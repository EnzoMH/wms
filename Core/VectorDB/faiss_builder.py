#!/usr/bin/env python3
"""
ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ Faiss êµ¬ì¶•ê¸°
==============================

AGV, EMS, RTV, CNV ë“± ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ë…¼ë¬¸ì˜ 
JSON ì²­í¬ íŒŒì¼ë“¤ì„ Faiss ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜í•˜ê³ 
ê³ ë„í™”ëœ RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ê³ ì„±ëŠ¥ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì‘ì„±ì: ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬íŒ€
ë‚ ì§œ: 2024ë…„ 1ì›” 15ì¼
ë²„ì „: 1.0.0
"""

import os
import json
import glob
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import logging
from datetime import datetime
import argparse

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import faiss
    import pandas as pd
    import torch
    from langchain_huggingface import HuggingFaceEmbeddings
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
    CUDA_AVAILABLE = torch.cuda.is_available()
    
except ImportError as e:
    print(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("ê¶Œì¥ ì„¤ì¹˜: pip install faiss-cpu langchain-huggingface torch pandas")
    exit(1)


class WarehouseAutomationFaissBuilder:
    """ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ë…¼ë¬¸ì˜ JSON ì²­í¬ íŒŒì¼ë“¤ì„ Faiss ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    
    def __init__(self, 
                 processed_data_dir: str = "../ProcessedData", 
                 vector_db_dir: str = "../VectorDB",
                 embedding_model: str = "korean_specialized"):
        """
        Faiss ë¹Œë”ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            processed_data_dir: ì²­í¬ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            vector_db_dir: Faiss ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
            embedding_model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸
        """
        self.processed_data_dir = Path(processed_data_dir)
        self.vector_db_dir = Path(vector_db_dir)
        self.embedding_model_name = embedding_model
        
        # Faiss ê´€ë ¨ ì†ì„±
        self.index = None
        self.documents = []
        self.metadatas = []
        self.embeddings_cache = []
        self.dimension = 768  # ko-sroberta-multitask ì„ë² ë”© ì°¨ì›
        
        self.setup_logging()
        self.setup_directories()
        self.setup_embedding_model()
        
    def setup_logging(self):
        """ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('faiss_builder.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.vector_db_dir.mkdir(parents=True, exist_ok=True)
        self.faiss_storage_dir = self.vector_db_dir / "faiss_storage"
        self.faiss_storage_dir.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"ì°½ê³  ìë™í™” Faiss DB ë””ë ‰í† ë¦¬ ì¤€ë¹„: {self.faiss_storage_dir}")
        
    def setup_embedding_model(self):
        """í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        self.logger.info(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {self.embedding_model_name}")
        
        device = 'cuda' if CUDA_AVAILABLE else 'cpu'
        self.logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        
        try:
            # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            self.embedding_model = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # ëª¨ë¸ ì°¨ì› í™•ì¸ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸
            test_embedding = self.embedding_model.embed_query("í…ŒìŠ¤íŠ¸")
            self.dimension = len(test_embedding)
            
            self.logger.info(f"âœ… í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            self.logger.info(f"   ëª¨ë¸: jhgan/ko-sroberta-multitask")
            self.logger.info(f"   ë””ë°”ì´ìŠ¤: {device}")
            self.logger.info(f"   ì°¨ì›: {self.dimension}")
            
        except Exception as e:
            self.logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_embedding(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        try:
            embedding = self.embedding_model.embed_query(text)
            return np.array(embedding, dtype=np.float32)
        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros(self.dimension, dtype=np.float32)
    
    def get_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        try:
            embeddings = self.embedding_model.embed_documents(texts)
            return np.array(embeddings, dtype=np.float32)
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros((len(texts), self.dimension), dtype=np.float32)
    
    def load_chunk_files(self) -> List[Dict]:
        """ì²­í¬ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        self.logger.info("ì²­í¬ íŒŒì¼ë“¤ ë¡œë“œ ì¤‘...")
        
        chunk_files = list(self.processed_data_dir.glob("chunks_*.json"))
        self.logger.info(f"ë°œê²¬ëœ ì²­í¬ íŒŒì¼ ìˆ˜: {len(chunk_files)}")
        
        all_chunks = []
        
        for chunk_file in chunk_files:
            self.logger.info(f"ğŸ“‚ ë¡œë“œ ì¤‘: {chunk_file.name}")
            
            try:
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    paper_data = json.load(f)
                
                paper_info = {
                    'source': paper_data.get('source', 'unknown'),
                    'filename': paper_data.get('filename', chunk_file.name),
                    'total_chars': paper_data.get('total_chars', 0),
                    'total_chunks': paper_data.get('total_chunks', 0)
                }
                
                for chunk in paper_data.get('chunks', []):
                    chunk_with_paper_info = {
                        **chunk,
                        'paper_source': paper_info['source'],
                        'paper_filename': paper_info['filename'],
                        'paper_total_chunks': paper_info['total_chunks']
                    }
                    all_chunks.append(chunk_with_paper_info)
                
                self.logger.info(f"  âœ… {len(paper_data.get('chunks', []))} ì²­í¬ ë¡œë“œë¨")
                
            except Exception as e:
                self.logger.error(f"âŒ {chunk_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        self.logger.info(f"ğŸ‰ ì´ {len(all_chunks)} ì²­í¬ ë¡œë“œ ì™„ë£Œ")
        return all_chunks
    
    def build_vector_database(self):
        """Faiss ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        self.logger.info("ğŸš€ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ Faiss ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘...")
        
        # ê¸°ì¡´ ë°ì´í„° í™•ì¸
        index_path = self.faiss_storage_dir / "warehouse_automation_knowledge.index"
        if index_path.exists():
            response = input("ê¸°ì¡´ Faiss ì¸ë±ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ êµ¬ì¶•í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() != 'y':
                self.logger.info("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
                return self.load_existing_index()
        
        # ì²­í¬ ë°ì´í„° ë¡œë“œ
        chunks = self.load_chunk_files()
        
        if not chunks:
            self.logger.error("âŒ ë¡œë“œí•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # Faiss ì¸ë±ìŠ¤ ì´ˆê¸°í™” (HNSW ì¸ë±ìŠ¤ ì‚¬ìš© - ë†’ì€ ì„±ëŠ¥)
        self.logger.info(f"ğŸ“Š Faiss HNSW ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ì°¨ì›: {self.dimension})")
        self.index = faiss.IndexHNSWFlat(self.dimension, 32)  # 32ëŠ” ì—°ê²° ìˆ˜
        self.index.hnsw.efConstruction = 200  # êµ¬ì¶• ì‹œ í’ˆì§ˆ
        
        # ë°ì´í„° ì¤€ë¹„
        self.documents = []
        self.metadatas = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        batch_size = 50  # FaissëŠ” ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        
        self.logger.info(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {total_batches} ë°°ì¹˜, ë°°ì¹˜ë‹¹ {batch_size} ì²­í¬")
        
        all_embeddings = []
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(chunks))
            batch_chunks = chunks[start_idx:end_idx]
            
            self.logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_chunks)} ì²­í¬)")
            
            # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            batch_texts = []
            batch_metadatas = []
            
            for chunk in batch_chunks:
                chunk_id = f"paper_{chunk['paper_filename']}_{chunk['id']:03d}"
                batch_texts.append(chunk['content'])
                
                metadata = {
                    'id': chunk_id,
                    'paper_filename': chunk['paper_filename'],
                    'paper_source': chunk['paper_source'],
                    'chunk_id': chunk['id'],
                    'chunk_size': chunk['size'],
                    'sentences': chunk['sentences'],
                    'paper_total_chunks': chunk['paper_total_chunks'],
                    'content': chunk['content']  # ê²€ìƒ‰ ì‹œ ì‚¬ìš©
                }
                batch_metadatas.append(metadata)
            
            # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            try:
                batch_embeddings = self.get_embeddings_batch(batch_texts)
                all_embeddings.append(batch_embeddings)
                
                # ë©”íƒ€ë°ì´í„°ì™€ ë¬¸ì„œ ì €ì¥
                self.documents.extend(batch_texts)
                self.metadatas.extend(batch_metadatas)
                
                self.logger.info(f"  âœ… ë°°ì¹˜ {batch_idx + 1} ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                
            except Exception as e:
                self.logger.error(f"âŒ ë°°ì¹˜ {batch_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ëª¨ë“  ì„ë² ë”©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        if all_embeddings:
            embeddings_matrix = np.vstack(all_embeddings)
            self.logger.info(f"ğŸ“Š ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ í˜•íƒœ: {embeddings_matrix.shape}")
            
            # Faiss ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embeddings_matrix)
            
            # ì¸ë±ìŠ¤ ì €ì¥
            self.save_index()
            
            # ìµœì¢… í†µê³„
            self.logger.info("=" * 60)
            self.logger.info("ğŸ‰ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ Faiss ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
            self.logger.info(f"ğŸ“Š ì´ ì €ì¥ëœ ì²­í¬ ìˆ˜: {len(self.documents)}")
            self.logger.info(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {self.faiss_storage_dir}")
            self.logger.info("=" * 60)
            
            return len(self.documents)
        else:
            self.logger.error("âŒ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤!")
            return 0
    
    def save_index(self):
        """Faiss ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        self.logger.info("ğŸ’¾ Faiss ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
        
        # Faiss ì¸ë±ìŠ¤ ì €ì¥
        index_path = self.faiss_storage_dir / "warehouse_automation_knowledge.index"
        faiss.write_index(self.index, str(index_path))
        
        # ë¬¸ì„œ ë‚´ìš© ì €ì¥
        documents_path = self.faiss_storage_dir / "documents.json"
        with open(documents_path, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = self.faiss_storage_dir / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadatas, f, ensure_ascii=False, indent=2)
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        config = {
            'dimension': self.dimension,
            'total_documents': len(self.documents),
            'embedding_model': 'jhgan/ko-sroberta-multitask',
            'index_type': 'HNSW',
            'created_at': datetime.now().isoformat()
        }
        
        config_path = self.faiss_storage_dir / "config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_path}")
        self.logger.info(f"âœ… ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: {documents_path}")
        self.logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")
        self.logger.info(f"âœ… ì„¤ì • ì €ì¥ ì™„ë£Œ: {config_path}")
    
    def load_existing_index(self):
        """ê¸°ì¡´ Faiss ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            self.logger.info("ğŸ“‚ ê¸°ì¡´ Faiss ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
            
            # ì¸ë±ìŠ¤ ë¡œë“œ
            index_path = self.faiss_storage_dir / "warehouse_automation_knowledge.index"
            self.index = faiss.read_index(str(index_path))
            
            # ë¬¸ì„œ ë¡œë“œ
            documents_path = self.faiss_storage_dir / "documents.json"
            with open(documents_path, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = self.faiss_storage_dir / "metadata.json"
            with open(metadata_path, 'r', encoding='utf-8') as f:
                self.metadatas = json.load(f)
            
            # ì„¤ì • ë¡œë“œ
            config_path = self.faiss_storage_dir / "config.json"
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    self.logger.info(f"ğŸ“Š ì¸ë±ìŠ¤ ì •ë³´: {config}")
            
            self.logger.info(f"âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({len(self.documents)} ë¬¸ì„œ)")
            return len(self.documents)
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return 0
    
    def test_search(self, query: str = "AGV ê²½ë¡œ ê³„íš", top_k: int = 5):
        """ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if self.index is None:
            self.logger.error("âŒ ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return
        
        self.logger.info(f"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.get_embedding(query)
            query_embedding = query_embedding.reshape(1, -1)  # Faiss í˜•íƒœë¡œ ë³€í™˜
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            scores, indices = self.index.search(query_embedding, top_k)
            
            self.logger.info(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ({top_k}ê°œ):")
            
            for i, (idx, score) in enumerate(zip(indices[0], scores[0])):
                if idx < len(self.metadatas):
                    metadata = self.metadatas[idx]
                    document = self.documents[idx]
                    
                    # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
                    similarity = 1 / (1 + score)
                    
                    self.logger.info(f"\n{i+1}. ğŸ“„ {metadata['paper_filename']}")
                    self.logger.info(f"   ğŸ¯ ìœ ì‚¬ë„: {similarity:.3f}")
                    self.logger.info(f"   ğŸ“ ì²­í¬ #{metadata['chunk_id']}")
                    self.logger.info(f"   ğŸ“Š í¬ê¸°: {metadata['chunk_size']} chars, {metadata['sentences']} sentences")
                    self.logger.info(f"   ğŸ“ƒ ë‚´ìš©: {document[:200]}...")
            
            return {
                'query': query,
                'results': [
                    {
                        'document': self.documents[idx],
                        'metadata': self.metadatas[idx],
                        'score': float(score),
                        'similarity': float(1 / (1 + score))
                    }
                    for idx, score in zip(indices[0], scores[0])
                    if idx < len(self.metadatas)
                ]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def get_database_stats(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        if not self.documents:
            if not self.load_existing_index():
                self.logger.error("âŒ ë¡œë“œí•  ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return
        
        count = len(self.documents)
        self.logger.info("ğŸ“Š ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ Faiss ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
        self.logger.info(f" ì´ ì²­í¬ ìˆ˜: {count}")
        self.logger.info(f" ë²¡í„° ì°¨ì›: {self.dimension}")
        self.logger.info(f" ì¸ë±ìŠ¤ íƒ€ì…: HNSW")
        
        if count > 0:
            # ë…¼ë¬¸ë³„ í†µê³„
            papers = {}
            for metadata in self.metadatas:
                filename = metadata['paper_filename']
                if filename not in papers:
                    papers[filename] = 0
                papers[filename] += 1
            
            self.logger.info(f"   ğŸ“š ë…¼ë¬¸ ìˆ˜: {len(papers)}")
            self.logger.info(f"   ğŸ“Š ë…¼ë¬¸ë‹¹ í‰ê·  ì²­í¬: {count / len(papers):.1f}")
            
            # ìƒìœ„ 5ê°œ ë…¼ë¬¸
            top_papers = sorted(papers.items(), key=lambda x: x[1], reverse=True)[:5]
            self.logger.info("   ğŸ† ìƒìœ„ ë…¼ë¬¸ë“¤:")
            for filename, chunk_count in top_papers:
                display_name = filename[:50] + "..." if len(filename) > 50 else filename
                self.logger.info(f"      - {display_name}: {chunk_count} ì²­í¬")


def main():
    """ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ Faiss êµ¬ì¶•ê¸° ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ Faiss ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ê¸°")
    parser.add_argument("--processed-data", default="../ProcessedData", 
                       help="ì°½ê³  ìë™í™” ì²­í¬ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--vector-db", default="../VectorDB", 
                       help="ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ Faiss DBë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬")
    parser.add_argument("--action", choices=['build', 'test', 'stats'],
                       default='build', help="ìˆ˜í–‰í•  ì‘ì—…")
    parser.add_argument("--test-query", default="AGV ê²½ë¡œ ê³„íš",
                       help="í…ŒìŠ¤íŠ¸ìš© ì°½ê³  ìë™í™” ê²€ìƒ‰ ì¿¼ë¦¬")
    
    args = parser.parse_args()
    
    # ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ Faiss ë¹Œë” ì´ˆê¸°í™”
    builder = WarehouseAutomationFaissBuilder(
        processed_data_dir=args.processed_data,
        vector_db_dir=args.vector_db,
        embedding_model="korean_specialized"
    )
    
    if args.action == 'build':
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        builder.build_vector_database()
        
    elif args.action == 'test':
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ í›„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        builder.load_existing_index()
        builder.test_search(query=args.test_query)
        
    elif args.action == 'stats':
        # í†µê³„ ì¡°íšŒ
        builder.get_database_stats()


if __name__ == "__main__":
    main()
