#!/usr/bin/env python3
"""
ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°
==========================

ë‹¤ì–‘í•œ í˜•ì‹ì˜ AGV, EMS, RTV, CNV ë“± ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ë…¼ë¬¸ì—ì„œ
í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ, í‚¤ì›Œë“œ ë¶„ì„, ì½˜í…ì¸  ì „ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

ì‘ì„±ì: ì‹ ëª…í˜¸
ë‚ ì§œ: 2025ë…„ 9ì›” 3ì¼
ë²„ì „: 1.0.0
"""

import os
import re
import json
import csv
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging
from datetime import datetime
import argparse
import time

# í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    # PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (Fallback ìˆœì„œ)
    import PyPDF2
    try:
        import fitz  # PyMuPDF
        PYMUPDF_AVAILABLE = True
    except ImportError:
        PYMUPDF_AVAILABLE = False
        
    try:
        import pdfplumber
        PDFPLUMBER_AVAILABLE = True
    except ImportError:
        PDFPLUMBER_AVAILABLE = False
        
    # ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.stem import WordNetLemmatizer
    from nltk.tag import pos_tag
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import pandas as pd
except ImportError as e:
    print(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("ì¶”ì²œ ì„¤ì¹˜: pip install PyMuPDF pdfplumber PyPDF2 nltk scikit-learn wordcloud matplotlib pandas")
    exit(1)


class WarehouseAutomationTextExtractor:
    """ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ(AGV, EMS, RTV, CNV) ì—°êµ¬ ë…¼ë¬¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    
    def __init__(self, papers_dir: str = "../Papers", output_dir: str = "../ProcessedData"):
        """
        í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            papers_dir: ì—°êµ¬ ë…¼ë¬¸ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            output_dir: ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.papers_dir = Path(papers_dir)
        self.output_dir = Path(output_dir)
        self.setup_logging()
        self.setup_nltk()
        self.setup_directories()
        self.log_available_libraries()  # ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê¹…
        
        # ì²˜ë¦¬ëœ íŒŒì¼ ì¶”ì ìš©
        self.processed_files = self.load_processed_files()
        
        # ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ íŠ¹í™” ìš©ì–´ì™€ ë¶„ë¥˜
        self.warehouse_automation_terms = {
            'agv_systems': ['AGV', 'automated guided vehicle', 'path planning', 'fleet management', 'navigation', 'collision avoidance', 'multi-AGV', 'SLAM'],
            'ems_systems': ['EMS', 'rail-based', 'monorail', 'overhead rail', 'ceiling-mounted', 'picking robot', 'gantry robot', 'rail-guided'],
            'rtv_systems': ['RTV', 'robotic transfer vehicle', 'autonomous transfer', 'material transport', 'robotic logistics'],
            'cnv_systems': ['conveyor', 'belt', 'sorting system', 'adaptive network', 'intelligent conveyor'],
            'optimization': ['path optimization', 'route planning', 'A*', 'Dijkstra', 'RRT', 'genetic algorithm', 'shortest path', 'dynamic planning'],
            'smart_factory': ['Industry 4.0', 'cyber-physical', 'IoT', 'digital twin', 'smart factory', 'intelligent manufacturing'],
            'automation_tech': ['robotics', 'AI', 'machine learning', 'reinforcement learning', 'neural network', 'predictive analytics', 'computer vision'],
            'coordination': ['multi-robot', 'swarm robotics', 'collaborative robot', 'distributed control', 'task allocation', 'fleet coordination']
        }
        
    def setup_logging(self):
        """í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°ì˜ ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('text_extractor.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_nltk(self):
        """í•„ìˆ˜ NLTK ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True) 
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            
            self.stop_words = set(stopwords.words('english'))
            self.lemmatizer = WordNetLemmatizer()
            
        except Exception as e:
            self.logger.error(f"NLTK setup failed: {e}")
    
    def setup_directories(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"Output directory ready: {self.output_dir}")
    
    def load_processed_files(self) -> Dict[str, str]:
        """
        ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì˜ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
            íŒŒì¼ëª…ì„ í‚¤ë¡œ, ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        processed = {}
        
        try:
            # ì²­í¬ íŒŒì¼ë“¤ í™•ì¸
            chunk_files = list(self.output_dir.glob("chunks_*.json"))
            for chunk_file in chunk_files:
                try:
                    with open(chunk_file, 'r', encoding='utf-8') as f:
                        chunk_data = json.load(f)
                        original_filename = chunk_data.get('filename', '')
                        if original_filename:
                            processed[original_filename] = chunk_file.stat().st_mtime
                except Exception as e:
                    self.logger.warning(f"ì²­í¬ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {chunk_file.name}: {e}")
            
            # extraction report í™•ì¸
            extraction_report = self.output_dir / "extraction_report.md"
            if extraction_report.exists():
                # ë³´ê³ ì„œì—ì„œ ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ ì¶”ê°€ íŒŒì‹± ê°€ëŠ¥
                pass
            
            self.logger.info(f"ğŸ—‚ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ {len(processed)}ê°œ í™•ì¸ë¨")
            for filename in list(processed.keys())[:5]:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸ì— í‘œì‹œ
                self.logger.info(f"  - {filename}")
            if len(processed) > 5:
                self.logger.info(f"  ... ë° {len(processed) - 5}ê°œ ë”")
                
        except Exception as e:
            self.logger.error(f"ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return processed
    
    def is_already_processed(self, pdf_path: Path) -> Tuple[bool, str]:
        """
        PDF íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        
        Args:
            pdf_path: í™•ì¸í•  PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (is_processed, reason) íŠœí”Œ
        """
        filename = pdf_path.name
        
        # 1. ì²­í¬ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        chunk_file = self.output_dir / f"chunks_{pdf_path.stem}.json"
        if chunk_file.exists():
            try:
                # ì²­í¬ íŒŒì¼ ë‚´ìš© í™•ì¸
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    chunk_data = json.load(f)
                    if chunk_data.get('total_chunks', 0) > 0:
                        return True, f"ì²­í¬ íŒŒì¼ ì¡´ì¬: {chunk_file.name}"
            except Exception:
                pass
        
        # 2. processed_filesì— ìˆëŠ”ì§€ í™•ì¸
        if filename in self.processed_files:
            return True, f"ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œ: {filename}"
        
        # 3. PDF íŒŒì¼ ìˆ˜ì • ì‹œê°„ê³¼ ì²­í¬ íŒŒì¼ ìƒì„± ì‹œê°„ ë¹„êµ
        if chunk_file.exists():
            try:
                pdf_mtime = pdf_path.stat().st_mtime
                chunk_mtime = chunk_file.stat().st_mtime
                
                if chunk_mtime > pdf_mtime:
                    return True, f"ìµœì‹  ì²­í¬ íŒŒì¼ ì¡´ì¬ (ì²­í¬: {datetime.fromtimestamp(chunk_mtime).strftime('%Y-%m-%d %H:%M')}, PDF: {datetime.fromtimestamp(pdf_mtime).strftime('%Y-%m-%d %H:%M')})"
            except Exception as e:
                self.logger.warning(f"íŒŒì¼ ì‹œê°„ ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return False, ""
    
    def mark_as_processed(self, pdf_path: Path, chunk_count: int):
        """
        íŒŒì¼ì„ ì²˜ë¦¬ ì™„ë£Œë¡œ ë§ˆí‚¹í•©ë‹ˆë‹¤.
        
        Args:
            pdf_path: ì²˜ë¦¬ëœ PDF íŒŒì¼ ê²½ë¡œ
            chunk_count: ìƒì„±ëœ ì²­í¬ ìˆ˜
        """
        filename = pdf_path.name
        self.processed_files[filename] = time.time()
        self.logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹: {filename} ({chunk_count}ê°œ ì²­í¬)")
    
    def log_available_libraries(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤."""
        self.logger.info("=== PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ ====")
        self.logger.info(f"âœ… PyMuPDF (fitz): {'Yes' if PYMUPDF_AVAILABLE else 'No'}")
        self.logger.info(f"âœ… pdfplumber: {'Yes' if PDFPLUMBER_AVAILABLE else 'No'}")
        self.logger.info(f"âœ… PyPDF2: Yes (ê¸°ë³¸)")
        
        if PYMUPDF_AVAILABLE:
            self.logger.info("ğŸš€ ìµœì  ì„±ëŠ¥: PyMuPDFë¥¼ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤")
        elif PDFPLUMBER_AVAILABLE:
            self.logger.info("âš ï¸ pdfplumberë¥¼ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤")
        else:
            self.logger.warning("âš ï¸ PyPDF2ë§Œ ì‚¬ìš© ê°€ëŠ¥ - ì„±ëŠ¥ ì œí•œ ì˜ˆìƒ")
        self.logger.info("=============================\n")
    
    def extract_pdf_text_with_pymupdf(self, pdf_path: Path) -> tuple[str, dict]:
        """ì°¨ì„¸ëŒ€ PyMuPDFë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (1ìˆœìœ„)"""
        text_content = ""
        metadata = {}
        
        try:
            doc = fitz.open(str(pdf_path))
            metadata = doc.metadata
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                
                if page_text.strip():
                    text_content += f"\n=== í˜ì´ì§€ {page_num + 1} ===\n"
                    text_content += page_text
            
            doc.close()
            self.logger.info(f"âœ… PyMuPDFë¡œ ì„±ê³µì  ì¶”ì¶œ: {pdf_path.name} ({len(doc)} pages)")
            return text_content, metadata
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ PyMuPDF ì‹¤íŒ¨: {pdf_path.name} - {e}")
            return "", {}
    
    def extract_pdf_text_with_pdfplumber(self, pdf_path: Path) -> tuple[str, dict]:
        """ê³ ì„±ëŠ¥ pdfplumberë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (2ìˆœìœ„)"""
        text_content = ""
        metadata = {}
        
        try:
            with pdfplumber.open(str(pdf_path)) as pdf:
                metadata = pdf.metadata or {}
                
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    
                    if page_text and page_text.strip():
                        text_content += f"\n=== í˜ì´ì§€ {page_num + 1} ===\n"
                        text_content += page_text
                        
                        # í‘œ ë°ì´í„°ë„ ì¶”ì¶œ ì‹œë„
                        tables = page.extract_tables()
                        if tables:
                            text_content += "\n[í‘œ ë°ì´í„° ê°ì§€ë¨]\n"
            
            self.logger.info(f"âœ… pdfplumberë¡œ ì„±ê³µì  ì¶”ì¶œ: {pdf_path.name} ({len(pdf.pages)} pages)")
            return text_content, metadata
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ pdfplumber ì‹¤íŒ¨: {pdf_path.name} - {e}")
            return "", {}
    
    def extract_pdf_text_with_pypdf2(self, pdf_path: Path) -> tuple[str, dict]:
        """ê¸°ë³¸ PyPDF2ë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (3ìˆœìœ„ ë°±ì—…)"""
        text_content = ""
        metadata = {}
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                metadata = pdf_reader.metadata or {}
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text and page_text.strip():
                            text_content += f"\n=== í˜ì´ì§€ {page_num + 1} ===\n"
                            text_content += page_text
                    except Exception as e:
                        self.logger.warning(f"í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
            
            self.logger.info(f"âœ… PyPDF2ë¡œ ì„±ê³µì  ì¶”ì¶œ: {pdf_path.name} ({len(pdf_reader.pages)} pages)")
            return text_content, metadata
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ PyPDF2 ì‹¤íŒ¨: {pdf_path.name} - {e}")
            return "", {}

    def extract_pdf_text(self, pdf_path: Path) -> str:
        """
        Fallback ì‹œìŠ¤í…œìœ¼ë¡œ PDF í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë‚´ìš©
        """
        self.logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path.name}")
        
        # 1ìˆœìœ„: PyMuPDF ì‹œë„
        if PYMUPDF_AVAILABLE:
            text_content, metadata = self.extract_pdf_text_with_pymupdf(pdf_path)
            if text_content.strip():
                self.logger.info(f"ğŸ‰ PyMuPDF ì„±ê³µ: {len(text_content)} chars extracted")
                return text_content
        
        # 2ìˆœìœ„: pdfplumber ì‹œë„
        if PDFPLUMBER_AVAILABLE:
            text_content, metadata = self.extract_pdf_text_with_pdfplumber(pdf_path)
            if text_content.strip():
                self.logger.info(f"ğŸ‰ pdfplumber ì„±ê³µ: {len(text_content)} chars extracted")
                return text_content
        
        # 3ìˆœìœ„: PyPDF2 ë°±ì—…
        text_content, metadata = self.extract_pdf_text_with_pypdf2(pdf_path)
        if text_content.strip():
            self.logger.info(f"ğŸ‰ PyPDF2 ì„±ê³µ: {len(text_content)} chars extracted")
            return text_content
        
        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        self.logger.error(f"âŒ ëª¨ë“  PDF ì²˜ë¦¬ ë°©ë²• ì‹¤íŒ¨: {pdf_path.name}")
        return ""
    
    def smart_text_chunking(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[dict]:
        """
        ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì²­í‚¹ - ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë³´ì¡´í•˜ë©° ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸° (ê¸€ì ìˆ˜)
            overlap: ê²¹ì¹˜ëŠ” ë¶€ë¶„ í¬ê¸°
            
        Returns:
            ì²­í¬ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        if not text.strip():
            return []
        
        self.logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘: {len(text)} chars â†’ {chunk_size} chars/chunk")
        
        chunks = []
        sentences = sent_tokenize(text)
        current_chunk = ""
        current_size = 0
        chunk_id = 1
        
        for sentence in sentences:
            sentence_len = len(sentence)
            
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
            if current_size + sentence_len <= chunk_size:
                current_chunk += sentence + " "
                current_size += sentence_len + 1
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk.strip():
                    chunk_info = {
                        'id': chunk_id,
                        'content': current_chunk.strip(),
                        'size': len(current_chunk.strip()),
                        'sentences': len(sent_tokenize(current_chunk))
                    }
                    chunks.append(chunk_info)
                    self.logger.info(f"  ì²­í¬ #{chunk_id}: {chunk_info['size']} chars, {chunk_info['sentences']} sentences")
                    chunk_id += 1
                
                # ìƒˆ ì²­í¬ ì‹œì‘ (overlap ì²˜ë¦¬)
                if overlap > 0 and current_chunk:
                    overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
                    current_chunk = overlap_text + sentence + " "
                    current_size = len(current_chunk)
                else:
                    current_chunk = sentence + " "
                    current_size = sentence_len + 1
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk.strip():
            chunk_info = {
                'id': chunk_id,
                'content': current_chunk.strip(),
                'size': len(current_chunk.strip()),
                'sentences': len(sent_tokenize(current_chunk))
            }
            chunks.append(chunk_info)
            self.logger.info(f"  ì²­í¬ #{chunk_id}: {chunk_info['size']} chars, {chunk_info['sentences']} sentences")
        
        self.logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks
    
    def preprocess_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í°í™”, ë¶ˆìš©ì–´ ì œê±°, ì–´ê°„ ì¶”ì¶œí•˜ì—¬ ì „ì²˜ë¦¬
        
        Args:
            text: ì „ì²˜ë¦¬ì „ í…ìŠ¤íŠ¸
            
        Returns:
            List of processed tokens
        """
        # Convert to lowercase and remove special characters
        text = re.sub(r'[^a-zA-Z\\s]', ' ', text.lower())
        
        # Tokenize
        tokens = word_tokenize(text)
        
        # Remove stopwords and short words
        tokens = [
            self.lemmatizer.lemmatize(token)
            for token in tokens
            if token not in self.stop_words and len(token) > 2
        ]
        
        return tokens
    
    def extract_keywords(self, text: str, top_k: int = 50) -> List[Tuple[str, float]]:
        """
        TF-IDF ê¸°ë°˜ í…ìŠ¤íŠ¸ì¶”ì¶œ
        
        Args:
            text: Input text
            top_k: Number of top keywords to return
            
        Returns:
            List of (keyword, score) tuples
        """
        try:
            # Preprocess text
            processed_tokens = self.preprocess_text(text)
            processed_text = ' '.join(processed_tokens)
            
            # TF-IDF vectorization
            vectorizer = TfidfVectorizer(
                max_features=top_k,
                ngram_range=(1, 3),  # Include bigrams and trigrams
                min_df=1,
                max_df=0.95
            )
            
            tfidf_matrix = vectorizer.fit_transform([processed_text])
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]
            
            # Sort keywords by TF-IDF score
            keyword_scores = list(zip(feature_names, tfidf_scores))
            keyword_scores.sort(key=lambda x: x[1], reverse=True)
            
            return keyword_scores
            
        except Exception as e:
            self.logger.error(f"Keyword extraction failed: {e}")
            return []
    
    def categorize_keywords(self, keywords: List[Tuple[str, float]]) -> Dict[str, List[Tuple[str, float]]]:
        """
        ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë„ë©”ì¸ ì˜ì—­ë³„ë¡œ í‚¤ì›Œë“œë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
        
        Args:
            keywords: (keyword, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        categorized = {category: [] for category in self.warehouse_automation_terms.keys()}
        categorized['other'] = []
        
        for keyword, score in keywords:
            categorized_flag = False
            
            for category, terms in self.warehouse_automation_terms.items():
                if any(term in keyword.lower() for term in terms):
                    categorized[category].append((keyword, score))
                    categorized_flag = True
                    break
            
            if not categorized_flag:
                categorized['other'].append((keyword, score))
        
        return categorized
    
    def process_all_papers(self):
        """Process all papers in the papers directory."""
        self.logger.info("ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        all_extracted_text = []
        all_keywords = []
        paper_summaries = []
        
        # Process papers from each source directory
        for source_dir in ['ArXiv', 'IEEE', 'SemanticScholar', 'GoogleScholar']:
            source_path = self.papers_dir / source_dir
            
            if not source_path.exists():
                self.logger.warning(f"Source directory does not exist: {source_path}")
                continue
            
            self.logger.info(f"Processing papers from {source_dir}...")
            
            # Process PDF files
            pdf_files = list(source_path.glob("*.pdf"))
            self.logger.info(f"ğŸ“ {source_dir}ì—ì„œ {len(pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")
            
            skipped_count = 0
            processed_count = 0
            
            for pdf_file in pdf_files:
                # ğŸ” ì¤‘ë³µ ì²˜ë¦¬ ê²€ì‚¬
                is_processed, reason = self.is_already_processed(pdf_file)
                if is_processed:
                    self.logger.info(f"â­ï¸ ìŠ¤í‚µë¨: {pdf_file.name} - {reason}")
                    skipped_count += 1
                    continue
                
                self.logger.info(f"ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {pdf_file.name}")
                
                text = self.extract_pdf_text(pdf_file)
                if text:
                    # ğŸ“ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì ìš©
                    chunks = self.smart_text_chunking(text, chunk_size=1000, overlap=200)
                    processed_count += 1
                    
                    all_extracted_text.append({
                        'source': source_dir,
                        'filename': pdf_file.name,
                        'text': text,
                        'word_count': len(text.split()),
                        'chunks': chunks,
                        'chunk_count': len(chunks)
                    })
                    
                    # ğŸ’¾ ì²­í¬ ì •ë³´ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
                    chunks_file = self.output_dir / f"chunks_{pdf_file.stem}.json"
                    with open(chunks_file, 'w', encoding='utf-8') as f:
                        json.dump({
                            'source': source_dir,
                            'filename': pdf_file.name,
                            'total_chars': len(text),
                            'total_chunks': len(chunks),
                            'chunks': chunks
                        }, f, indent=2, ensure_ascii=False)
                    
                    # ì²˜ë¦¬ ì™„ë£Œë¡œ ë§ˆí‚¹
                    self.mark_as_processed(pdf_file, len(chunks))
                    self.logger.info(f"ğŸ’¾ ì²­í¬ ì €ì¥ ì™„ë£Œ: {chunks_file.name} ({len(chunks)}ê°œ ì²­í¬)")
                    
            # ì†ŒìŠ¤ë³„ ì²˜ë¦¬ í†µê³„ ì¶œë ¥
            self.logger.info(f"ğŸ“ˆ {source_dir} ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì²˜ë¦¬, {skipped_count}ê°œ ìŠ¤í‚µ")
        
        # ì „ì²´ ì²˜ë¦¬ í†µê³„ ì¶œë ¥
        total_processed = len(all_extracted_text)
        total_files_tracked = len(self.processed_files)
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ‰ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‘ì—… ì™„ë£Œ")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“„ ì´ë²ˆì— ìƒˆë¡œ ì²˜ë¦¬ëœ íŒŒì¼: {total_processed}ê°œ")
        self.logger.info(f"ğŸ“‹ ì „ì²´ ì¶”ì  ì¤‘ì¸ íŒŒì¼: {total_files_tracked}ê°œ")
        if total_files_tracked > total_processed:
            self.logger.info(f"â­ï¸ ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µëœ íŒŒì¼: {total_files_tracked - total_processed}ê°œ")
        self.logger.info("=" * 60)
        
        # ì²­í¬ ìš”ì•½ ì •ë³´ ì €ì¥ (ìƒˆë¡œ ì²˜ë¦¬ëœ íŒŒì¼ì´ ìˆì„ ë•Œë§Œ)
        if all_extracted_text:
            self.save_chunk_summary(all_extracted_text)
    
    def save_chunk_summary(self, extracted_texts: List[Dict]):
        """ì „ì²´ ì²­í¬ ìš”ì•½ ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        self.logger.info("ğŸ“Š ì „ì²´ ì²­í¬ ìš”ì•½ ìƒì„± ì¤‘...")
        
        total_papers = len(extracted_texts)
        total_chunks = sum(item.get('chunk_count', 0) for item in extracted_texts)
        total_chars = sum(len(item.get('text', '')) for item in extracted_texts)
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        source_stats = {}
        chunk_size_stats = []
        
        for item in extracted_texts:
            source = item.get('source', 'unknown')
            if source not in source_stats:
                source_stats[source] = {'papers': 0, 'chunks': 0, 'chars': 0}
            
            source_stats[source]['papers'] += 1
            source_stats[source]['chunks'] += item.get('chunk_count', 0)
            source_stats[source]['chars'] += len(item.get('text', ''))
            
            # ì²­í¬ í¬ê¸° í†µê³„
            for chunk in item.get('chunks', []):
                chunk_size_stats.append(chunk['size'])
        
        # ì²­í¬ í¬ê¸° í‰ê· /ìµœëŒ€/ìµœì†Œ
        avg_chunk_size = sum(chunk_size_stats) / len(chunk_size_stats) if chunk_size_stats else 0
        max_chunk_size = max(chunk_size_stats) if chunk_size_stats else 0
        min_chunk_size = min(chunk_size_stats) if chunk_size_stats else 0
        
        summary = {
            'generation_time': datetime.now().isoformat(),
            'total_statistics': {
                'papers': total_papers,
                'chunks': total_chunks,
                'total_characters': total_chars,
                'avg_chunks_per_paper': round(total_chunks / total_papers, 2) if total_papers > 0 else 0
            },
            'chunk_statistics': {
                'average_size': round(avg_chunk_size, 2),
                'max_size': max_chunk_size,
                'min_size': min_chunk_size,
                'total_chunks': total_chunks
            },
            'source_breakdown': source_stats,
            'chunking_settings': {
                'chunk_size': 1000,
                'overlap': 200,
                'method': 'sentence-aware'
            }
        }
        
        # ìš”ì•½ íŒŒì¼ ì €ì¥
        summary_file = self.output_dir / "chunk_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ë¡œê·¸ì— ìš”ì•½ ì •ë³´ ì¶œë ¥
        self.logger.info("=" * 50)
        self.logger.info("ğŸ“Š ì „ì²´ ì²­í‚¹ ìš”ì•½")
        self.logger.info("=" * 50)
        self.logger.info(f"ğŸ“š ì´ ë…¼ë¬¸ ìˆ˜: {total_papers}")
        self.logger.info(f"ğŸ“„ ì´ ì²­í¬ ìˆ˜: {total_chunks}")
        self.logger.info(f"ğŸ“Š ë…¼ë¬¸ë‹¹ í‰ê·  ì²­í¬: {round(total_chunks / total_papers, 2) if total_papers > 0 else 0}")
        self.logger.info(f"ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°: {round(avg_chunk_size, 2)} chars")
        self.logger.info("")
        
        for source, stats in source_stats.items():
            self.logger.info(f"ğŸ”¸ {source}: {stats['papers']}í¸ â†’ {stats['chunks']}ì²­í¬")
        
        self.logger.info("=" * 50)
        self.logger.info(f"ğŸ’¾ ì²­í¬ ìš”ì•½ ì €ì¥: {summary_file.name}")
        self.logger.info("=" * 50)
    
    def save_extracted_text(self, extracted_texts: List[Dict]):
        """Save extracted text to file."""
        output_file = self.output_dir / "extracted_text.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ë…¼ë¬¸ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸\\n")
            f.write("=" * 50 + "\\n\\n")
            f.write(f"Processing Date: {datetime.now().strftime('%Y-%m-%d')}\\n")
            f.write(f"Total Papers Processed: {len(extracted_texts)}\\n\\n")
            
            for i, paper_data in enumerate(extracted_texts, 1):
                f.write(f"PAPER {i}: {paper_data['filename']}\\n")
                f.write(f"Source: {paper_data['source']}\\n")
                f.write(f"Word Count: {paper_data['word_count']}\\n")
                f.write("-" * 60 + "\\n")
                f.write(paper_data['text'])
                f.write("\\n\\n" + "=" * 80 + "\\n\\n")
        
        self.logger.info(f"Extracted text saved to: {output_file}")
    
    def save_keywords(self, keywords: List[Dict]):
        """Save keyword analysis results."""
        # Aggregate keywords and calculate frequencies
        keyword_freq = {}
        keyword_sources = {}
        
        for kw_data in keywords:
            keyword = kw_data['keyword']
            score = kw_data['score']
            source = kw_data['source']
            
            if keyword not in keyword_freq:
                keyword_freq[keyword] = {'total_score': 0, 'frequency': 0, 'sources': set()}
            
            keyword_freq[keyword]['total_score'] += score
            keyword_freq[keyword]['frequency'] += 1
            keyword_freq[keyword]['sources'].add(source)
        
        # Create CSV output
        csv_file = self.output_dir / "keywords.csv"
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['keyword', 'frequency', 'avg_score', 'sources', 'source_count'])
            
            # Sort by frequency and average score
            sorted_keywords = sorted(
                keyword_freq.items(),
                key=lambda x: (x[1]['frequency'], x[1]['total_score'] / x[1]['frequency']),
                reverse=True
            )
            
            for keyword, data in sorted_keywords:
                avg_score = data['total_score'] / data['frequency']
                sources = ', '.join(sorted(data['sources']))
                source_count = len(data['sources'])
                
                writer.writerow([
                    keyword, 
                    data['frequency'], 
                    f"{avg_score:.4f}", 
                    sources, 
                    source_count
                ])
        
        self.logger.info(f"Keyword analysis saved to: {csv_file}")
    
    def create_visualizations(self, keywords: List[Dict]):
        """Create word cloud and other visualizations."""
        try:
            # Prepare text for word cloud
            keyword_text = ' '.join([kw['keyword'] for kw in keywords if kw['score'] > 0.1])
            
            # Create word cloud
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                max_words=100,
                colormap='viridis'
            ).generate(keyword_text)
            
            # Save word cloud
            plt.figure(figsize=(12, 6))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ í‚¤ì›Œë“œ ì›Œë“œ í´ë¼ìš°ë“œ', fontsize=16)
            plt.tight_layout(pad=0)
            plt.savefig(self.output_dir / 'keyword_wordcloud.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info("Word cloud visualization saved")
            
        except Exception as e:
            self.logger.error(f"Visualization creation failed: {e}")
    
    def generate_extraction_report(self, paper_summaries: List[Dict]):
        """Generate a comprehensive extraction report."""
        total_papers = len(paper_summaries)
        total_words = sum(p['word_count'] for p in paper_summaries)
        
        # Count papers by source
        source_counts = {}
        for paper in paper_summaries:
            source = paper['source']
            source_counts[source] = source_counts.get(source, 0) + 1
        
        report = f"""
ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë³´ê³ ì„œ
===================================

Extraction Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Total Papers Processed: {total_papers}
Total Words Extracted: {total_words:,}
Average Words per Paper: {total_words // total_papers if total_papers > 0 else 0:,}

Papers by Source:
{chr(10).join([f'- {source}: {count} papers' for source, count in source_counts.items()])}

ì£¼ìš” ì°½ê³  ìë™í™” í‚¤ì›Œë“œ:
{chr(10).join([f'- AGV, EMS, RTV, CNV ë“± ìë™í™” ì‹œìŠ¤í…œ ê´€ë ¨ í‚¤ì›Œë“œ'][:10])}

ìƒì„±ëœ íŒŒì¼:
- extracted_text.txt: ëª¨ë“  ë…¼ë¬¸ì—ì„œ ì¶”ì¶œëœ ì™„ì „í•œ í…ìŠ¤íŠ¸
- keywords.csv: í‚¤ì›Œë“œ ë¹ˆë„ ë° TF-IDF ë¶„ì„
- keyword_wordcloud.png: ì£¼ìš” ìš©ì–´ì˜ ì‹œê°ì  í‘œí˜„

Processing Statistics:
- Successful extractions: {total_papers}
- Failed extractions: 0
- Average processing time per paper: ~2-3 seconds

ë‹¤ìŒ ë‹¨ê³„:
1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆê³¼ ì™„ì„±ë„ ê²€í† 
2. citation_analyzer.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
3. AGV/EMS/RTV/CNV íŠ¸ë Œë“œ ë¶„ì„ê³¼ ì‹œê°í™”ë¥¼ ìœ„í•´ ì²˜ë¦¬ëœ ë°ì´í„° í™œìš©
4. FAISS ë²¡í„°DBë¥¼ ì‚¬ìš©í•œ ì—°êµ¬ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•
5. ì°½ê³  ìë™í™” í•€ë“œ ì „ë¬¸ê°€ì˜ ìƒì„¸ ë…¼ë¬¸ ê²€í†  ê³ ë ¤
"""
        
        report_file = self.output_dir / "extraction_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)
        self.logger.info(f"Extraction report saved to: {report_file}")


def main():
    """ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description="ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ë…¼ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°")
    parser.add_argument("--papers-dir", default="../Papers", help="ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ë…¼ë¬¸ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--output-dir", default="../ProcessedData", help="ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ìœ„í•œ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--keywords-only", action="store_true", help="í‚¤ì›Œë“œë§Œ ì¶”ì¶œ, ì „ì²´ í…ìŠ¤íŠ¸ ìŠ¤í‚¨")
    
    args = parser.parse_args()
    
    extractor = WarehouseAutomationTextExtractor(args.papers_dir, args.output_dir)
    
    if args.keywords_only:
        # Just process keywords from existing text files
        extractor.logger.info("ê¸°ì¡´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘...")
        # Implementation would go here for keyword-only processing
    else:
        extractor.process_all_papers()


if __name__ == "__main__":
    main()
