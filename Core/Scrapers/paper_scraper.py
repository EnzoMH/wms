#!/usr/bin/env python3
"""
ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ ìˆ˜ì§‘ê¸°
========================

ArXiv, IEEE Xplore, Semantic Scholar, Google Scholar ë“±
ì—¬ëŸ¬ í•™ìˆ  ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ AGV, EMS, RTV, CNV ë“±
ì°½ê³  ìë™í™” ë° ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ëŠ” í¬ê´„ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤.

ì‘ì„±ì: ì‹ ëª…í˜¸
ë‚ ì§œ: 2025ë…„ 9ì›” 3ì¼
ë²„ì „: 1.0.0
"""

import os
import json
import csv
import re
import requests
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import argparse
import logging

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    import arxiv
    import scholarly
    from scholarly import scholarly as gs
    import bibtexparser
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except ImportError as e:
    print(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("ì‹¤í–‰: pip install arxiv scholarly bibtexparser requests")
    exit(1)


class WarehouseAutomationPaperScraper:
    """ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ(AGV, EMS, RTV, CNV) ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    
    def __init__(self, output_dir: str = "../Papers"):
        """
        ë…¼ë¬¸ ìˆ˜ì§‘ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            output_dir: ë‹¤ìš´ë¡œë“œëœ ë…¼ë¬¸ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.output_dir = output_dir
        self.setup_logging()
        self.setup_directories()
        self.session = self.setup_session()
        
        # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê¸°ì¡´ ë…¼ë¬¸ ì •ë³´ ë¡œë“œ
        self.existing_papers = self.load_existing_papers()
        
        self.warehouse_automation_keywords = [ 
            # === AGV (Automated Guided Vehicle) ê´€ë ¨ ===
            "automated guided vehicle",
            "AGV path planning",
            "AGV fleet management", 
            "AGV navigation system",
            "AGV collision avoidance",
            "AGV scheduling optimization",
            "multi-AGV coordination",
            "AGV SLAM navigation",
            
            # === EMS (Electric Monorail System) ê´€ë ¨ ===
            "rail-based picking robot",
            "overhead rail robot system",
            "EMS picking automation",
            "rail-guided robot warehouse",
            "ceiling-mounted picking robot",
            "rail robot material handling",
            "automated picking rail system",
            "overhead crane robot picking",
            "rail-based storage retrieval",
            "gantry robot warehouse",
            
            # === RTV (Robotic Transfer Vehicle) ê´€ë ¨ ===
            "robotic transfer vehicle",
            "RTV material handling",
            "automated material transport",
            "robotic logistics system",
            "autonomous transfer robot",
            "RTV warehouse automation",
            
            # === CNV (Conveyor) ì‹œìŠ¤í…œ ê´€ë ¨ ===
            "intelligent conveyor system",
            "smart conveyor belt",
            "automated conveyor control",
            "conveyor sorting system",
            "adaptive conveyor network",
            "conveyor AGV integration",
            
            # === ê²½ë¡œ ìµœì í™” ë° A* ì•Œê³ ë¦¬ì¦˜ ===
            "A* algorithm warehouse",
            "path optimization warehouse",
            "warehouse route planning",
            "dynamic path planning factory",
            "multi-robot path planning",
            "warehouse navigation algorithm",
            "shortest path warehouse logistics",
            "Dijkstra algorithm warehouse",
            "RRT path planning warehouse",
            "genetic algorithm warehouse optimization",
            
            # === ì°½ê³  ìµœì í™” ë™ì„  ===
            "warehouse layout optimization",
            "optimal warehouse design",
            "warehouse traffic flow optimization",
            "storage location optimization",
            "warehouse space utilization",
            "picking route optimization",
            "warehouse congestion management",
            "material flow optimization",
            
            # === ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬ í†µí•© ì‹œìŠ¤í…œ ===
            "smart factory automation",
            "Industry 4.0 robotics",
            "intelligent manufacturing system",
            "cyber-physical system factory",
            "IoT smart factory",
            "digital twin warehouse",
            "smart factory logistics",
            "autonomous manufacturing system",
            
            # === ë¡œë´‡ í˜‘ì—… ë° ì¡°ì • ===
            "multi-robot coordination",
            "robot fleet management",
            "collaborative robot system",
            "swarm robotics warehouse", 
            "distributed robot control",
            "robot task allocation",
            
            # === ì‹¤ì‹œê°„ ì œì–´ ë° ëª¨ë‹ˆí„°ë§ ===
            "real-time warehouse monitoring",
            "predictive maintenance AGV",
            "warehouse digital twin",
            "smart sensor warehouse",
            "RFID warehouse tracking",
            "computer vision warehouse",
            
            # === ë¨¸ì‹ ëŸ¬ë‹ ë° AI ìµœì í™” ===
            "machine learning warehouse optimization",
            "AI-driven logistics",
            "reinforcement learning AGV",
            "neural network path planning",
            "deep learning warehouse management",
            "predictive analytics warehouse"
        ]
        
        # ê¸°ì¡´ WMS ì¼ë°˜ í‚¤ì›Œë“œ (ì£¼ì„ ì²˜ë¦¬ë¨)
        # ê¸°ì¡´ ì¼ë°˜ì ì¸ WMS í‚¤ì›Œë“œ (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        # self.warehouse_automation_keywords = [
        #     "warehouse management system",
        #     "WMS optimization", 
        #     "warehouse automation",
        #     "inventory management system",
        #     "automated storage retrieval",
        #     "AGV warehouse",
        #     "warehouse robotics",
        #     "smart warehouse", 
        #     "warehouse IoT",
        #     "supply chain automation"
        # ]
        # í˜„ì¬ëŠ” ìœ„ì˜ AGV, EMS, RTV, CNV ë“± ë” êµ¬ì²´ì ì¸ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©
    
    def setup_logging(self):
        """ìˆ˜ì§‘ê¸°ì˜ ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('paper_scraper.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_directories(self):
        """ë…¼ë¬¸ ì €ì¥ì„ ìœ„í•œ í•„ìˆ˜ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        directories = [
            f"{self.output_dir}/ArXiv",
            f"{self.output_dir}/IEEE", 
            f"{self.output_dir}/SemanticScholar",
            f"{self.output_dir}/GoogleScholar"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            self.logger.info(f"Created directory: {directory}")
    
    def setup_session(self) -> requests.Session:
        """ì¬ì‹œë„ ì „ëµì„ í¬í•¨í•œ HTTP ì„¸ì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session
    
    def load_existing_papers(self) -> Dict[str, set]:
        """
        ê¸°ì¡´ ë…¼ë¬¸ë“¤ì˜ ì •ë³´ë¥¼ ë¡œë“œí•˜ì—¬ ì¤‘ë³µ ë°©ì§€ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Returns:
            ê¸°ì¡´ ë…¼ë¬¸ì˜ ì œëª©, URL, IDë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        existing = {
            'titles': set(),
            'urls': set(), 
            'ids': set(),
            'files': set()
        }
        
        try:
            # ê° ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ì—ì„œ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° í™•ì¸
            sources = ['ArXiv', 'IEEE', 'SemanticScholar', 'GoogleScholar']
            
            for source in sources:
                source_path = f"{self.output_dir}/{source}"
                
                # ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ í™•ì¸
                metadata_files = [
                    f"{source_path}/metadata.json",
                    f"{source_path}/search_results.json"
                ]
                
                for metadata_file in metadata_files:
                    if os.path.exists(metadata_file):
                        try:
                            with open(metadata_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                papers = data.get('papers', [])
                                
                                for paper in papers:
                                    # ì œëª© ì¶”ê°€ (ì •ê·œí™”)
                                    title = paper.get('title', '').strip().lower()
                                    if title:
                                        existing['titles'].add(title)
                                    
                                    # URL ì¶”ê°€
                                    url = paper.get('url') or paper.get('pdf_url')
                                    if url:
                                        existing['urls'].add(url)
                                    
                                    # ID ì¶”ê°€
                                    paper_id = paper.get('id') or paper.get('paperId')
                                    if paper_id:
                                        existing['ids'].add(str(paper_id))
                                        
                        except Exception as e:
                            self.logger.warning(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {metadata_file}: {e}")
                
                # ê¸°ì¡´ íŒŒì¼ë“¤ í™•ì¸
                if os.path.exists(source_path):
                    for file in os.listdir(source_path):
                        if file.endswith('.pdf'):
                            existing['files'].add(file)
            
            self.logger.info(f"ê¸°ì¡´ ë…¼ë¬¸ ì •ë³´ ë¡œë“œ ì™„ë£Œ:")
            self.logger.info(f"  - ì œëª©: {len(existing['titles'])}ê°œ")
            self.logger.info(f"  - URL: {len(existing['urls'])}ê°œ") 
            self.logger.info(f"  - ID: {len(existing['ids'])}ê°œ")
            self.logger.info(f"  - íŒŒì¼: {len(existing['files'])}ê°œ")
            
        except Exception as e:
            self.logger.error(f"ê¸°ì¡´ ë…¼ë¬¸ ì •ë³´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return existing
    
    def is_duplicate_paper(self, paper_data: Dict) -> Tuple[bool, str]:
        """
        ë…¼ë¬¸ì´ ì¤‘ë³µì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        
        Args:
            paper_data: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°
            
        Returns:
            (is_duplicate, reason) íŠœí”Œ
        """
        title = paper_data.get('title', '').strip().lower()
        url = paper_data.get('url') or paper_data.get('pdf_url')
        paper_id = paper_data.get('id') or paper_data.get('paperId')
        
        # ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ê²€ì‚¬
        if title and title in self.existing_papers['titles']:
            return True, f"ì¤‘ë³µ ì œëª©: {title[:50]}..."
        
        # URLë¡œ ì¤‘ë³µ ê²€ì‚¬
        if url and url in self.existing_papers['urls']:
            return True, f"ì¤‘ë³µ URL: {url}"
        
        # IDë¡œ ì¤‘ë³µ ê²€ì‚¬  
        if paper_id and str(paper_id) in self.existing_papers['ids']:
            return True, f"ì¤‘ë³µ ID: {paper_id}"
        
        return False, ""
    
    def add_to_existing_papers(self, paper_data: Dict):
        """
        ìƒˆë¡œìš´ ë…¼ë¬¸ ì •ë³´ë¥¼ ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            paper_data: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°
        """
        title = paper_data.get('title', '').strip().lower()
        url = paper_data.get('url') or paper_data.get('pdf_url')
        paper_id = paper_data.get('id') or paper_data.get('paperId')
        
        if title:
            self.existing_papers['titles'].add(title)
        if url:
            self.existing_papers['urls'].add(url)
        if paper_id:
            self.existing_papers['ids'].add(str(paper_id))
    
    def scrape_arxiv_papers(self, max_results: int = 50) -> List[Dict]:
        """
        Scrape papers from ArXiv.
        
        Args:
            max_results: Maximum number of papers to retrieve
            
        Returns:
            List of paper metadata dictionaries
        """
        self.logger.info("ArXivì—ì„œ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        papers = []
        
        try:
            # ì—¬ëŸ¬ ê²€ìƒ‰ ì¡°ê±´ì„ ê²°í•©
            search_query = " OR ".join([f'"{keyword}"' for keyword in self.warehouse_automation_keywords[:5]])
            
            search = arxiv.Search(
                query=search_query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            
            for result in search.results():
                paper_data = {
                    "id": result.entry_id,
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "abstract": result.summary,
                    "published": result.published.strftime("%Y-%m-%d"),
                    "categories": result.categories,
                    "pdf_url": result.pdf_url,
                    "source": "ArXiv"
                }
                
                # ì¤‘ë³µ ê²€ì‚¬
                is_duplicate, reason = self.is_duplicate_paper(paper_data)
                if is_duplicate:
                    self.logger.info(f"â­ï¸ ì¤‘ë³µ ë…¼ë¬¸ ìŠ¤í‚µ: {reason}")
                    continue
                
                papers.append(paper_data)
                
                # ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ì— ì¶”ê°€
                self.add_to_existing_papers(paper_data)
                
                # ê°€ëŠ¥í•œ ê²½ìš° PDF ë‹¤ìš´ë¡œë“œ
                try:
                    filename = f"{len(papers):03d}_{result.title[:50]}.pdf".replace(" ", "_")
                    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)  # íŒŒì¼ëª… ì •ê·œí™”
                    
                    # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    file_path = f"{self.output_dir}/ArXiv/{filename}"
                    if os.path.exists(file_path):
                        self.logger.info(f"ğŸ“„ íŒŒì¼ ì´ë¯¸ ì¡´ì¬: {filename}")
                    else:
                        result.download_pdf(dirpath=f"{self.output_dir}/ArXiv", filename=filename)
                        self.logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
                        self.existing_papers['files'].add(filename)
                except Exception as e:
                    self.logger.warning(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                time.sleep(1)  # ì†ë„ ì œí•œ
            
            # Save metadata
            with open(f"{self.output_dir}/ArXiv/metadata.json", 'w', encoding='utf-8') as f:
                json.dump({
                    "collection_info": {
                        "created_date": datetime.now().strftime("%Y-%m-%d"),
                        "total_papers": len(papers),
                        "source": "ArXiv",
                        "search_keywords": self.warehouse_automation_keywords
                    },
                    "papers": papers
                }, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ArXiv ì°½ê³  ìë™í™” ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ. {len(papers)}ê°œ ë…¼ë¬¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"ArXiv scraping failed: {e}")
        
        return papers
    
    def scrape_semantic_scholar(self, max_results: int = 50) -> List[Dict]:
        """
        Scrape papers from Semantic Scholar API.
        
        Args:
            max_results: Maximum number of papers to retrieve
            
        Returns:
            List of paper metadata dictionaries
        """
        self.logger.info("Semantic Scholarì—ì„œ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        papers = []
        
        base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        
        try:
            # Rate limiting ê°œì„ : ë” ì ì€ í‚¤ì›Œë“œì™€ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
            for keyword in self.warehouse_automation_keywords[:2]:  # Limit to 2 keywords to avoid rate limits
                params = {
                    "query": keyword,
                    "limit": min(max_results // 2, 50),  # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°
                    "fields": "paperId,title,authors,year,venue,citationCount,abstract,url"
                }
                
                self.logger.info(f"Querying Semantic Scholar for: {keyword}")
                response = self.session.get(base_url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                for paper in data.get('data', []):
                    paper_data = {
                        "id": paper.get("paperId"),
                        "title": paper.get("title"),
                        "authors": [author.get("name", "") for author in paper.get("authors", [])],
                        "abstract": paper.get("abstract", ""),
                        "year": paper.get("year"),
                        "venue": paper.get("venue"),
                        "citation_count": paper.get("citationCount", 0),
                        "url": paper.get("url"),
                        "source": "Semantic Scholar",
                        "search_keyword": keyword
                    }
                    
                    # ğŸ” ì¤‘ë³µ ê²€ì‚¬
                    is_duplicate, reason = self.is_duplicate_paper(paper_data)
                    if is_duplicate:
                        self.logger.info(f"â­ï¸ Semantic Scholar ì¤‘ë³µ ìŠ¤í‚µ: {reason}")
                        continue
                    
                    papers.append(paper_data)
                    # ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ì— ì¶”ê°€
                    self.add_to_existing_papers(paper_data)
                
                self.logger.info(f"Found {len(data.get('data', []))} papers for keyword: {keyword}")
                time.sleep(3)  # ë” ê¸´ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ rate limiting íšŒí”¼
            
            # Save search results
            with open(f"{self.output_dir}/SemanticScholar/search_results.json", 'w', encoding='utf-8') as f:
                json.dump({
                    "search_metadata": {
                        "query_date": datetime.now().strftime("%Y-%m-%d"),
                        "total_results": len(papers),
                        "source": "Semantic Scholar",
                        "search_keywords": self.warehouse_automation_keywords[:3]
                    },
                    "papers": papers
                }, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"Semantic Scholar ì°½ê³  ìë™í™” ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ. {len(papers)}ê°œ ë…¼ë¬¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"Semantic Scholar scraping failed: {e}")
        
        return papers
    
    def scrape_google_scholar(self, max_results: int = 30) -> List[Dict]:
        """
        Scrape papers from Google Scholar using scholarly library.
        
        Args:
            max_results: Maximum number of papers to retrieve
            
        Returns:
            List of paper metadata dictionaries
        """
        self.logger.info("Google Scholarì—ì„œ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        papers = []
        abstracts_text = []
        
        try:
            for keyword in self.warehouse_automation_keywords[:2]:  # Limit to avoid blocking
                search_query = gs.search_pubs(keyword)
                
                count = 0
                for paper in search_query:
                    if count >= max_results // 2:
                        break
                    
                    try:
                        paper_data = {
                            "title": paper.get("bib", {}).get("title", ""),
                            "authors": paper.get("bib", {}).get("author", []),
                            "year": paper.get("bib", {}).get("pub_year"),
                            "venue": paper.get("bib", {}).get("venue", ""),
                            "citation_count": paper.get("num_citations", 0),
                            "url": paper.get("pub_url", ""),
                            "abstract": paper.get("bib", {}).get("abstract", ""),
                            "source": "Google Scholar",
                            "search_keyword": keyword
                        }
                        
                        # ğŸ” ì¤‘ë³µ ê²€ì‚¬
                        is_duplicate, reason = self.is_duplicate_paper(paper_data)
                        if is_duplicate:
                            self.logger.info(f"â­ï¸ Google Scholar ì¤‘ë³µ ìŠ¤í‚µ: {reason}")
                            continue
                        
                        papers.append(paper_data)
                        # ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ì— ì¶”ê°€
                        self.add_to_existing_papers(paper_data)
                        
                        # Collect abstracts for text file
                        if paper_data["abstract"]:
                            abstracts_text.append(f"Title: {paper_data['title']}")
                            abstracts_text.append(f"Authors: {', '.join(paper_data['authors']) if isinstance(paper_data['authors'], list) else paper_data['authors']}")
                            abstracts_text.append(f"Year: {paper_data['year']}")
                            abstracts_text.append(f"Abstract: {paper_data['abstract']}")
                            abstracts_text.append("-" * 80)
                        
                        count += 1
                        time.sleep(2)  # ì†ë„ ì œí•œ for Google Scholar
                        
                    except Exception as e:
                        self.logger.warning(f"Error processing paper: {e}")
                        continue
            
            # Save abstracts to text file
            with open(f"{self.output_dir}/GoogleScholar/abstracts.txt", 'w', encoding='utf-8') as f:
                f.write("Google Scholar Search Results - ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ë…¼ë¬¸\\n")
                f.write("=" * 50 + "\\n\\n")
                f.write(f"Search Date: {datetime.now().strftime('%Y-%m-%d')}\\n")
                f.write(f"Total Papers: {len(papers)}\\n\\n")
                f.write("\\n".join(abstracts_text))
            
            self.logger.info(f"Google Scholar ì°½ê³  ìë™í™” ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ. {len(papers)}ê°œ ë…¼ë¬¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"Google Scholar scraping failed: {e}")
        
        return papers
    
    def generate_citation_file(self, papers: List[Dict], source: str):
        """Generate BibTeX citation file for IEEE papers."""
        if source == "IEEE":
            citations = []
            for i, paper in enumerate(papers):
                citation = f"""@article{{paper_{i+1:03d},
  title={{{paper.get('title', 'Unknown Title')}}},
  author={{{', '.join(paper.get('authors', ['Unknown Author']))}}},
  journal={{{paper.get('venue', 'Unknown Journal')}}},
  year={{{paper.get('year', 'Unknown Year')}}},
  publisher={{IEEE}}
}}"""
                citations.append(citation)
            
            with open(f"{self.output_dir}/IEEE/citations.bib", 'w', encoding='utf-8') as f:
                f.write("\\n\\n".join(citations))
    
    def run_full_scraping(self, max_results_per_source: int = 50):
        """
        ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ëŠ” ì™„ì „í•œ ê³¼ì •ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            max_results_per_source: ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜
        """
        self.logger.info("ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ ì¢…í•© ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        all_papers = {}
        
        # Scrape from all sources
        all_papers['arxiv'] = self.scrape_arxiv_papers(max_results_per_source)
        time.sleep(5)  # Break between sources
        
        all_papers['semantic_scholar'] = self.scrape_semantic_scholar(max_results_per_source)
        time.sleep(5)
        
        # Google ScholarëŠ” captcha ë¬¸ì œë¡œ ë¹„í™œì„±í™”
        # all_papers['google_scholar'] = self.scrape_google_scholar(max_results_per_source)
        self.logger.info("Google Scholar ìˆ˜ì§‘ì€ captcha ë¬¸ì œë¡œ ë¹„í™œì„±í™”ë¨")
        all_papers['google_scholar'] = []
        
        # Generate citation files
        self.generate_citation_file(all_papers.get('google_scholar', []), "IEEE")
        
        # Generate summary report
        self.generate_scraping_report(all_papers)
        
        self.logger.info("Scraping process completed successfully!")
    
    def generate_scraping_report(self, papers_by_source: Dict[str, List]):
        """ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ ìˆ˜ì§‘ ê³¼ì •ì˜ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        total_papers = sum(len(papers) for papers in papers_by_source.values())
        
        report = f"""
ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ ìˆ˜ì§‘ ë³´ê³ ì„œ
=================================

ìˆ˜ì§‘ ë‚ ì§œ : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ì´ ë…¼ë¬¸ ìˆ˜ : {total_papers}

í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬: AGV, EMS, RTV, CNV, ê²½ë¡œìµœì í™”, ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬, AI/ML

ì†ŒìŠ¤ë³„ ë¶„í• :
- ArXiv(ì•„ì¹´ì´ë¸Œë…¼ë¬¸): {len(papers_by_source.get('arxiv', []))} ë…¼ë¬¸
- Semantic Scholar(ì‹œë§¨í‹± ì…€ëŸ¬): {len(papers_by_source.get('semantic_scholar', []))} ë…¼ë¬¸  
- Google Scholar(êµ¬ê¸€ ìŠ¤ì¿¨ëŸ¬): {len(papers_by_source.get('google_scholar', []))} ë…¼ë¬¸

Search Keywords Used:
{chr(10).join([f'- {keyword}' for keyword in self.warehouse_automation_keywords])}

Files Generated:
- ArXiv/metadata.json(ë©”íƒ€ë°ì´í„°)
- SemanticScholar/search_results.json(ê²€ìƒ‰ ê²°ê³¼)
- GoogleScholar/abstracts.txt(ì´ˆë¡)
- IEEE/citations.bib (from Google Scholar data)

ë‹¤ìŒ ë‹¨ê³„:
1. text_extractor.pyë¥¼ ì‹¤í–‰í•˜ì—¬ PDFì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
2. citation_analyzer.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì¸ìš© ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
3. trend_visualizer.pyë¥¼ ì‹¤í–‰í•˜ì—¬ AGV/EMS/RTV/CNV íŠ¸ë Œë“œ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
4. FAISS ë²¡í„°DBì— ì €ì¥í•˜ì—¬ ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""
        
        with open(f"{self.output_dir}/../scraping_report.txt", 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)


def main():
    """ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ë…¼ë¬¸ ìˆ˜ì§‘ê¸°ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description="ì°½ê³  ìë™í™” ì‹œìŠ¤í…œ ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ê¸°")
    parser.add_argument("--output-dir", default="../Papers", help="Output directory for papers")
    parser.add_argument("--max-results", type=int, default=50, help="Maximum results per source")
    parser.add_argument("--source", choices=['arxiv', 'semantic', 'google', 'all'], 
                       default='all', help="Source to scrape from")
    
    args = parser.parse_args()
    
    scraper = WarehouseAutomationPaperScraper(args.output_dir)
    
    if args.source == 'arxiv':
        scraper.scrape_arxiv_papers(args.max_results)
    elif args.source == 'semantic':
        scraper.scrape_semantic_scholar(args.max_results)
    elif args.source == 'google':
        scraper.scrape_google_scholar(args.max_results)
    else:
        scraper.run_full_scraping(args.max_results)


if __name__ == "__main__":
    main()
