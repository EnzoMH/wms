#!/usr/bin/env python3
"""
WMS ChromaDB êµ¬ì¶•ê¸°
=================

JSON ì²­í¬ íŒŒì¼ë“¤ì„ ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜í•˜ê³ 
RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì‘ì„±ì: WMS ì—°êµ¬íŒ€
ë‚ ì§œ: 2024ë…„ 1ì›” 15ì¼
ë²„ì „: 1.0.0
"""

import os
import json
import glob
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
from datetime import datetime
import argparse

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import chromadb
    from chromadb.config import Settings
    import pandas as pd
    # ì„ë² ë”© ëª¨ë¸ - ì—¬ëŸ¬ ì˜µì…˜ ì œê³µ
    try:
        from sentence_transformers import SentenceTransformer
        SENTENCE_TRANSFORMERS_AVAILABLE = True
    except ImportError:
        SENTENCE_TRANSFORMERS_AVAILABLE = False
        
    try:
        import openai
        OPENAI_AVAILABLE = True
    except ImportError:
        OPENAI_AVAILABLE = False
        
except ImportError as e:
    print(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("ê¶Œì¥ ì„¤ì¹˜: pip install chromadb sentence-transformers pandas openai")
    exit(1)


class WMSChromaDBBuilder:
    """JSON ì²­í¬ íŒŒì¼ë“¤ì„ ChromaDBë¡œ ë³€í™˜í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    
    def __init__(self, 
                 processed_data_dir: str = "../ProcessedData", 
                 vector_db_dir: str = "../VectorDB",
                 embedding_model: str = "sentence-transformers"):
        """
        ChromaDB ë¹Œë”ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            processed_data_dir: ì²­í¬ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            vector_db_dir: ChromaDBë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
            embedding_model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ('sentence-transformers' or 'openai')
        """
        self.processed_data_dir = Path(processed_data_dir)
        self.vector_db_dir = Path(vector_db_dir)
        self.embedding_model_type = embedding_model
        
        self.setup_logging()
        self.setup_directories()
        self.setup_embedding_model()
        self.setup_chromadb()
        
    def setup_logging(self):
        """ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('chromadb_builder.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.vector_db_dir.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"ë²¡í„° DB ë””ë ‰í† ë¦¬ ì¤€ë¹„: {self.vector_db_dir}")
        
    def setup_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        self.logger.info(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {self.embedding_model_type}")
        
        if self.embedding_model_type == "sentence-transformers":
            if not SENTENCE_TRANSFORMERS_AVAILABLE:
                raise ImportError("sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # í•œêµ­ì–´ë„ ì§€ì›í•˜ëŠ” ë©€í‹°ë§êµ¬ì–¼ ëª¨ë¸ ì‚¬ìš©
            model_name = "sentence-transformers/all-MiniLM-L6-v2"  # ë¹ ë¥´ê³  ì¢‹ì€ ì„±ëŠ¥
            # model_name = "sentence-transformers/all-mpnet-base-v2"  # ë” ì¢‹ì€ ì„±ëŠ¥, ëŠë¦¼
            
            self.embedding_model = SentenceTransformer(model_name)
            self.logger.info(f"âœ… SentenceTransformer ë¡œë“œ ì™„ë£Œ: {model_name}")
            
        elif self.embedding_model_type == "openai":
            if not OPENAI_AVAILABLE:
                raise ImportError("openaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # OpenAI API í‚¤ í™•ì¸
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            self.embedding_model = openai.Client()
            self.logger.info("âœ… OpenAI ì„ë² ë”© API ì¤€ë¹„ ì™„ë£Œ")
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ëª¨ë¸: {self.embedding_model_type}")
    
    def setup_chromadb(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.logger.info("ChromaDB ì´ˆê¸°í™” ì¤‘...")
        
        # Persistent í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ë°ì´í„° ì˜êµ¬ ì €ì¥)
        self.chroma_client = chromadb.PersistentClient(
            path=str(self.vector_db_dir / "chroma_storage")
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        collection_name = "wms_research_papers"
        try:
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                metadata={"description": "WMS ì—°êµ¬ ë…¼ë¬¸ ì²­í¬ ë²¡í„° DB"}
            )
            self.logger.info(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")
        except Exception:
            self.collection = self.chroma_client.get_collection(collection_name)
            self.logger.info(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: {collection_name}")
    
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        if self.embedding_model_type == "sentence-transformers":
            embedding = self.embedding_model.encode(text, convert_to_numpy=True)
            return embedding.tolist()
            
        elif self.embedding_model_type == "openai":
            response = self.embedding_model.embeddings.create(
                input=text,
                model="text-embedding-ada-002"
            )
            return response.data[0].embedding
    
    def load_chunk_files(self) -> List[Dict]:
        """ì²­í¬ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        self.logger.info("ì²­í¬ íŒŒì¼ë“¤ ë¡œë“œ ì¤‘...")
        
        chunk_files = list(self.processed_data_dir.glob("chunks_*.json"))
        self.logger.info(f"ë°œê²¬ëœ ì²­í¬ íŒŒì¼ ìˆ˜: {len(chunk_files)}")
        
        all_chunks = []
        
        for chunk_file in chunk_files:
            self.logger.info(f"ğŸ“‚ ë¡œë“œ ì¤‘: {chunk_file.name}")
            
            try:
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    paper_data = json.load(f)
                
                paper_info = {
                    'source': paper_data.get('source', 'unknown'),
                    'filename': paper_data.get('filename', chunk_file.name),
                    'total_chars': paper_data.get('total_chars', 0),
                    'total_chunks': paper_data.get('total_chunks', 0)
                }
                
                for chunk in paper_data.get('chunks', []):
                    chunk_with_paper_info = {
                        **chunk,
                        'paper_source': paper_info['source'],
                        'paper_filename': paper_info['filename'],
                        'paper_total_chunks': paper_info['total_chunks']
                    }
                    all_chunks.append(chunk_with_paper_info)
                
                self.logger.info(f"  âœ… {len(paper_data.get('chunks', []))} ì²­í¬ ë¡œë“œë¨")
                
            except Exception as e:
                self.logger.error(f"âŒ {chunk_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        self.logger.info(f"ğŸ‰ ì´ {len(all_chunks)} ì²­í¬ ë¡œë“œ ì™„ë£Œ")
        return all_chunks
    
    def build_vector_database(self):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        self.logger.info("ğŸš€ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ë°ì´í„° í™•ì¸
        existing_count = self.collection.count()
        if existing_count > 0:
            self.logger.info(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° {existing_count}ê°œ ë°œê²¬")
            response = input("ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ êµ¬ì¶•í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() == 'y':
                self.chroma_client.delete_collection("wms_research_papers")
                self.collection = self.chroma_client.create_collection(
                    name="wms_research_papers",
                    metadata={"description": "WMS ì—°êµ¬ ë…¼ë¬¸ ì²­í¬ ë²¡í„° DB"}
                )
                self.logger.info("ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
            else:
                self.logger.info("ê¸°ì¡´ ë°ì´í„° ìœ ì§€, ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€í•©ë‹ˆë‹¤")
        
        # ì²­í¬ ë°ì´í„° ë¡œë“œ
        chunks = self.load_chunk_files()
        
        if not chunks:
            self.logger.error("âŒ ë¡œë“œí•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        batch_size = 100
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        
        self.logger.info(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {total_batches} ë°°ì¹˜, ë°°ì¹˜ë‹¹ {batch_size} ì²­í¬")
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(chunks))
            batch_chunks = chunks[start_idx:end_idx]
            
            self.logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_chunks)} ì²­í¬)")
            
            # ì„ë² ë”© ìƒì„±
            documents = []
            metadatas = []
            ids = []
            
            for chunk in batch_chunks:
                chunk_id = f"paper_{chunk['paper_filename']}_{chunk['id']:03d}"
                documents.append(chunk['content'])
                
                metadata = {
                    'paper_filename': chunk['paper_filename'],
                    'paper_source': chunk['paper_source'],
                    'chunk_id': chunk['id'],
                    'chunk_size': chunk['size'],
                    'sentences': chunk['sentences'],
                    'paper_total_chunks': chunk['paper_total_chunks']
                }
                metadatas.append(metadata)
                ids.append(chunk_id)
            
            # ChromaDBì— ì¶”ê°€ (ìë™ìœ¼ë¡œ ì„ë² ë”© ìƒì„±ë¨)
            try:
                self.collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                self.logger.info(f"  âœ… ë°°ì¹˜ {batch_idx + 1} ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ ë°°ì¹˜ {batch_idx + 1} ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ìµœì¢… í†µê³„
        final_count = self.collection.count()
        self.logger.info("=" * 60)
        self.logger.info("ğŸ‰ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        self.logger.info(f"ğŸ“Š ì´ ì €ì¥ëœ ì²­í¬ ìˆ˜: {final_count}")
        self.logger.info(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {self.vector_db_dir}")
        self.logger.info("=" * 60)
        
        return final_count
    
    def test_search(self, query: str = "warehouse automation", top_k: int = 5):
        """ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        self.logger.info(f"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")
        
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k
        )
        
        self.logger.info(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ({top_k}ê°œ):")
        
        for i, (doc, metadata, distance) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0], 
            results['distances'][0]
        )):
            self.logger.info(f"\n{i+1}. ğŸ“„ {metadata['paper_filename']}")
            self.logger.info(f"   ğŸ¯ ìœ ì‚¬ë„: {1-distance:.3f}")
            self.logger.info(f"   ğŸ“ ì²­í¬ #{metadata['chunk_id']}")
            self.logger.info(f"   ğŸ“Š í¬ê¸°: {metadata['chunk_size']} chars, {metadata['sentences']} sentences")
            self.logger.info(f"   ğŸ“ƒ ë‚´ìš©: {doc[:200]}...")
        
        return results
    
    def get_database_stats(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        count = self.collection.count()
        self.logger.info("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
        self.logger.info(f"   ğŸ’¾ ì´ ì²­í¬ ìˆ˜: {count}")
        
        if count > 0:
            # ìƒ˜í”Œ ë°ì´í„°ë¡œ í†µê³„ í™•ì¸
            sample = self.collection.get(limit=min(100, count), include=['metadatas'])
            
            # ë…¼ë¬¸ë³„ í†µê³„
            papers = {}
            for metadata in sample['metadatas']:
                filename = metadata['paper_filename']
                if filename not in papers:
                    papers[filename] = 0
                papers[filename] += 1
            
            self.logger.info(f"   ğŸ“š ë…¼ë¬¸ ìˆ˜: {len(papers)}")
            self.logger.info(f"   ğŸ“Š ë…¼ë¬¸ë‹¹ í‰ê·  ì²­í¬: {count / len(papers):.1f}")
            
            # ìƒìœ„ 5ê°œ ë…¼ë¬¸
            top_papers = sorted(papers.items(), key=lambda x: x[1], reverse=True)[:5]
            self.logger.info("   ğŸ† ìƒìœ„ ë…¼ë¬¸ë“¤:")
            for filename, chunk_count in top_papers:
                self.logger.info(f"      - {filename[:50]}...: {chunk_count} ì²­í¬")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="WMS ChromaDB êµ¬ì¶•ê¸°")
    parser.add_argument("--processed-data", default="../ProcessedData", 
                       help="ì²­í¬ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--vector-db", default="../VectorDB", 
                       help="ChromaDBë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬")
    parser.add_argument("--embedding-model", choices=['sentence-transformers', 'openai'],
                       default='sentence-transformers', help="ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸")
    parser.add_argument("--action", choices=['build', 'test', 'stats'],
                       default='build', help="ìˆ˜í–‰í•  ì‘ì—…")
    parser.add_argument("--test-query", default="warehouse automation",
                       help="í…ŒìŠ¤íŠ¸ìš© ê²€ìƒ‰ ì¿¼ë¦¬")
    
    args = parser.parse_args()
    
    # ChromaDB ë¹Œë” ì´ˆê¸°í™”
    builder = WMSChromaDBBuilder(
        processed_data_dir=args.processed_data,
        vector_db_dir=args.vector_db,
        embedding_model=args.embedding_model
    )
    
    if args.action == 'build':
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        builder.build_vector_database()
        
    elif args.action == 'test':
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        builder.test_search(query=args.test_query)
        
    elif args.action == 'stats':
        # í†µê³„ ì¡°íšŒ
        builder.get_database_stats()


if __name__ == "__main__":
    main()
