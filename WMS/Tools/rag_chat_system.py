#!/usr/bin/env python3
"""
WMS RAG ì±„íŒ… ì‹œìŠ¤í…œ
==================

ChromaDBì™€ LLMì„ ì—°ê²°í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ChatOllama, OpenAI GPT, Claude ë“± ë‹¤ì–‘í•œ LLM ì§€ì›.

ì‘ì„±ì: WMS ì—°êµ¬íŒ€
ë‚ ì§œ: 2024ë…„ 1ì›” 15ì¼
ë²„ì „: 1.0.0
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
from datetime import datetime
import argparse

try:
    import chromadb
    from chromadb.config import Settings
    # LLM ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
    try:
        from langchain_community.llms import Ollama
        OLLAMA_AVAILABLE = True
    except ImportError:
        OLLAMA_AVAILABLE = False
        
    try:
        import openai
        OPENAI_AVAILABLE = True
    except ImportError:
        OPENAI_AVAILABLE = False
        
    try:
        import anthropic
        ANTHROPIC_AVAILABLE = True
    except ImportError:
        ANTHROPIC_AVAILABLE = False
        
except ImportError as e:
    print(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("ê¶Œì¥ ì„¤ì¹˜: pip install chromadb langchain-community openai anthropic")
    exit(1)


class WMSRAGChatSystem:
    """WMS ì—°êµ¬ ë…¼ë¬¸ RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤."""
    
    def __init__(self, 
                 vector_db_dir: str = "../VectorDB",
                 llm_provider: str = "ollama",
                 model_name: str = "llama3.1"):
        """
        RAG ì±„íŒ… ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            vector_db_dir: ChromaDBê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
            llm_provider: LLM ì œê³µì ('ollama', 'openai', 'claude')
            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…
        """
        self.vector_db_dir = Path(vector_db_dir)
        self.llm_provider = llm_provider
        self.model_name = model_name
        
        self.setup_logging()
        self.setup_chromadb()
        self.setup_llm()
        
    def setup_logging(self):
        """ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('rag_chat.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_chromadb(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.logger.info("ChromaDB ì—°ê²° ì¤‘...")
        
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=str(self.vector_db_dir / "chroma_storage")
            )
            self.collection = self.chroma_client.get_collection("wms_research_papers")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
            count = self.collection.count()
            self.logger.info(f"âœ… ChromaDB ì—°ê²° ì™„ë£Œ: {count}ê°œ ì²­í¬ ë¡œë“œë¨")
            
            if count == 0:
                raise ValueError("ChromaDBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. chromadb_builder.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                
        except Exception as e:
            self.logger.error(f"âŒ ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def setup_llm(self):
        """LLMì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        self.logger.info(f"LLM ì´ˆê¸°í™” ì¤‘: {self.llm_provider} - {self.model_name}")
        
        if self.llm_provider == "ollama":
            if not OLLAMA_AVAILABLE:
                raise ImportError("langchain-communityê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            try:
                self.llm = Ollama(model=self.model_name, base_url="http://localhost:11434")
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                response = self.llm.invoke("Hello")
                self.logger.info(f"âœ… ChatOllama ì—°ê²° ì™„ë£Œ: {self.model_name}")
            except Exception as e:
                self.logger.error(f"âŒ ChatOllama ì—°ê²° ì‹¤íŒ¨: {e}")
                self.logger.info("ğŸ’¡ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")
                raise
                
        elif self.llm_provider == "openai":
            if not OPENAI_AVAILABLE:
                raise ImportError("openaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            self.llm = openai.Client()
            self.logger.info("âœ… OpenAI API ì—°ê²° ì™„ë£Œ")
            
        elif self.llm_provider == "claude":
            if not ANTHROPIC_AVAILABLE:
                raise ImportError("anthropicì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if not os.getenv("ANTHROPIC_API_KEY"):
                raise ValueError("ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            self.llm = anthropic.Anthropic()
            self.logger.info("âœ… Claude API ì—°ê²° ì™„ë£Œ")
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {self.llm_provider}")
    
    def search_relevant_chunks(self, query: str, top_k: int = 5) -> List[Dict]:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì²­í¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        self.logger.info(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘: '{query[:50]}...' (ìƒìœ„ {top_k}ê°œ)")
        
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k
        )
        
        chunks = []
        for i, (doc, metadata, distance) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        )):
            chunks.append({
                'content': doc,
                'metadata': metadata,
                'similarity': 1 - distance,
                'rank': i + 1
            })
        
        self.logger.info(f"âœ… {len(chunks)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
        return chunks
    
    def generate_answer_ollama(self, query: str, context_chunks: List[Dict]) -> str:
        """ChatOllamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        context = "\n\n".join([
            f"ğŸ“„ {chunk['metadata']['paper_filename']} (ìœ ì‚¬ë„: {chunk['similarity']:.3f})\n{chunk['content']}"
            for chunk in context_chunks
        ])
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
ë‹¹ì‹ ì€ WMS(ì°½ê³ ê´€ë¦¬ì‹œìŠ¤í…œ) ì „ë¬¸ ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì œê³µëœ ì—°êµ¬ ë…¼ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ì°¸ê³  ìë£Œ:
{context}

ë‹µë³€ ì§€ì¹¨:
1. ì œê³µëœ ì—°êµ¬ ìë£Œë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ë…¼ë¬¸ëª…ê³¼ í˜ì´ì§€ë¥¼ ì¸ìš©í•˜ì„¸ìš”  
3. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ìì„¸í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "ìë£Œì—ì„œ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”

ë‹µë³€:
"""
        
        try:
            response = self.llm.invoke(prompt)
            return response
        except Exception as e:
            self.logger.error(f"âŒ Ollama ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def generate_answer_openai(self, query: str, context_chunks: List[Dict]) -> str:
        """OpenAI GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        context = "\n\n".join([
            f"ë…¼ë¬¸: {chunk['metadata']['paper_filename']}\në‚´ìš©: {chunk['content']}"
            for chunk in context_chunks
        ])
        
        messages = [
            {"role": "system", "content": """
ë‹¹ì‹ ì€ WMS(ì°½ê³ ê´€ë¦¬ì‹œìŠ¤í…œ) ì „ë¬¸ ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì œê³µëœ ì—°êµ¬ ë…¼ë¬¸ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ë…¼ë¬¸ëª…ì„ ëª…ì‹œí•˜ë©° í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
"""},
            {"role": "user", "content": f"ì§ˆë¬¸: {query}\n\nì°¸ê³ ìë£Œ:\n{context}"}
        ]
        
        try:
            response = self.llm.chat.completions.create(
                model="gpt-4",
                messages=messages,
                temperature=0.1
            )
            return response.choices[0].message.content
        except Exception as e:
            self.logger.error(f"âŒ OpenAI ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def generate_answer_claude(self, query: str, context_chunks: List[Dict]) -> str:
        """Claudeë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        context = "\n\n".join([
            f"ë…¼ë¬¸: {chunk['metadata']['paper_filename']}\në‚´ìš©: {chunk['content']}"
            for chunk in context_chunks
        ])
        
        prompt = f"""
WMS(ì°½ê³ ê´€ë¦¬ì‹œìŠ¤í…œ) ì—°êµ¬ ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ì°¸ê³  ë…¼ë¬¸ ìë£Œ:
{context}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ë…¼ë¬¸ëª…ì„ ë°˜ë“œì‹œ ì¸ìš©í•˜ì„¸ìš”.
"""
        
        try:
            response = self.llm.messages.create(
                model="claude-3-5-sonnet-20240620",
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        except Exception as e:
            self.logger.error(f"âŒ Claude ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def ask_question(self, query: str, top_k: int = 5) -> Dict:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        self.logger.info(f"ğŸ’¬ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: {query}")
        
        # 1ë‹¨ê³„: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_chunks = self.search_relevant_chunks(query, top_k)
        
        # 2ë‹¨ê³„: LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        if self.llm_provider == "ollama":
            answer = self.generate_answer_ollama(query, relevant_chunks)
        elif self.llm_provider == "openai":
            answer = self.generate_answer_openai(query, relevant_chunks)
        elif self.llm_provider == "claude":
            answer = self.generate_answer_claude(query, relevant_chunks)
        else:
            answer = "ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µìì…ë‹ˆë‹¤."
        
        # ê²°ê³¼ íŒ¨í‚¤ì§•
        result = {
            'query': query,
            'answer': answer,
            'sources': [
                {
                    'paper': chunk['metadata']['paper_filename'],
                    'chunk_id': chunk['metadata']['chunk_id'],
                    'similarity': chunk['similarity']
                }
                for chunk in relevant_chunks
            ],
            'timestamp': datetime.now().isoformat(),
            'llm_provider': self.llm_provider
        }
        
        self.logger.info("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
        return result
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        print("ğŸ¤– WMS ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
        print(f"ğŸ“Š {self.collection.count()}ê°œ ë…¼ë¬¸ ì²­í¬ ë¡œë“œë¨")
        print(f"ğŸ§  LLM: {self.llm_provider} - {self.model_name}")
        print("\nğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:")
        print("- WMSì—ì„œ ê°€ì¥ íš¨ìœ¨ì ì¸ ë¬¼ì²´ ì¸ì‹ ë°©ë²•ì€?")
        print("- Amazon ë¡œë´‡ê³¼ ë¹„êµí–ˆì„ ë•Œ ìš°ë¦¬ê°€ ì§‘ì¤‘í•´ì•¼ í•  ê¸°ìˆ ì€?")
        print("- ì°½ê³  ìë™í™”ì—ì„œ ROIê°€ ê°€ì¥ ë†’ì€ ê¸°ìˆ ì€?")
        print("- ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit' ì…ë ¥\n")
        
        while True:
            try:
                query = input("ğŸ™‹ ì§ˆë¬¸: ").strip()
                
                if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print("ğŸ‘‹ WMS ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not query:
                    continue
                
                print("ğŸ” ê²€ìƒ‰ ì¤‘...")
                result = self.ask_question(query)
                
                print(f"\nğŸ¤– ë‹µë³€:\n{result['answer']}")
                print(f"\nğŸ“š ì°¸ê³  ë…¼ë¬¸:")
                for source in result['sources']:
                    print(f"- {source['paper']} (ìœ ì‚¬ë„: {source['similarity']:.3f})")
                print("-" * 80 + "\n")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ WMS ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="WMS RAG ì±„íŒ… ì‹œìŠ¤í…œ")
    parser.add_argument("--vector-db", default="../VectorDB", 
                       help="ChromaDBê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬")
    parser.add_argument("--llm-provider", choices=['ollama', 'openai', 'claude'],
                       default='ollama', help="ì‚¬ìš©í•  LLM ì œê³µì")
    parser.add_argument("--model-name", default="llama3.1",
                       help="ì‚¬ìš©í•  ëª¨ë¸ëª…")
    parser.add_argument("--query", help="ë‹¨ì¼ ì§ˆë¬¸ (ëŒ€í™”í˜• ëª¨ë“œ ëŒ€ì‹ )")
    
    args = parser.parse_args()
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = WMSRAGChatSystem(
        vector_db_dir=args.vector_db,
        llm_provider=args.llm_provider,
        model_name=args.model_name
    )
    
    if args.query:
        # ë‹¨ì¼ ì§ˆë¬¸ ëª¨ë“œ
        result = rag_system.ask_question(args.query)
        print(f"ì§ˆë¬¸: {result['query']}")
        print(f"ë‹µë³€: {result['answer']}")
        print(f"ì°¸ê³  ë…¼ë¬¸: {len(result['sources'])}ê°œ")
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        rag_system.interactive_chat()


if __name__ == "__main__":
    main()
