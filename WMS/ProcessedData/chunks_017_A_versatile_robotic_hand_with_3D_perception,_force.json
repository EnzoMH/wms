{
  "source": "ArXiv",
  "filename": "017_A_versatile_robotic_hand_with_3D_perception,_force.pdf",
  "total_chars": 21840,
  "total_chunks": 32,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nA versatile robotic hand with 3D perception, force\nsensing for autonomous manipulation\nNikolaus Correll, Dylan Kriegman, Stephen Otto and James Watson\nDepartment of Computer Science, University of Colorado Boulder, Boulder, Colorado 80309–0430\nEmail: ncorrell@colorado.edu\nAbstract—We describe a force-controlled robotic gripper with\nbuilt-in tactile and 3D perception. We also describe a com-\nplete autonomous manipulation pipeline consisting of object\ndetection, segmentation, point cloud processing, force-controlled\nmanipulation,andsymbolic(re)-planning.Thedesignemphasizes\nversatility in terms of applications, manufacturability, use of\ncommercial off-the-shelf parts, and open-source software. We\nvalidate the design by characterizing force control (achieving\nup to 32N, controllable in steps of 0.08N), force measurement,\nand two manipulation demonstrations: assembly of the Siemens\ngear assembly problem, and a sensor-based stacking task re-\nquiring replanning.",
      "size": 983,
      "sentences": 3
    },
    {
      "id": 2,
      "content": "o 32N, controllable in steps of 0.08N), force measurement,\nand two manipulation demonstrations: assembly of the Siemens\ngear assembly problem, and a sensor-based stacking task re-\nquiring replanning. These demonstrate robust execution of long\nsequences of sensor-based manipulation tasks, which makes the\nresultingplatformasolidfoundationforresearchersintask-and-\nmotionplanning,educators,andquickprototypingofhousehold,\nindustrial and warehouse automation tasks. I. INTRODUCTION\nAutonomous manipulation remains a challenge in dig-\nitizing and automating value streams from manufacturing\nto recycling. These tasks include picking, placing, assem-\nbly/disassembly, and packing. They involve a large variety\nof compliance, for example when tightening a metal nut vs. Fig. 1. All-in-one manipulation architecture that emphasizes manufactura-\ngraspingadelicateraspberry.Often,tasksofvaryingdexterity bility, active compliance, robust execution, and versatility.",
      "size": 957,
      "sentences": 9
    },
    {
      "id": 3,
      "content": "ut vs. Fig. 1. All-in-one manipulation architecture that emphasizes manufactura-\ngraspingadelicateraspberry.Often,tasksofvaryingdexterity bility, active compliance, robust execution, and versatility. A light-weight\nand compliance need to be combined, requiring a highly software architecture combines object detection, segmentation, point cloud\nprocessing, with planning, inference and execution, creating a platform for\nversatile hardware and software system. Implementing such\nresearchintask-and-motionplanningforcomplexmanipulationproblems. a system is difficult. Commercially available grippers have\nlimitedcapabilities,particularlytheabsenceoftorquecontrol, the underactuated Pisa hand which has 19 joints driven by\nand therefore lack of active compliance [1]. Integration with only a single actuator. Benchmarking an end-effector design\nvision systems is not straightforward; wrist-mounted, table- is a challenging problem and subject to research.",
      "size": 953,
      "sentences": 10
    },
    {
      "id": 4,
      "content": "liance [1]. Integration with only a single actuator. Benchmarking an end-effector design\nvision systems is not straightforward; wrist-mounted, table- is a challenging problem and subject to research. It is difficult\ntop, and palm-mounted cameras can be occluded at different to disentangle grasp planning from perception [3], robot arm\ntimes during operation. Finally, software frameworks such as from end-effector performance [19], and versatility from spe-\nROS take considerable effort to commission and maintain, cialization, e.g. cloth manipulation [16] or warehouse picking\nwhich is often unnecessary in particular when the goal is to [10]. investigate only one aspect of a manipulation pipeline such One way to evaluate the capability of a hand design is\nas vision or high-level reasoning.",
      "size": 795,
      "sentences": 7
    },
    {
      "id": 5,
      "content": "necessary in particular when the goal is to [10]. investigate only one aspect of a manipulation pipeline such One way to evaluate the capability of a hand design is\nas vision or high-level reasoning. In this paper, we present to subject it to a wide variety of household tasks [22] that\na lightweight manipulation architecture geared at researchers, overlapwithupper-limbprostheticapplications.Insuchatest,\neducators, and hobbyists, that is able to perform a wide underactuated designs such as the Pisa hand [5] and a 100-\nvariety of multi-step manipulation tasks, while being simple year old claw design that has only a single actuated finger\nto manufacture and affordable (Figure 1). [20] have outperformed all other designs in [22].",
      "size": 735,
      "sentences": 4
    },
    {
      "id": 6,
      "content": "lti-step manipulation tasks, while being simple year old claw design that has only a single actuated finger\nto manufacture and affordable (Figure 1). [20] have outperformed all other designs in [22]. Although\nThe “gold standard” in manipulation is the human hand, the Pisa hand’s compliance is advantageous when operating\nwhich is able to perform an extensive range of tasks, in- scissors, e.g., a simple mechanical design is intriguing due\ncluding tool usage and in-hand manipulation [13]. Prominent to its manufacturability, robustness, and simpler control [14]\ndesigns in this category are the “ShadowHand” that is able to while still being highly versatile. manipulate a Rubik’s cube [2], the RBO hand [4], and the Although versatility is not important in conventional au-\nPisa/IIT hand [7].",
      "size": 795,
      "sentences": 5
    },
    {
      "id": 7,
      "content": "“ShadowHand” that is able to while still being highly versatile. manipulate a Rubik’s cube [2], the RBO hand [4], and the Although versatility is not important in conventional au-\nPisa/IIT hand [7]. These three examples also span a large tomationusinghighlyspecializedend-effectorssuchasduring\nspectrum of degrees of freedom from 24 (ShadowHand) to warehouse picking or assembly of the same part, versatility\n4202\nbeF\n8\n]OR.sc[\n1v81060.2042:viXra\n=== 페이지 2 ===\nThe 4-bar linkage design (Figure 2) provides a maximal\nfield of view for the palm-mounted camera (Intel RealSense\nD405), providing a maximum aperture of w = 106.24mm. All pieces have been 3D printed using PLA. Although PLA\nprinted parts have lesser stiffness than Delrin or ABS sheets,\nlasercuttersarenotavailabletomosteducatorsandhobbyists. AlllinksuseM5bearingsthatarepress-fittedintoopeningsin\nthe PLA material and M3 screws as the axis.",
      "size": 901,
      "sentences": 6
    },
    {
      "id": 8,
      "content": "lesser stiffness than Delrin or ABS sheets,\nlasercuttersarenotavailabletomosteducatorsandhobbyists. AlllinksuseM5bearingsthatarepress-fittedintoopeningsin\nthe PLA material and M3 screws as the axis. We found using\nbearings to greatly reduce friction, while press-fitting reduces\nplay and thereby increases the accuracy of the mechanism. The AX-12 servo motors (Robotis, Korea) have a stall-\ntorque τ of 1.5Nm. The AX-12’s digital interface allows\nsettingandmeasuringtheactualmotortorqueviacurrentcon-\ntrol, thereby enabling compliant control and detecting objects\nFig.2. TorqueDiagramtoCalculateEstimatedForceontheObject. within the gripper. Independently controlling the fingers helps\nwill becomecritical forrecovering fromerrors orwhen atask to prevent the gripper to get stuck when one of its fingers\nnot only requires picking, but precise placement, e.g. during is impeded, e.g. when picking items from clutter, as well as\npacking. grasping objects off center [12].",
      "size": 969,
      "sentences": 11
    },
    {
      "id": 9,
      "content": "get stuck when one of its fingers\nnot only requires picking, but precise placement, e.g. during is impeded, e.g. when picking items from clutter, as well as\npacking. grasping objects off center [12]. In previous work, we have designed an industrial-grade The Intel RealSense camera has an operational range from\nrobotic gripper following these design principles that inte- 7cmto50cmwithafieldofviewof87o ×58o andaresolution\ngrated 3D perception, force control and computation [12] to of1280×720.Itcandetectobjectsassmallas0.1mm,making\npick up items as small as a washer and as large as a commer- it ideal for in-hand perception applications. The camera is\ncial toner cartridge. The system is gentle enough to handle a mounted in the palm [12], allowing the hand to see an object\nstrawberry, and strong enough to tighten a nut.",
      "size": 826,
      "sentences": 7
    },
    {
      "id": 10,
      "content": "applications. The camera is\ncial toner cartridge. The system is gentle enough to handle a mounted in the palm [12], allowing the hand to see an object\nstrawberry, and strong enough to tighten a nut. In this paper, until contact is made, minimizing distortion and occlusion,\nwe describe a design with much-improved manufacturability when compared to wrist- and table-mounted cameras. Figure\nby relying on commercial, off-the-shelf motors and cameras, 6 (center), shows a snapshot through the camera, with only\nforgoingcostlycustomintegrationofcomputationandcabling. minimal occlusion by the fingers. Although sacrificing ruggedness and stand-alone capability,\nthis design can be manufactured at a fraction of the cost B.",
      "size": 719,
      "sentences": 7
    },
    {
      "id": 11,
      "content": "tlycustomintegrationofcomputationandcabling. minimal occlusion by the fingers. Although sacrificing ruggedness and stand-alone capability,\nthis design can be manufactured at a fraction of the cost B. Software\n(around $460 in parts including camera), and less than half\nOur grasping pipeline follows the outline described in [11]:\nof the weight (414g), allowing operation on a wide range of\nobjects are detected from RGB camera images using pixel-\nrobotic arms, while requiring only a 3D printer and off-the- wise segmentation based on Yolo v52. Segmentation masks\nshelf parts such as stand-offs and bearings. are then used to isolate 3D point cloud information from\nBased on experience with a wide range of robotic competi-\nindividual objects.",
      "size": 743,
      "sentences": 6
    },
    {
      "id": 12,
      "content": "Segmentation masks\nshelf parts such as stand-offs and bearings. are then used to isolate 3D point cloud information from\nBased on experience with a wide range of robotic competi-\nindividual objects. We determine the orientation of the point\ntionsrangingfromwarehousepicking[10],householdautoma-\ncloud using principal component analysis (PCA), compute a\ntion[20],roboticassembly[24]andmobilemanipulation[21],\ngrasp pose based on the object’s bounding box, and use an\nwe have developed a perception and planning pipeline that\ninverse kinematics solver from Peter Corke’s robotic toolbox\ncombinesrecentadvancesinlow-cost3Dperceptionwithdeep\n[9] to move the robotic arm to the desired pose. Together,\nlearningforinstance-segmentation,andconventionalplanning. these components create a robust pipeline in which individual\nHere, the emphasis of our architecture is to robustly deal with\nelements can be replaced for research or education.",
      "size": 932,
      "sentences": 5
    },
    {
      "id": 13,
      "content": "conventionalplanning. these components create a robust pipeline in which individual\nHere, the emphasis of our architecture is to robustly deal with\nelements can be replaced for research or education. All of\nsequencing hundreds of behaviors based on visual and tactile\nthe behaviors are embedded into Behavior Tree (BT) [8]\ncues. The software stack is released under the MIT License1. nodes that allow for robust engineering of multi-state robot\ncontrollers. II. DESIGN\nIn order to be able to manage long task sequences com-\nA. Hardware\nprising hundreds of steps, e.g.",
      "size": 567,
      "sentences": 8
    },
    {
      "id": 14,
      "content": "e MIT License1. nodes that allow for robust engineering of multi-state robot\ncontrollers. II. DESIGN\nIn order to be able to manage long task sequences com-\nA. Hardware\nprising hundreds of steps, e.g. as required during assembly of\nOurdesignisdrivenbytherequirementtoperformassembly complex mechatronics [24], our planner automatically gener-\ntasksinvolvingpartsassmallasM3nutsdrivenbytherobotic ates BTs from a high-level task description of the scene in\nassemblydomain[24]andhouseholdtasksasin[22].Trading PDDL 2.1 [15], which can be efficiently solved by the AI\nstrength and size for manufacturability and cost, we designed planning framework FastDownward [17] that we interface via\nthesystemtopickup88%oftheitemsintheYCBbenchmark py2PDDL[18].Here,theplanningproblemispopulatedfrom\n[6], excluding the hammer, the power drill, the pan, and other the Yolo v5 object recognition pipeline after every step and\nlarge objects.",
      "size": 922,
      "sentences": 6
    },
    {
      "id": 15,
      "content": "benchmark py2PDDL[18].Here,theplanningproblemispopulatedfrom\n[6], excluding the hammer, the power drill, the pan, and other the Yolo v5 object recognition pipeline after every step and\nlarge objects. With this, we were able to specify the gripper the plan is recomputed to catch potential execution failures. aperture and motor strength, assuming a conservative friction Here, spatial relationships between objects (e.g.",
      "size": 420,
      "sentences": 3
    },
    {
      "id": 16,
      "content": "le to specify the gripper the plan is recomputed to catch potential execution failures. aperture and motor strength, assuming a conservative friction Here, spatial relationships between objects (e.g. “on top”) are\ncoefficient of µ=0.4 and a safety factor of 1.5. hard-coded.ThisframeworkisillustratedinFigure6,showing\n1https://github.com/correlllab/MAGPIE 2https://github.com/ultralytics/yolov5\n=== 페이지 3 ===\nBillofMaterials\nID Item Quantity Cost\n1 TopBase(PLA) 1 $1.51\n2 TopBaseCover(PLA) 1 $2.05\n3 BottomBase(PLA) 1 $1.17\n4 BottomBaseCover(PLA) 1 $1.15\n5 ServoCrank(PLA) 2 $0.47\n6 ServoCoupler(PLA) 4 $1.56\n7 ServoRocker(PLA) 2 $0.28\n8 Finger 2 $0.86\n9 OpenRB-150board 1 $24.90\n10 Intel®RealSense™D405 1 $272.00\n11 AX-12AServoMotor 2 $99.80\n12 3M6mmstandoff 4 $9.99\n13 3M8mmstandoff 8 $9.99\n14 2.5M10mmstandoff 4 $9.99\n15 ElectricalWire 2 $0.45\n16 3MNutsandBolts 32 $12.49\n17 2MNutsandBolts 8 $9.99\n18 5Mbearings 8 $9.99\nTotal $458.63\nFig. 3.",
      "size": 944,
      "sentences": 4
    },
    {
      "id": 17,
      "content": "3M6mmstandoff 4 $9.99\n13 3M8mmstandoff 8 $9.99\n14 2.5M10mmstandoff 4 $9.99\n15 ElectricalWire 2 $0.45\n16 3MNutsandBolts 32 $12.49\n17 2MNutsandBolts 8 $9.99\n18 5Mbearings 8 $9.99\nTotal $458.63\nFig. 3. Left: CAD Drawings of the gripper from the top (A), bottom (B), and exploded view (C). Each motor independently actuates a finger, providing\nindependenttorque-basedcontrol.Thecameraisintegratedintothepalm.Right:BillofMaterialswithapproximatedcost.PLAwithacostof$0.05percm2. anexampleofsuccessfultaskcompletiondespiteerrorsduring Figure 6 shows snapshots of the experiment, a view of the\nexecution by continuous replanning. camera image, the resulting labeled 3D objects, and the\ncorresponding PDDL domain. FastDownward automatically\nIII. EVALUATION\ngenerates a plan, which our system translates into a behavior\na) Hardware: In order to test the torque-controlling tree to execute the necessary steps.",
      "size": 899,
      "sentences": 8
    },
    {
      "id": 18,
      "content": "ain. FastDownward automatically\nIII. EVALUATION\ngenerates a plan, which our system translates into a behavior\na) Hardware: In order to test the torque-controlling tree to execute the necessary steps. Running this loop once\nability of the servo, we measure the force at the finger as a correspondstotheclassicalsense-plan-actapproach.Updating\nfunction of the dimensionless Dynamixel settings in the range thePDDLdomainandreplanningaccordinglyallowsformore\nof 0–1023. A force gauge (Shimpo FGV-10XY) was locked sophisticated behavior, including reacting to changes in the\ninto a vice to hold it securely for all measurements. The force environment or recovering from errors in execution. We ran a\ngauge gives an error of ±.2%.",
      "size": 724,
      "sentences": 7
    },
    {
      "id": 19,
      "content": "havior, including reacting to changes in the\ninto a vice to hold it securely for all measurements. The force environment or recovering from errors in execution. We ran a\ngauge gives an error of ±.2%. The force increases linearly single step, moving one block from one tower to another, 50\n(R2 =.995,y =0.0316x−3.0194)from0to9Nat“400”and times.Ofthe42successfulattempts,30werecompletedatthe\nthen increases to 32N following a second-degree polynomial first trial, 6 required two trials, and 3, 2, and 1 experiments\nfunction (R2 = .999). In the linear regime, we are able to required 3, 4, and 6 trials. The other 8 failed in a way that\nchange force in increments of 0.08N. could not be recovered with this simple program, for example\nWe also demonstrate the gripper’s ability to sense when a block was moved out of the field of view of the hand. force/torque. Figure 4 shows the force profile for the right\n(‘Motor1’)andtheleft(‘Motor2’)fingervs.gripperaperture\nIV.",
      "size": 963,
      "sentences": 9
    },
    {
      "id": 20,
      "content": "s ability to sense when a block was moved out of the field of view of the hand. force/torque. Figure 4 shows the force profile for the right\n(‘Motor1’)andtheleft(‘Motor2’)fingervs.gripperaperture\nIV. DISCUSSIONANDCONCLUSION\nas the gripper closes on the mustard container from the YCB The proposed design exceeds the capabilities of [12] in\ndataset. The container was placed off-center, with the right terms of perception and in the dimensions of objects that can\nmotor making contact first. Instead of moving the mustard, be picked up. Yet, 3D-printed PLA plastics cannot reach the\nthe finger remains in position until the second finger makes levelofstiffnessandaccuracythatasheet-metal-baseddesign\ncontact.Weobserveminimaltorqueswhilemovingthefinger, can,whichisadisadvantagewhengraspingverysmallpieces,\nwith Motor 1 having more friction than Motor 2 due to such as an M3 screw that requires exact alignment of the\nvariations in manufacturing. fingers.",
      "size": 953,
      "sentences": 8
    },
    {
      "id": 21,
      "content": "n,whichisadisadvantagewhengraspingverysmallpieces,\nwith Motor 1 having more friction than Motor 2 due to such as an M3 screw that requires exact alignment of the\nvariations in manufacturing. fingers. To demonstrate the precision and accuracy of the design, The current design requires three external cables that need\nwe perform a robotic assembly task with tight tolerances to be routed along a robotic arm: a USB 3.1 connection for\n(< 0.5mm). Snapshots of the task are shown in Figure 5. thecamera,aUSB2.0connectionforthemotorcontrolboard,\nPrecisionandaccuracyofgraspingtheelementsfromaknown and a power cable that can provide up to 3A at 12V (36W). position on the kitting mat (modeling the industrial assembly All of these could be met by a single USB-C PD connection,\nchallenge requirements from [24]) is sufficient to reliably in particular when 5V to 12V step-up circuits with sufficient\nassemble all parts into a functioning mechanical system. The power are available.",
      "size": 975,
      "sentences": 6
    },
    {
      "id": 22,
      "content": "lenge requirements from [24]) is sufficient to reliably in particular when 5V to 12V step-up circuits with sufficient\nassemble all parts into a functioning mechanical system. The power are available. open-loopassemblytasksucceeded8outof10trials;withone The emphasis on the software pipeline is to demonstrate\nfailure each due to an angular misalignment of the small peg modularity (via the BT framework and STRIPS planner),\nand the large gear becoming jammed on the large peg. These rather than performance on specific subtasks. Indeed, compo-\nresults motivate a sensor-based re-planning approach such as nentssuchasobjectdetectioncouldbeeasilytunedtoachieve\ndescribed in [25]. 100%one-shotsuccessrateinthetowerassemblytask.Failure\nb) Software: We evaluated the system using a tower- has been instructive as it will be pervasive in autonomous\nassembly task using three colored, wooden cubes from [6]. settings, and remain a problem if it occurs with a non-zero\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\nFig.4.",
      "size": 994,
      "sentences": 7
    },
    {
      "id": 23,
      "content": "structive as it will be pervasive in autonomous\nassembly task using three colored, wooden cubes from [6]. settings, and remain a problem if it occurs with a non-zero\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\nFig.4. GrippergraspingthemustardcontainerfromtheYCBdatasetfromanoff-centerposition(left).Limitingtorqueduringapproachpreventsthemustard\nfrommovingastherightfingermakescontact.Dashedverticallinesindicatecontactbytherightfinger,contactwithbothfingers,andopening(fromleftto\nright). Fig.5. Demonstrationofaccuracyandprecisionbyreliablyassemblingthe“Siemensgearassemblyproblem”[23]withsub-millimeteraccuracyrequirements. Fig.6. Snapshotsfromatowerconstructiontaskthatissolvedbycontinuousreplanning.Therobotaccidentallyhitsthetower(1)whileplacingtheblue\nblock (2), which fails as the tower has moved (3), the robot re-analyzes the scene (4), and places the blue block (5).",
      "size": 859,
      "sentences": 7
    },
    {
      "id": 24,
      "content": "vedbycontinuousreplanning.Therobotaccidentallyhitsthetower(1)whileplacingtheblue\nblock (2), which fails as the tower has moved (3), the robot re-analyzes the scene (4), and places the blue block (5). Point cloud and image data are used\nforobjectidentificationandsegmentation,resultingintoalabeledscenethatgetsparsedintoaPDDL2.1problemdescriptionandsolvedbyFastDownward. Theplanisrepeateduntiltheproblemissolved. probability. Here, we are particularly interested in reasoning cup. underuncertaintyandgeneratinghigh-levelplansforrecovery\nACKNOWLEDGEMENTS\nfrom error using semantic domain knowledge. For example,\nwhen an object moves outside of the robot’s field of view, This work has been supported by a grant by the National\nthe robot could search for it and move objects out of the Science Foundation “USDA-NIFA NRI INT: Autonomous\nway if it has to. We plan to further explore this in the future Restoration and Revegetation of Degraded Ecosystems”.",
      "size": 950,
      "sentences": 8
    },
    {
      "id": 25,
      "content": "it and move objects out of the Science Foundation “USDA-NIFA NRI INT: Autonomous\nway if it has to. We plan to further explore this in the future Restoration and Revegetation of Degraded Ecosystems”. by interfacing PDDL descriptions with large language models\nREFERENCES\nto (1) generate problem descriptions from image/3D data,\n[1] A. Albu-Schaffer, O. Eiberger, M. Grebenstein, S. Had-\n(2) generate PDDL descriptions from natural language, and\ndadin, C. Ott, T. Wimbock, S. Wolf, and G. Hirzinger. (3) use domain knowledge to increase the robot’s reasoning\nSoft robotics. IEEE Robotics & Automation Magazine,\nabilities.Finally,weareinterestedinbuildinguponthisdesign\n15(3):20–30, 2008.\ntoperformbetteronheavyitemsbyaddingathumb,operating\n[2] O. M. Andrychowicz, B. Baker, M. Chociej, R. Joze-\ntools such as a power-drill by adding an additional degree of\nfowicz,B.McGrew,J.Pachocki,A.Petron,M.Plappert,\nfreedom to one finger, and fast picking by adding a suction\nG. Powell, A. Ray, et al.",
      "size": 988,
      "sentences": 5
    },
    {
      "id": 26,
      "content": "ze-\ntools such as a power-drill by adding an additional degree of\nfowicz,B.McGrew,J.Pachocki,A.Petron,M.Plappert,\nfreedom to one finger, and fast picking by adding a suction\nG. Powell, A. Ray, et al. Learning dexterous in-hand\n=== 페이지 5 ===\nmanipulation. The International Journal of Robotics for expressing temporal planning domains. Journal of\nResearch, 39(1):3–20, 2020. artificial intelligence research, 20:61–124, 2003. [3] Y. Bekiroglu, N. Marturi, M. A. Roa, K. J. M. Adjigble, [16] I. Garcia-Camacho, M. Lippi, M. C. Welle, H. Yin,\nT. Pardi, C. Grimm, R. Balasubramanian, K. Hang, and R.Antonova,A.Varava,J.Borras,C.Torras,A.Marino,\nR. Stolkin. Benchmarking protocol for grasp planning G. Alenya, et al. Benchmarking bimanual cloth manip-\nalgorithms. IEEE Robotics and Automation Letters, 5 ulation. IEEE Robotics and Automation Letters, 5(2):\n(2):315–322, 2019. 1111–1118, 2020. [4] A.Bhatt,A.Sieler,S.Puhlmann,andO.Brock. Surpris- [17] M. Helmert. The fast downward planning system.",
      "size": 992,
      "sentences": 13
    },
    {
      "id": 27,
      "content": "rs, 5 ulation. IEEE Robotics and Automation Letters, 5(2):\n(2):315–322, 2019. 1111–1118, 2020. [4] A.Bhatt,A.Sieler,S.Puhlmann,andO.Brock. Surpris- [17] M. Helmert. The fast downward planning system. Jour-\ningly robust in-hand manipulation: An empirical study. nalofArtificialIntelligenceResearch,26:191–246,2006. arXiv preprint arXiv:2201.11503, 2022. [18] R. Karem. py2pddl. https://github.com/remykarem/\n[5] M.Bonilla,C.D.Santina,A.Rocchi,E.Luberto,G.San- py2pddl, 2022.\ntaera, E. Farnioli, C. Piazza, F. Bonomo, A. Brando, [19] A. S. Morgan, K. Hang, W. G. Bircher, F. M. Alladkani,\nA. Raugi, et al. Advanced grasping with the pisa/iit A. Gandhi, B. Calli, and A. M. Dollar. Benchmarking\nsofthand. In Robotic Grasping and Manipulation: First clutteredrobotpick-and-placemanipulationwiththebox\nRobotic Grasping and Manipulation Challenge, RGMC and blocks test. IEEE Robotics and Automation Letters,\n2016, Held in Conjunction with IROS 2016, Daejeon, 5(2):454–461, 2019.",
      "size": 972,
      "sentences": 16
    },
    {
      "id": 28,
      "content": "emanipulationwiththebox\nRobotic Grasping and Manipulation Challenge, RGMC and blocks test. IEEE Robotics and Automation Letters,\n2016, Held in Conjunction with IROS 2016, Daejeon, 5(2):454–461, 2019. South Korea, October 10–12, 2016, Revised Papers 1, [20] R. Patel, J. Segil, and N. Correll. Manipulation using\npages 19–38. Springer, 2018. the “utah” prosthetic hand: The role of stiffness in ma-\n[6] B. Calli, A. Singh, J. Bruce, A. Walsman, K. Konolige, nipulation. InRoboticGraspingandManipulation:First\nS. Srinivasa, P. Abbeel, and A. M. Dollar. Yale-cmu- Robotic Grasping and Manipulation Challenge, RGMC\nberkeley dataset for robotic manipulation research. The 2016, Held in Conjunction with IROS 2016, Daejeon,\nInternational Journal of Robotics Research, 36(3):261– South Korea, October 10–12, 2016, Revised Papers 1,\n268, 2017. pages 107–116. Springer, 2018. [7] M. G. Catalano, G. Grioli, E. Farnioli, A. Serio, C. Pi- [21] M. A. Roa, M. R. Dogar, J. Pages, C. Vivas,\nazza, and A. Bicchi.",
      "size": 997,
      "sentences": 11
    },
    {
      "id": 29,
      "content": "2, 2016, Revised Papers 1,\n268, 2017. pages 107–116. Springer, 2018. [7] M. G. Catalano, G. Grioli, E. Farnioli, A. Serio, C. Pi- [21] M. A. Roa, M. R. Dogar, J. Pages, C. Vivas,\nazza, and A. Bicchi. Adaptive synergies for the design A. Morales, N. Correll, M. Gorner, J. Rosell, S. Foix,\nand control of the pisa/iit softhand. The International R. Memmesheimer, et al. Mobile manipulation\nJournal of Robotics Research, 33(5):768–782, 2014. hackathon: Moving into real world applications. IEEE\n[8] M. Colledanchise and P. O¨gren. Behavior trees in Robotics&AutomationMagazine,28(2):112–124,2021. robotics and AI: An introduction. CRC Press, 2018. [22] Y. Sun, J. Falco, N. Cheng, H. R. Choi, E. D. Engeberg,\n[9] P. Corke and J. Haviland. Not your grandmother’s N. Pollard, M. Roa, and Z. Xia. Robotic grasping\ntoolbox–the robotics toolbox reinvented for python. In and manipulation competition: task pool.",
      "size": 904,
      "sentences": 16
    },
    {
      "id": 30,
      "content": ",\n[9] P. Corke and J. Haviland. Not your grandmother’s N. Pollard, M. Roa, and Z. Xia. Robotic grasping\ntoolbox–the robotics toolbox reinvented for python. In and manipulation competition: task pool. In Robotic\n2021 IEEE International Conference on Robotics and GraspingandManipulation:FirstRoboticGraspingand\nAutomation (ICRA), pages 11357–11363. IEEE, 2021. ManipulationChallenge,RGMC2016,HeldinConjunc-\n[10] N. Correll, K. E. Bekris, D. Berenson, O. Brock, tionwithIROS2016,Daejeon,SouthKorea,October10–\nA. Causo, K. Hauser, K. Okada, A. Rodriguez, J. M. 12, 2016, Revised Papers 1, pages 1–18. Springer, 2018. Romano, and P. R. Wurman. Analysis and observations [23] M.Vecerik,O.Sushkov,D.Barker,T.Rotho¨rl,T.Hester,\nfrom the first amazon picking challenge. IEEE Trans- and J. Scholz. A practical approach to insertion with\nactions on Automation Science and Engineering, 15(1): variable socket position using deep reinforcement learn-\n172–188, 2016. ing.",
      "size": 958,
      "sentences": 12
    },
    {
      "id": 31,
      "content": "e. IEEE Trans- and J. Scholz. A practical approach to insertion with\nactions on Automation Science and Engineering, 15(1): variable socket position using deep reinforcement learn-\n172–188, 2016. ing. In 2019 international conference on robotics and\n[11] N. Correll, B. Hayes, C. Heckman, and A. Roncone. In- automation (ICRA), pages 754–760. IEEE, 2019.\ntroductiontoAutonomousRobots:Mechanisms,Sensors, [24] F. Von Drigalski, C. Schlette, M. Rudorfer, N. Correll,\nActuators, and Algorithms. Mit Press, 2022. J. C. Triyonoputro, W. Wan, T. Tsuji, and T. Watan-\n[12] N. J. Correll, A. K. Miller, and B. Romero. Systems, abe. Robots assembling machines: learning from the\ndevices,components,andmethodsforacompactrobotic worldrobotsummit2018assemblychallenge. Advanced\ngripper with palm-mounted sensing, grasping, and com- Robotics, 34(7-8):408–421, 2020.\nputingdevicesandcomponents,Oct.192021. USPatent [25] J. Watson, A. Miller, and N. Correll.",
      "size": 942,
      "sentences": 11
    },
    {
      "id": 32,
      "content": "lychallenge. Advanced\ngripper with palm-mounted sensing, grasping, and com- Robotics, 34(7-8):408–421, 2020.\nputingdevicesandcomponents,Oct.192021. USPatent [25] J. Watson, A. Miller, and N. Correll. Autonomous in-\n11,148,295. dustrial assembly using force, torque, and rgb-d sensing. [13] R. Coulson, C. Li, C. Majidi, and N. S. Pollard. The Advanced Robotics, 34(7-8):546–559, 2020.\nelliott and connolly benchmark: A test for evaluating\nthe in-hand dexterity of robot hands. In 2020 IEEE-\nRAS20thInternationalConferenceonHumanoidRobots\n(Humanoids), pages 238–245. IEEE, 2021. [14] R.S.Fearing. Simplifiedgraspingandmanipulationwith\ndextrous robot hands. IEEE Journal on Robotics and\nAutomation, 2(4):188–195, 1986. [15] M. Fox and D. Long. Pddl2.1: An extension to pddl",
      "size": 771,
      "sentences": 13
    }
  ]
}