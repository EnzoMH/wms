{
  "source": "ArXiv",
  "filename": "044_Learning_2D_to_3D_Lifting_for_Object_Detection_in_.pdf",
  "total_chars": 42549,
  "total_chunks": 63,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nLearning 2D to 3D Lifting for Object Detection in 3D\nfor Autonomous Vehicles\nSiddharth Srivastava1, Frederic Jurie2 and Gaurav Sharma3\nAbstract—We address the problem of 3D object detection BirdGAN 3D representation\ne.g. BEV images\nfrom 2D monocular images in autonomous driving scenarios. We propose to lift the 2D images to 3D representations\nusing learned neural networks and leverage existing networks\nworking directly on 3D data to perform 3D object detection\nandlocalization.Weshowthat,withcarefullydesignedtraining\nmechanism and automatically selected minimally noisy data,\nsuch a method is not only feasible, but gives higher results 2D image 3D detections\nthan many methods working on actual 3D inputs acquired\nfrom physical sensors.",
      "size": 756,
      "sentences": 3
    },
    {
      "id": 2,
      "content": "ically selected minimally noisy data,\nsuch a method is not only feasible, but gives higher results 2D image 3D detections\nthan many methods working on actual 3D inputs acquired\nfrom physical sensors. On the challenging KITTI benchmark,\nwe show that our 2D to 3D lifted method outperforms many\nrecentcompetitive3Dnetworkswhilesignificantlyoutperform-\nGround plane\ning previous state-of-the-art for 3D detection from monocular\nestimation\nimages. We also show that a late fusion of the output of the\nnetwork trained on generated 3D images, with that trained\nAuxiliary network\non real 3D images, improves performance. We find the results\nvery interesting and argue that such a method could serve as\nFig. 1.",
      "size": 702,
      "sentences": 5
    },
    {
      "id": 3,
      "content": "k trained on generated 3D images, with that trained\nAuxiliary network\non real 3D images, improves performance. We find the results\nvery interesting and argue that such a method could serve as\nFig. 1. We aim to do 3D detection from 2D monocular images, by\na highly reliable backup in case of malfunction of expensive generating (i) 3D representations using state-of-the-art GANs and (ii) 3D\n3D sensors, if not potentially making them redundant, at least dataforgroundplaneestimationusingrecent3Dnetworks.Weshowthat\nin the case of low human injury risk autonomous navigation itispossibletoachievecompetitive3Ddetectionwithouthavingactual3D\nscenarios like warehouse automation. dataattesttime. I. INTRODUCTION\nWe address the important problem of object detection in\ndetection in 3D at test time without the need for explicit\n3D data while only using monocular images at inference. 3D data.",
      "size": 886,
      "sentences": 8
    },
    {
      "id": 4,
      "content": "me. I. INTRODUCTION\nWe address the important problem of object detection in\ndetection in 3D at test time without the need for explicit\n3D data while only using monocular images at inference. 3D data. We show that it is possible to utilize 3D data,\nTraditionally, two approaches have been widespread for 3D\ncollectedonce,totrainanetworktogenerate3Dinformation\nobject detection problems. First is to detect objects in 2D\nat test time from 2D images, which can be used as a drop-\nusing monocular images and then infer in 3D [1], [2],\nin replacement to many object detection pipelines, and still\n[3], and second is to use 3D data (e.g.LiDAR) to detect\nprovidesurprisinglygoodresults.Inparticular,wetargetthe\nbounding boxes directly in 3D [4].",
      "size": 738,
      "sentences": 6
    },
    {
      "id": 5,
      "content": "cement to many object detection pipelines, and still\n[3], and second is to use 3D data (e.g.LiDAR) to detect\nprovidesurprisinglygoodresults.Inparticular,wetargetthe\nbounding boxes directly in 3D [4]. However, on 3D object\nBird’sEyeView(BEV)and3DObjectDetectionchallenges\ndetectionandlocalizationbenchmarks,themethodsbasedon\nof KITTI’s evaluation benchmark and show that with 3D\nmonocularimagessignificantlylagbehindthelatter,limiting\ninformation generated at test time from 2D images, we can\ntheir deployment in practical scenarios. A reason for such a\nachieve better or comparable results to numerous recent and\ndisparityinperformanceisthatmethodsbasedonmonocular\ncompetitive techniques working directly on 3D data. We\nimages attempt at implicitly inferring 3D information from\nbelievetheresultsareofimportanceas(i)theeffortsthatare\nthe input.",
      "size": 844,
      "sentences": 4
    },
    {
      "id": 6,
      "content": "odsbasedonmonocular\ncompetitive techniques working directly on 3D data. We\nimages attempt at implicitly inferring 3D information from\nbelievetheresultsareofimportanceas(i)theeffortsthatare\nthe input. Additionally, availability of depth information\ndirected towards collecting high quality 3D data can help in\n(derived or explicit), greatly increases the performance of\nscenarios where explicit 3D data cannot be acquired at test\nsuchmethods[4].Moreover,for3Dnetworkstoworkattest\ntime. (ii) the method can be used as a plug-and-play module\ntime,alimitationistheneedtodeployexpensive(thousands\nwithanyexisting3DmethodwhichworkswithBEVimages,\nof dollars) and bulky (close to a kilogram) 3D scanners\nallowing operations with seamless switching between RGB\ncf.cheaper and lighter 2D cameras. Hence, a monocular\nand3Dscannerswhileleveragingthesameunderlyingobject\nimage based 3D object detection method closing the gap\ndetection platform.",
      "size": 932,
      "sentences": 5
    },
    {
      "id": 7,
      "content": "tching between RGB\ncf.cheaper and lighter 2D cameras. Hence, a monocular\nand3Dscannerswhileleveragingthesameunderlyingobject\nimage based 3D object detection method closing the gap\ndetection platform. in performance with the methods requiring explicit 3D data\nThepresentedworkmakesuseofprogressininterpretation\nwill be highly practical. of 2D scans in 3D, such as 3D reconstruction from single\nIn this paper, we show that we can learn a mapping,\nimages [5], [6] and depth estimation [7].",
      "size": 486,
      "sentences": 4
    },
    {
      "id": 8,
      "content": "ogressininterpretation\nwill be highly practical. of 2D scans in 3D, such as 3D reconstruction from single\nIn this paper, we show that we can learn a mapping,\nimages [5], [6] and depth estimation [7]. To the best of our\nleveraging existing 3D data, and use it to perform object\nknowledge,wearethefirsttogenerate(twodifferentvariants\n1SiddharthSrivastavaiswithIndianInstituteofTechnologyDelhi,India of)BEVimagefromasingleRGBimage,usingstate-of-the-\neez127506@ee.iitd.ac.in art image to image translation models, and use it for object\n2Frederic Jurie is with Normandie Univ., UNICAEN, ENSICAEN, detectionusingexisting3DCNNs.Theresultsshowthatwe\nCNRS,Francefrederic.jurie@unicaen.fr\nsignificantlyoutperformthestate-of-the-artmonocularimage\n3Gaurav Sharma is with NEC Labs America, USA\ngrvsharma@gmail.com based3Dobjectdetectionswhilealsoperformingbetterthan\n9102\ntcO\n11\n]VC.sc[\n2v49480.4091:viXra\n[표 데이터 감지됨]\n\n=== 페이지 2 ===\nmany recent methods requiring explicit 3D data.",
      "size": 967,
      "sentences": 3
    },
    {
      "id": 9,
      "content": "America, USA\ngrvsharma@gmail.com based3Dobjectdetectionswhilealsoperformingbetterthan\n9102\ntcO\n11\n]VC.sc[\n2v49480.4091:viXra\n[표 데이터 감지됨]\n\n=== 페이지 2 ===\nmany recent methods requiring explicit 3D data. Finally, networkextractinginformationfromothercorrespondingdo-\nwhile the performance of a method with generated data is mains (e.g. image). [22] uses a GAN to generate 3D objects\nexpectedtobeatmostasgoodastheunderlyingnetwork,we from a single depth image, by combining autoencoders and\nshow that by fusing the outputs of the base network and the conditional GAN. [23] uses a GAN to generate 3D from 2D\nnetwork trained on generated data, the performance of the images, and perform shape completion from occluded 2.5D\nbasenetworkisfurtherimproved,outperformingthemethods views, building on [18] using Wasserstein objective. based on 3D data captured using scanners. Image to image translation: Our work addresses the spe-\nII.",
      "size": 923,
      "sentences": 7
    },
    {
      "id": 10,
      "content": "orkisfurtherimproved,outperformingthemethods views, building on [18] using Wasserstein objective. based on 3D data captured using scanners. Image to image translation: Our work addresses the spe-\nII. RELATEDWORK cific task of 3D object detection by translating RGB images\nObject Detection in 3D: Object detection in 3D is one to BEV. Image translation has recently received attention\nof the main tasks of 3D scene understanding. Many works for style transfer applications, e.g.pix2pix [24] or the recent\nhave addressed 3D object detection using 3D data like work of [25]. While perhaps being less challenging than a\nLiDAR images [4], [8], [9] and stereo images [10], while full and accurate 3D scene generation, 3D object detection\nsome have also used only monocular images [1], [11]. The is still a very challenging and relevant task for autonomous\napproachesfor3Dobjectdetectionvaryfromproposingnew driving use cases.",
      "size": 919,
      "sentences": 8
    },
    {
      "id": 11,
      "content": "t detection\nsome have also used only monocular images [1], [11]. The is still a very challenging and relevant task for autonomous\napproachesfor3Dobjectdetectionvaryfromproposingnew driving use cases. Here, we generate 3D data as an inter-\nneural network architectures, e.g.BirdNet [8], MV3D [4], mediate step, but instead of focusing on the quality of the\nto novel object representations such as the work on 3DVP generated 3D data as in [24], [25], we design and evaluate\n[9].Someworksalsoutilizeothermodalitiesalongwith3D, our method directly on the task of 3D object detection from\nsuch as corresponding 2D images [4] and structure from monocular images. motion [12]. Among the neural network based approaches,\nIII.",
      "size": 717,
      "sentences": 5
    },
    {
      "id": 12,
      "content": "th3D, our method directly on the task of 3D object detection from\nsuch as corresponding 2D images [4] and structure from monocular images. motion [12]. Among the neural network based approaches,\nIII. APPROACH\nmany competitive approaches follow the success of 2D\nThe proposed approach aims at generating 3D data from\nobject detection methods and are based on 3D proposal\n2D images for performing 3D object detection in the gener-\nnetworks and classifying them, e.g.MV3D (multi-view 3D)\nateddatawith3Dobjectdetectors.WegenerateBEVimages\n[4], AVOD[13]. directly from a single RGB image, (i) by designing a high\nWebuildonsuchrecentarchitectureswhichworkdirectly\nfidelity GAN architecture, and (ii) by carefully curating a\nwith 3D representations, i.e., previous works that took mul-\ntraining mechanism, which includes selecting of minimally\ntiview projections of the 3D data to use with 2D image\nnoisy data for training. networks followed by fusion mechanisms [14].",
      "size": 961,
      "sentences": 6
    },
    {
      "id": 13,
      "content": "s that took mul-\ntraining mechanism, which includes selecting of minimally\ntiview projections of the 3D data to use with 2D image\nnoisy data for training. networks followed by fusion mechanisms [14]. However,\nSince training GAN based networks is hard, we initially\ninstead of feeding them with real 3D data we use generated\nexplored the idea of obtaining 3D point clouds image to\ndata as input. Since the two architectures we use as our\ndepth networks [7], and project BEV from them. However,\nbasenetworks,i.e.BirdNet[8]andMV3D[4],takeinputsof\nthe generated BEVs were sparse and did not have enough\ndifferent nature we propose appropriate generation networks\ndensity/informationin theobject regions.This motivated the\nand a carefully designed training data processing pipeline. current approach of directly generating BEV using GAN.",
      "size": 832,
      "sentences": 6
    },
    {
      "id": 14,
      "content": "riate generation networks\ndensity/informationin theobject regions.This motivated the\nand a carefully designed training data processing pipeline. current approach of directly generating BEV using GAN. Inferring3DusingRGBimages:Amongmethodsinferring The proposed method can work with different variants of\n3D information from RGB images, [15] work on predicting the BEV image considered. Specifically, we show results\n2D keypoint heat maps and 3D objects structure recovery. with two recent competitive 3D object detection networks,\n[16] use single RGB image to obtain detailed 3D structure BirdNet [8] and MV3D [4], which originally take different\nusing MRFs on small homogeneous patches to predict plane formats of BEV inputs obtained form 3D LiDAR inputs. parameters encoding 3D locations and orientations of the Both of the networks process the LiDAR input to obtain (i)\npatches.",
      "size": 881,
      "sentences": 6
    },
    {
      "id": 15,
      "content": "ches to predict plane formats of BEV inputs obtained form 3D LiDAR inputs. parameters encoding 3D locations and orientations of the Both of the networks process the LiDAR input to obtain (i)\npatches. [17] learn to predict 3D human pose from single Bird’s Eye View (BEV) images, of two different variations,\nimage using a fine discretization of the 3D space around the and (ii) 3D point clouds for ground plane estimation. Ad-\nsubject and predicting per voxel likelihoods for each joint, ditionally, MV3D also takes the corresponding front view\nand using a coarse-to-fine scheme. and RGB images as input. The BEV images generated\nGenerating 3D data from 2D: Many works have proposed from 2D images by the proposed method are effective with\nvariants of generative models for 3D data generation. [18] such 3D detection algorithm, both at train and test times.",
      "size": 856,
      "sentences": 7
    },
    {
      "id": 16,
      "content": "Many works have proposed from 2D images by the proposed method are effective with\nvariants of generative models for 3D data generation. [18] such 3D detection algorithm, both at train and test times. use Generative Adversarial Networks (GANs) to gener- While, BirdNet is based on processing BEV input, MV3D\nate 3D objects using volumetric networks, extending the is takes multi-modal input. We show results on both, and\nvanilla GAN and VAE GAN to 3D. [19] propose projective hence demonstrate that the proposed method is general\ngenerative adversarial networks (PrGAN) for obtaining 3D and is capable of working across a spectrum of methods\nstructures from multiple 2D views. [20] synthesize novel using (variations of) BEV input. We now gives details of\nviews from a single image by inferring geometrical infor- our generation network, and our training data processing,\nmation followed by image completion, using a combination followed by the details of two instantiations of our method.",
      "size": 988,
      "sentences": 7
    },
    {
      "id": 17,
      "content": "ring geometrical infor- our generation network, and our training data processing,\nmation followed by image completion, using a combination followed by the details of two instantiations of our method. of adversarial and perceptual loss. [6] propose Perspective\nA. Generating BEV images from 2D images (BirdGAN)\nTransformer Nets (PTNs), an encoder-decoder network with\na novel projection loss using perspective transformation, for The network for generating the BEV images from input\nlearning to use 2D observations without explicit 3D super- 2D RGB images are based on the Generative Adversarial\nvision. [21] generate 3D models with an enhancer neural Networks for image to image translation [24]. GANs have\n=== 페이지 3 ===\nInput image\nBirdGAN\n2D aligned\nBEV Image BirdNet\ndetections in BEV\nImage to 3D Net Ground plane estimation 3D detections 2D detections\nFig.2. ProposedpipelinewithBirdNet.AGANbasedgeneratortranslatesthe2DRGBimageintoBEVimagecompatiblewiththeBirdNetarchitecture.",
      "size": 981,
      "sentences": 7
    },
    {
      "id": 18,
      "content": "n BEV\nImage to 3D Net Ground plane estimation 3D detections 2D detections\nFig.2. ProposedpipelinewithBirdNet.AGANbasedgeneratortranslatesthe2DRGBimageintoBEVimagecompatiblewiththeBirdNetarchitecture. An RGB to 3D network, like the Perspective Transformer Network [6], gives us 3D information for ground planes estimation. The BEV detections are\nthenconvertedto3Ddetectionsusingthegroundplaneestimation. Input image\n(M+2) channel\nBirdGANs BEV Image\n2D and 3D detections\nDepth Depth to Front view\nimage point cloud image MV3D\nImage to Depth Net\nFig.3. ProposedpipelinewithMV3D.InthecaseofMV3D,multipleGANbasedgeneratorsindependentlytranslatethe2DRGBimageintothedifferent\nchannelsoftheMV3DcompatibleBEVimage.InadditionauxiliarynetworkisusedforFrontView(FV)imagegenerationfromRGBimage.Allthree,\ni.e.RGB,FVandBEVimages,arethenfedtotheMV3Darchitecturetopredictin3D.",
      "size": 859,
      "sentences": 6
    },
    {
      "id": 19,
      "content": "ifferent\nchannelsoftheMV3DcompatibleBEVimage.InadditionauxiliarynetworkisusedforFrontView(FV)imagegenerationfromRGBimage.Allthree,\ni.e.RGB,FVandBEVimages,arethenfedtotheMV3Darchitecturetopredictin3D. become very popular recently and here we use them for B. BirdNet 3D object detection\nThe default BirdNet [8] pipeline uses a 3 channel Bird’s\nturning 2D RGB images to BEV images containing 3D\nEye View (BEV) image consisting of height, density and\ninformation about the scene. Specifically, the image trans-\nintensity of the points as the main input. In addition to\nlation BirdGAN network consists of a VGG-16 CNN to\nthe BEV image input, BirdNet also requires ground plane\nencode the images, followed by DCGAN [26] conditioned\nestimation for determining the height of the 3D bounding\nover the encoded vector to generate the BEV image. The\nboxes.",
      "size": 844,
      "sentences": 5
    },
    {
      "id": 20,
      "content": "also requires ground plane\nencode the images, followed by DCGAN [26] conditioned\nestimation for determining the height of the 3D bounding\nover the encoded vector to generate the BEV image. The\nboxes. Both of these inputs are normally extracted from\nfullBirdGANpipelineistrainedend-to-endusingthepaired\nthe full LIDAR point cloud which is the main input to the\nmonocular and BEV images available. system. The quality of data used to train the GAN makes a big\nIn the proposed method ((Fig. 2), we generate the two\ndifference in the final performance. With this in mind, we\ninputs, i.e.the BEV image and the 3D point cloud using\npropose and experiment with two methods for training the\ntwo neural networks learned on auxiliary data, respectively. GAN for generating BEV images. In the first, we take all\nThe first network is the GAN based network explained in\nthe objects in the scene, i.e.the whole image, to generate\nthe previous section (Sec. III-A).",
      "size": 950,
      "sentences": 10
    },
    {
      "id": 21,
      "content": "generating BEV images. In the first, we take all\nThe first network is the GAN based network explained in\nthe objects in the scene, i.e.the whole image, to generate\nthe previous section (Sec. III-A). It takes the RGB image as\nthe BEV image, while in the second we take only the ‘well-\ninput and outputs the 3 channel BEV image. The 3 channels\ndefined’objectsinthescene,i.e.,thoseclosesttothecamera. of the BEV image in this case are the height, density and\nThe former is the natural choice which makes use of all\nintensity of the points. the data available, while the latter is motivated by the fact\nThe second network reconstructs a 3D model using the\nthat the point clouds become relatively noisy, and possibly\nRGB image as input. The 3D reconstruction network takes\nuninformative for object detection, as the distance increases\nthe 3 channel RGB image as input and generates either the\ndue to very small objects and occlusions.",
      "size": 929,
      "sentences": 8
    },
    {
      "id": 22,
      "content": ". The 3D reconstruction network takes\nuninformative for object detection, as the distance increases\nthe 3 channel RGB image as input and generates either the\ndue to very small objects and occlusions. In the second\npoint clouds or their voxelized version as the 3D model. case,whileasignificantamountofdataisdiscarded(e.g.the\nThe generated 3D model is then used to obtain the ground\nobjectshighlightedwithredarrowsintheBEVimageonthe\nestimation for constructing the 3D bounding boxes around\nright in Fig. 4), the quality of retained data is better as the\nthe detected objects. nearby objects are bigger and have fewer occlusions etc.,\nespecially in the RGB image. In both of the cases, we work C. MV3D as base architecture\nwith the RGB images and corresponding LiDAR clouds of MV3D[4]takesthreeinputs,theRGBimage,theLiDAR\nthe area around the car (Fig. 4).",
      "size": 853,
      "sentences": 8
    },
    {
      "id": 23,
      "content": "image. In both of the cases, we work C. MV3D as base architecture\nwith the RGB images and corresponding LiDAR clouds of MV3D[4]takesthreeinputs,theRGBimage,theLiDAR\nthe area around the car (Fig. 4). FrontView[27]andtheBEVimage.ItdiffersfromBirdNet\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\nof shape prior for ground plane estimation significantly im-\nproves the performance on 3D object localization. Recently\nit was also shown that a class specific end-to-end learning\nframework,evenonsyntheticdataset,couldprovideaccurate\nposeandshapeinformation.Executingsuchnetworksattest\ntime is fast, as it usually involves a single forward pass. Therefore, we choose the former paradigm with Perspective\nTransformer Network and reconstruct the 3D object/scene. The ground plane is then estimated by fitting a plane using\nRANSAC [31]. IV. EXPERIMENTS\nFig.4.",
      "size": 831,
      "sentences": 9
    },
    {
      "id": 24,
      "content": "we choose the former paradigm with Perspective\nTransformer Network and reconstruct the 3D object/scene. The ground plane is then estimated by fitting a plane using\nRANSAC [31]. IV. EXPERIMENTS\nFig.4. TheRGBimageonlyshowsthefrontviewwhilethetopmounted\nLiDARpointcloudalsohavedatafromthebackandsidesofthecar.We Dataset: We evaluate our method on the standard challeng-\ncrop the LiDAR point cloud appropriately so that only the corresponding ing KITTI Object Detection Benchmark [32]. The dataset\ninformationinthetwomodalitiesremain.WealsopruneoutfarawayBEV\nconsists of 7,481 training images and 7,518 images for\npoints,astheyarehighlyoccludedintheRGBimage,potentiallyloosing\nsomeobjectse.g.thosehighlightedwithredarrows. testing, however, the test set is not publicly available. We\nsplit the train set into training and validation used in [33],\nand report results with these splits, as done by many recent\nin the format of BEV input it accepts, while BirdNet takes works.",
      "size": 969,
      "sentences": 8
    },
    {
      "id": 25,
      "content": "lable. We\nsplit the train set into training and validation used in [33],\nand report results with these splits, as done by many recent\nin the format of BEV input it accepts, while BirdNet takes works. While we focus on techniques which use Bird’s Eye\na 3 channel BEV image, i.e.height, intensity and density, View (BEV) images for detecting objects, we also compare\nMV3D pre-processes the height channel to encode more against published state-of-the-art results to demonstrate the\ndetailed height information. It divides the point cloud into competitiveness of the proposed method. M slices and computes a height map for each slice giving Training data for BirdGAN: We use RGB image as input\na BEV image of M +2 channels. Hence, to generate the for training BEV GAN for BirdNet with the 3 channels,\nappropriate input for MV3D, we use multiple independently height, intensity and density, BEV image as required by\ntrained BirdGANs to generate the M height channels of the BirdNet.",
      "size": 978,
      "sentences": 6
    },
    {
      "id": 26,
      "content": "th the 3 channels,\nappropriate input for MV3D, we use multiple independently height, intensity and density, BEV image as required by\ntrained BirdGANs to generate the M height channels of the BirdNet. In case of MV3D, the BEV is different than\nBEV image. We also experimented with directly generating Birdnet; the height channel consists of M channels which\nthe M + 2 channel BEV image, but the results indicated are produced by slicing the point cloud in M slices along\nthatindependentlytrainedGANsprovidedbetterresults.We the height dimensions. We experiment with a single GAN\nusetheRGBimagetoobtainthecorrespondingdepthimage to generate M+2 channels as output and as well as multiple\nusing a neural network [7], and use the depth map image to independent GANs for each of the slices. obtain the 3D point cloud, for constructing the LiDAR front TheresultsarereportedbytrainingBirdGANontwotypes\nview. Following [4], [27] we project the information on to a of training data for both the above cases.",
      "size": 998,
      "sentences": 6
    },
    {
      "id": 27,
      "content": "point cloud, for constructing the LiDAR front TheresultsarereportedbytrainingBirdGANontwotypes\nview. Following [4], [27] we project the information on to a of training data for both the above cases. cylindricalplaneobtainingthefrontview.Wefinallyfeedall\n(w/oclipping)—WeusethedatainthefieldofviewofRGB\nthese three inputs, i.e.the BEV image, the front view image\nimages i.e.90◦ in the front view. This setting is referred to\nand the RGB image, to the MV3D network to obtain 3D\nas without clipping. object detections (Fig. 3 illustrates the full pipeline). (clipping) — In KITTI dataset, the objects that are far, are\nD. Ground plane estimation difficult to detect mostly due to occlusion [11].",
      "size": 692,
      "sentences": 7
    },
    {
      "id": 28,
      "content": "ng. object detections (Fig. 3 illustrates the full pipeline). (clipping) — In KITTI dataset, the objects that are far, are\nD. Ground plane estimation difficult to detect mostly due to occlusion [11]. Using such\nIn the proposed pipelines, the ground plane estimation is data could hinder training the GAN, as then the GAN is\nneededinbothcases.BirdNetusesthegroundplane,i.e.the expected to generate objects which are not clearly visible in\nbottom-most points, to estimate the height of the object for the RGB image. We experiment with using only the nearby\nconstructing the 3D bounding boxes. MV3D obtains the 3D objectsfortrainingtheGANs,i.e.weremovetheBEVimage\nlocalizations by projecting the 3D bounding boxes to the data corresponding to points which are more than 25 meters\nground plane. The ground plane estimation is an important away and use these modified BEV images to train the GAN\nstep here, especially for MV3D, as it governs the size of the based translation model.",
      "size": 977,
      "sentences": 8
    },
    {
      "id": 29,
      "content": "round plane. The ground plane estimation is an important away and use these modified BEV images to train the GAN\nstep here, especially for MV3D, as it governs the size of the based translation model. This data setting is referred to as\nprojected objects on the BEV impacting the quality of 3D (with) clipping. object localization. ParameterSettings:FortrainingMV3D,BirdNet,PTNand\nThere are two ways to obtain the ground plane, (i) imagetodepthnetworks,weusethethedefaultexperimental\nby reconstructing a 3D model from a single RGB image settings from the respective publications. using techniques such as Perspective Transformer Network Proposed methods: In the experimental results, following\n(PTN)[6], Point Set generation [5], depth estimation [7] notations are used for various proposed methods.",
      "size": 798,
      "sentences": 6
    },
    {
      "id": 30,
      "content": "h as Perspective Transformer Network Proposed methods: In the experimental results, following\n(PTN)[6], Point Set generation [5], depth estimation [7] notations are used for various proposed methods. etc.andestimatethegroundplane,or(ii)usingtheimageto Ours (w/o clipping)-BirdNet — Generated BEVs are used\ndirectly estimate the ground plane without transforming the for training and testing the BirdNet. The GAN is trained on\nimage to 3D [11], [28], [29]. The quality of the latter case data without clipping. For constructing the training pairs for\nusually requires explicit presence of strong 2D object pro- BirdGAN with BirdNet, the input consists of RGB images\nposals or texture/color pattern.",
      "size": 697,
      "sentences": 5
    },
    {
      "id": 31,
      "content": "t clipping. For constructing the training pairs for\nusually requires explicit presence of strong 2D object pro- BirdGAN with BirdNet, the input consists of RGB images\nposals or texture/color pattern. [30] also noted that presence while the output consists of BEV images generated using\n=== 페이지 5 ===\nIoU=0.5 IoU=0.7\nMethod Data Easy Moderate Hard Easy Moderate Hard = 0.7 is a difficult task in itself [4], the hard examples are\nMono3D[1] Mono 30.5 22.39 19.16 5.22 5.19 4.13 heavily occluded and are relatively far in many cases. Since\n3DOP[33] Stereo 55.04 41.25 34.55 12.63 9.49 7.59\nXuet.al[2] Mono 55.02 36.73 31.27 22.03 13.63 11.60 BirdGANistrainedspecificallyoncloseobjects,thefaraway\nBirdNet[8] LIDAR 90.43 71.45 71.34 72.32 54.09 54.50\nDoBEM[34] LIDAR 88.07 88.52 88.19 73.09 75.16 75.24 andheavilyoccludedobjectsarebadlygeneratedintheBEV\nVeloFCN[27] LIDAR 79.68 63.82 62.80 40.14 32.08 30.47 image.",
      "size": 909,
      "sentences": 4
    },
    {
      "id": 32,
      "content": "90.43 71.45 71.34 72.32 54.09 54.50\nDoBEM[34] LIDAR 88.07 88.52 88.19 73.09 75.16 75.24 andheavilyoccludedobjectsarebadlygeneratedintheBEV\nVeloFCN[27] LIDAR 79.68 63.82 62.80 40.14 32.08 30.47 image. MV3D(BEV+FV)[4] LIDAR 95.74 88.57 88.13 86.18 77.32 76.33\nMV3D(...+RGB)[4] LI+Mo 96.34 89.39 88.67 86.55 78.10 76.67 Amongthetwobasenetworks,BirdNethasnearlyhalfthe\nFrustumPointNets[35] LIDAR - - - 88.16 84.02 76.44\nOurs AP loc of MV3D (36.1 vs.60.13). The drop in performances,\n(w/oclipping)-BirdNet Mono 58.40 49.54 48.20 45.60 31.10 29.54\ncomparedtotheirrespectivebaselinemethodstrainedonreal\n(w/oclipping)-MV3D Mono 71.35 47.43 43.25 58.70 38.20 36.56\n(clipping)-BirdNet Mono 84.40 64.18 58.70 68.2 42.1 36.1 data, is more in case of BirdNet (54.5 to 36.1) than MV3D\n(clipping)-MV3D Mono 90.24 79.50 80.16 81.32 68.40 60.13\n(76.67 to 60.13) showing that MV3D is better than BirdNet\nTABLEI in the case of both real and generated data.",
      "size": 937,
      "sentences": 3
    },
    {
      "id": 33,
      "content": "BirdNet (54.5 to 36.1) than MV3D\n(clipping)-MV3D Mono 90.24 79.50 80.16 81.32 68.40 60.13\n(76.67 to 60.13) showing that MV3D is better than BirdNet\nTABLEI in the case of both real and generated data. 3Dlocalizationperformance:AVERAGEPRECISION(AP\nLOC\n,%)OF The proposed methods with clipped data perform ∼ 10-\nBIRD’SEYEVIEWBOXESONKITTIvalidationSET.FORMONO3DAND 25% better than corresponding networks trained with data\n3DOP,WEUSETHERESULTSWITH3DBOXREGRESSIONFROM[4] without clipping. This shows that reducing noisy examples\nduring training, allows GAN to learn a better mapping\nfrom 2D image domain to BEV image. While this leads to\ndiscardingdataattrainingtime,overallthelessnoisytraining\nthe technique of BirdNet. Once the BirdGAN is trained, the\nimproves performance at test time by learning better quality\nBirdNet is trained using BirdGAN generated BEVs from\nBEV generators. input RGB images.",
      "size": 895,
      "sentences": 6
    },
    {
      "id": 34,
      "content": "hnique of BirdNet. Once the BirdGAN is trained, the\nimproves performance at test time by learning better quality\nBirdNet is trained using BirdGAN generated BEVs from\nBEV generators. input RGB images. At test time, the input image is again\n3D Object Detection: The results for 3D Object Detection\nmappedtoaBEVimagebyBirdGANandpassestoBirdNet\ni.e.detection of 3D bounding boxes are shown in Table\nfor further processing. II. The performance is evaluated by computing IoU over-\nOurs(w/oclipping)-MV3D—ThisreferstousingBirdGAN\nlap with ground-truth 3D bounding boxes using Average\ntrained on data without clipping. Here the output of\nPrecision (AP ) metric. With IoU = 0.25, the Mono3D\n3D\nBirdGAN is of M+2 where the ground truth BEV is gener-\nand 3DOP outperform the BirdNet and MV3D trained on\nated using the technique of [4]. unclipped dataset.",
      "size": 843,
      "sentences": 9
    },
    {
      "id": 35,
      "content": ") metric. With IoU = 0.25, the Mono3D\n3D\nBirdGAN is of M+2 where the ground truth BEV is gener-\nand 3DOP outperform the BirdNet and MV3D trained on\nated using the technique of [4]. unclipped dataset. However, with clipped dataset, MV3D\nOurs (clipping)-BirdNet — This setting refers to using performs better than both Mono3D and 3DOP which are\nclipped training for training BirdGAN and its subsequent based on monocular images. With IoU 0.5 and 0.7, the\nuse to train and test BirdNet described above. proposed pipeline with or without clipped data still outper-\nOurs (clipping)-MV3D — Similar to the previous case with formsMono3Dand3DOP.Infact,MV3Dongenerateddata\nthe training and testing pairs consist of clipped data. performs∼30%(w/oclipping)to∼40%(clipped)betteron\nIoU 0.25 and 0.5 than Mono3D and 3DOP.",
      "size": 807,
      "sentences": 7
    },
    {
      "id": 36,
      "content": "case with formsMono3Dand3DOP.Infact,MV3Dongenerateddata\nthe training and testing pairs consist of clipped data. performs∼30%(w/oclipping)to∼40%(clipped)betteron\nIoU 0.25 and 0.5 than Mono3D and 3DOP. Except MV3D\nA. Quantitative results\nand Frustum PointNets, the proposed MV3D with clipped\nBEV Object Detection: Table I shows the results for 3D data still outperforms existing methods which explicitly use\nobject localization of Bird’s Eye View boxes on KITTI real 3D data like VeloFCN [27]. Benchmark. The 3D localization refers to the oriented ob- Similar to the case of 3D localization, we observe that\nject bounding boxes in the KITTI bird’s eye view images. the networks trained with clipped data achieve a significant\nThe evaluation is performed for both IoU = 0.5 and IoU increase in the AP as compared to the networks trained\n3D\n= 0.7 where the performance is evaluated on Average on unclipped data. The performance increase is ∼ 30% on\nPrecision (AP ) for bird’s eye view boxes.",
      "size": 987,
      "sentences": 7
    },
    {
      "id": 37,
      "content": "in the AP as compared to the networks trained\n3D\n= 0.7 where the performance is evaluated on Average on unclipped data. The performance increase is ∼ 30% on\nPrecision (AP ) for bird’s eye view boxes. We observe IoU 0.25 for both BirdNet and MV3D. The results indicate\nloc\nthat the proposed methods significantly outperform state- that with clipped data, the detected bounding boxes are\nof-the-art results based on monocular (Mono3D) and stereo close to the actual object. However, with IoU of 0.7, the\n(3DOP) images. Specifically, the proposed method outper- increase in performance for networks trained on unclipped\nformsMono3Dby27.9%oneasyimagesandby19.54%on data vs.clipped data is reduced to ∼ 4% for BirdNet\nhard examples with BirdNet, where the BEVs are obtained and ∼ 8% for MV3D. This indicates that BEV generated\nusing BirdGAN trained on unclipped data.",
      "size": 862,
      "sentences": 7
    },
    {
      "id": 38,
      "content": "ta vs.clipped data is reduced to ∼ 4% for BirdNet\nhard examples with BirdNet, where the BEVs are obtained and ∼ 8% for MV3D. This indicates that BEV generated\nusing BirdGAN trained on unclipped data. with clipped data allows learning of models that have a\nWith the more restricted evaluation setting of IoU = 0.7, larger number of bounding boxes closer to the ground-truth\nbothMono3Dand3DOPsufferasignificantdropinAP i.e. annotations,whichalsohavealargeroverlapwiththem.The\nloc\n∼ 15–25%, while the drop for pipelines based on proposed increase for MV3D is interesting as it uses LIDAR Front\nnetwork is more graceful at ∼ 10–15% on an average. View which is also generated via depth map obtained only\nSimilartrendisobtainedforothercomparedmethodsaswell. from the RGB image.",
      "size": 772,
      "sentences": 6
    },
    {
      "id": 39,
      "content": "uses LIDAR Front\nnetwork is more graceful at ∼ 10–15% on an average. View which is also generated via depth map obtained only\nSimilartrendisobtainedforothercomparedmethodsaswell. from the RGB image. Further, when the networks are trained with clipped We also observe an increase of ∼ 15% in case of hard\ndataset, the improvement in AP , in most cases, is 2– examples with an IoU of 0.7 for MV3D with clipped\nloc\n3 times that of Mono3D and 3DOP. In fact, with clipped cf.unclipped data.",
      "size": 485,
      "sentences": 5
    },
    {
      "id": 40,
      "content": "% in case of hard\ndataset, the improvement in AP , in most cases, is 2– examples with an IoU of 0.7 for MV3D with clipped\nloc\n3 times that of Mono3D and 3DOP. In fact, with clipped cf.unclipped data. This again supports our hypothesis that\ndataset, the performance of both BirdNet and MV3D are clipping the data and training on closer objects, that have\nwithin ∼ 5–10%, except on hard examples with IoU = 0.7. better visibility in the RGB image and lesser occlusions,\nThe reason could be that while detecting examples with IoU helps the BirdGAN to learn a better mapping from RGB\n=== 페이지 6 ===\nIoU=0.25 IoU=0.5 IoU=0.7\nMethod Data\nEasy Moderate Hard Easy Mod Hard Easy Mod Hard\nMono3D[1] Mono 62.94 48.2 42.68 25.19 18.2 15.52 2.53 2.31 2.31\n3DOP[33] Stereo 85.49 68.82 64.09 46.04 34.63 30.09 6.55 5.07 4.1\nBirdNet[8] LIDAR 93.45 71.43 73.58 88.92 67.56 68.59 17.24 15.63 14.20\nVeloFCN[27] LIDAR 89.04 81.06 75.93 67.92 57.57 52.56 15.20 13.66 15.98\nMV3D(BEV+FV)[4] LIDAR 96.03 88.85 88.39 95.19 87.65 80.11 71.19 56.60 55.30\nMV3D(BEV+FV+RGB)[4] LIDAR+Mono 96.52 89.56 88.94 96.02 89.05 88.38 71.29 62.68 56.56\nFrustumPointNets[35] LIDAR - - - - - - 83.76 70.92 63.65\nOurs(w/oclipping)-BirdNet Mono 55.70 38.42 37.20 51.27 37.41 30.28 8.43 4.26 3.12\nOurs(w/oclipping)-MV3D Mono 61.4 46.18 44.20 59.23 45.46 41.72 46.42 38.70 25.35\nOurs(clipping)-BirdNet Mono 89.4 68.3 64.3 80.34 51.20 46.7 12.24 10.70 8.64\nOurs(clipping)-MV3D Mono 91.20 81.42 75.57 84.18 78.64 74.50 58.26 42.48 40.72\nTABLEII\n3Ddetectionperformance:AVERAGEPRECISION(AP3D)(IN%)OF3DBOXESONKITTIvalidationSET.FORMONO3DAND3DOP,WEUSETHE\nRESULTSWITH3DBOXREGRESSIONASREPORTEDIN[4]\n3DOP [33] VeloFCN [27] MV3D[4] Ours(clipped)-MV3D Ours(clipped)-BirdNet\nFig.5.",
      "size": 1722,
      "sentences": 3
    },
    {
      "id": 41,
      "content": "AGEPRECISION(AP3D)(IN%)OF3DBOXESONKITTIvalidationSET.FORMONO3DAND3DOP,WEUSETHE\nRESULTSWITH3DBOXREGRESSIONASREPORTEDIN[4]\n3DOP [33] VeloFCN [27] MV3D[4] Ours(clipped)-MV3D Ours(clipped)-BirdNet\nFig.5. QualitativeResultson3DDetectionforvarioustechniquesagainsttheproposedpipelines.TheresultsforOurs(clipped)-BirdNetandOurs(clipped)-\nMV3Dshowthecomputedresultsingreen.TheBird’sEyeViewimagesshownintheresultsfortheproposedpipelinesaregeneratedusingBirdGAN.To\nhighlightthequalityofdetectionsusingtheproposedtechniques,thedetectionsusingdefaultMV3DarealsomarkedintheRGB(pink)andBEV(blue)\nimagesforOurs(clipped)-MV3DandOurs(clipped)-BirdNet. 3DObjectDetection 3DObjectLocalization Method BBOpt. Data Easy Mod. Hard\nMethod Easy Moderate Hard Easy Moderate Hard Chabotetal. [36] 2D Mono 96.40 90.10 80.79\nBirdNet [8] 14.75 13.44 12.04 50.81 75.52 50.00 Xuetal.",
      "size": 851,
      "sentences": 6
    },
    {
      "id": 42,
      "content": "3DObjectLocalization Method BBOpt. Data Easy Mod. Hard\nMethod Easy Moderate Hard Easy Moderate Hard Chabotetal. [36] 2D Mono 96.40 90.10 80.79\nBirdNet [8] 14.75 13.44 12.04 50.81 75.52 50.00 Xuetal. [2] 2D Mono 90.43 87.33 76.78\nMV3D [4] 71.09 62.35 55.12 86.02 76.90 68.49 AVOD[13] 3D LiDAR 88.08 89.73 80.14\nAVOD [13] 81.94 71.88 66.38 88.53 83.79 77.90 MV3D[4] 3D LiDAR 90.37 88.90 79.81\nOurs(clipping)-BirdNet 10.01 9.42 7.20 46.01 65.31 41.45 Ours(clipping)-MV3D 3D Mono 85.41 81.72 65.56\nOurs(clipping)-MV3D 66.30 59.31 42.80 82.90 73.45 61.16 Ours(clipping)-AVOD 3D Mono 82.93 83.51 68.20\nOurs(clipping)-AVOD 77.24 61.52 52.30 84.40 78.41 72.20\nTABLEIV\nTABLEIII\nRESULTSONKITTItestSETFOR3DOBJ.DETECTIONAND\nPERFORMANCEONKITTItestSETFOR2DOBJECTDETECTION.BB\nLOCALIZATION. OPT.INDICATESTHEOPTIMIZATIONOFBOUNDINGBOXESIN2DOR3D. of six cases, i.e. on localization, moderate and hard, and\nimage to the Bird’s Eye View image. detection easy.",
      "size": 938,
      "sentences": 9
    },
    {
      "id": 43,
      "content": "R2DOBJECTDETECTION.BB\nLOCALIZATION. OPT.INDICATESTHEOPTIMIZATIONOFBOUNDINGBOXESIN2DOR3D. of six cases, i.e. on localization, moderate and hard, and\nimage to the Bird’s Eye View image. detection easy. Thus the proposed method is general and\nGeneralization Ability: In Table III, the results on KITTI can work with different state of the art 3D networks giving\ntest set are shown. We can observe that the performance competitive performance. with proposed method is again close to the performance 2DObjectDetection:InTableIV,theresultsonKITTItest\nof the base network. Additionally, to demonstrate that the setfor2Ddetectionsareshown.Itcanbeobservedthateven\nproposed method can be used as a drop-in replacement, with entirely generated data the method also performs close\nwe show the results with another base architecture, AVOD to the base networks for 2D object detection. [13] which uses the same BEV as MV3D.",
      "size": 909,
      "sentences": 10
    },
    {
      "id": 44,
      "content": "ith entirely generated data the method also performs close\nwe show the results with another base architecture, AVOD to the base networks for 2D object detection. [13] which uses the same BEV as MV3D. It also generates\nB. Qualitative Results\n3D anchor grids, for which we provide the point clouds\ngenerated using the method discussed in Section III-C. We Figure 5 shows some qualitative results for object detec-\nobserve that the performance with the proposed method is tion in 3D, on actual BEV images for compared methods\nvery close to AVOD’s performance with real 3D data, and (first three columns) and on generated BEV images for the\nthat it outperforms MV3D, with real 3D data, on three out proposed methods. For comparison we chose 3DOP which\n=== 페이지 7 ===\nisbasedonstereoimages,VeloFCNisbasedonLIDARand dataset and train BirdNet and MV3D with it. However, we\nMV3D, which is based on multi modal fusion of LIDAR, observethattheperformancedropsabout12%onanaverage\nBEV and RGB image.",
      "size": 986,
      "sentences": 5
    },
    {
      "id": 45,
      "content": "oFCNisbasedonLIDARand dataset and train BirdNet and MV3D with it. However, we\nMV3D, which is based on multi modal fusion of LIDAR, observethattheperformancedropsabout12%onanaverage\nBEV and RGB image. In the first row, we can observe that in this setting on the easy, moderate and hard examples. We\nOurs(clipped)-MV3D detects four cars in the scene while hypothesize that this could be because the network was not\nmissing the occluded car. However, Ours(clipped)-BirdNet able to optimize over the combined dataset which includes\ndetects three cars closer to the camera while misses out two BEVs for the same image, one real and one generated. on the car at the back, which is visible but is far away ThetwoBEVswouldhavedifferentstatisticsandmightthus\nfrom the camera. We can also observe that the BEV image confuse the detector when used together for training.",
      "size": 859,
      "sentences": 7
    },
    {
      "id": 46,
      "content": "back, which is visible but is far away ThetwoBEVswouldhavedifferentstatisticsandmightthus\nfrom the camera. We can also observe that the BEV image confuse the detector when used together for training. and the marked detections in Ours(clipped)-MV3D, which However, since the networks independently perform well,\nuses generated BEVs, are very close to the full MV3D with we perform their deep fusion, similar to that in MV3D. actual BEV images. However, for Ours(clipped)-BirdNet, We combine their outputs with a join operation e.g.con-\nthe detections in BEV do not overlap completely with the catenation or mean, and feed that to another layer before\nobjectexplainingthedropinperformancewhenhigherIoUs prediction, i.e.we add another layer whose input is the join\nare considered with BirdNet for 3D Object localization and of the outputs from the two networks given by\ndetection (Table I and II).",
      "size": 894,
      "sentences": 5
    },
    {
      "id": 47,
      "content": "rIoUs prediction, i.e.we add another layer whose input is the join\nare considered with BirdNet for 3D Object localization and of the outputs from the two networks given by\ndetection (Table I and II). f =f ⊕f (1)\nThe second row shows a simpler case, where only BirdNetfus BirdNet (BirdGAN+BirdNet)\nthree cars are present in the image. It can be seen that f MV3Dfus =f MV3D ⊕f (BirdGAN+MV3D) (2)\nOurs(clipping)-MV3D is able to detect all three of the\nwhere f and f are networks pretrained\ncars with high overlap with default MV3D. However, with BirdNet MV3D\non ground-truth data while f and\nOurs(clipping)-BirdNet,onlytwocarsaredetected.Thethird (BirdGAN+BirdNet)\nf are BirdNet and MV3D networks pre-\ncar (right most) is not detected potentially because BirdNet (BirdGAN+MV3D)\ntrained on generated BEV. f and f is\nis highly sensitive to occlusion and the presence of the pole BirdNetfus MV3Dfus\nthe combination of the respective outputs using the join\nin front of that car is confusing it.",
      "size": 987,
      "sentences": 5
    },
    {
      "id": 48,
      "content": "nerated BEV. f and f is\nis highly sensitive to occlusion and the presence of the pole BirdNetfus MV3Dfus\nthe combination of the respective outputs using the join\nin front of that car is confusing it. operation ⊕, which is a mean operation in our case. The\nWhencomparedtothegeneratedBEVforMV3D(M+2\nresults are shown in Table VI. We observe that deep fusion\nchannel)and theground-truth (row2, firstthreefigures), the\nimproves the performance of the base network for both\ngenerated BEV for BirdNet (only 3 channels) has missing\n3D localization (Table I) and 3D detection (Table II) by\nregions. These regions have better reconstruction in the first\n2-5% for easy and moderate examples.",
      "size": 681,
      "sentences": 6
    },
    {
      "id": 49,
      "content": "for BirdNet (only 3 channels) has missing\n3D localization (Table I) and 3D detection (Table II) by\nregions. These regions have better reconstruction in the first\n2-5% for easy and moderate examples. Interestingly, the\ncase,potentiallybecausetheBEVconsistsofmultipleheight\naddition of generated data for 3D object localization on\nchannels, which might be allowing it to distinguish the\neasy examples, provides an AP of 89.24, which is higher\nregion where car ends while the pole continues (from the loc\nthan that of a state-of-the-art method, Frustum PointNets\nperspective of converting RGB to BEV image). This case\n(88.16).However,incaseofhardexamples,theperformance\nalso shows the effectiveness of the BirdGAN in generating\ndrops.",
      "size": 731,
      "sentences": 4
    },
    {
      "id": 50,
      "content": "t method, Frustum PointNets\nperspective of converting RGB to BEV image). This case\n(88.16).However,incaseofhardexamples,theperformance\nalso shows the effectiveness of the BirdGAN in generating\ndrops. This could be due to the fact that the hard examples\nclose to actual Bird’s Eye View images where it respected\ncontain heavily occluded objects and hence the introduction\nthe occlusions due to cars and large objects, which appear\nof generated BEVs reduces the performance cf.the network\nas empty blank region in the BEV images, e.g. behind the\ntrainedonground-truthannotations.Whileinthis,oneofthe\ncars in the generated as well as actual BEV image. networks uses LIDAR data, the results are interesting, as we\nC. Ablation Studies can generate data from the available training data, and use\nthat to improve the performance without needing additional\nWe first study the impact of various channels within BEV\ntraining samples.",
      "size": 923,
      "sentences": 5
    },
    {
      "id": 51,
      "content": "udies can generate data from the available training data, and use\nthat to improve the performance without needing additional\nWe first study the impact of various channels within BEV\ntraining samples. We believe this is one of the first time\non 3D object detection and localization. Table V shows\nperformance improvment has been reported by augmenting\nthe results on using each of the channels of BEV image\nwithgenerateddataonarealworldcomputervisionproblem. separately. We observe that by removing the density and\nheight channels, there is a high drop in performance for\nV. CONCLUSION\nboth BirdNet and MV3D. Incase of BirdNet, using only the\nWe demonstrated that using GANs to generate 3D data\ndensityortheheightchannelprovidesperformancecloserto\nfrom 2D images can lead to performances close to the\nthecasewhenallthreechannelshavebeenused.Incontrast, state-of-the-art 3D object detectors which use actual 3D\nforMV3D,thedropinrelativeperformanceiscomparatively data at test time.",
      "size": 979,
      "sentences": 6
    },
    {
      "id": 52,
      "content": "mances close to the\nthecasewhenallthreechannelshavebeenused.Incontrast, state-of-the-art 3D object detectors which use actual 3D\nforMV3D,thedropinrelativeperformanceiscomparatively data at test time. The proposed method outperformed\nhigher (∼ 5-12%) when using density or height channel the state-of-the-art monocular image based 3D object\ndetectionmethodsbyasignificantmargin.Weproposedtwo\nalone as compared to BirdNet where the relative drop is\ngeneration mechanisms to work with two different recent\n∼ 2-5%. This indicates that both the channels encode\n3D object detection architectures, and proposed training\ndistinctive and/or complementary information which results strategies which lead to high detection performances. We\nin significant boost when all the channels are combined. also gave ablation studies and compared the results from\nNext,weexperimentwithusinggenerateddataalongwith the architectures using real 3D data vs.generated 3D data,\nat train and test time.",
      "size": 974,
      "sentences": 5
    },
    {
      "id": 53,
      "content": "combined. also gave ablation studies and compared the results from\nNext,weexperimentwithusinggenerateddataalongwith the architectures using real 3D data vs.generated 3D data,\nat train and test time. We also showed that late fusion\nthe real data to analyze if the generated data can improve\nof networks trained with real and generated 3D data,\nthe detection and localization performance by augmenting\nrespectively, improves performance over both individually. the real data.",
      "size": 473,
      "sentences": 4
    },
    {
      "id": 54,
      "content": "ata can improve\nof networks trained with real and generated 3D data,\nthe detection and localization performance by augmenting\nrespectively, improves performance over both individually. the real data. We first try merging the ground-truth training We believe it is one of the first time that results have been\nimages and the generated BEV image to make a common reported where training data augmentation by generating\n=== 페이지 8 ===\nI D H BirdNet[8] Ours(clipped)-BirdNet MV3D[4] Ours(clipped)-MV3D\nEasy Moderate Hard Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard\n(cid:88) (cid:88) (cid:88) 72.32 54.09 54.50 68.2 42.1 36.1 86.18 77.32 76.33 81.32 68.40 60.13\n(cid:88) 55.04 41.16 38.56 51.32 28.21 18.65 68.20 65.66 62.14 62.10 45.36 42.64\n(cid:88) 70.94 53.00 53.30 65.50 38.42 33.20 75.30 69.45 68.32 72.20 58.49 48.24\n(cid:88) 69.80 52.90 53.69 64.70 36.10 32.44 75.11 73.10 67.50 72.45 59.22 48.68\nTABLEV\nPERFORMANCEONBIRDEYE’SVIEWDETECTION(AP\nLOC\n)ONTHEKITTIvalidationSETFORCARUSINGDIFFERENTCHANNELOFBIRDEYE’S\nVIEWIMAGEASANINPUT.IREPRESENTSINTENSITY,DREPRESENTSDENSITYANDHREPRESENTSHEIGHTCHANNELSOFABIRD’SEYEVIEW.",
      "size": 1128,
      "sentences": 3
    },
    {
      "id": 55,
      "content": "EONBIRDEYE’SVIEWDETECTION(AP\nLOC\n)ONTHEKITTIvalidationSETFORCARUSINGDIFFERENTCHANNELOFBIRDEYE’S\nVIEWIMAGEASANINPUT.IREPRESENTSINTENSITY,DREPRESENTSDENSITYANDHREPRESENTSHEIGHTCHANNELSOFABIRD’SEYEVIEW. 3DObjectLocalization(APloc) 3DObjectDetection(AP3D) [14] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view\nMethod Easy Moderate Hard Easy Moderate Hard convolutional neural networks for 3d shape recognition,” in ICCV,\n2015. BirdNetfus 75.64 57.09 55.48 21.20 17.21 12.10\n[15] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum, A. Torralba,\nMV3Dfus 89.24 79.12 73.10 75.31 64.70 52.80\nandW.T.Freeman,“Singleimage3dinterpreternetwork,”inECCV,\nTABLEVI 2016. [16] A. Saxena, M. Sun, and A. Y. Ng, “Make3d: Learning 3d scene\nPERFORMANCEOFDEEPFUSIONOFOUTPUTSFROMNETWORKS\nstructurefromasinglestillimage,”TPAMI,2009. TRAINEDONREALANDGENERATEDBEVIMAGESFOR3DOBJECT [17] G.Pavlakos,X.Zhou,K.G.Derpanis,andK.Daniilidis,“Coarse-to-\nLOCALIZATIONANDDETECTIONONKITTI.",
      "size": 969,
      "sentences": 6
    },
    {
      "id": 56,
      "content": "FROMNETWORKS\nstructurefromasinglestillimage,”TPAMI,2009. TRAINEDONREALANDGENERATEDBEVIMAGESFOR3DOBJECT [17] G.Pavlakos,X.Zhou,K.G.Derpanis,andK.Daniilidis,“Coarse-to-\nLOCALIZATIONANDDETECTIONONKITTI. finevolumetricpredictionforsingle-image3dhumanpose,”inCVPR,\n2017. [18] J.Wu,C.Zhang,T.Xue,W.T.Freeman,andJ.B.Tenenbaum,“Learn-\ning a probabilistic latent space of object shapes via 3d generative-\nimages leads to improved performance for the challenging adversarialmodeling,”inNeurIPS,2016. task of 3D object detection. The setting used in our [19] M.Gadelha,S.Maji,andR.Wang,“3dshapeinductionfrom2dviews\nexperiments is very practical as urban environments remain ofmultipleobjects,”in3DV,2017. [20] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C. Berg,\nrelatively similar in local areas. Hence the RGB image\n“Transformation-grounded image generation network for novel 3d\nto 3D BEV image generator once trained, say in parts\nviewsynthesis,”inCVPR,2017.",
      "size": 952,
      "sentences": 8
    },
    {
      "id": 57,
      "content": "relatively similar in local areas. Hence the RGB image\n“Transformation-grounded image generation network for novel 3d\nto 3D BEV image generator once trained, say in parts\nviewsynthesis,”inCVPR,2017. of a certain city, can be used with good generalization in\n[21] J.Zhu,J.Xie,andY.Fang,“Learningadversarial3dmodelgeneration\ndifferent areas of the same and or nearby cities. An avenue with2dimageenhancer.”inAAAI,2018. for future work is to move towards more challenging setting [22] B. Yang, H. Wen, S. Wang, R. Clark, A. Markham, and N. Trigoni,\nwith unknown camera parameters (e.g.the lines of [37]). “3d object reconstruction from a single depth view with adversarial\nlearning,”inICCV,2017. Acknowledgements. This work was partly funded by [23] E.SmithandD.Meger,“Improvedadversarialsystemsfor3dobject\nthe MOBI-DEEP ANR-17-CE33-0011 program. generationandreconstruction,”arXiv:1707.09557,2017. [24] P. Isola, J.-Y. Zhu, T. Zhou, and A.",
      "size": 937,
      "sentences": 11
    },
    {
      "id": 58,
      "content": "by [23] E.SmithandD.Meger,“Improvedadversarialsystemsfor3dobject\nthe MOBI-DEEP ANR-17-CE33-0011 program. generationandreconstruction,”arXiv:1707.09557,2017. [24] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image\ntranslationwithconditionaladversarialnetworks,”inCVPR,2017. REFERENCES\n[25] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang,\nandE.Shechtman,“Towardmultimodalimage-to-imagetranslation,”\n[1] X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun,\ninNeurIPS,2017. “Monocular 3d object detection for autonomous driving,” in CVPR,\n[26] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation\n2016.\nlearning with deep convolutional generative adversarial networks,”\n[2] B.XuandZ.Chen,“Multi-levelfusionbased3dobjectdetectionfrom\narXivpreprintarXiv:1511.06434,2015.\nmonocularimages,”inCVPR,2018.",
      "size": 848,
      "sentences": 9
    },
    {
      "id": 59,
      "content": "16.\nlearning with deep convolutional generative adversarial networks,”\n[2] B.XuandZ.Chen,“Multi-levelfusionbased3dobjectdetectionfrom\narXivpreprintarXiv:1511.06434,2015.\nmonocularimages,”inCVPR,2018. [27] B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3d lidar using\n[3] C.Li,M.Z.Zia,Q.-H.Tran,X.Yu,G.D.Hager,andM.Chandraker,\nfullyconvolutionalnetwork,”2016. “Deepsupervisionwithintermediateconcepts,”TPAMI,2018. [28] A. Cherian, V. Morellas, and N. Papanikolopoulos, “Accurate 3d\n[4] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object\ngroundplaneestimationfromasingleimage,”inICRA,2009. detectionnetworkforautonomousdriving,”inCVPR,2017. [29] B.Wang,V.Fre´mont,andS.A.R.Florez,“Color-basedroaddetection\n[5] H.Fan,H.Su,andL.J.Guibas,“Apointsetgenerationnetworkfor\nanditsevaluationonthekittiroadbenchmark,”inIVS,2014. 3dobjectreconstructionfromasingleimage.”inCVPR,2017.",
      "size": 889,
      "sentences": 7
    },
    {
      "id": 60,
      "content": ".Florez,“Color-basedroaddetection\n[5] H.Fan,H.Su,andL.J.Guibas,“Apointsetgenerationnetworkfor\nanditsevaluationonthekittiroadbenchmark,”inIVS,2014. 3dobjectreconstructionfromasingleimage.”inCVPR,2017. [30] M.etal.,“Shapepriorsforreal-timemonocularobjectlocalizationin\n[6] X. Yan, J. Yang, E. Yumer, Y. Guo, and H. Lee, “Perspective trans-\ndynamicenvironments,”inIROS,2017. formernets:Learningsingle-view3dobjectreconstructionwithout3d\n[31] S.Choi,J.Park,J.Byun,andW.Yu,“Robustgroundplanedetection\nsupervision,”inNeurIPS,2016. from3dpointclouds,”inICCAS2014,2014. [7] C.Godard,O.MacAodha,andG.J.Brostow,“Unsupervisedmonoc-\n[32] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\nulardepthestimationwithleft-rightconsistency,”inCVPR,2017. driving?thekittivisionbenchmarksuite,”inCVPR,2012.",
      "size": 800,
      "sentences": 7
    },
    {
      "id": 61,
      "content": "“Unsupervisedmonoc-\n[32] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\nulardepthestimationwithleft-rightconsistency,”inCVPR,2017. driving?thekittivisionbenchmarksuite,”inCVPR,2012. [8] J. Beltrn, C. Guindel, F. M. Moreno, D. Cruzado, F. Garca, and [33] X. Chen, K. Kundu, Y. Zhu, H. Ma, S. Fidler, and R. Urtasun,\nA. de la Escalera, “Birdnet: A 3d object detection framework from “3dobjectproposalsforaccurateobjectclassdetection,”inNeurIPS,\nlidarinformation,”inITSC,2018. 2015. [9] Y. Xiang, W. Choi, Y. Lin, and S. Savarese, “Data-driven 3d voxel [34] S.-L. Yu, T. Westfechtel, R. Hamada, K. Ohno, and S. Tadokoro,\npatternsforobjectcategoryrecognition,”inCVPR,2015. “Vehicledetectionandlocalizationonbirdseyeviewelevationimages\n[10] X. Chen, K. Kundu, Y. Zhu, H. Ma, S. Fidler, and R. Urtasun, usingconvolutionalneuralnetwork,”inSSRR,2017.",
      "size": 860,
      "sentences": 6
    },
    {
      "id": 62,
      "content": "recognition,”inCVPR,2015. “Vehicledetectionandlocalizationonbirdseyeviewelevationimages\n[10] X. Chen, K. Kundu, Y. Zhu, H. Ma, S. Fidler, and R. Urtasun, usingconvolutionalneuralnetwork,”inSSRR,2017. “3d object proposals using stereo imagery for accurate object class [35] C.R.Qi,W.Liu,C.Wu,H.Su,andL.J.Guibas,“Frustumpointnets\ndetection,”TPAMI,vol.40,no.5,pp.1259–1272,2018. for3dobjectdetectionfromrgb-ddata,”CVPR,2018. [11] S. Song and M. Chandraker, “Joint sfm and detection cues for [36] F. Chabot, M. Chaouch, J. Rabarisoa, C. Teuliere, and T. Chateau,\nmonocular3dlocalizationinroadscenes,”inCVPR,2015. “Deepmanta:Acoarse-to-finemany-tasknetworkforjoint2dand3d\n[12] V. Dhiman, Q. Tran, J. J. Corso, and M. Chandrakar, “A continuous vehicleanalysisfrommonocularimage,”inCVPR,2017. occlusionmodelforroadsceneunderstanding,”inCVPR,2016.",
      "size": 839,
      "sentences": 7
    },
    {
      "id": 63,
      "content": "tasknetworkforjoint2dand3d\n[12] V. Dhiman, Q. Tran, J. J. Corso, and M. Chandrakar, “A continuous vehicleanalysisfrommonocularimage,”inCVPR,2017. occlusionmodelforroadsceneunderstanding,”inCVPR,2016. [37] B. Zhuang, Q. Tran, G. Lee, L. Cheong, and M. Chandraker, “De-\n[13] J.Ku,M.Mozifian,J.Lee,A.Harakeh,andS.Waslander,“Joint3d generacy in self-calibration revisted and a deep learning solution for\nproposal generation and object detection from view aggregation,” in uncalibratedslam,”inIROS,2019. IROS,2018.",
      "size": 509,
      "sentences": 4
    }
  ]
}