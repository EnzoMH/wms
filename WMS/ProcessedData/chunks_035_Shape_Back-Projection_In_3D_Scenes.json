{
  "source": "ArXiv",
  "filename": "035_Shape_Back-Projection_In_3D_Scenes.pdf",
  "total_chars": 35519,
  "total_chunks": 51,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nShape Back-Projection In 3D Scenes\nAshish Kumar1, L. Behera1, Senior Member IEEE, K. S. Venkatesh1\n{https://github.com/ashishkumar822}\nAbstract—Inthiswork,weproposeanovelframeworkshape\nback-projection for computationally efficient point cloud\nprocessing in a probabilistic manner. The primary component\nof the technique is shape histogram and a back-projection\nprocedure. The technique measures similarity between 3D sur-\nfaces,byanalyzingtheirgeometricalproperties.Itisanalogous\nto color back-projection which measures similarity between\n(a)Pointcloud (b)Predictedplanarregions\nimages, simply by looking at their color distributions. In the\noverall process, first, shape histogram of a sample surface (e.g. planar) is computed, which captures the profile of surface\nnormals around a point in form of a probability distribution.",
      "size": 842,
      "sentences": 5
    },
    {
      "id": 2,
      "content": "ions. In the\noverall process, first, shape histogram of a sample surface (e.g. planar) is computed, which captures the profile of surface\nnormals around a point in form of a probability distribution. Later,thehistogramisback-projectedontoatestsurfaceanda\nlikelihoodscoreisobtained.Thescoredepictsthathowlikelya\npoint in the test surface behaves similar to the sample surface,\ngeometrically. Shape back-projection finds its application in\nbinary surface classification, high curvature edge detection in (c)Predictedcurvedregions (d)Predictededges\nunorganized point cloud, automated point cloud labeling for\nFig. 1: Output of the algorithm for tasks such as plane seg-\n3D-CNNs (convolutional neural network) etc. The algorithm\nmentation, curved region segmentation, and edge detection.",
      "size": 783,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "nt cloud labeling for\nFig. 1: Output of the algorithm for tasks such as plane seg-\n3D-CNNs (convolutional neural network) etc. The algorithm\nmentation, curved region segmentation, and edge detection. can also be used for real-time robotic operations such as\nautonomous object picking in warehouse automation, ground\nplane extraction for autonomous vehicles and can be deployed\neasily on computationally limited platforms (UAVs). cameras such as Microsoft Kinect, Intel real sense, IDS-\nEnsenso etc. Whereas, the LIDAR [4] sensors are a major\nI. INTRODUCTION\nsource of unorganized point clouds. Due to long range and\nAccurate depth sensing and its efficient processing is high accuracy, they are preferred in autonomous driving and\ncrucialforaroboticsystemtoreliablyperformtaskssuchas navigation [5], [6], warehouse automation, robotic object\nobject pick, place or autonomous navigation. The depth data manipulation and other industrial applications.",
      "size": 949,
      "sentences": 9
    },
    {
      "id": 4,
      "content": "ystemtoreliablyperformtaskssuchas navigation [5], [6], warehouse automation, robotic object\nobject pick, place or autonomous navigation. The depth data manipulation and other industrial applications. is acquired by the depth sensors and is commonly known as In order to advance the state-of-art in the above ar-\npointclouds.Thecurrentdepthsensorscanprovidemillions eas, several worldwide robotic challenges have been hosted\nof point cloud data in near real time. However, processing previously such as Amazon Picking Challenge, 2015 and\nthem, in general, requires huge computing power. Hence, 2016, Amazon Robotics Challenge, 2017 for warehouse au-\ndriven by the significance of the depth information, in this tomation, MBZIRC, DARPA humanoid [7] and autonomous\npaper, we focus on exploring local geometrical properties driving [8] challenges.",
      "size": 843,
      "sentences": 5
    },
    {
      "id": 5,
      "content": "au-\ndriven by the significance of the depth information, in this tomation, MBZIRC, DARPA humanoid [7] and autonomous\npaper, we focus on exploring local geometrical properties driving [8] challenges. Interestingly, all of them have a large\nof a point cloud such that multiple tasks in the area of 3D technologicaloverlapbetweenthemwhichprimarilyincludes\nperception can be reasoned by computing only once. object detection and segmentation of images and point\nTypically,apointcloudiseitherorganizedorunorganized. clouds, edge detection (2D or 3D), object pose estimation,\nAn organized cloud is represented as a 2D matrix in which 3D model fitting for robot grasping. each location corresponds to a 3D point similar to a pixel The state-of-art (SOA) algorithms for object detection\nin an image. Such clouds offers straightforward use of [9], [10], [11], [12], [13], [14], [15] in images are based\nthe 2D image processing techniques (e.g.",
      "size": 934,
      "sentences": 6
    },
    {
      "id": 6,
      "content": "f-art (SOA) algorithms for object detection\nin an image. Such clouds offers straightforward use of [9], [10], [11], [12], [13], [14], [15] in images are based\nthe 2D image processing techniques (e.g. edge detection) on Convolutional Neural Network (CNN) architectures [16],\nand facilitates fast nearest neighbor computations. On the [17], [18]. The CNNs have also been employed in 3D\nother hand, unorganized clouds are simply a collection of perception, such as point cloud segmentation [19], semantic\n3D points which does not convey any spatial or structural 3Dscenecompletion[20],and3Dobjectsegmentation[21]. connectivity. These clouds are often represented in form These variants of CNNs are known as 3D-CNNs. The 3D-\nof kD-trees [1] or Octrees [2] which facilitate efficient CNNsrequirehugeamountoflabeledpointclouddatawhich\n(but slower then organized) nearest neighbor search [3] as comes at a cost of specialized softwares and exhaustive\nwell as reduced memory storage.",
      "size": 975,
      "sentences": 8
    },
    {
      "id": 7,
      "content": "nt CNNsrequirehugeamountoflabeledpointclouddatawhich\n(but slower then organized) nearest neighbor search [3] as comes at a cost of specialized softwares and exhaustive\nwell as reduced memory storage. Most common sources of manual efforts. Despite the accuracies, their computational\norganized point cloud are stereo cameras, Time-of-Flight and memory intensive nature limits their scope for real time\napplications [22], [23] on limited computing platforms such\n1Mr. Ashish Kumar, Dr. L. Behera and Dr. K. S. Venkatesh are with\nas Unmanned Aerial Vehicles (UAVs).",
      "size": 562,
      "sentences": 4
    },
    {
      "id": 8,
      "content": "ture limits their scope for real time\napplications [22], [23] on limited computing platforms such\n1Mr. Ashish Kumar, Dr. L. Behera and Dr. K. S. Venkatesh are with\nas Unmanned Aerial Vehicles (UAVs). the Department of Electrical Engineering, Indian Institute of Technology,\nKanpur{krashish,lbehera,venkats}@iitk.ac.in As an alternative, traditional feature matching [24], [25]\n1202\nnaJ\n61\n]VC.sc[\n1v90460.1012:viXra\n=== 페이지 2 ===\nand consensus based model fitting [26] methods are pre-\n·10−2\nferred.Theformerinvolvecomputingofhandcraftedfeatures 10\n8\nbased on local geometrical information of a model point 6\n4\ncloud and matching them with the features computed for 0 2\n0 180 360\na target point cloud. The feature matching procedure is Colorintensity\na compute intensive task and its performance is severely (a)\ndeteriorated even for minor 3D surface variations between\nthe model and the target.",
      "size": 895,
      "sentences": 4
    },
    {
      "id": 9,
      "content": "loud. The feature matching procedure is Colorintensity\na compute intensive task and its performance is severely (a)\ndeteriorated even for minor 3D surface variations between\nthe model and the target. Inconsistent depth data is the\nprimary reason for this which often results in inaccurate\nfeature estimation and matching. On the other hand, the consensus based methods of\nRANSAC [27], LMeds [28] are a primary choice to fit\nprimitive shapes (e.g. plane, cylinder or sphere) into point\nclouds. These methods perform iterative random sampling\nof the input points and estimates model parameters (plane\ncoefficients,cylinderorsphereradii).However,samplingand\nthe estimation process becomes computationally inefficient\nwhen a large number of points are irrelevant to the model\nto be fit.",
      "size": 782,
      "sentences": 6
    },
    {
      "id": 10,
      "content": "rameters (plane\ncoefficients,cylinderorsphereradii).However,samplingand\nthe estimation process becomes computationally inefficient\nwhen a large number of points are irrelevant to the model\nto be fit. In case of multiple model instances, the algorithm\nis iterated exactly equal to number of instances, which in\nsomeapplications,isnotknownbeforehand.Moreover,their\ncapabilities to only deal with primitive shapes, limit their\napplicability in real world scenarios, because many objects\nareoftencomplexshaped,i.e.neitheraplanenoracylinder. Furthermore, robust edge detection in the point clouds is\nquite important for various robotic applications. It can be\nachievedintheorganizedpointclouds(depthimages)simply\nby using the techniques of edge detection in RGB images. Whereas, unorganized cloud needs special treatment. Such\ncasesaregenerallyoccurswhenaraworganizedpointcloud\nundergoes noise removal preprocessing operations and loses\nits spatial and structural connectivity (unorganized cloud).",
      "size": 992,
      "sentences": 6
    },
    {
      "id": 11,
      "content": "eeds special treatment. Such\ncasesaregenerallyoccurswhenaraworganizedpointcloud\nundergoes noise removal preprocessing operations and loses\nits spatial and structural connectivity (unorganized cloud). Sometimes, the depth sensors (LIDARs) directly provides\nunorganized point cloud data. Due to the lost spatial rela-\ntions, edge detection in unorganized point clouds becomes\nquitechallengingandhavegainedonlyalittleattention[29]. TheapproachisbasedonEigenvalueanalysiswithallresults\nreported only for synthetic data. In this paper, we propose a novel probabilistic frame-\nwork Shape Back-projection which addresses all of\nthe above limitations in a non-iterative manner. The most\ncrucial part of the algorithm is shape histograms which\nencodes the 3D information in such a way that it can be\nutilizedforanumberofapplicationswhileavoidingcomplex\ncomputations.",
      "size": 857,
      "sentences": 7
    },
    {
      "id": 12,
      "content": "ive manner. The most\ncrucial part of the algorithm is shape histograms which\nencodes the 3D information in such a way that it can be\nutilizedforanumberofapplicationswhileavoidingcomplex\ncomputations. We experimentally demonstrate that, shape\nback-projection can be deployed independently for the tasks\nof point cloud classification, edge detection, especially in\nthe unorganized clouds. Unlike consensus based methods,\nthe proposed algorithm can deal with any number of in-\nstances, without manual specification. The algorithm also\noutperforms a recent algorithm [29] of edge detection in\nunorganized point clouds.",
      "size": 614,
      "sentences": 5
    },
    {
      "id": 13,
      "content": "ds,\nthe proposed algorithm can deal with any number of in-\nstances, without manual specification. The algorithm also\noutperforms a recent algorithm [29] of edge detection in\nunorganized point clouds. Moreover, the algorithm can also\nbeeffectivetofacilitateautomatedlabelingof3Dpointcloud\ndata, required for 3D-CNNs\nIn the following sections, first, we lay a preliminary\ngroundonwhichthewholeideaisbased(Sec.II).Then,we\ndiscusscoreofthealgorithm(Sec.III).Attheend,wereport\n)I|roloc(P Hue\nSat\nVal\n(b) (c) (d)\nFig.2:(a)Sampleimage,(b)correspondingHSVhistogram,\n(c) test image, and (d) back-projection of the hue histogram\nof the object pixels in the sample image onto the test image. comprehensive experimental analysis (Sec.IV) and discuss\nthe primary applications of shape back-projection. II. PRELIMINARIES\nA.",
      "size": 809,
      "sentences": 6
    },
    {
      "id": 14,
      "content": "istogram\nof the object pixels in the sample image onto the test image. comprehensive experimental analysis (Sec.IV) and discuss\nthe primary applications of shape back-projection. II. PRELIMINARIES\nA. Color Histograms\nColor histogram is a discretization of a color space such\nas (Red, Green, Blue), (Hue, Saturation, Value), etc and\nrepresents a frequency distribution over the pixel colors in\nan image. Each component of a color space is referred as a\nchannel. Let C be a k-bin color histogram to be computed\noverasinglecolorchannel.First,abin-id(Eq.1)forapixel\nis obtained and the corresponding bin value is incremented\nby one. This process is repeated for all or only a desired\nset of pixels. The obtained frequency distribution (C) is\nthen normalized (C ) with the number of pixels which\nn\nparticipatedinthehistogramcomputation.Thenormalization\nstep essentially scales all the bin values to sum them up-to\none so that it can follow the properties of a valid probability\ndensity function. Fig.",
      "size": 995,
      "sentences": 10
    },
    {
      "id": 15,
      "content": "ticipatedinthehistogramcomputation.Thenormalization\nstep essentially scales all the bin values to sum them up-to\none so that it can follow the properties of a valid probability\ndensity function. Fig. 2b shows single channel normalized\nhistograms of H, S, V components of a sample image (Fig. 2a). (cid:106)color(cid:107)\nbin-id= (1)\nk\nB. Color Back-projection\nBack-projection [30] is a technique to identify test data\npatterns, behaving almost similar to that of a given distri-\nbution. In the context of images, color histograms are back-\nprojected to localize color patterns similar to a given color\nhistogram. In the first step of back-projection, normalized\ncolor histogram of the pixels of interest in a sample image\niscomputed(objectpixelsinFig.2a).Later,foreachpixelin\natestimage(Fig.2c),bin-idisobtainedandassignedascore\n(Eq. 2) equal to the value of bin-id in the color histogram.",
      "size": 889,
      "sentences": 9
    },
    {
      "id": 16,
      "content": "s of interest in a sample image\niscomputed(objectpixelsinFig.2a).Later,foreachpixelin\natestimage(Fig.2c),bin-idisobtainedandassignedascore\n(Eq. 2) equal to the value of bin-id in the color histogram. The score is referred as back-projection likelihood which\ndepicts that how likely a pixel in the test image belongs to\nthe object in the sample image. P(pixel=color | C )=C (bin-id) (2)\nn n\nIn general, hue component of the HSV color space is\npreferred for color back-projection because it carries the\nchrominance information about a pixel-color. However, the\nselection of color component may vary from application to\napplication. Color back-projection has been explored previ-\nouslyinvariousapplicationssuchasrealtimeobjecttracking\nin images using Mean-Shift [31], Cam-Shift [32]. [표 데이터 감지됨]\n\n=== 페이지 3 ===\nIII.",
      "size": 812,
      "sentences": 7
    },
    {
      "id": 17,
      "content": "to\napplication. Color back-projection has been explored previ-\nouslyinvariousapplicationssuchasrealtimeobjecttracking\nin images using Mean-Shift [31], Cam-Shift [32]. [표 데이터 감지됨]\n\n=== 페이지 3 ===\nIII. SHAPEBACK-PROJECTION\nShapeback-projectionisinspiredbycolorback-projection\nand comprises of shape-histogram analogous to a color\nhistogram.Byback-projectingtheshapehistogramsonto3D\nsurfaces, points of a particular interest can be obtained in a\nprobabilistic manner, similar to obtaining pixels of interest\n(a) (b) (c) (d)\nin the color back-projection process. A. Shape Histograms 0\nσ(\n5\nd\n0\neg)\n100\n0\nA shape histogram has been designed by carefully ob-\nserving the geometrical properties of 3D surfaces such as 50\nnormals,andcurvature.Tounderstandthis,consideraplanar\n100\nand a curved surface (S) as shown in Fig. 3a and 3c. Let\np ,p ∈S beapointanditsneighborrespectivelyandn ,n\ni j i j\nbe their normals. For each (p ,p ) pair, we define an angle\ni j\nα between n and n .",
      "size": 969,
      "sentences": 9
    },
    {
      "id": 18,
      "content": "and a curved surface (S) as shown in Fig. 3a and 3c. Let\np ,p ∈S beapointanditsneighborrespectivelyandn ,n\ni j i j\nbe their normals. For each (p ,p ) pair, we define an angle\ni j\nα between n and n . An individual α does not convey\nij i j ij\nmuch information, however, the α s for all neighbors of\nij\np governs the behavior of local surface variations around\ni\np . By the visual inspection of Fig.3b and 3d, it can be\ni\ninferred that larger the α, more is the surface curved around\np . Based on this fact, we exploit the information contained\ni\nin the α s which we call Inter Normal Angle Difference\nij\n(INAD).",
      "size": 609,
      "sentences": 7
    },
    {
      "id": 19,
      "content": "can be\ni\ninferred that larger the α, more is the surface curved around\np . Based on this fact, we exploit the information contained\ni\nin the α s which we call Inter Normal Angle Difference\nij\n(INAD). We parameterize INAD as a mean and standard\ndeviation(µ,σ)valuepairoftheα sforeachp .TheINAD\nij i\npairessentiallycapturesaGaussiandistributionN(µ,σ2)of\ncurvature around p which is given by Eq.3, 4.\ni\nN\n1 (cid:88)\nµ = α (3)\ni N ij\nj=1\n(cid:118)\n(cid:117) N (cid:117) 1 (cid:88)\nσ i =(cid:116) N (α ij −µ i )2 (4)\nj=1\nFurther, in order to obtain the shape histogram of a given\nsurface, firstly, INAD for each p ∈ S is computed. Later,\ni\na cumulative distribution of these INAD pairs is obtained in\na way similar to that of color histogram computation. It is\nimportant to note that the INAD is composed of two values:\nµ,σ.",
      "size": 819,
      "sentences": 5
    },
    {
      "id": 20,
      "content": "uted. Later,\ni\na cumulative distribution of these INAD pairs is obtained in\na way similar to that of color histogram computation. It is\nimportant to note that the INAD is composed of two values:\nµ,σ. Thus, a shape histogram is simply two dimensional\nand to accommodate this, the Eq.1 can be rewritten as Eq.5,\nwhere, k ,k are the number of bins in the µ and σ axis\nµ σ\n(cid:106)µ (cid:107) (cid:106)σ (cid:107)\nbin-id = i , bin-id = i (5)\nµi k σi k\nµ σ\nNext, the obtained cumulative distribution is normalized\nby dividing it with the maximum value in the distribu-\ntion.The normalized cumulative distribution is termed as\na shape-histogram. Fig.3e-3h depicts the shape his-\ntogram of a synthetically generated planar and a curved\nsurface for different values of neighborhood search radii r.\nThe INAD computations are entirely based on surface\nnormals. Therefore, noisy normals can severely affect the\nshape histograms.",
      "size": 918,
      "sentences": 6
    },
    {
      "id": 21,
      "content": "d a curved\nsurface for different values of neighborhood search radii r.\nThe INAD computations are entirely based on surface\nnormals. Therefore, noisy normals can severely affect the\nshape histograms. Most of the noise comes directly from\nthe depth sensors which is aggregated with the approximate\nsolutionserrors,introducedbythenormalestimationprocess. Hence, it becomes necessary to deal with the case of noisy\nnormals. Ideally, it would be impossible to obtain 100%\n)ged(µ\nσ(deg)\n0 50 100\n0\n50\n100\n(e)r1\n)ged(µ\nσ(deg)\n0 50 100\n0\n50\n100\n(f)r2\n)ged(µ\nσ(deg)\n0 50 100\n0\n50\n100\n(g)r1\n)ged(µ\n(h)r2\nFig. 3: (a), (c) Sample planar and curved surfaces. (b), (d)\ncorresponding profile of surface normals, points (red) and\nnormals(blue). (e), (f) shape histograms of planar surface,\nand (g), (h) curved surface for different values of neighbor-\nhood search radii r1 and r2, where r1 < r2 Higher green,\nhigher probability. noise free normals.",
      "size": 933,
      "sentences": 9
    },
    {
      "id": 22,
      "content": "(f) shape histograms of planar surface,\nand (g), (h) curved surface for different values of neighbor-\nhood search radii r1 and r2, where r1 < r2 Higher green,\nhigher probability. noise free normals. Thus, we employ a noise cancellation\nprocedure based on simple statistical outlier removal tech-\nnique. Before discussing the noise cancellation procedure,\nwe briefly introduce the normal estimation process below. 1) Normal Estimation: Let p be any point in R3 and\ni\nN ⊂ S be the set of all neighboring points around P in\na spherical region of radii r. A covariance matrix (C) in\nR3 is computed from a point set P ∪N and decomposed\ninto Eigen vectors. The matrix C essentially captures the\nspatial behavior (variance or spread) of neighboring points\naround p and an Eigen vector quantifies both the direction\ni\nand amount of spread. In general, a surface has minimal\nspreadinadirectionnormaltoit.Therefore,theEigenvector\nhavinglowest Eigenvaluecan betakenas anapproximation\nto the normal at p .",
      "size": 993,
      "sentences": 7
    },
    {
      "id": 23,
      "content": "the direction\ni\nand amount of spread. In general, a surface has minimal\nspreadinadirectionnormaltoit.Therefore,theEigenvector\nhavinglowest Eigenvaluecan betakenas anapproximation\nto the normal at p . The above methodology is the most i\nsimpleandfastestapproximationofnormalsanditspractical\nimplementations are publicly available in the Point Cloud\nLibrary (PCL) [33]. 2) Noise Removal: After the normal estimation process,\nα sforeachp arecomputedusinginversecosinerule1.At\nij i\nthis stage, we assume that some of the α s may be noisy. ij\nTo filter such values, we perform statistical outlier rejection\noperation on α s. This step eliminates noisy normals up-to\nij\na great extent, leading to consistent normals in the output. The mathematical treatment of the operation is given by Eq. (3, 4, 6). Here, c is referred as an outlier rate which controls\nthe number of outliers in the output. Decision for a point to\nbe an outlier or inlier is given by Eq. 6.",
      "size": 954,
      "sentences": 10
    },
    {
      "id": 24,
      "content": "the operation is given by Eq. (3, 4, 6). Here, c is referred as an outlier rate which controls\nthe number of outliers in the output. Decision for a point to\nbe an outlier or inlier is given by Eq. 6. (cid:40)\nInlier, if |αij−µi| ≤c\np\nj\nis = σi (6)\nOutlier, Otherwise\nThe filtered α s are now plugged into Eq. 3, 4 in order ij\n1αij =cos−1 (cid:16)\n|n\nn\ni\ni\n|\n. .|\nn\nn\nj\nj|\n(cid:17)\n=== 페이지 4 ===\nacquired by Microsoft Kinect sensor. Each cloud contains a\nclutter of household objects placed on top of a table. The\nclouds are deliberately converted into unorganized format. Due to the unavailability of ground truths for the purpose of\nbinarysurfaceclassificationandedgedetection,wemanually\ngeneratethemusingCloudCompare[34].Alltheexperiments\nare performed only on a i7-6850-K CPU and 64GB RAM. Ss St P(p∈Ss) Ss St P(p∈Ss)\nIt must be noticed that shape-histograms does not represent\nany feature e.g. [24], [25], therefore we have limited our\nFig.",
      "size": 944,
      "sentences": 13
    },
    {
      "id": 25,
      "content": "performed only on a i7-6850-K CPU and 64GB RAM. Ss St P(p∈Ss) Ss St P(p∈Ss)\nIt must be noticed that shape-histograms does not represent\nany feature e.g. [24], [25], therefore we have limited our\nFig. 4: Shape back-projection for various S -S pair\ns t\ndiscussion only to variations in hyper-parameters of shape-\nhistograms and its utilities. to compute the new values of µ and σ which collectively\ni i\nA. Varying “r”\nrepresents INAD at p . It must be noted that any point p ∈\ni j\nN marked as an outlier is not included into computation of The INAD score for a point depends on the number of\nµ and σ . Fig. 3e and 3g shows shape histogram computed the spatial neighbors, which in turn is governed by r. Fig. i i\nfor a planar and a curved surface. 3e-3h shows shape histograms of Fig. 3a and 3c for varying\nr. It can be seen that as r is increased, the peak in the shape\nB.",
      "size": 870,
      "sentences": 11
    },
    {
      "id": 26,
      "content": "ich in turn is governed by r. Fig. i i\nfor a planar and a curved surface. 3e-3h shows shape histograms of Fig. 3a and 3c for varying\nr. It can be seen that as r is increased, the peak in the shape\nB. Shape Histogram Back-projection\nhistogramofaplanersurfacedoesn’tchangewhereasitshifts\nShapehistogramback-projectionisfunctionallyanalogous towardshigherbinsforthecaseofacurvedsurface.Thus,we\ntothecolorhistogramback-projection.Inthiscase,theback- makeanimportantobservationthatthevalueofr essentially\nprojection likelihood score depicts that: “how likely a point captures the information about the curvature at a point. We\nin a test surface is following geometrical properties similar exploit the observation and show that by computing shape\nto that of represented by the shape histogram of a sample histogram of a surface only once, we can use it for different\nsurface”. The mathematical expression for such a likelihood purposes as given below.",
      "size": 945,
      "sentences": 7
    },
    {
      "id": 27,
      "content": "at of represented by the shape histogram of a sample histogram of a surface only once, we can use it for different\nsurface”. The mathematical expression for such a likelihood purposes as given below. isgiveninEq.7,whereS andSH referstoasamplesurface\ns 1) Binary Surface Classification: Surface segmentation\nand its shape histogram respectively. is a most fundamental operation which is required in order\nto segregate points with similar geometrical properties into\nP( p ∈S | SH,r)=SH(bin-id , bin-id ) (7)\ni s µi σi\ndifferent clusters. In this direction, the learning based [21],\nInordertoback-projectashapehistogram,firsttheINAD [19] approaches exists, however, they require tremendous\nforeachpointinatestsurfaceS t iscomputed(Eq.3,4).The amounts of data and GPUs for their operations. Here, we\nINAD is then used to obtain bin-id µ and bin-id σ which are demonstrate that shape back-projection can achieve high\nthen plugged into Eq.7 to obtain the likelihood.",
      "size": 960,
      "sentences": 6
    },
    {
      "id": 28,
      "content": "PUs for their operations. Here, we\nINAD is then used to obtain bin-id µ and bin-id σ which are demonstrate that shape back-projection can achieve high\nthen plugged into Eq.7 to obtain the likelihood. accuraciesforbinarysurfaceclassificationmerelyonaCPU. Fig.4 shows examples of the shape back-projection pro- The 3D surfaces can be broadly classified into two cate-\ncedure. The column-1 represents the case when shape his- goriesi.e.planarandcurved(non-planar).Wecansaythatif\ntograms of a planar and a curved surface (S s ) are back- apointismorelikelytobeonaplanarsurface,itwillbeless\nprojected onto a test surface (S t ), having three orthogonal likely to be on a curved surface and vice-versa. Therefore\nplanes.Itcanbenoticedthatthelikelihoodobtainedishigher if we have shape histogram of a planar surface, the Eq. 8\n( ) when both S s and S t belong to similar kind of surfaces holds. (planar-planar). Whereas, it is lower ( ) when S and S are\ns t\nof different kinds (curved-planar).",
      "size": 986,
      "sentences": 9
    },
    {
      "id": 29,
      "content": "m of a planar surface, the Eq. 8\n( ) when both S s and S t belong to similar kind of surfaces holds. (planar-planar). Whereas, it is lower ( ) when S and S are\ns t\nof different kinds (curved-planar). Similarly, the column-2 P(p∈S |SH,r) + P(p∈S |SH,r) =1 (8)\nplane curved\nrepresents the case when shape histogram of the planar and\ncurved sample surfaces is back-projected onto a curved test To verify this, we compute shape histogram of a planar\nsurface S . surface and back-project onto the real 3D scenes as shown\nt\nIntheperformanceofshapeback-projection,theneighbor- in column-(1) Fig. 5. Corresponding color coded likelihoods\nhood search radii r is a crucial parameter . The variations P(p ∈ S plane |SH,r) and P(p ∈ S curved |SH,r) are shown\nin the values of r lead to different utilities of shape back- in column-(2,3) Fig. 5. The likelihood score proves the\nprojection, which we have experimentally demonstrated in validity of Eq. 8 qualitatively.",
      "size": 954,
      "sentences": 12
    },
    {
      "id": 30,
      "content": "alues of r lead to different utilities of shape back- in column-(2,3) Fig. 5. The likelihood score proves the\nprojection, which we have experimentally demonstrated in validity of Eq. 8 qualitatively. The Table I depicts the pre-\nthe following section. cision, recall, F1 and mIoU [35] scores to assess the binary\nclassification quality. These values are heavily dependent on\nIV. EXPERIMENTS\nthequalityofthegroundtruthwhichwehavemanuallygen-\nIn this section, we provide quantitative and qualitative re- erated. The planar class have high precision and high recall\nsults of Shape Back-projection. The experimental evaluation values. Whereas, high recall and relatively low precision for\nis done by varying the values of parameters such as number curved surfaces can be accounted by the misclassification of\nof histograms bins (k), neighbor search radii (r).",
      "size": 855,
      "sentences": 11
    },
    {
      "id": 31,
      "content": "relatively low precision for\nis done by varying the values of parameters such as number curved surfaces can be accounted by the misclassification of\nof histograms bins (k), neighbor search radii (r). In order planar points as well as inaccurate ground truths near the\nto evaluate the algorithm against noise and demonstrate its high curvature edges. Despite the relatively low precisions\nreal world usefulness, we choose publicly available point for the curved surfaces, the qualitative analysis of binary\ncloud dataset [33] containing 108 organized point clouds, classification (Fig. 5 column-2-5) is quite pleasant. === 페이지 5 ===\n1.0\n0.0\nTest P(p∈Splane) P(p∈Splane) P(p∈Scurved) P(p∈Scurved) P(p∈Sedge) P(p∈Sedge)\nsurfaces Groundtruth Predicted Groundtruth Predicted Groundtruth Predicted\n)r,HS|ecafruS∈p(P\nFig.",
      "size": 814,
      "sentences": 5
    },
    {
      "id": 32,
      "content": "t. === 페이지 5 ===\n1.0\n0.0\nTest P(p∈Splane) P(p∈Splane) P(p∈Scurved) P(p∈Scurved) P(p∈Sedge) P(p∈Sedge)\nsurfaces Groundtruth Predicted Groundtruth Predicted Groundtruth Predicted\n)r,HS|ecafruS∈p(P\nFig. 5: Qualitative results of shape back-projection for binary surface classification and edge detection\n2) Edge Detection in Unorganized Point Clouds: We\ntarget this task as a utility of shape back-projection by\nemploying two facts: first, the INAD captures the amount\nof curvature at a point and second, the edges have high\ncurvature as compared to a plane. Therefore, we reduce r (a)Syntheticcloud (b)Groundtruthedges (c)Predictededges\ntoasmallervalue(∼6mm)andthenback-projecttheshape Fig. 6: Edge detection in synthetic unorganized point cloud\nhistogram of a planar surface onto the 3D scenes as shown\nin column-1 Fig. 5. Since, the edges are also curved regions\nin a cloud, therefore, the Eq. 8 can be rewritten as given\nletthenoisehampertheunderlyinggeometricalinformation. below.",
      "size": 982,
      "sentences": 8
    },
    {
      "id": 33,
      "content": "3D scenes as shown\nin column-1 Fig. 5. Since, the edges are also curved regions\nin a cloud, therefore, the Eq. 8 can be rewritten as given\nletthenoisehampertheunderlyinggeometricalinformation. below. Hence, the shape histograms inherently deals with surface\nP(p∈S |SH,r) + P(p∈S |SH,r) =1 (9) noise which is desirable for many practical applications. plane edge\nTableIIshowstheprecision,recallandtheF1-Scoreforedge\nC. Timing Analysis and Point Cloud Density\ndetection using shape back-projection and a recent method\n[29]. It can be inferred that the recent method performs well The INAD score heavily relies on number of nearest\nfor synthetic data which they have reported in their paper, neighbors (k-NNs) which depends on the cloud density. In\nhowever, performs quite inferior on real data. This is where, general, the real point cloud data is noisy and posses non-\nthe shape back-projection marks its importance to reliably uniform density.",
      "size": 943,
      "sentences": 10
    },
    {
      "id": 34,
      "content": "n\nhowever, performs quite inferior on real data. This is where, general, the real point cloud data is noisy and posses non-\nthe shape back-projection marks its importance to reliably uniform density. Therefore, rather changing the radii, timing\ndeal with real data. Fig. 5 column-(6-7) shows the color analysis is performed by explicitly varying the number of\ncoded edge likelihood P(p ∈ S edge |SH,r) corresponding nearest neighbors in a synthetic point cloud (Fig. 6). Table\ntothecloudsincolumn-(1).Itcanbeseenthatthepredicted III shows the INAD computing performance for a single\nedges appears quite similar to the ground truth. pointwithdifferentnumberofnearestneighbors.Onascale,\n10 and 500 neighbors roughly corresponds to a radii of\nB. Number of Bins (k ,k ) and its Effect on Noise\nµ σ 0.005m and 0.03m on surface of resolution 0.001m.",
      "size": 843,
      "sentences": 9
    },
    {
      "id": 35,
      "content": "ferentnumberofnearestneighbors.Onascale,\n10 and 500 neighbors roughly corresponds to a radii of\nB. Number of Bins (k ,k ) and its Effect on Noise\nµ σ 0.005m and 0.03m on surface of resolution 0.001m. The\nTostudytheeffectofnoise,wedon’trelyonaddingnoise methodisquitefastevenonsinglethread.Thisenablesshape\nin the synthetic point clouds as performed in [29], instead back-projection to be deployed on computationally limited\nthe chosen point clouds fulfills the purpose, as they are platforms. The speeds can be increased several times by\nfull of random noise and inconsistence depth measurements. taking advantage of multi-threading and GPUs, if available. TableIIdepictstheprecision,recallandF1measureforhigh\ncurvature edge detection in the real point clouds data well\nD. Rotation and Translation Invariancy\nas a synthetic cloud (Fig. 6).",
      "size": 839,
      "sentences": 7
    },
    {
      "id": 36,
      "content": "s, if available. TableIIdepictstheprecision,recallandF1measureforhigh\ncurvature edge detection in the real point clouds data well\nD. Rotation and Translation Invariancy\nas a synthetic cloud (Fig. 6). It is noticeable that the best\nF1 (in blue) on real data is obtained with higher number Shape back-projection exploits local geometrical proper-\nof bins while for the synthetic point cloud, it is achieved ties. In this process, the INAD score for a point is computed\nwith lower number of bins. It happens because noisy point using surface normals which are in turn computed locally\ncloud contains local surface variations which can not be and invariant to 3D affine transformation. Therefore, the\nrepresented for small values of k. On the other hand, high INAD score and the shape histograms remain rotational and\nvaluesofkaccuratelyencodessuchinformationanddoesnot translational invariant.",
      "size": 890,
      "sentences": 7
    },
    {
      "id": 37,
      "content": ", the\nrepresented for small values of k. On the other hand, high INAD score and the shape histograms remain rotational and\nvaluesofkaccuratelyencodessuchinformationanddoesnot translational invariant. [표 데이터 감지됨]\n\n=== 페이지 6 ===\nin Fig.4 can be used to generate dataset to train a 3D CNN\nfor purpose of surface segmentation and to detect edges. VI. CONCLUSION\nIn this work, we have presented a novel algorithm of\nshape back-projection in 3D scenes. Its inspiration lies in\nthe working of color back-projection which obtains color\nsimilarity between two images by analyzing their color his-\nFig.7:Ourroboticsetupperforminganautonomouspicking\ntograms. Here, we have developed a novel shape histogram,\noperation on the grasp location provided by shape back-\nby means of which, a probabilistic measure of similarity\nprojection. between two 3D point clouds can be obtained. The utility of\ntheshapeback-projectionrangesfromwarehouseautomation\nto automated labeled data generation for 3D-CNNs.",
      "size": 984,
      "sentences": 8
    },
    {
      "id": 38,
      "content": "ic measure of similarity\nprojection. between two 3D point clouds can be obtained. The utility of\ntheshapeback-projectionrangesfromwarehouseautomation\nto automated labeled data generation for 3D-CNNs. The\nE. A Promising Alternative to Model Consensus\nexisting feature based 3D object detection methods can also\nConsider the point cloud in the Fig.5 row-3. In order be benefitted by extracting salient points using shape back-\nto extract all the cylindrical items from the cloud, model projection. The algorithm is a robust and a computationally\nconsensus needs to be deployed iteratively and number of efficient alternative to the SOA algorithms for the above\ncylinders (instances) must be known beforehand. On the purposes. Therefore, it can be easily deployed on computa-\nother hand, binary surface classification using shape back- tionallylimitedplatforms(UAVs)forcomplextasksofobject\nprojection can classify all instances without requiring to manipulation.",
      "size": 959,
      "sentences": 8
    },
    {
      "id": 39,
      "content": "on computa-\nother hand, binary surface classification using shape back- tionallylimitedplatforms(UAVs)forcomplextasksofobject\nprojection can classify all instances without requiring to manipulation. have the total number of instances apriori and can also deal\nREFERENCES\nwith variety of shapes, unlike model consensus which are\nprimitive only. Table IV shows the precision, recall and F1- [1] J.L.Bentley,“Multidimensionalbinarysearchtreesusedforassocia-\ntivesearching,”CommunicationsoftheACM,vol.18,no.9,pp.509–\nscore of shape back-projection and RANSAC to extract all\n517,1975.\ncylinders in the the the cloud-Id 33 (Fig.5, row-3). [2] D. J. Meagher, Octree encoding: A new technique for the rep-\nresentation, manipulation and display of arbitrary 3-d objects by\nV. PRACTICALAPPLICATIONS computer.ElectricalandSystemsEngineeringDepartmentRensseiaer\nPolytechnicInstituteImageProcessingLaboratory,1980.",
      "size": 901,
      "sentences": 4
    },
    {
      "id": 40,
      "content": "ntation, manipulation and display of arbitrary 3-d objects by\nV. PRACTICALAPPLICATIONS computer.ElectricalandSystemsEngineeringDepartmentRensseiaer\nPolytechnicInstituteImageProcessingLaboratory,1980. 1) Warehouse Automation: A large number of novel [3] M.MujaandD.G.Lowe,“Flann,fastlibraryforapproximatenearest\nneighbors,” in International Conference on Computer Vision Theory\nitems arrive in the warehouses on a daily basis. Dealing\nandApplications(VISAPP’09),vol.3,INSTICCPress,2009. with such items becomes a major challenge because not [4] Velodyne,“VelodyneHDL-64E:AhighdefinitionLIDARsensorfor\nevery item can be included in the dataset of the learning 3D applications,” tech. rep., Velodyne, October 2007. Available at\nwww.velodyne.com/lidar/products/whitepaper. basedperceptionalgorithms.Insuchcases,thebinarysurface\n[5] D. Lavrinc, “Ford unveils its first autonomous vehicle pro-\nclassification using shape back-projection can be utilized in totype http://www. wired.",
      "size": 975,
      "sentences": 8
    },
    {
      "id": 41,
      "content": "rceptionalgorithms.Insuchcases,thebinarysurface\n[5] D. Lavrinc, “Ford unveils its first autonomous vehicle pro-\nclassification using shape back-projection can be utilized in totype http://www. wired. com/autopia/2013/12/ford-fusion-hybrid-\norder to segregate items in the cluttered containers on the autonomous,”AccessedDecember16th,2013. [6] E. Ackerman, “Tesla model S: Summer software update will enable\nbasisoftheir3Dshapeandthesegregateditemsthencanbe\nautonomousdriving,”IEEESpectrumCarsThatThink,2015. sent to different destinations for further processing. [7] G.PrattandJ.Manzo,“Thedarparoboticschallenge[competitions],”\n2) SuctionGraspLocationEstimation: Throughourpar- IEEE Robotics & Automation Magazine, vol. 20, no. 2, pp. 10–12,\n2013.\nticipation in the Amazon challenges APC’16 and ARC’17,\n[8] M.Buehler,K.Iagnemma,andS.Singh,TheDARPAurbanchallenge:\nwehaverealizedthatthecentroidbasedapproach[36]isnot autonomousvehiclesincitytraffic,vol.56. springer,2009.",
      "size": 969,
      "sentences": 10
    },
    {
      "id": 42,
      "content": "on challenges APC’16 and ARC’17,\n[8] M.Buehler,K.Iagnemma,andS.Singh,TheDARPAurbanchallenge:\nwehaverealizedthatthecentroidbasedapproach[36]isnot autonomousvehiclesincitytraffic,vol.56. springer,2009. suitable for suction based grasping in the presence of partial [9] C. Zhu, Y. Zheng, K. Luu, and M. Savvides, “Cms-rcnn: contextual\nmulti-scale region-based cnn for unconstrained face detection,” in\nocclusions, and especially when a smaller object lies on top\nDeepLearningforBiometrics,pp.57–79,Springer,2017. ofalargerobject.Asasolution,shape-histogramofaplanar [10] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international\nor curved surface (depending on the target object shape) is conferenceoncomputervision,pp.1440–1448,2015. [11] S.Ren,K.He,R.Girshick,andJ.Sun,“Fasterr-cnn:Towardsreal-\nback-projected onto the point cloud of a target object.",
      "size": 859,
      "sentences": 5
    },
    {
      "id": 43,
      "content": "ding on the target object shape) is conferenceoncomputervision,pp.1440–1448,2015. [11] S.Ren,K.He,R.Girshick,andJ.Sun,“Fasterr-cnn:Towardsreal-\nback-projected onto the point cloud of a target object. Then,\ntimeobjectdetectionwithregionproposalnetworks,”inAdvancesin\nan adaptive mean-shift operation is performed on the back- neuralinformationprocessingsystems,pp.91–99,2015. projection likelihood and its convergence point is chosen as [12] J.Long,E.Shelhamer,andT.Darrell,“Fullyconvolutionalnetworks\nfor semantic segmentation,” in Proceedings of the IEEE Conference\nthesuctiongrasplocation(Fig.7).Thisstrategyisquitefast,\nonComputerVisionandPatternRecognition,pp.3431–3440,2015. effective and also eliminates a need of time consuming 6D [13] H.Zhao,J.Shi,X.Qi,X.Wang,andJ.Jia,“Pyramidsceneparsing\npose estimation such as Super4PCS [37]. network,”inProceedingsofIEEEConferenceonComputerVisionand\nPatternRecognition(CVPR),2017.",
      "size": 926,
      "sentences": 6
    },
    {
      "id": 44,
      "content": "nsuming 6D [13] H.Zhao,J.Shi,X.Qi,X.Wang,andJ.Jia,“Pyramidsceneparsing\npose estimation such as Super4PCS [37]. network,”inProceedingsofIEEEConferenceonComputerVisionand\nPatternRecognition(CVPR),2017. 3) Automated data labeling for 3D-CNNs: In order to\n[14] K.He,G.Gkioxari,P.Dolla´r,andR.Girshick,“Maskr-cnn,”CVPR,\ntrain 3D-CNNs for the task of surface classification, a huge 2017.\namount of data is required. Manual labeling of 3D point [15] G. Lin, A. Milan, C. Shen, and I. D. Reid, “Refinenet: Multi-path\nrefinement networks for high-resolution semantic segmentation.,” in\nclouds is quite exhaustive and challenging as compared to\nCvpr,vol.1,p.5,2017. images and requires specialized softwares. Hence, shape [16] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage\nback-projection can be deployed to segment specific kind recognition,” in Proceedings of the IEEE conference on computer\nvisionandpatternrecognition,pp.770–778,2016.",
      "size": 940,
      "sentences": 6
    },
    {
      "id": 45,
      "content": "Sun,“Deepresiduallearningforimage\nback-projection can be deployed to segment specific kind recognition,” in Proceedings of the IEEE conference on computer\nvisionandpatternrecognition,pp.770–778,2016. of surfaces from a 3D scene and its output can be used to\n[17] G.Huang,Z.Liu,K.Q.Weinberger,andL.vanderMaaten,“Densely\ngenerategroundtruths.Forexample,theoutputsofalgorithm connectedconvolutionalnetworks,”CVPR,2017. === 페이지 7 ===\nTABLE I: Performanceanalysisofbinarysurfaceclassification\nkµ×kσ\n10×10 20×20\nScene-Id\nPlanar Curved Planar Curved\nmIoU mIoU\nPrecision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1\n4 0.99 0.95 0.97 0.50 0.98 0.66 0.95 0.99 0.93 0.96 0.38 0.99 0.55 0.93\n20 1.00 0.95 0.97 0.63 1.00 0.67 0.95 0.92 0.96 0.92 0.51 1.00 0.68 0.92\n33 0.97 0.95 0.96 0.74 0.82 0.78 0.92 0.99 0.91 0.95 0.66 0.97 0.79 0.91\n44 0.98 0.96 0.97 0.71 0.89 0.79 0.95 0.99 0.93 0.96 0.61 0.99 0.76 0.93\nTABLE II: Performanceanalysisofhighcurvatureedgedetection,S.B.",
      "size": 986,
      "sentences": 3
    },
    {
      "id": 46,
      "content": ".95 0.96 0.74 0.82 0.78 0.92 0.99 0.91 0.95 0.66 0.97 0.79 0.91\n44 0.98 0.96 0.97 0.71 0.89 0.79 0.95 0.99 0.93 0.96 0.61 0.99 0.76 0.93\nTABLE II: Performanceanalysisofhighcurvatureedgedetection,S.B. TABLE IV: Performanceanalysisformulti-instancecylinderextraction\nreferrestoShapeBack-projection\nMethod Precision Recall F1\nScene-Id Method kµ×kσ Precision Recall F1 Time(S) S.B. 0.66 0.97 0.79\nS.B. 10×10 0.98 0.51 0.67 2.2 RANSAC[27] 0.16 0.92 0.27\n4 S.B. 20×20 0.94 0.72 0.82 2.2\n[29] - 0.05 1.00 0.09 2.7\nS.B. 10×10 0.95 0.63 0.75 2.0\n20 S.B. 20×20 0.90 0.83 0.86 2.0 [29] D.Bazazian,J.R.Casas,andJ.Ruiz-Hidalgo,“Fastandrobustedge\n[29] - 0.05 1.00 0.11 2.5 extractioninunorganizedpointclouds,”inDigitalImageComputing:\nS.B. 10×10 0.87 0.48 0.62 1.5 TechniquesandApplications(DICTA),2015InternationalConference\n33 S.B. 20×20 0.78 0.75 0.77 1.5 on,pp.1–8,IEEE,2015. [29] - 0.05 1.00 0.09 2.1 [30] M.J.SwainandD.H.Ballard,“Colorindexing,”Internationaljournal\nS.B.",
      "size": 961,
      "sentences": 10
    },
    {
      "id": 47,
      "content": "dApplications(DICTA),2015InternationalConference\n33 S.B. 20×20 0.78 0.75 0.77 1.5 on,pp.1–8,IEEE,2015. [29] - 0.05 1.00 0.09 2.1 [30] M.J.SwainandD.H.Ballard,“Colorindexing,”Internationaljournal\nS.B. 10×10 0.94 0.62 0.75 1.4 ofcomputervision,vol.7,no.1,pp.11–32,1991. 44 S.B. 20×20 0.84 0.82 0.83 1.4 [31] D.ComaniciuandP.Meer,“Robustanalysisoffeaturespaces:color\n[29] - 0.04 1.00 0.08 2.0 image segmentation,” in Computer Vision and Pattern Recognition,\nS.B. 10×10 0.98 0.98 0.98 1.5 1997. Proceedings., 1997 IEEE Computer Society Conference on,\nFig.6a S.B. 20×20 0.42 0.96 0.59 1.5 pp.750–755,IEEE,1997. [29] - 0.95 0.98 0.97 1.6 [32] G.R.Bradski,“Computervisionfacetrackingforuseinaperceptual\nuserinterface,”1998. [33] R.B.RusuandS.Cousins,“3dishere:Pointcloudlibrary(pcl),”in\nTABLE III: TiminganalysisofINADcomputationsperpoint Roboticsandautomation(ICRA),2011IEEEInternationalConference\non,pp.1–4,IEEE,2011.",
      "size": 912,
      "sentences": 11
    },
    {
      "id": 48,
      "content": "98. [33] R.B.RusuandS.Cousins,“3dishere:Pointcloudlibrary(pcl),”in\nTABLE III: TiminganalysisofINADcomputationsperpoint Roboticsandautomation(ICRA),2011IEEEInternationalConference\non,pp.1–4,IEEE,2011. k-NNs 10 100 200 300 400 500\n[34] D. Girardeau-Montaut, “Cloud compare—3d point cloud and mesh\nTime(µS) 4.2 23.1 45.5 67.6 87.5 100.7 processingsoftware,”OpenSourceProject,2015. [35] M.Everingham,L.VanGool,C.K.Williams,J.Winn,andA.Zisser-\nman,“Thepascalvisualobjectclasses(voc)challenge,”International\njournalofcomputervision,vol.88,no.2,pp.303–338,2010. [18] K. Simonyan and A. Zisserman, “Very deep convolutional networks [36] C.F.Lehnert,A.English,C.McCool,A.W.Tow,andT.Perez,“Au-\nforlarge-scaleimagerecognition,”CoRR,vol.abs/1409.1556,2014. tonomous sweet pepper harvesting for protected cropping systems.,”\n[19] J. Huang and S. You, “Point cloud labeling using 3d convolutional IEEE Robotics and Automation Letters, vol. 2, no. 2, pp.",
      "size": 939,
      "sentences": 8
    },
    {
      "id": 49,
      "content": "014. tonomous sweet pepper harvesting for protected cropping systems.,”\n[19] J. Huang and S. You, “Point cloud labeling using 3d convolutional IEEE Robotics and Automation Letters, vol. 2, no. 2, pp. 872–879,\nneural network,” in Pattern Recognition (ICPR), 2016 23rd Interna- 2017.\ntionalConferenceon,pp.2670–2675,IEEE,2016. [37] N.Mellado,D.Aiger,andN.J.Mitra,“Super4pcsfastglobalpoint-\n[20] S.Song,F.Yu,A.Zeng,A.X.Chang,M.Savva,andT.Funkhouser, cloud registration via smart indexing,” Computer Graphics Forum,\n“Semanticscenecompletionfromasingledepthimage,”inComputer vol.33,no.5,pp.205–215,2014. Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on,\npp.190–198,IEEE,2017. [21] B.Wu,A.Wan,X.Yue,andK.Keutzer,“Squeezeseg:Convolutional\nneural nets with recurrent crf for real-time road-object segmentation\nfrom3dlidarpointcloud,”arXivpreprintarXiv:1710.07368,2017.",
      "size": 874,
      "sentences": 7
    },
    {
      "id": 50,
      "content": "EEE,2017. [21] B.Wu,A.Wan,X.Yue,andK.Keutzer,“Squeezeseg:Convolutional\nneural nets with recurrent crf for real-time road-object segmentation\nfrom3dlidarpointcloud,”arXivpreprintarXiv:1710.07368,2017. [22] F. Caccavale, G. Giglio, G. Muscio, and F. Pierri, “Adaptive control\nfor uavs equipped with a robotic arm,” IFAC Proceedings Volumes,\nvol.47,no.3,pp.11049–11054,2014. [23] T. W. Danko, K. P. Chaney, and P. Y. Oh, “A parallel manipulator\nfor mobile manipulating uavs,” in Technologies for Practical Robot\nApplications(TePRA),2015IEEEInternationalConferenceon,pp.1–\n6,IEEE,2015. [24] R. B. Rusu, N. Blodow, and M. Beetz, “Fast point feature his-\ntograms(fpfh)for3dregistration,”inRoboticsandAutomation,2009. ICRA’09.IEEEInternationalConferenceon,pp.3212–3217,Citeseer,\n2009. [25] F. Tombari, S. Salti, and L. Di Stefano, “Unique signatures of\nhistogramsforlocalsurfacedescription,”inEuropeanconferenceon\ncomputervision,pp.356–369,Springer,2010.",
      "size": 947,
      "sentences": 7
    },
    {
      "id": 51,
      "content": ",pp.3212–3217,Citeseer,\n2009. [25] F. Tombari, S. Salti, and L. Di Stefano, “Unique signatures of\nhistogramsforlocalsurfacedescription,”inEuropeanconferenceon\ncomputervision,pp.356–369,Springer,2010. [26] F.TombariandL.DiStefano,“Objectrecognitionin3dsceneswith\nocclusions and clutter by hough voting,” in 2010 Fourth Pacific-Rim\nSymposiumonImageandVideoTechnology,pp.349–355,IEEE,2010. [27] M. A. Fischler and R. C. Bolles, “Random sample consensus: a\nparadigm for model fitting with applications to image analysis and\nautomatedcartography,”CommunicationsoftheACM,vol.24,no.6,\npp.381–395,1981. [28] R.Garcia,J.Batlle,andX.Cufi,“Asystemtoevaluatetheaccuracy\nofavisualmosaickingmethodology,”inOCEANS,2001.MTS/IEEE\nConferenceandExhibition,vol.4,pp.2570–2576,IEEE,2001. [표 데이터 감지됨]",
      "size": 778,
      "sentences": 6
    }
  ]
}