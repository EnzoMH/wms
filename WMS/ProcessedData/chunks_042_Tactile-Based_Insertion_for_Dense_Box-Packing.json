{
  "source": "ArXiv",
  "filename": "042_Tactile-Based_Insertion_for_Dense_Box-Packing.pdf",
  "total_chars": 39628,
  "total_chunks": 55,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nTactile-Based Insertion for Dense Box-Packing\nSiyuan Dong and Alberto Rodriguez\nMassachusetts Institute of Technology\n<sydong,albertor>@mit.edu\nAbstract—We study the problem of using high-resolution\ntactile sensors to control the insertion of objects in a box-\npackingscenario.Inthispaper,weproposeaninsertionstrategy\nthat leverages tactile sensing to: 1) safely probe the box with\nthegraspedobjectwhilemonitoringincipientsliptomaintaina\nstable grasp on the object. 2) estimate and correct for residual\npositionuncertaintiestoinserttheobjectintoadesignatedgap\nwithout disturbing the environment. Ourproposedmethodologyisbasedontwoneuralnetworks\nthat estimate the error direction and error magnitude, from\na stream of tactile imprints, acquired by two GelSlim fingers,\nduring the insertion process.",
      "size": 811,
      "sentences": 3
    },
    {
      "id": 2,
      "content": "rproposedmethodologyisbasedontwoneuralnetworks\nthat estimate the error direction and error magnitude, from\na stream of tactile imprints, acquired by two GelSlim fingers,\nduring the insertion process. The system is trained on four\nobjectswithbasicgeometricshapes,whichweshowgeneralizes\ntofourothercommonobjects.Basedontheestimatedpositional\nerrors, a heuristic controller iteratively adjusts the position\nof the object and eventually inserts it successfully without\nrequiring prior knowledge of the geometry of the object. The key insight is that dense tactile feedback contains useful\ninformationwithrespecttothecontactinteractionbetweenthe\ngrasped object and its environment. We achieve high success\nrate and show that unknown objects can be inserted with an\naverage of 6 attempts of the probe-correct loop. The method’s\nabilitytogeneralizetonovelobjectsmakesitagoodfitforbox\npacking in warehouse automation. I. INTRODUCTION\nWarehouse automation plays an important role in retail Fig.1.",
      "size": 987,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "probe-correct loop. The method’s\nabilitytogeneralizetonovelobjectsmakesitagoodfitforbox\npacking in warehouse automation. I. INTRODUCTION\nWarehouse automation plays an important role in retail Fig.1. Densepackingtask.Therobotisinsertingacylindricaltapeinto\naboxcontainingotherpackedobjects.Withthepositionerror,thetapeis\nfor improving efficiency, increasing reliability, and reducing\nblockedbythesurroundingobject.TheerrorisdetectedbyGelSlimsensor\ncost. Recently, automatic item picking has experienced an andpassedtothemachinelearningmodelandacontroller.Thecontroller\nincreased interest in the robotics community, due in part to decidestomovetothe+xdirectionby10mm,whichresultsinasuccessful\ninsertion. the Amazon Robotics Challenge [1].",
      "size": 736,
      "sentences": 7
    },
    {
      "id": 4,
      "content": "elandacontroller.Thecontroller\nincreased interest in the robotics community, due in part to decidestomovetothe+xdirectionby10mm,whichresultsinasuccessful\ninsertion. the Amazon Robotics Challenge [1]. Automatic item packing is the dual of item picking in\nwarehousesettings.Packingitemsdenselyimprovesthestor-\ncapturing the small changes to the tactile imprints left in\nage capacity, decreases the delivery cost and saves packing\nthe sensor by a grasped object that accidentally collides\nmaterials. It is however a demanding manipulation task\nwith the box or surrounding objects. The captured images\nwhich has not been thoroughly explored by the research\nare processed by two neural networks that estimate position\ncommunity. errors. Based on these estimated errors, a controller adjusts\nDense box-packing requires an accurate vision system\ntheinsertionpositioninthenextattempt.Duringthepacking\nto estimate the packing position.",
      "size": 926,
      "sentences": 7
    },
    {
      "id": 5,
      "content": "ors. Based on these estimated errors, a controller adjusts\nDense box-packing requires an accurate vision system\ntheinsertionpositioninthenextattempt.Duringthepacking\nto estimate the packing position. To avoid collisions, the\nprocess,GelSlimalsomonitorsincipientslipbetweenfingers\nperceptionandcontrolsystemsneedtousespatialmarginsof\nand object to avoid breaking the grasp. We demonstrate\nseveralcentimeters,wastingspace.Figure1showsacollision\nthat 1) by monitoring incipient slip signals, the system can\nevent in a dense box-packing task. effectively prevent items from slipping out of the gripper\nToaddressthelimitationsofvisionsystems,weproposea\nor being crushed by a hard collision, 2) by estimating the\npackingstrategybasedonhigh-resolutiontactileinformation.",
      "size": 763,
      "sentences": 5
    },
    {
      "id": 6,
      "content": "from slipping out of the gripper\nToaddressthelimitationsofvisionsystems,weproposea\nor being crushed by a hard collision, 2) by estimating the\npackingstrategybasedonhigh-resolutiontactileinformation. relative position between the target object in-hand and the\nIn particular, we use GelSlim [2], a sensor capable of\nenvironment objects, small position errors can be corrected,\n3)bytrainingwith4objectswithdifferentshapes,thesystem\nThis work was supported by the Amazon Research Awards, and the\nToyota Research Institute (TRI). This article solely reflects the opinions can generalize to packing new objects. andconclusionsofitsauthorsandnotAmazonorToyota. Dense packing task is related to the much more explored\nWe thank Wen Xiong and Rachel Holladay for proofreading the\nmanuscriptandDaolinMaforhelpfuldiscussions. peg-in-hole insertion problem, but with more tolerance on\n9102\npeS\n21\n]OR.sc[\n1v62450.9091:viXra\n=== 페이지 2 ===\npositional errors and larger object variability.",
      "size": 973,
      "sentences": 6
    },
    {
      "id": 7,
      "content": "scriptandDaolinMaforhelpfuldiscussions. peg-in-hole insertion problem, but with more tolerance on\n9102\npeS\n21\n]OR.sc[\n1v62450.9091:viXra\n=== 페이지 2 ===\npositional errors and larger object variability. Most solutions and tilted modes with force/moment sensors. These model-\nto the classic peg-in-hole problem based on either hardware based methods leveraged the geometry of the peg, which\naidsorforcefeedbackcontrol[3],[4]requiretheknowledge was usually cylindrical. These methods are ineffective for\nof geometry of the object and the hole, which are not the packing task, where object geometry varies and is often\neffective for the dense packing task. not known. The key insight in this paper is that dense high-resolution Learning-based methods instead utilize patterns of data\ntactile information is better suited to the packing problem, acquiredbythesensorindifferentsituationstoguidethehole\nthan suggested measurements such as with force torque search and motion corrections.",
      "size": 978,
      "sentences": 7
    },
    {
      "id": 8,
      "content": "actile information is better suited to the packing problem, acquiredbythesensorindifferentsituationstoguidethehole\nthan suggested measurements such as with force torque search and motion corrections. These sensors are most often\nsensors, with which it is more difficult to give a geometric forcetorquesensors.Newmanetal. [9]proposedamethodto\ninterpretation in terms of contact with the environment interprettheforceandthemomentumsignalinapreliminary\nwithout knowledge of the geometry of the object and the assembly attempt, and used it to find holes in the compliant\nenvironment. Our method does not require prior information peg-in-hole task. Gullapalli et al. [10] trained a policy that\nof the objects and can conform to objects with different learnedthemappingbetweenthecontactforceresultingfrom\ngeometries. In addition, we only gently and vertically poke misalignmentandthemotionthatreducedthemisalignment.",
      "size": 910,
      "sentences": 7
    },
    {
      "id": 9,
      "content": "conform to objects with different learnedthemappingbetweenthecontactforceresultingfrom\ngeometries. In addition, we only gently and vertically poke misalignmentandthemotionthatreducedthemisalignment. the surrounding objects, preventing the surrounding objects As the computation power increases, more and more vision\nfromshiftingorfalling.Thetactile-basedpackingsystemwe sensors are used to solve the peg-in-hole problem. Levine et\npropose enables closed-loop control for eliminating position al. [11] trained an end-to-end strategy which took images\nuncertainties. from an external camera as the input and directly outputted\napplied robot motor torques. They demonstrated the policy\nII. RELATEDWORK\nin several peg-in-hole like assembly tasks. Lee et al. [12]\nDensepackingintoaclutteredenvironmenthassimilarities proposed a deep reinforcement learning method with both\nwith the peg-in-hole problem. Therefore, in section II-A we images and forces as the input.",
      "size": 959,
      "sentences": 11
    },
    {
      "id": 10,
      "content": "ensepackingintoaclutteredenvironmenthassimilarities proposed a deep reinforcement learning method with both\nwith the peg-in-hole problem. Therefore, in section II-A we images and forces as the input. They used self-supervised\nreview different methods for the peg-in-hole problem and learning to learn the representation of the images and the\ndiscuss whether these methods can be implemented in the forces in advance, which improved the data efficiency of the\ndensepackingtask.Sinceweadoptadeeplearningapproach learning process. In addition, the policy can be generalized\nto process image sequences acquired by the vision-based to several pegs with different shapes. tactilesensor,wereviewdeeplearningapproachesforimage Our method belongs to the learning-based active sensing\nsequence processing in Section II-B. methods. Different from the peg-in-hold problem, the sur-\nrounding objects in the dense packing task are not fastened.",
      "size": 930,
      "sentences": 7
    },
    {
      "id": 11,
      "content": "elongs to the learning-based active sensing\nsequence processing in Section II-B. methods. Different from the peg-in-hold problem, the sur-\nrounding objects in the dense packing task are not fastened. A. Peg-in-hole\nThereforethesearchingstrategieswithforcecontrol[3],[4],\nAs an active research topic in robotics for decades, the [13] that rely on an known geometry are not appropriate. peg-in-hole problem is a typical contact manipulation task To retain the position of surrounding objects, our method\nthat requires precise position and some form of compliance, performsaverticalpokingtothesurroundingobjects,instead\neitherpassiveoractive.Itrepresentsalargeclassofassembly ofslidingintothem.It’sworthpointingoutthattheGelSlim\ntasks in industry. Many approaches have been proposed to sensor uses a soft gel layer as the contact surface, yielding\nsolve the problem. One class of methods is based on pas- some compliance to facilitate the insertion task. sive compliance hardware and control algorithms.",
      "size": 1000,
      "sentences": 8
    },
    {
      "id": 12,
      "content": "gel layer as the contact surface, yielding\nsolve the problem. One class of methods is based on pas- some compliance to facilitate the insertion task. sive compliance hardware and control algorithms. Drake [5]\nB. Deep Learning for Image Sequence Processing\ndesigned a passive compliance device called Remote Center\nCompliance (RCC) for correcting small uncertainties in the Deeplearningapproachesforprocessingimagesequences\nassembly task. Whitney [6] further analyzed deformations are mostly used in action classification tasks in computer\nof the geometry and the forces of rigid part mating. The vision.Accordingtowhether2Dor3Dconvolutionalkernels\nparameters of the RCC device were tuned to adjust the are deployed, the neural networks can be classified into two\npeg orientation relative to the hole. Jain et al. [7] demon- mainclasses.TheConvNet+LSTM[14]methodweusehere\nstratedthataddingcompliancewithtwoionicpolymermetal has 2D kernels.",
      "size": 938,
      "sentences": 9
    },
    {
      "id": 13,
      "content": "be classified into two\npeg orientation relative to the hole. Jain et al. [7] demon- mainclasses.TheConvNet+LSTM[14]methodweusehere\nstratedthataddingcompliancewithtwoionicpolymermetal has 2D kernels. It employs CNN to extract spatial features\ncomposite (IPMC) compliant fingers had advantages in peg- and LSTM to capture temporal features. This method can\nin-hole assembly. Park et al. [3] proposed a strategy that benefit from classical CNN models pretrained with Ima-\nadoptedhybridforce/positioncontrolandpassivecompliance geNet. Carreira et al. [15] proposed the state of the art 3D\ncontrol for successful peg-in-hole assembly. convolution method, which is basically an inflated Inception\nAnother class of methods is model-based active sensing. net [16]. It requires not only the raw RGB image sequence\nBy utilizing feedback from sensors to identify the configu- but also the corresponding optical images at the input.",
      "size": 920,
      "sentences": 12
    },
    {
      "id": 14,
      "content": "del-based active sensing. net [16]. It requires not only the raw RGB image sequence\nBy utilizing feedback from sensors to identify the configu- but also the corresponding optical images at the input. This\nrations or errors in the assembly process, these methods are type of two-stream network fusion style can be naturally\nusually more adaptable to new environments. Bruyninckx et adaptedtoourtask,becausewecanobtaintheopticalimages\nal. [8] proposed a model-based method to model different easily by tracking the markers on GelSlim sensor. contactsituationsanddeployedthemodelandfeedbackfrom There are also related works in robotics that extract\na force sensor to explicitly find the hole and align the axes useful information from sequential image signals with deep\nof the peg with the hole. Kim [4] developed an insertion learning.Nguyenetal.",
      "size": 844,
      "sentences": 8
    },
    {
      "id": 15,
      "content": "a force sensor to explicitly find the hole and align the axes useful information from sequential image signals with deep\nof the peg with the hole. Kim [4] developed an insertion learning.Nguyenetal. [17]proposedanewmethodtotrans-\nalgorithm according to the quasi-static analysis of normal late videos to commands in robotic manipulations, enabling\n=== 페이지 3 ===\nrobots to perform manipulation tasks by watching human\nbehaviors. Finn et al. [18] developed a video prediction\nmodelthatpredictedthepixelmotion(objectmotion).Based\non the actions and the initial frames of the robot, the model\npredicts the next several frames in a robot pushing task. The architecture of the model belongs to the family of\nCNN+LSTM. Lee et al. [19] designed a convolutional and\ntemporal model that allowed robots to learn new activities\nfrom unlabeled example videos of humans. The basic idea\nwas to learn the temporal structure of human activity and\nthen apply to the motor executions of robots.",
      "size": 975,
      "sentences": 9
    },
    {
      "id": 16,
      "content": "allowed robots to learn new activities\nfrom unlabeled example videos of humans. The basic idea\nwas to learn the temporal structure of human activity and\nthen apply to the motor executions of robots. Several prior works use video signals from vision-based\ntactile sensors to extract physical properties or motion in-\nformation. Yuan et al. [20] demonstrated that the GelSight Fig. 2. Example image sequence of a cylindrical object in the collision\ntactilesensorcouldestimatethehardnessofobjects,sincethe process,capturedbytheGelSlimsensor.Toprows:rawimages(frame1,6\nand8).Bottomrow:differencesbetweenthecorrespondingrawimageand\nsensor generated different image sequences when contacting\nframe1. with soft and hard objects. They trained a CNN+LSTM\nmodel to learn the hardness of objects directly from the\nimage sequences in the contact period.",
      "size": 841,
      "sentences": 9
    },
    {
      "id": 17,
      "content": "d different image sequences when contacting\nframe1. with soft and hard objects. They trained a CNN+LSTM\nmodel to learn the hardness of objects directly from the\nimage sequences in the contact period. They also applied a\ncontact event is correlated with the contact formation, and\nsimilar principle to infer the physical properties of different\nthe relative position and orientation between object and gap. clothes [21]. Li et al. [22] trained a neural network with\nimage sequences from GelSight and an external camera GelSlim sensor To detect the motion of the object during\nto detect slip. Zhang et al. [23] performed an analogous contact, we use the GelSlim [2] vision-based tactile sensor\nexperiment with a different tactile sensor. The slip detection (Fig. 1). The contact surface of the sensor is a piece of soft\ntaskwithtactilesensorsissimilartoourtasktosomeextent, elastomericgel,coveredwithatexturedwear-resistantcloth.",
      "size": 927,
      "sentences": 12
    },
    {
      "id": 18,
      "content": "sensor. The slip detection (Fig. 1). The contact surface of the sensor is a piece of soft\ntaskwithtactilesensorsissimilartoourtasktosomeextent, elastomericgel,coveredwithatexturedwear-resistantcloth. because both require tracking local motion of the object in Black dots that move with applied shear force are uniformly\nhand. markedonthegel.Acameracapturesthedeformationofthe\ngelsurface,aswellasthemotionofthemarkers.Force/strain\nIII. BOX-PACKINGTASK\ninformationcanbeestimatedfromthemotionofthemarkers\non the gel surface [24]. The soft sensor surface exhibits\nTask Description We perform the dense packing task under\ncompliance, enabling the sensor to detect the 3D motion of\nposition uncertainties. We assume the position of the gap\nthe object in-hand and slip [25]. The motion of the object\nhas been roughly estimated by a vision system and the\nduringcontactisencodedinthesequenceofimagescaptured\ninitial position of the object to pack is known. The packing\nby the sensor, shown in Fig. 2.",
      "size": 991,
      "sentences": 12
    },
    {
      "id": 19,
      "content": "has been roughly estimated by a vision system and the\nduringcontactisencodedinthesequenceofimagescaptured\ninitial position of the object to pack is known. The packing\nby the sensor, shown in Fig. 2. To estimate the relative\nenvironment is shown in Fig. 6. We introduce controlled\npositionoftheobjectandtheholefromtheimagesequence,\nposition errors in x direction (translation) and in yaw (ro-\nwe use a deep learning network. tation). The range of translation errors and rotation errors\nare −30%∼30% of the object’s width and −15◦ ∼−15◦,\nClassification of error directionsYuetal. [26]categorized\nrespectively. The assembly clearance is about 2 mm. The\nthe contact formation of a rectangular prism with a parallel\ntwo objects on the side of the hole are not fixed in test\ngap as in our case, into 8 classes, according to which edge\nexperiments.Weassumenopriorknowledgeofthegeometry\nof the object was contacted with the environment.",
      "size": 928,
      "sentences": 11
    },
    {
      "id": 20,
      "content": "side of the hole are not fixed in test\ngap as in our case, into 8 classes, according to which edge\nexperiments.Weassumenopriorknowledgeofthegeometry\nof the object was contacted with the environment. Because\nofthetargetobject.Weperformthetaskonaseriesofobjects\nwe are aimed at generalizing the method to objects with\nwith different shapes. differentshapesandestimatingthedirectionoftheerror,here\nPerformance Criteria We ask the insertion algorithm three we categorize the contact situations into 8 classes according\ngoals: 1) the target object is firmly grasped in the gripper to different combinations of the directions of two errors (x\nin the insertion process. 2) the robot does not change the and θ).",
      "size": 703,
      "sentences": 4
    },
    {
      "id": 21,
      "content": "according\ngoals: 1) the target object is firmly grasped in the gripper to different combinations of the directions of two errors (x\nin the insertion process. 2) the robot does not change the and θ). We set T θ =5◦ as the threshold of rotation errors\npositionsofthesurroundingtwoobjects.3)therobotisable and T x =2.5 mm as the threshold of translation errors in\nto correct the position error within several trials for realistic our experiment, as shown by the red and green dash lines in\napplications. Fig.3.Forexample,theregionintheupperleftcorner(class\n3)representsthetranslationerroralong−xdirectionandthe\nIV. METHOD rotationerroralong+θ direction.Theneighboringregionon\nHumans can roughly estimate a correction signal in a therightreferstoonlytherotationerroralong+θ direction. blind insertion process, especially if the positional error is Our hypothesis isthat different error directionswill result in\nrelatively small.",
      "size": 924,
      "sentences": 6
    },
    {
      "id": 22,
      "content": "therightreferstoonlytherotationerroralong+θ direction. blind insertion process, especially if the positional error is Our hypothesis isthat different error directionswill result in\nrelatively small. The tactile sensors in our compliant fingers distinguishable tactile imprints. can detect the small motion of the object in-hand when the When the object held by two GelSlim fingers collides\nin-handobjectcollideswiththesurroundingobjects.Thekey with the surrounding objects, it rotates along the edge of the\nideaweexploitisthatthetactilesignalgeneratedduringthat environment object. If there is only a translation error, the\n=== 페이지 4 ===\nnetworks to classify the error directions and to regress the\nerror magnitude, using a Convolutional Neural Networks\n(CNN) + Recurrent Neural Networks (RNN) architecture\n(Fig. 4). The CNN with Alexnet [27] architecture (the last\nlayer removed) extracts 4096 features from each image.",
      "size": 920,
      "sentences": 7
    },
    {
      "id": 23,
      "content": "a Convolutional Neural Networks\n(CNN) + Recurrent Neural Networks (RNN) architecture\n(Fig. 4). The CNN with Alexnet [27] architecture (the last\nlayer removed) extracts 4096 features from each image. The input data is two streams of images from two GelSlim\nsensors.Becausethecontactperiodisveryshort,weselect8\nframes starting from the frame that first captures the motion\nof the object. We pair the 16 images into 8 pairs by the\nsequence order. The two lists of features from each pair\nFig.3. (1)8regionscategorizedbytheerrordirectionintheerrorspace. of images are concatenated into a list of 8192 features. (2)Schematicillustrationoftheerrormagnitudeestimation.Thelengthof\nAfterward, the 8 lists of features are processed by an RNN\nthearrowrepresentsthemagnitudeoftheerror.Thedirectionofthearrow\nalignswiththeerrordirection.",
      "size": 824,
      "sentences": 9
    },
    {
      "id": 24,
      "content": "rationoftheerrormagnitudeestimation.Thelengthof\nAfterward, the 8 lists of features are processed by an RNN\nthearrowrepresentsthemagnitudeoftheerror.Thedirectionofthearrow\nalignswiththeerrordirection. model (LSTM) [28] with 1 hidden layer and 170 hidden\nunits.TheoutputofDirectionNNisthenumberofregions\ninerrorspace,andtheoutputofMagnitudeNNisestimated\nerrorsinx(∆x )andθ (∆θ ).Thoughthearchitecturesofthe\ne e\ntwo networks are the same, they are trained independently. Control strategy Based on the estimation of the error\nmagnitude and direction, we propose an heuristic controller\ntocorrectthepositionerror.Theclassnumberobtainedfrom\nthe DirectionNN is converted to the signs of the errors\nin x (S ) and θ (S ). S and S can take a value of 1,\nx θ x θ\n0, or -1 with -1 and 1 indicating errors in the positive or\nnegative direction and 0 indicating no error. The outputs of\nthe controller areC andC , which are the corrections in x\nx θ\nFig. 4. Architecture of the CNN+LSTM neural network.",
      "size": 987,
      "sentences": 7
    },
    {
      "id": 25,
      "content": "rors in the positive or\nnegative direction and 0 indicating no error. The outputs of\nthe controller areC andC , which are the corrections in x\nx θ\nFig. 4. Architecture of the CNN+LSTM neural network. The image and θ directions. sequences from two GelSlim sensors are processed by the Alexnet and We formulate the control strategy with Eq. 1. When\nthen concatenated. The output features from Alexnet feed the LSTM for\nthe estimations from the two models are consistent, the\npredictingtheerrorsoftheinsertionposition. corrections are directly -∆x and -∆θ , shrunken by a factor\ne e\n0.7, which experimentally helps avoid overshooting. When\nrotation of the object is parallel to the gel surface, resulting the DirectionNN indicates no error in the direction, we\nin marker motions on the sensor surface. However, if a further decrease the factor to 0.3.",
      "size": 848,
      "sentences": 12
    },
    {
      "id": 26,
      "content": "the object is parallel to the gel surface, resulting the DirectionNN indicates no error in the direction, we\nin marker motions on the sensor surface. However, if a further decrease the factor to 0.3. We observe from experi-\nrotation error occurs, the rotation direction of the object can ments that estimating the error direction is easier and more\nbedecomposedintothedirectionsparallelandperpendicular robust than estimating the error magnitude. Therefore, we\nto the gel surface. It results in both marker motions and trust the DirectionNN prediction if the two models are\nnormalpressurechangesonthegelsurface.Thisfactlaysthe contradictory. In this case, a constant step (3 mm or 3◦) is\nfoundation of distinguishing error directions from the image chosen for C x and C θ . To prevent overshooting after one\nsequences with a neural network.",
      "size": 840,
      "sentences": 7
    },
    {
      "id": 27,
      "content": "In this case, a constant step (3 mm or 3◦) is\nfoundation of distinguishing error directions from the image chosen for C x and C θ . To prevent overshooting after one\nsequences with a neural network. Furthermore, after obtain- trial, we clip the magnitude ofC x andC θ at 4 mm and 4◦for\ning the error signal, we can reduce the error by moving the later trials. We summarize the workflow of the box-packing\nobject in the direction that negates the error gradually. Since approach in Figure 5.\ntheoutputoftheclassifieraretheprobabilitiesofeachclass, \n−0.7×∆x if S ×∆x >0\nit interpreted the confidence of the neural network in the  e x e\ncurrent state. We call this neural network DirectionNN. C x = −0.3×∆x e if S x =0\n\n−3.0×S if S ×∆x <0\nx x e\nRegressionoferrormagnitudeInsertingtheobjectintothe (1)\n\nhole with a minimal amount of trials requires more than the  −0.7×∆θ e if S θ ×∆θ e >0\ndirection of the error.",
      "size": 917,
      "sentences": 7
    },
    {
      "id": 28,
      "content": "\n−3.0×S if S ×∆x <0\nx x e\nRegressionoferrormagnitudeInsertingtheobjectintothe (1)\n\nhole with a minimal amount of trials requires more than the  −0.7×∆θ e if S θ ×∆θ e >0\ndirection of the error. We experimentally observe that errors C = −0.3×∆θ if S =0\nθ e θ\nwith different magnitudes but same direction also produce \n−3.0×S if S ×∆θ <0\nθ θ e\ndistinctive image sequences. Therefore, we train another\nneural network to estimate the error magnitude. Regression\nV. EXPERIMENTS\nwithaneuralnetworkismoredifficultthanclassification,but In this section, we explain the details of the experimental\neven a rough estimate of the error is helpful for efficiency. setup, data collection process, parameter tuning for model\nWe name this neural network MagnitudeNN. We combine training and the test experiments for evaluating the system. the outputs of two independent models, which can boost the\nA. Experimental setup\nperformance and robustness of the model. The experimental setup shown in Fig.",
      "size": 988,
      "sentences": 9
    },
    {
      "id": 29,
      "content": "t experiments for evaluating the system. the outputs of two independent models, which can boost the\nA. Experimental setup\nperformance and robustness of the model. The experimental setup shown in Fig. 6 is a simplified\nModel As discussed above, we train two independent neural version of the grasping system developed in [29]. The\n=== 페이지 5 ===\nFig.5. Workflowofourcontrolstrategy.Therobotstartstheinsertionandifsuccessful,theprocessisdone.Iftheobjectcollideswiththeenvironment,\nthemotionoftheobjectinthecontactperiodiscapturedbytheimagesequencesofGelSlimsensors.Theimagesequencesareusedtotraintwoneural\nnetworks.TheclassoferrorisestimatedbyDirectionNNandthemagnitudeoftheerrorispredictedbyMagnitudeNN.Theoutputfromtheneural\nnetworksispassedtoaheuristiccontroller.Thecontrollerdecidesadjustmentoftheinsertionposition.Ifatthenewpositiontheinsertionissuccessful,\ntheprocessends.Otherwise,thenewerrorisagaindetectedbyGelSlimsensors.Theloopsrununtilsuccess.",
      "size": 952,
      "sentences": 7
    },
    {
      "id": 30,
      "content": "ontroller.Thecontrollerdecidesadjustmentoftheinsertionposition.Ifatthenewpositiontheinsertionissuccessful,\ntheprocessends.Otherwise,thenewerrorisagaindetectedbyGelSlimsensors.Theloopsrununtilsuccess. the slip detection function [25] of GelSlim sensor monitors\nslip. If the slip detection is triggered earlier than expected,\nthe object is blocked by the surrounding objects. In this\ncase, we log the last 30 frames of GelSlim images and the\ncorrespondingpositionerror.Otherwise,therobotinsertsthe\nobject successfully, and we do not need to log the data. The robot repeats the process described above with different\nerrors until enough data is acquired. To better observe the\nmotionoftheobject,wesetthegraspingforceslightlyhigher\nthan the minimum force required. The pose of the object in\nhandchangesafterseveralpokes.Toacquireconsistentdata,\nthe robot puts the object back to the fixture to correct the\nFig. 6. Experimental setup.",
      "size": 929,
      "sentences": 9
    },
    {
      "id": 31,
      "content": "an the minimum force required. The pose of the object in\nhandchangesafterseveralpokes.Toacquireconsistentdata,\nthe robot puts the object back to the fixture to correct the\nFig. 6. Experimental setup. The system is composed of two GelSlim poseafterevery5trials.Wecollectabout7500dataforeach\nsensors,4fixtures(onlyoneisshown)foradjustingtheposeoftheobject object and about 30000 data in total. inthegripper,twoenvironmentobjectsand4objectsfortrainingtheneural\nnetworks. C. Model Training\nData preprocess Since circle and hexagon objects do not\nsystemincludesa6-DOFABB-1600robotarm,twoGelSlim\nhaveclass7and8errors(onlyrotationerrorsandnotransla-\nsensors attached to a WSG-50 parallel gripper. The packing\ntionerrors),thedatabecomesunbalanced.Wemanuallydou-\nenvironment is designed as two 3D printed rigid blocks (top\nble the entire data of class 7 and 8 with data augmentations\nsurface: 45×155 mm) with 56 mm gap between them. To\n(random cropping).",
      "size": 945,
      "sentences": 9
    },
    {
      "id": 32,
      "content": "llydou-\nenvironment is designed as two 3D printed rigid blocks (top\nble the entire data of class 7 and 8 with data augmentations\nsurface: 45×155 mm) with 56 mm gap between them. To\n(random cropping). Because Alexnet is pretrained for object\nensurethatthedatalabellingisaccurate,thetwoenvironment\nrecognition, the absolute location of the tactile imprints in\nobjects are fixed in the data collection process. In the test\nthesensorimageisneglected.However,therelativeposition\nexperiments, the two objects are free to move. information for us is crucial. Therefore, instead of using\nWe 3D print 4 target objects of different shapes. The base\nthe raw images, we use image differences as the input data\nshapesareacircle(Ø=25.5mm),arectangle(51×80mm)\n(example in Fig. 2 second row)\n, an ellipse (a =105 mm, b =25.5 mm) and a hexagon\n(R=25.5 mm). We prepare a fixture to precisely relocate\nTraining specifics For faster training, we freeze the convo-\neach object after a few experiments. Fig.",
      "size": 985,
      "sentences": 10
    },
    {
      "id": 33,
      "content": "se (a =105 mm, b =25.5 mm) and a hexagon\n(R=25.5 mm). We prepare a fixture to precisely relocate\nTraining specifics For faster training, we freeze the convo-\neach object after a few experiments. Fig. 6 shows the fixture\nlutionallayers ofAlexnet andtrain asmallerneural network\nfor the ellipse object. with 2 fully connected layers of CNN and LSTM. We use\nB. Data Collection ADAM optimizer [30], NVIDIA Titan X Pascal GPU and\nPytorch package for training. We use a self-supervised scheme to collect and label the\npacking data for a set of training objects under controlled D. Test experiment\npositional errors. We first sample translation and rotation\nerrors uniformly from −15 mm < x < 15 mm, −15◦ < TestwithtrainingobjectsWefirsttestthesystemwiththe4\nθ < 15◦. As the position of the gap is known, we can training objects to evaluate whether the neural networks can\nintroduce controlled error by adding the position error to learnusefulfeatures.Thetwoenvironmentobjectswitha56\nthe gap position.",
      "size": 994,
      "sentences": 10
    },
    {
      "id": 34,
      "content": "n, we can training objects to evaluate whether the neural networks can\nintroduce controlled error by adding the position error to learnusefulfeatures.Thetwoenvironmentobjectswitha56\nthe gap position. The robot moves down vertically while mmgaparereleasedandfreetomoveorfallduringtest.We\n=== 페이지 6 ===\ntest 26 packing attempts with random positional errors. To\ntest the method in the extreme error conditions, we also test\nthe four extreme error conditions where ∆x and ∆θ saturate\nin both positive and negative directions. In the experiment, the robot picks the target object from\na known position and moved it to a noisy position above\nthe gap. It vertically inserts the object and stops if incipient\nslip is detected. If the object is blocked by the environment\nobjects, the GelSlims signals during the contact period will\nbe fed to the two neural networks and then the estimation\nof the errors from the neural networks will be sent to the\ncontroller.",
      "size": 953,
      "sentences": 6
    },
    {
      "id": 35,
      "content": "vironment\nobjects, the GelSlims signals during the contact period will\nbe fed to the two neural networks and then the estimation\nof the errors from the neural networks will be sent to the\ncontroller. The robot adjusts the pose according to the C\nx\nand C produced by the controller and start the next trial. θ\nThe process repeats until the object is successfully inserted\nor it stops after 15 trials. We record the number of trials\nand perturbations to surrounding objects to evaluate the\nperformance. Test with new objects We further test the system with\n4 daily objects that the neural networks have never seen. The objects (Table I) have different base shapes, weights, Fig.7. ConfusionmatrixoftheDirectionNNfortrainingobjects.The\ndiagonalelementsrepresenttheclassificationaccuracyforeachclass.The\nand widths. The vitamin bottle is cylindrical, similar to the\noff-diagonalelementsaretheerrorratesofclassificationstoawrongclass. cylinder object in the training.",
      "size": 962,
      "sentences": 9
    },
    {
      "id": 36,
      "content": "assificationaccuracyforeachclass.The\nand widths. The vitamin bottle is cylindrical, similar to the\noff-diagonalelementsaretheerrorratesofclassificationstoawrongclass. cylinder object in the training. However, the bottle cap part\nthe robot grasps in the experiment has sharp textures. The\nradiusisalsosmallerthanthatofthebase.Thewhiteboxisa following the classifier, a simple controller that moves small\nrectanglebutwithdifferentsizesfromtheboxinthetraining. steps along the opposite direction can highly probably insert\nBoxpackingorstackingisverycommoninwarehouse,soit the object into the designated gap. The training loss of the\nis important that the method works for boxes with different MagnitudeNN is 10.5 and the validation loss is 14.4. The\nsizes. The mustard container and the metal can are heavy averaged error for translation error ∆x and rotation error ∆θ\nand their base shapes are rounded rectangles with different are estimated to be -1.9 mm∼1.9 mm and −1.9◦ ∼1.9◦,\ncurvatures.",
      "size": 989,
      "sentences": 9
    },
    {
      "id": 37,
      "content": "can are heavy averaged error for translation error ∆x and rotation error ∆θ\nand their base shapes are rounded rectangles with different are estimated to be -1.9 mm∼1.9 mm and −1.9◦ ∼1.9◦,\ncurvatures. They all have flat bottoms because the method assuming the two errors contribute equally to the mean\nwe propose only considers the contact problem in 2D space. squareloss.Thoughtheerrormagnitudeestimationperforms\nTakingintoaccountthedifferentsizesofthe4dailyobjects, better than expected, we still use a discount factor in the\nwe change the width of the gap to keep the clearance to be controller to avoid overshooting. 2mm when testing each object. The range of the translation\nerror x is also adapted to the specific object, from -30% to B. Experimental results with training objects\n30% of the width of the object.",
      "size": 817,
      "sentences": 6
    },
    {
      "id": 38,
      "content": ". 2mm when testing each object. The range of the translation\nerror x is also adapted to the specific object, from -30% to B. Experimental results with training objects\n30% of the width of the object. The rest of the experiment\nIn 120 tests of the 4 training objects, the objects were\nis the same as the experiment of training objects described\ninserted successfully 100% within 15 trials. The two sur-\nabove. rounding objects were stable during the test. We plot the\nnumberoftrialsvs.errorxanderrorθ foreachtestinFig.8. VI. RESULTS\nThe colors of the points represent the number of trials. We\nA. Model accuracy\nsummarize the successful rate, mean and max number of\nThetrainingaccuracyoftheDirectionNNis80.5%and trials for each object in Table I.\nvalidation accuracy is 74.4%. The relatively similar values The rectangle object is the most difficult case, because\noftrainingandvalidationaccuraciesindicatethatthesystem even a slight rotation error will result in a collision.",
      "size": 973,
      "sentences": 13
    },
    {
      "id": 39,
      "content": "The relatively similar values The rectangle object is the most difficult case, because\noftrainingandvalidationaccuraciesindicatethatthesystem even a slight rotation error will result in a collision. The\ndoesnotoverfit.TheconfusionmatrixinFig.7demonstrates average number of trials of inserting the rectangle object is\nthe effectiveness of the trained classifier. The large diagonal 4.7,twicehigherthanthatoftheother3objects.Inaddition,\nvalues confirm the high accuracy. The off-diagonal elements from Fig. 8, almost all tests that require more trials fall in\nalso give hints about the types of misclassification. For the region of large rotation error θ. It is due to the fact that\nexample, the top row indicates that the model is confused a large rotation error reduces the object motion parallel with\nby class 3 and 4 when the truth is class 1.",
      "size": 846,
      "sentences": 7
    },
    {
      "id": 40,
      "content": "rror θ. It is due to the fact that\nexample, the top row indicates that the model is confused a large rotation error reduces the object motion parallel with\nby class 3 and 4 when the truth is class 1. It makes sense the sensor plane that is easy to capture, and increases the\nthat they all have the -x error, lying on the left-hand side motion perpendicular to the sensor surface that is hard to\nof the error space (see Fig. 3). Similarly, the bottom row capture. shows that it is more difficult for the model to distinguish These results demonstrate that the two neural networks\nclass8fromclass3andclass5,becausetheyallhaveerrors andthesimplecontrolstrategyperformwellonthe4training\nin +θ direction. The model can always distinguish -x from objects. The 2D textures of the 4 objects are very different,\n+x directions and it is similar for the rotation errors.",
      "size": 859,
      "sentences": 8
    },
    {
      "id": 41,
      "content": "formwellonthe4training\nin +θ direction. The model can always distinguish -x from objects. The 2D textures of the 4 objects are very different,\n+x directions and it is similar for the rotation errors. By but the neural network can extract useful information for\n=== 페이지 7 ===\nTABLEI\nSUCCESSRATE,MEANANDMAXNUMBEROFTRIALSOFTHETESTSFOR4TRAININGOBJECTSAND4NEWOBJECTS\nRectangle Circle Ellipse Hexagon Whitebox Vitaminbottle Metalcan Mustardcontainer\nSuccessRate 100.0% 100% 100.0% 100.0% 93.3% 96.7% 93.3% 100%\nMeannumberoftrials 4.7 2.97 2.77 2.4 6.53 2.97 7.4 3.6\nMaxnumberoftrials 11 5 8 6 >15 >15 >15 9\nFig. 9. Number of trials before a successful insertion for the 4 new\nFig. 8. Number of trials before a successful insertion for the 4 training\nobjects with different initial position errors. The rotation error θ changes objects with different initial position errors. The rotation error θ changes\nfrom-15◦to15◦andthetranslationerrorxvariesfrom-15mmto15mm.",
      "size": 956,
      "sentences": 10
    },
    {
      "id": 42,
      "content": "different initial position errors. The rotation error θ changes objects with different initial position errors. The rotation error θ changes\nfrom-15◦to15◦andthetranslationerrorxvariesfrom-15mmto15mm. from-15◦to15◦andthetranslationerrorxvariesfrom-15mmto15mm. The color of the points corresponds to the number of trials. The dark red\nThecolorofthepointscorrespondstothenumberoftrials. crossesindicatefailedinsertionsafter15trials. the insertion task. Though the accuracy of a single neural\nmean number of trials of the trained rectangle object, the\nnetwork is not high, the complimentary contributions of the\nrobot needs 2 or 3 more trials to insert the two new objects. two neural networks and the control strategy significantly\nIfweremovethe2failurecases,theyonlyrequireonemore\nbenefit the decision making. By estimating the error magni-\ntrial than rounded shapes.",
      "size": 865,
      "sentences": 11
    },
    {
      "id": 43,
      "content": "ts. two neural networks and the control strategy significantly\nIfweremovethe2failurecases,theyonlyrequireonemore\nbenefit the decision making. By estimating the error magni-\ntrial than rounded shapes. All of the failure cases happened\ntude, MagnitudeNN reduces the number of trials when the\nunderbiggerrotationerrorandtranslationerror.Anincorrect\nerror is large, as shown by the large error regions in Fig. 8.\nestimation of the error direction in the first trial makes the\nC. Experiment results with new objects controllerdivergeandittakesseveralattemptstorecoverand\ncapture useful information to locate the gap. We also achieved a high successful rate for the 4 new\nOverall, our method can be directly generalized to the\nobjects. In 30 tests of each object, the mustard container\ninsertiontaskofobjectswithroundedbaseshape.Forobjects\nhas 100% success rate.",
      "size": 856,
      "sentences": 7
    },
    {
      "id": 44,
      "content": "he 4 new\nOverall, our method can be directly generalized to the\nobjects. In 30 tests of each object, the mustard container\ninsertiontaskofobjectswithroundedbaseshape.Forobjects\nhas 100% success rate. There is only 1 failure case for the\nwith rectangle base shape, our method performs a little bit\nVitamin bottle and 2 failure cases for the other 2 objects. worse than the result of the trained object. Considering all\nWe plot the number of trials at each error point in Fig. 9\ntested new objects, it takes averagely 6 trials to insert the\nand summarize the successful rate, mean and max number\nobject into the gap successfully. of trials in Tabel I. The dark red crosses in Fig. 9 represent\nthefailurecases,whichalwayshappenintheregionoflarge\nVII. DISCUSSION\nrotation or translation errors. The number of trials of the\nfailures is set to 16 for calculating the mean.",
      "size": 866,
      "sentences": 11
    },
    {
      "id": 45,
      "content": "ses in Fig. 9 represent\nthefailurecases,whichalwayshappenintheregionoflarge\nVII. DISCUSSION\nrotation or translation errors. The number of trials of the\nfailures is set to 16 for calculating the mean. In this paper, we propose a box-packing strategy with a\nThe vitamin bottle and the mustard container both have vision-based high-resolution tactile sensor GelSlim [2]. The\nrounded edges and require only a few trials (averagely 2.97 contributionsoftheworkare:1)bydetectingincipientslipin\nand3.6).Thenumbersaresimilartotheaveragenumbersof the packing process with GelSlim, the robot can avoid hard\ntrials of the circle, ellipse and hexagon objects, confirming collision with the environment objects. It not only protects\nthe adaptability of the neural networks to new objects with theitemstopackbutalsomaintainsthestabilityofthegrasp\nrounded base shapes. The base shapes of the metal can and andthatoftheenvironmentobjects,enablingfuturetrials.2)\nthe white box are close to a rectangle.",
      "size": 984,
      "sentences": 8
    },
    {
      "id": 46,
      "content": "mstopackbutalsomaintainsthestabilityofthegrasp\nrounded base shapes. The base shapes of the metal can and andthatoftheenvironmentobjects,enablingfuturetrials.2)\nthe white box are close to a rectangle. Comparing to the TheimagesequencesacquiredbyGelSlimduringthecontact\n[표 데이터 감지됨]\n\n=== 페이지 8 ===\nperiodareusedtoestimateboththedirectionandmagnitude [9] W.S.Newman,Y.Zhao,andY.-H.Pao,“Interpretationofforceand\nof the position error. Based on the estimation, the heuristic momentsignalsforcompliantpeg-in-holeassembly,”inProceedings\n2001ICRA.IEEEICRA(Cat.No.01CH37164),vol.1. IEEE,2001,\ncontrolstrategyenablestherobottoinsertnewobjectswithin\npp.571–576. a few trials. The averaged number of trials for new objects [10] V. Gullapalli, A. G. Barto, and R. A. Grupen, “Learning admittance\nis comparable to that of trained objects. mappingsforforce-guidedassembly,”inProceedingsofthe1994IEEE\nInternationalConferenceonRoboticsandAutomation.",
      "size": 931,
      "sentences": 8
    },
    {
      "id": 47,
      "content": "G. Barto, and R. A. Grupen, “Learning admittance\nis comparable to that of trained objects. mappingsforforce-guidedassembly,”inProceedingsofthe1994IEEE\nInternationalConferenceonRoboticsandAutomation. IEEE,1994,\nHere we briefly comment on the limitations of the pro-\npp.2633–2638. posed strategy and future research directions: [11] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training\nof deep visuomotor policies,” The Journal of Machine Learning\n• Thecontrol strategyisbasedonahand-craftedheuris-\nResearch,vol.17,no.1,pp.1334–1373,2016. tic that combines the learned estimations of the error [12] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei,\ndirection and magnitude. It does not take into account A. Garg, and J. Bohg, “Making sense of vision and touch: Self-\nsupervised learning of multimodal representations for contact-rich\nthe sequence of past actions and observations.",
      "size": 911,
      "sentences": 6
    },
    {
      "id": 48,
      "content": "not take into account A. Garg, and J. Bohg, “Making sense of vision and touch: Self-\nsupervised learning of multimodal representations for contact-rich\nthe sequence of past actions and observations. Due\ntasks,”arXivpreprintarXiv:1810.10191,2018.\nto the lack of full observability of the contact state [13] S. R. Chhatpar and M. S. Branicky, “Search strategies for peg-in-\nbetween the object and the packed box, a model-based hole assemblies with position uncertainty,” in Intelligent Robots and\nSystems,2001.Proceedings.2001IEEE/RSJInternationalConference\nreinforcement learning (RL) framework could come\non,vol.3. IEEE,2001,pp.1465–1470. up with a more efficient packing policy with better [14] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach,\ngeneralization.",
      "size": 770,
      "sentences": 4
    },
    {
      "id": 49,
      "content": "earning (RL) framework could come\non,vol.3. IEEE,2001,pp.1465–1470. up with a more efficient packing policy with better [14] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach,\ngeneralization. S. Venugopalan, K. Saenko, and T. Darrell, “Long-term recurrent\nconvolutional networks for visual recognition and description,” in\n• The dimension of the error space is small: translation\nProceedingsoftheIEEEconferenceonCVPR,2015,pp.2625–2634. errors in axis x and rotation errors in yaw θ. A more [15] J.CarreiraandA.Zisserman,“Quovadis,actionrecognition?anew\nadvanced packing policy would also consider tilting modelandthekineticsdataset,”inproceedingsoftheIEEEConference\nonCVPR,2017,pp.6299–6308. the object, which increases the dimension of the ac-\n[16] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\ntion space, but can also provide natural robustness to D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with\npacking.",
      "size": 946,
      "sentences": 7
    },
    {
      "id": 50,
      "content": "e ac-\n[16] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\ntion space, but can also provide natural robustness to D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with\npacking. convolutions,”inProceedingsoftheIEEEconferenceonCVPR,2015,\npp.1–9. • Therepresentationofthealignmenterrorisanotdirect\n[17] A. Nguyen, D. Kanoulas, L. Muratore, D. G. Caldwell, and N. G.\nobservation, but something that we have to estimate. It Tsagarakis,“Translatingvideostocommandsforroboticmanipulation\nwouldbeinterestingtocomparethismethodtoapurely with deep recurrent neural networks,” in 2018 IEEE ICRA. IEEE,\n2018,pp.1–9. RLcontrollerthatis trained directlyontactileimprints. [18] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for\n• The objects all have a flat base and all rigid. Exploring physicalinteractionthroughvideoprediction,”inAdvancesinneural\ngeneralization to a larger set of objects is important. informationprocessingsystems,2016,pp.64–72.",
      "size": 973,
      "sentences": 9
    },
    {
      "id": 51,
      "content": "a flat base and all rigid. Exploring physicalinteractionthroughvideoprediction,”inAdvancesinneural\ngeneralization to a larger set of objects is important. informationprocessingsystems,2016,pp.64–72. [19] J. Lee and M. S. Ryoo, “Learning robot activities from first-person\nThe proposed box-packing strategy can be further im-\nhuman videos using convolutional future regression,” in Proceedings\nproved in several ways, but provides a highly flexible oftheIEEEConferenceonCVPRWorkshops,2017,pp.1–2. baseline that can be applied, for example, on warehouse [20] W. Yuan, C. Zhu, A. Owens, M. A. Srinivasan, and E. H. Adelson,\n“Shape-independent hardness estimation using deep learning and a\nautomation. gelsighttactilesensor,”in2017IEEEICRA. IEEE,2017,pp.951–\n958. REFERENCES [21] W.Yuan,Y.Mo,S.Wang,andE.H.Adelson,“Activeclothingmaterial\nperception using tactile sensing and deep learning,” in 2018 IEEE\n[1] N.Correll,K.E.Bekris,D.Berenson,O.Brock,A.Causo,K.Hauser,\nICRA. IEEE,2018,pp.1–8.",
      "size": 985,
      "sentences": 9
    },
    {
      "id": 52,
      "content": "o,S.Wang,andE.H.Adelson,“Activeclothingmaterial\nperception using tactile sensing and deep learning,” in 2018 IEEE\n[1] N.Correll,K.E.Bekris,D.Berenson,O.Brock,A.Causo,K.Hauser,\nICRA. IEEE,2018,pp.1–8. K.Okada,A.Rodriguez,J.M.Romano,andP.R.Wurman,“Analysis\n[22] J.Li,S.Dong,andE.Adelson,“Slipdetectionwithcombinedtactile\nand observations from the first amazon picking challenge,” IEEE\nandvisualinformation,”inICRA. IEEE/RSJ,2018. TransactionsonAutomationScienceandEngineering,vol.15,no.1,\n[23] Y.Zhang,Z.Kan,Y.A.Tse,Y.Yang,andM.Y.Wang,“Fingervision\npp.172–188,2018. tactile sensor design and slip detection using convolutional lstm\n[2] E. Donlon, S. Dong, M. Liu, J. Li, E. Adelson, and A. Rodriguez,\nnetwork,”arXivpreprintarXiv:1810.02653,2018. “Gelslim: A high-resolution, compact, robust, and calibrated tactile-\n[24] D. Ma, E. Donlon, S. Dong, and A. Rodriguez, “Dense tactile force\nsensingfinger,”in2018IEEE/RSJIROS. IEEE,2018,pp.1927–1934.",
      "size": 943,
      "sentences": 8
    },
    {
      "id": 53,
      "content": "“Gelslim: A high-resolution, compact, robust, and calibrated tactile-\n[24] D. Ma, E. Donlon, S. Dong, and A. Rodriguez, “Dense tactile force\nsensingfinger,”in2018IEEE/RSJIROS. IEEE,2018,pp.1927–1934. distributionestimationusinggelslimandinversefem,”inIEEEICRA,\n[3] H. Park, J.-H. Bae, J.-H. Park, M.-H. Baeg, and J. Park, “Intuitive\n2018.\npeg-in-holeassemblystrategywithacompliantmanipulator,”inIEEE\n[25] S.Dong,D.Ma,E.Donlon,andA.Rodriguez,“Maintaininggrasps\nISR2013. IEEE,2013,pp.1–5. within slipping bound by monitoring incipient slip,” in IEEE ICRA,\n[4] I.-W.Kim,D.-J.Lim,andK.-I.Kim,“Activepeg-in-holeofchamfer-\n2018.\nlesspartsusingforce/momentsensor,”inProceedings1999IEEE/RSJ\n[26] K.-T.YuandA.Rodriguez,“Realtimestateestimationwithtactileand\nInternational Conference on Intelligent Robots and Systems. Human\nvisualsensingforinsertingasuction-heldobject,”in2018IEEE/RSJ\nand Environment Friendly Robots with High Intelligence and Emo-\nIROS. IEEE,2018,pp.1628–1635. tional Quotients (Cat. No.",
      "size": 996,
      "sentences": 10
    },
    {
      "id": 54,
      "content": "nd Systems. Human\nvisualsensingforinsertingasuction-heldobject,”in2018IEEE/RSJ\nand Environment Friendly Robots with High Intelligence and Emo-\nIROS. IEEE,2018,pp.1628–1635. tional Quotients (Cat. No. 99CH36289), vol. 2. IEEE, 1999, pp. [27] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassification\n948–953. with deep convolutional neural networks,” in Advances in neural\n[5] S. H. Drake, “Using compliance in lieu of sensory feedback for informationprocessingsystems,2012,pp.1097–1105. automatic assembly.” Ph.D. dissertation, Massachusetts Institute of [28] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,”Neural\nTechnology,1978. computation,vol.9,no.8,pp.1735–1780,1997. [6] D.E.Whitney,“Quasi-staticassemblyofcompliantlysupportedrigid\n[29] A.Zeng,S.Song,K.-T.Yu,E.Donlon,F.Hogan,M.Bauza,D.Ma,\nparts,”JournalofDynamicSystems,Measurement,andControl,vol. O. Taylor, M. Liu, E. Romo, N. Fazeli, F. Alet, N. Chavan-Dafle,\n104,no.1,pp.65–77,1982.",
      "size": 953,
      "sentences": 14
    },
    {
      "id": 55,
      "content": ".Zeng,S.Song,K.-T.Yu,E.Donlon,F.Hogan,M.Bauza,D.Ma,\nparts,”JournalofDynamicSystems,Measurement,andControl,vol. O. Taylor, M. Liu, E. Romo, N. Fazeli, F. Alet, N. Chavan-Dafle,\n104,no.1,pp.65–77,1982. R. Holladay, I. Morona, P. Q. Nair, D. Green, I. Taylor, W. Liu,\n[7] R. K. Jain, S. Majumder, and A. Dutta, “Scara based peg-in-hole\nT. Funkhouser, and A. Rodriguez, “Robotic pick-and-place of novel\nassembly using compliant ipmc micro gripper,” Robotics and Au-\nobjects in clutter with multi-affordance grasping and cross-domain\ntonomousSystems,vol.61,no.3,pp.297–311,2013. imagematching,”inICRA. IEEE,2018. [8] H. Bruyninckx, S. Dutre, and J. De Schutter, “Peg-on-hole: a model [30] D.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization,”\nbased solution to peg and hole alignment,” in Proceedings of 1995 arXivpreprintarXiv:1412.6980,2014. IEEE International Conference on Robotics and Automation, vol. 2. IEEE,1995,pp.1919–1924.",
      "size": 931,
      "sentences": 10
    }
  ]
}