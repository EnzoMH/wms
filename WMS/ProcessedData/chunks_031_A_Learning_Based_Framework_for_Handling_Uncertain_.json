{
  "source": "ArXiv",
  "filename": "031_A_Learning_Based_Framework_for_Handling_Uncertain_.pdf",
  "total_chars": 20733,
  "total_chunks": 32,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nA Learning Based Framework for Handling Uncertain Lead Times in\nMulti-Product Inventory Management\nHardikMeisheri1,SomjitNath1,MayankBaranwal1,2,HarshadKhadilkar1,2\n1TCSResearch,Mumbai\n{hardik.meisheri,somjit.nath,baranwal.mayank,harshad.khadilkar}@tcs.com\n2IITBombay,Mumbai\n{mbaranwal,harshadk}@iitb.ac.in\nAbstract andHussain2016),uncertaintyinmanufacturingorprocure-\nment lead times (Dolgui et al. 2013), and poor end-to-end\nMostexistingliteratureonsupplychainandinventoryman-\nvisibility (Goh et al. 2009), i.e., unavailability of precise,\nagementconsiderstochasticdemandprocesseswithzeroor\nconstantleadtimes.Whileitistruethatincertainnichesce- real-time info regarding current stock levels to suppliers.",
      "size": 720,
      "sentences": 3
    },
    {
      "id": 2,
      "content": ", i.e., unavailability of precise,\nagementconsiderstochasticdemandprocesseswithzeroor\nconstantleadtimes.Whileitistruethatincertainnichesce- real-time info regarding current stock levels to suppliers. narios, uncertainty in lead times can be ignored, most real- Theproblembecomesparticularlycriticalwhenthereplen-\nworldscenariosexhibitstochasticityinleadtimes.Theseran- ishment strategies need to be designed concurrently for a\ndomfluctuationscanbecausedduetouncertaintyinarrival verylargenumberofproducts. of raw materials at the manufacturer’s end, delay in trans- Existing supply-chain management strategies incorpo-\nportation,anunforeseensurgeindemands,andswitchingto\nrate one or only a few factors into their decision making,\nadifferentvendor,tonameafew.Stochasticityinleadtimes\nresulting in an inefficient and sub-optimal replenishment\nis known to severely degrade the performance in an inven-\nsystem.",
      "size": 906,
      "sentences": 3
    },
    {
      "id": 3,
      "content": "their decision making,\nadifferentvendor,tonameafew.Stochasticityinleadtimes\nresulting in an inefficient and sub-optimal replenishment\nis known to severely degrade the performance in an inven-\nsystem. For instance, the economic-order-quantity model\ntory management system, and it is only fair to abridge this\nand the dynamic-economic-lotsize model, two of the most\ngap in supply chain system through a principled approach. Motivatedbytherecentlyintroduceddelay-resolveddeepQ- commonly used replenishment policies work under the as-\nlearning (DRDQN) algorithm, this paper develops a rein- sumption of deterministic demands (Zipkin 2000). Conse-\nforcementlearningbasedparadigmforhandlinguncertainty quently,arobustlinearprogrammingframeworkwasdevel-\ninleadtimes(actiondelay).Throughempiricalevaluations, oped(BertsimasandThiele2004)toaddresstheshortcom-\nitisfurthershownthattheinventorymanagementwithuncer- ings of the aforementioned policies.",
      "size": 940,
      "sentences": 4
    },
    {
      "id": 4,
      "content": "evel-\ninleadtimes(actiondelay).Throughempiricalevaluations, oped(BertsimasandThiele2004)toaddresstheshortcom-\nitisfurthershownthattheinventorymanagementwithuncer- ings of the aforementioned policies. The framework, origi-\ntainleadtimesisnotonlyequivalenttothatofdelayininfor- nallydesignedtohandlesingle-stagesingle-product,iscom-\nmationsharingacrossmultipleechelons(observationdelay),\nputationally scalable but cannot handle cross-product con-\namodeltrainedtohandleonekindofdelayiscapabletohan-\nstraints and backlogging of excess demands. Subsequently,\ndledelaysofanotherkindwithoutrequiringtoberetrained.",
      "size": 606,
      "sentences": 3
    },
    {
      "id": 5,
      "content": "but cannot handle cross-product con-\namodeltrainedtohandleonekindofdelayiscapabletohan-\nstraints and backlogging of excess demands. Subsequently,\ndledelaysofanotherkindwithoutrequiringtoberetrained. a more recent series of work concerns with multi-product,\nFinally,weapplythedelay-resolvedframeworktoscenarios\nmulti-echelon replenishment strategies (Ben-Tal, Golany,\ncomprisingofmultipleproductssubjectedtostochasticityin\nleadtimes,andelucidatehowthedelay-resolvedframework andShtern2009;AkbariandKarimi2015;Ca´rdenas-Barro´n\nnegates the effect of any delay to achieve near-optimal per- and Trevin˜o-Garza 2014; Harifi et al. 2020; Mousavi et al. formance. 2014; Yang et al. 2017), albeit under assumptions of deter-\nministicornegligible(zero)leadtimes.",
      "size": 753,
      "sentences": 7
    },
    {
      "id": 6,
      "content": "lay to achieve near-optimal per- and Trevin˜o-Garza 2014; Harifi et al. 2020; Mousavi et al. formance. 2014; Yang et al. 2017), albeit under assumptions of deter-\nministicornegligible(zero)leadtimes. 1 Introduction&RelatedWork A significant disadvantage with the aforementioned\nstrategies is their dependency on solving robust optimiza-\nInventory replenishment (Axsa¨ter 2015) is one of the key\ntion problems at each time point, which scales poorly with\nfactors determining efficient flow of goods through the en-\nthenumberofproducts.Inreality,amoderatelylargeretail\ntiresupplychainsystem.Inventoryreplenishmentprimarily\nbusiness may have each of its store selling up to 100,000\nconcernswithdesigningstrategiesthatgovernmovementof\nproduct types 1.",
      "size": 747,
      "sentences": 6
    },
    {
      "id": 7,
      "content": "eratelylargeretail\ntiresupplychainsystem.Inventoryreplenishmentprimarily\nbusiness may have each of its store selling up to 100,000\nconcernswithdesigningstrategiesthatgovernmovementof\nproduct types 1. Fueled by the recent advances and suc-\ninventoryfromreservestorage(e.g.awarehouse)toprimary\ncesses of modern deep learning algorithms, there is a sig-\nstorage(e.g.aretailoutlet).Devisingbetterstrategiesforre-\nnificantshifttowardsadoptingreinforcementlearning(RL)\nplenishment translates directly to increased profit margins\nfor handling large product size (Meisheri et al. 2021; Yan\nforabusiness.Thus,itisonlyfairthattheproblemofinven-\netal.2021).Recallthattheultimategoalfordesigningbetter\ntory replenishment remains one of the most studied prob-\nreplenishment strategies is to maximize the long-term (dis-\nlemsintheoperationsresearchcommunity. counted) reward subjected to constraints and uncertainties.",
      "size": 904,
      "sentences": 4
    },
    {
      "id": 8,
      "content": "ent remains one of the most studied prob-\nreplenishment strategies is to maximize the long-term (dis-\nlemsintheoperationsresearchcommunity. counted) reward subjected to constraints and uncertainties. There are several factors that impact the efficacy of in-\nThismakesRLasuitablecandidatetohelpdiscoveroptimal\nventory replenishment - stochastic demands (Lewis 2012),\nstrategiesinhighlydynamicenvironments.Despitesubstan-\nlimited capacity of vehicles (Sindhuchao et al.",
      "size": 467,
      "sentences": 3
    },
    {
      "id": 9,
      "content": "Lasuitablecandidatetohelpdiscoveroptimal\nventory replenishment - stochastic demands (Lewis 2012),\nstrategiesinhighlydynamicenvironments.Despitesubstan-\nlimited capacity of vehicles (Sindhuchao et al. 2005), fi-\ntialeffortstowardsincreasingthescopeofmoderninventory\nnite shelf life of goods (Haijema 2013), cross-product con-\nstraints(MinnerandTranschel2010),limitedholdingcapac-\nities at primary and secondary storage (Bertsimas, Kallus, 1https://www.retailtouchpoints.com/resources/how-many-\nproducts-does-amazon-carry\n2202\nraM\n9\n]GL.sc[\n2v58800.3022:viXra\n=== 페이지 2 ===\nmanagement strategies, uncertainty in lead times and poor 2 Methodology\nend-to-endvisibility,whichareknowntoresultinbullwhip\nFormulating the Supply Chain problem as a reinforcement\neffect (Lee, Padmanabhan, and Whang 1997), are often ig-\nlearning has been already explored before (Meisheri et al. nored largely due to increased complexity of the resulting\n2021).",
      "size": 934,
      "sentences": 3
    },
    {
      "id": 10,
      "content": "s a reinforcement\neffect (Lee, Padmanabhan, and Whang 1997), are often ig-\nlearning has been already explored before (Meisheri et al. nored largely due to increased complexity of the resulting\n2021). Generally, while modelling delays it has been ad-\nsupplychainsystem. dressedintheformofamultiagentReinforcementLearning\nThis paper fills the key gap in designing replenishment problem. While that can work well for constant and small\nstrategies in realistic scenarios comprising of all levels of number of unique delays, it is not generalizable. One crit-\ncomplexities,i.e.,stochasticdemandsandleadtimes,cross- ical failure would be in the case, where there are multiple\nproductconstraints,largenumberofproducts,limitedshelf- vendorswithstochasticleadtimes,thismodelwouldnotbe\nlives and capacities of products, and nonlinear business re- scalbaleandrobust. wards.",
      "size": 862,
      "sentences": 7
    },
    {
      "id": 11,
      "content": "ltiple\nproductconstraints,largenumberofproducts,limitedshelf- vendorswithstochasticleadtimes,thismodelwouldnotbe\nlives and capacities of products, and nonlinear business re- scalbaleandrobust. wards. The paper adopts RL as its core solution method- Delay Resolved Algorithms (Nath, Baranwal, and\nology to address scalability and fragility of supply chain Khadilkar2021)hasbeenasuccessinaddressingbothcon-\nsystem. Interestingly, lead times in delivery and manufac- stant and stochastic delays and can be applied to this sce-\nturing can be viewed as action delays, where the delay is nariodirectlywithfewmodifications.DelayResolvedAlgo-\nbetweentheeventswhentheorderisplacedtillitgetspro- rithmsdonotapproachthisproblembyassigningaseparate\ncured.Consequently,weequatetheeffectofuncertainlead model for every delay, but by modifying the Markov Deci-\ntimesininventoryreplenishmentproblemtolearninginde- sion Process (MDP). Generally, in the presence of delays,\nlayed environments.",
      "size": 975,
      "sentences": 5
    },
    {
      "id": 12,
      "content": "ncertainlead model for every delay, but by modifying the Markov Deci-\ntimesininventoryreplenishmentproblemtolearninginde- sion Process (MDP). Generally, in the presence of delays,\nlayed environments. The authors in (Nath, Baranwal, and reinforcement learning does not work as well, as the states\nKhadilkar2021)recentlyproposedadelay-resolvedframe- arenolongerMarkovandareaffectedbyactionswhichare\nwork to handle stochastic delays in highly dynamic envi- yet to be implemented on the environment. This makes the\nronments.Weleverageasimilarframeworktoaugmentour MDPpartiallyobservableandconsequentlyitisdifficultfor\nRL-basedreplenishmentstrategytoincorporateuncertainty RLalgorithmstolearn.DelayResolvedAlgorithmsaddress\nin lead times.",
      "size": 733,
      "sentences": 4
    },
    {
      "id": 13,
      "content": "ilarframeworktoaugmentour MDPpartiallyobservableandconsequentlyitisdifficultfor\nRL-basedreplenishmentstrategytoincorporateuncertainty RLalgorithmstolearn.DelayResolvedAlgorithmsaddress\nin lead times. As a happy consequence, our framework is this problem by appending the states with an action buffer\nalso capable to handle uncertainty in real-time information oftheun-implementedactions.Thesolution,albeitsimple,\nsharing across multiple echelons in a supply chain system. has a few interesting properties. It can be easily used with\nTheproposedframeworkadvancesthecurrentstate-of-the- anyRLalgorithmanditistheoreticallysound,i.e.optimiz-\nart methodologies in replenishment system by considering ing for the Mean Squared TD Error in the new augmented\nmultiplerealisticscenariosconcurrentlyinanefficientman- MDPleadstothesameoptimalpolicyastheoriginalMDP. ner.",
      "size": 858,
      "sentences": 5
    },
    {
      "id": 14,
      "content": "replenishment system by considering ing for the Mean Squared TD Error in the new augmented\nmultiplerealisticscenariosconcurrentlyinanefficientman- MDPleadstothesameoptimalpolicyastheoriginalMDP. ner. This can also be extended to stochastic delays by using a\nconstantlengthfortheactionbuffersuchthatthedelayval-\nWhile the work adopts RL-based solution as its core uesareupperboundedbythemaximumlengthofthebuffer. methodology, yet it promises a significant departure from Also,DelayResolvedAlgorithmshavetheaddedadvantage\nthe existing literature on using RL for optimizing inven- ofrobustnesstothesizeofthisbufferasituseszeropadding\ntory.",
      "size": 636,
      "sentences": 4
    },
    {
      "id": 15,
      "content": "ses a significant departure from Also,DelayResolvedAlgorithmshavetheaddedadvantage\nthe existing literature on using RL for optimizing inven- ofrobustnesstothesizeofthisbufferasituseszeropadding\ntory. Other than its ability to handle stochastic lead times for the action buffers which does not alter the final outputs\nand poor end-to-end visibility, the proposed framework is oftheRLagent.Forthispaper,weusedthedelayresolved\ndata-efficientonthreeaccounts,noneofwhichhasbeenad- versionofDQN(Mnihetal.2015)toaddresstheproblemof\ndressedintheliterature(Meisherietal.2021):(a)Nofore- actiondelaywhichcanbeinterpretedtobetheleadtimefor\ncasts:Ourframeworkdoesnotexplicitlyrequirethedemand theproductsthatisinherentlypresentinourenvironment. forecasts at all times, except for current timestep. Instead,\nThe supply chain replenishment can be formulated into\ntheframeworklearnstomanagereplenishmentthroughstep-\nMDPs with continuous state space and discrete actions as\nrewards.",
      "size": 966,
      "sentences": 4
    },
    {
      "id": 16,
      "content": "current timestep. Instead,\nThe supply chain replenishment can be formulated into\ntheframeworklearnstomanagereplenishmentthroughstep-\nMDPs with continuous state space and discrete actions as\nrewards. (b)Noroll-outs:Sincetheframeworkdoesnotre-\ndescribedin(Meisherietal.2021).Decisionsforeachprod-\nquireforecasteddemands,policyroll-outbasedonforecasts\nuctaretakenindependentlywhilesupplyingglobalinforma-\nis impossible. Despite the lack of policy roll-out (noisy in-\ntionaboutconstraintsinthestatesandrewards.State-space\nformation), our framework is capable of generating better\nis presented in table 1. x (t) denotes the current inventory\ni\nstrategiesforreplenishment. (c)Singleagentfordifferent levelinthestoreforith productattimestept.Metadatafor\nlead times: Since the framework is based on augmenting\neachproductisencapsulatedbyfeaturesv ,c andT which\ni i i\npastactionstoitsinformationstate,itcanhandleanyfinite- denotes, volume, weights and shelf life of ith product.",
      "size": 969,
      "sentences": 5
    },
    {
      "id": 17,
      "content": "ramework is based on augmenting\neachproductisencapsulatedbyfeaturesv ,c andT which\ni i i\npastactionstoitsinformationstate,itcanhandleanyfinite- denotes, volume, weights and shelf life of ith product. As\namountofdelay,andthusasingleagentcanbeusedtoop-\nmentioned before we only require a forecast for the next\ntimize replenishment of a product regardless of its current time step that is denoted by Wˆ (t). Features v(cid:124)Wˆ (t) and\n(stochastic)leadtimedelay. i\nc(cid:124)Wˆ (t)provideinformationaboutglobalconstraintswhich\nDeployment in real world: The problem statement is areacrossproductsandhencehelpindrivingpolicytowards\nmotivatedfromreal-worldscenariosandconstraintsinatyp- optimal decisions across products. We also adopt a simi-\nicalsupplychainmanagementsystem.Webelievethatmod- larrewardstructureasmentionedin(Meisherietal.2021).",
      "size": 841,
      "sentences": 5
    },
    {
      "id": 18,
      "content": "real-worldscenariosandconstraintsinatyp- optimal decisions across products. We also adopt a simi-\nicalsupplychainmanagementsystem.Webelievethatmod- larrewardstructureasmentionedin(Meisherietal.2021). eling lead time in a structured manner while taking care of Action space is 14 discrete actions denoting the amount of\ncomputational requirements can greatly enhance the prac- quantitytobereplenished. tical usability of such algorithms. Real-time inference time DRDQN in Supply Chains is an extension of the DQN\nhas been proven to be a bottleneck when dealing with mil- algorithm with augmented states. However, since we have\nlionsofproducts.",
      "size": 642,
      "sentences": 6
    },
    {
      "id": 19,
      "content": "ime inference time DRDQN in Supply Chains is an extension of the DQN\nhas been proven to be a bottleneck when dealing with mil- algorithm with augmented states. However, since we have\nlionsofproducts. a parallel forward pass for each of the products, the action\n=== 페이지 3 ===\nTable1:Statespacerepresentation\nNotation Explanation\nx (t) Currentinventorylevel\ni\nWˆ (t) Forecastaggregateordersin[t,t+1)\ni\nv Unitvolume\ni\nc Unitweight\ni\nT Shelf-life\ni\nv(cid:124)Wˆ (t) Totalvolumeofforecastforallproducts\nc(cid:124)Wˆ (t) Totalweightofforecastforallproducts\nFigure 2: Training results over 220 product datasets, solid\nline represents the mean over 10 random seeds and shaded\nregiondenotes95percentileconfidenceinterval. ...\n...\n... Figure 3: Training results over 100 product datasets, solid\nline represents the mean over 10 random seeds and shaded\nregiondenotes95percentileconfidenceinterval. Figure1:Illustrationofthereplenishmentsystemwithlead\ntimesandtheinformationstatebasedRLinputs.",
      "size": 981,
      "sentences": 6
    },
    {
      "id": 20,
      "content": "id\nline represents the mean over 10 random seeds and shaded\nregiondenotes95percentileconfidenceinterval. Figure1:Illustrationofthereplenishmentsystemwithlead\ntimesandtheinformationstatebasedRLinputs. to account for stochastic delays making it suitable to adapt\nto different lead times even during the training phase. This\nmakestheproposedframeworkquiteappealingforhandling\nbuffer would also include the delayed actions. Actions for\nuncertain lead times. Additionally, the complexity of the\neachoftheproductsmaybedifferentafterapplyingglobal\nframework grows linearly with the lead time, resulting in\nconstraints such as truck volume and weight capacity. We\nsignificantly less computational budget as opposed to with\naugment modified actions after applications of global con-\nthemulti-agentRLframework.",
      "size": 800,
      "sentences": 7
    },
    {
      "id": 21,
      "content": "nstraints such as truck volume and weight capacity. We\nsignificantly less computational budget as opposed to with\naugment modified actions after applications of global con-\nthemulti-agentRLframework. straintsforDRDQNinthestatespace,whereasoriginalde-\ncisionsofagentsarekeptasactionsinmemorybuffer.Infor-\n3 ResultsandDiscussion\nmationstateforDRDQNisshowninFigure1.Forstochas-\ntic delay cases, we assume that the delay changes only af- We have used two separate benchmark datasets each with\nteranepisodehasbeencompleted.Atstartofeachepisode, having different characteristics in demand distribution and\nwesampleleadtime(delay)uniformlyfrom(1,k ).This product metadata with 100 and 220 products respec-\nmax\ncorresponds to the practical scenario, where at the start of tively (Meisheri et al. 2021) with authors’ consent.",
      "size": 816,
      "sentences": 4
    },
    {
      "id": 22,
      "content": "delay)uniformlyfrom(1,k ).This product metadata with 100 and 220 products respec-\nmax\ncorresponds to the practical scenario, where at the start of tively (Meisheri et al. 2021) with authors’ consent. In our\na new season, one has to generate a contract with a vendor experiments, we consider lead time equal to delays in ac-\nwithknownleadtimeswhichremainsfixedforagivendu- tion implementations. We have used epsilon greedy as ex-\nration. plorationstrategiesandraneachexperimentfor10random\nIn addition, delay-resolved algorithms can be used for seedsforstatisticalsignificance.Wegaugetheperformance\nbothactionandobservationdelaysandhaveequivalentper- ofourmodelsonthebusinessreward. formance for both of them as highlighted in (Nath, Baran- Figures 2 and 3 shows the training graphs over 220 and\nwal, and Khadilkar 2021).",
      "size": 819,
      "sentences": 6
    },
    {
      "id": 23,
      "content": "ndelaysandhaveequivalentper- ofourmodelsonthebusinessreward. formance for both of them as highlighted in (Nath, Baran- Figures 2 and 3 shows the training graphs over 220 and\nwal, and Khadilkar 2021). Hence, in the supply chain sce- 100 datasetrespectively withdifferent fixedlead times and\nnario, Delay Resolved DQN is used to address both lead Figures4and5showtheeffectofleadtimesbetweenDQN\ntimedelaysaswelldelayininformationsharing. and DRDQN. From all the plots, it is evident that there\nThe proposed framework saves considerable compute is significant drop in performance (business reward) as the\ntimeaswedonotneedtotrainamodelforeveryleadtime. leadtimeincreasesforDQNwhereasthedecreaseismuch\nIt must be noted that the models trained to account for a lessforDRDQN.Forexample,DRDQNwith10leadtime\nspecificleadtimedonotgeneralizetootherleadtimes.On is similar in performance to DQN with half the lead time.",
      "size": 907,
      "sentences": 6
    },
    {
      "id": 24,
      "content": "noted that the models trained to account for a lessforDRDQN.Forexample,DRDQNwith10leadtime\nspecificleadtimedonotgeneralizetootherleadtimes.On is similar in performance to DQN with half the lead time. the other hand, the DRDQN-based framework can be used Thus, DRDQN is a more robust algorithm to changes in\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\ndelay and this can also be observed for stochastic delays, Wehavealsoexperimentedwithstochasticleadtimesce-\nas explained in the subsequent paragraphs. The difference, nario. For this as mentioned earlier, stochasticity is across\nthoughquitesmallintermsofmagnitudeofthebusinessre- the episode whereas for a particular episode lead time re-\nwards,canbequitesignificantwhentheoverallprofitmar- mains constant. Figure 7 shows the training results for 220\nginsareconsideredacrossalltheproducts. product dataset where value of lead time for each episode\nwaschosenatrandombetween1and50.",
      "size": 913,
      "sentences": 6
    },
    {
      "id": 25,
      "content": "profitmar- mains constant. Figure 7 shows the training results for 220\nginsareconsideredacrossalltheproducts. product dataset where value of lead time for each episode\nwaschosenatrandombetween1and50. Figure4:Effectofleadtimeon220Productdataset\nFigure7:Stochasticdelayofmaximum50leadtimeon220\nProductdataset,graphisplottedwithmovingaverageof20\n4 Conclusion\nThispaperaddressesakeyissueonmanaginguncertaintyin\nleadtimesandreal-timeinformationsharingacrossmultiple\nechelonsinasupplychainsystemforoptimizinginventory\nreplenishment.Theproposedworkleveragestherecentlyin-\ntroduced delay-resolved framework in the RL literature to\naccountforstochasticityatalllevelsinacomputationallyef-\nficientmanner,whereleadtimesareviewedasactiondelays\nassociatedwiththesupplychainsystem.Unliketheexisting\nFigure5:Effectofleadtimeon100Productdataset RL-basedinventorymanagementsystem,ourframeworkre-\nquires only one agent to be trained for different values of\nleadtimes.Indoingso,themodelsimplyaugmentsthepast\nWehaveconsideredleadtimesasactiondelays,however\nstocking decisions to its information state without needing\nwe can also consider having observation delays where we\nto work with forecasted demands or policy roll-outs.",
      "size": 1204,
      "sentences": 4
    },
    {
      "id": 26,
      "content": "eadtimesasactiondelays,however\nstocking decisions to its information state without needing\nwe can also consider having observation delays where we\nto work with forecasted demands or policy roll-outs. Thus,\nhaveolderstateswhiletakingadecisionsbuttherearenode-\nthemodelcanbetrainedefficientlyandscaledsuitablytoac-\nlaysinimplementationofactions.Figure6showstheequiv-\ncount for cross-product constraints. To the best of authors’\nalenceofactiondelaysandobservationdelayswhichisin-\nknowledge,thisisthefirstsuchworkthatconcurrentlytakes\nlinewithresultsreportedin(Nath,Baranwal,andKhadilkar\nintoaccountallaspectsofasupplychaininventorycontrol\n2021).WecanclearlyobservethattheDRDQNwithaction\ninacomputationallyefficientmanner. and observation delay is able to outperform DQN. Results\nareonlyreportedfor100productdatasetwith5delayboth\nReferences\nforactionandobservation.Wehaveobservedsimilarresults\nacrossdatasetanddelays. Akbari,A.A.;andKarimi,B.2015.",
      "size": 943,
      "sentences": 7
    },
    {
      "id": 27,
      "content": "able to outperform DQN. Results\nareonlyreportedfor100productdatasetwith5delayboth\nReferences\nforactionandobservation.Wehaveobservedsimilarresults\nacrossdatasetanddelays. Akbari,A.A.;andKarimi,B.2015. Anewrobustoptimiza-\ntion approach for integrated multi-echelon, multi-product,\nmulti-periodsupplychainnetworkdesignunderprocessun-\ncertainty. TheInternationalJournalofAdvancedManufac-\nturingTechnology,79(1-4):229–244. Axsa¨ter,S.2015. Inventorycontrol,volume225. Springer. Ben-Tal,A.;Golany,B.;andShtern,S.2009. Robustmulti-\nechelon multi-period inventory control. European Journal\nofOperationalResearch,199(3):922–935. Bertsimas,D.;Kallus,N.;andHussain,A.2016. Inventory\nmanagementintheeraofbigdata. ProductionandOpera-\ntionsManagement,25(12):2006–2009. Figure 6: Comparison of action and observation delay on\nBertsimas, D.; and Thiele, A. 2004. A robust optimiza-\n100productdatasetwith5Lead-time. tion approach to supply chain management.",
      "size": 940,
      "sentences": 18
    },
    {
      "id": 28,
      "content": "2):2006–2009. Figure 6: Comparison of action and observation delay on\nBertsimas, D.; and Thiele, A. 2004. A robust optimiza-\n100productdatasetwith5Lead-time. tion approach to supply chain management. In Interna-\n=== 페이지 5 ===\ntional Conference on Integer Programming and Combina- Supply Chain Management: Methodologies, State of the\ntorialOptimization,86–100.Springer. Art,andFutureOpportunities. StateoftheArt,andFuture\nCa´rdenas-Barro´n, L. E.; and Trevin˜o-Garza, G. 2014. An Opportunities(October4,2021). optimal solution to a three echelon supply chain network Yang,L.;Li,H.;Campbell,J.F.;andSweeney,D.C.2017. withmulti-productandmulti-period. AppliedMathematical Integratedmulti-perioddynamicinventoryclassificationand\nModelling,38(5-6):1911–1918. control. International Journal of Production Economics,\nDolgui, A.; Ben Ammar, O.; Hnaien, F.; and Louly, M.-A. 189:86–96. 2013. A state of the art on supply planning and inventory Zipkin, P. 2000.",
      "size": 951,
      "sentences": 17
    },
    {
      "id": 29,
      "content": ". control. International Journal of Production Economics,\nDolgui, A.; Ben Ammar, O.; Hnaien, F.; and Louly, M.-A. 189:86–96. 2013. A state of the art on supply planning and inventory Zipkin, P. 2000. Foundations of Inventory Manage-\ncontrol under lead time uncertainty. Studies in Informatics ment. McGraw-Hill Companies,Incorporated. ISBN\nandControl,22(3):255–268. 9780256113792. Goh, M.; De Souza, R.; Zhang, A. N.; He, W.; and Tan, P.\n2009. Supply chain visibility: A decision making perspec-\ntive. In20094thIEEEConferenceonIndustrialElectronics\nandApplications,2546–2551.IEEE. Haijema, R. 2013. A new class of stock-level dependent\nordering policies for perishables with a short maximum\nshelf life. International Journal of Production Economics,\n143(2):434–439. Harifi,S.;Khalilian,M.;Mohammadzadeh,J. ;andEbrahim-\nnejad, S. 2020. Optimization in solving inventory control\nproblemusingnatureinspiredEmperorPenguinsColonyal-\ngorithm. JournalofIntelligentManufacturing,1–15.",
      "size": 976,
      "sentences": 21
    },
    {
      "id": 30,
      "content": "halilian,M.;Mohammadzadeh,J. ;andEbrahim-\nnejad, S. 2020. Optimization in solving inventory control\nproblemusingnatureinspiredEmperorPenguinsColonyal-\ngorithm. JournalofIntelligentManufacturing,1–15. Lee, H. L.; Padmanabhan, V.; and Whang, S. 1997. The\nbullwhipeffectinsupplychains. Sloanmanagementreview,\n38:93–102. Lewis,C.2012. Demandforecastingandinventorycontrol. Routledge. Meisheri, H.; Sultana, N. N.; Baranwal, M.; Baniwal, V.;\nNath,S.;Verma,S.;Ravindran,B.;andKhadilkar,H.2021. Scalablemulti-productinventorycontrolwithleadtimecon-\nstraints using reinforcement learning. Neural Computing\nandApplications,1–23. Minner, S.; and Transchel, S. 2010. Periodic review\ninventory-control for perishable products under service-\nlevelconstraints. ORspectrum,32(4):979–996. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\nness,J.;Bellemare,M.G.;Graves,A.;Riedmiller,M. ;Fidje-\nland,A.K.;Ostrovski,G.;etal.2015. Human-levelcontrol\nthroughdeepRL. Nature,518(7540):529.",
      "size": 973,
      "sentences": 21
    },
    {
      "id": 31,
      "content": "nih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\nness,J.;Bellemare,M.G.;Graves,A.;Riedmiller,M. ;Fidje-\nland,A.K.;Ostrovski,G.;etal.2015. Human-levelcontrol\nthroughdeepRL. Nature,518(7540):529. Mousavi,S.M.;Hajipour,V.;Niaki,S.T.A.;andAalikar,N. 2014. Amulti-productmulti-periodinventorycontrolprob-\nlemunderinflationanddiscount:aparameter-tunedparticle\nswarm optimization algorithm. The International Journal\nof Advanced Manufacturing Technology, 70(9-12): 1739–\n1756. Nath,S.;Baranwal,M.;andKhadilkar,H.2021. Revisiting\nState Augmentation methods for Reinforcement Learning\nwithStochasticDelays. InProceedingsofthe30thACMIn-\nternationalConferenceonInformation&KnowledgeMan-\nagement,1346–1355. Sindhuchao,S.;Romeijn,H.E.;Akc¸ali,E. ;andBoondiskul-\nchok, R. 2005. An integrated inventory-routing system for\nmulti-itemjointreplenishmentwithlimitedvehiclecapacity. JournalofGlobalOptimization,32(1):93–118. Yan, Y.; Chow, A. H.; Ho, C. P.; Kuo, Y.-H.; Wu, Q.; and\nYing, C. 2021.",
      "size": 982,
      "sentences": 17
    },
    {
      "id": 32,
      "content": "inventory-routing system for\nmulti-itemjointreplenishmentwithlimitedvehiclecapacity. JournalofGlobalOptimization,32(1):93–118. Yan, Y.; Chow, A. H.; Ho, C. P.; Kuo, Y.-H.; Wu, Q.; and\nYing, C. 2021. Reinforcement Learning for Logistics and",
      "size": 239,
      "sentences": 4
    }
  ]
}