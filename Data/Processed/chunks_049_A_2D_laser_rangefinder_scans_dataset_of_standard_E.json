{
  "source": "ArXiv",
  "filename": "049_A_2D_laser_rangefinder_scans_dataset_of_standard_E.pdf",
  "total_chars": 19997,
  "total_chunks": 28,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nA 2D laser rangefinder scans dataset of standard EUR pallets\nIhab S. Mohameda,∗, Alessio Capitanellib, Fulvio Mastrogiovannib, Stefano Rovettab,\nRenato Zaccariab\naINRIA Sophia Antipolis - Méditerranée, Université Côte d’Azur, France\nbDepartment of Informatics, Bioengineering, Robotics and Systems Engineering (DIBRIS), University of\nGenoa, Italy\nAbstract\nIn the past few years, the technology of automated guided vehicles (AGVs) has notably\nadvanced. In particular, in the context of factory and warehouse automation, different\napproaches have been presented for detecting and localizing pallets inside warehouses and\nshop-floor environments. In a related research paper [1], we show that an AGVs can detect,\nlocalize, and track pallets using machine learning techniques based only on the data of an\non-board 2D laser rangefinder. Such sensor is very common in industrial scenarios due to its\nsimplicity and robustness, but it can only provide a limited amount of data.",
      "size": 984,
      "sentences": 4
    },
    {
      "id": 2,
      "content": "based only on the data of an\non-board 2D laser rangefinder. Such sensor is very common in industrial scenarios due to its\nsimplicity and robustness, but it can only provide a limited amount of data. Therefore, it\nhas been neglected in the past in favor of more complex solutions. In this paper, we release\nto the community the data we collected in [1] for further research activities in the field of\npallet localization and tracking. The dataset comprises a collection of 565 2D scans from\nreal-world environments, which are divided into 340 samples where pallets are present, and\n225 samples where they are not. The data have been manually labelled and are provided in\ndifferent formats. Keywords: 2D Laser Rangefinder, Object Detection, Robotics, Automated Guided Vehicle\n∗Corresponding author.",
      "size": 796,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "e they are not. The data have been manually labelled and are provided in\ndifferent formats. Keywords: 2D Laser Rangefinder, Object Detection, Robotics, Automated Guided Vehicle\n∗Corresponding author. Email addresses: ihab.mohamed@inria.fr (Ihab S. Mohamed),\nalessio.capitanelli@dibris.unige.it (Alessio Capitanelli), fulvio.mastrogiovanni@unige.it\n(Fulvio Mastrogiovanni), stefano.rovetta@unige.it (Stefano Rovetta), renato.zaccaria@unige.it\n(Renato Zaccaria)\nMarch 15, 2019\n9102\nraM\n31\n]OR.sc[\n2v46580.5081:viXra\n=== 페이지 2 ===\nSpecifications Table\nSubject area Engineering\nMore specific subject area Robotics, Object Detection, Automated Guided Vehicle\nType of data Raw depth data provided by the 2D range sensor\nProcessed 2D bitmap-like image representation of raw data\nHow data was acquired 2D laser rangefinder (SICK S3000 Pro CMS)\nData format Files in text format .txt\n2D images in .jpg & .png (681×533 & 250×250 pixels)\nMAT-files in MATLAB format .mat\nExperimental factors 2D depth data processed offline and converted into 2D images.",
      "size": 1040,
      "sentences": 4
    },
    {
      "id": 4,
      "content": "format Files in text format .txt\n2D images in .jpg & .png (681×533 & 250×250 pixels)\nMAT-files in MATLAB format .mat\nExperimental factors 2D depth data processed offline and converted into 2D images. Images have been manually tagged whether they include a pallet\nor not, and eventually paired with the respective region of interest. Experimental features Raw data have been acquired by moving a 2D laser scanner\nin a realistic reproduction of a factory workshop, featuring\npallets, people, robots and other equipment.",
      "size": 517,
      "sentences": 3
    },
    {
      "id": 5,
      "content": "n of interest. Experimental features Raw data have been acquired by moving a 2D laser scanner\nin a realistic reproduction of a factory workshop, featuring\npallets, people, robots and other equipment. Data source location EMARO Lab, Department of Informatics, Bioengineering, Robotics\nand Systems Engineering, University of Genoa, Genoa, Italy\n(44.402241, 8.960811)\nData accessibility Dataset and codes are archived in a GitHub repository at:\nhttps://github.com/EmaroLab/PDT\nRelated research article \"Detection, localisation and tracking of pallets using\nlearning techniques and 2D range data\" [1]\nValue of the Data\n• The 2D Laser Rangefinder dataset allows to develop novel techiques for pallet detec-\ntion, localization and tracking. • The 2D Laser Rangefinder dataset can be used as banchmark to compare the accuracy\nof different pallet detection, localization and tracking algorithms.",
      "size": 887,
      "sentences": 4
    },
    {
      "id": 6,
      "content": "pallet detec-\ntion, localization and tracking. • The 2D Laser Rangefinder dataset can be used as banchmark to compare the accuracy\nof different pallet detection, localization and tracking algorithms. • The 2D Laser Rangefinder dataset allows to improve Automated Guidance Vehicles in\nindustrial workshop environments. • The 2D Laser Rangefinder dataset can be used to simulate the 2D range sensor data\nof a mobile robot moving in an industrial workshop environment. • To our knowledge, this is the first dataset for pallet localization and tracking using\nonly 2D Laser Rangefinder data, as opposed to previous datasets aimed at generic\nAGV and/or more complex sensors [2–5]. March 15, 2019\n[표 데이터 감지됨]\n\n=== 페이지 3 ===\n1. Data\nIn this article, we present a dataset of 2D range data obtained from a laser scanner\nmoving inside an industrial workshop environment, where EUR standard pallets (see Fig. 3(a)), people, robots and other equipment are present.",
      "size": 951,
      "sentences": 8
    },
    {
      "id": 7,
      "content": "a dataset of 2D range data obtained from a laser scanner\nmoving inside an industrial workshop environment, where EUR standard pallets (see Fig. 3(a)), people, robots and other equipment are present. Each frame of the sensor trajectory\ncorresponds to: (i) a 2D range scan (see Table 1) obtained from a SICK S3000 Pro CMS\nlaser rangefinder (see Fig. 3(b)); (ii) a 2D image obtained by processing the 2D range scan\n(see Fig. 4); (iii) a tag attached by a human, indicating whether the scan includes a pallet\nor not; and (iv), the region of interest of the pallet in the image (if any), also defined by a\nhuman. The laser rangefinder has a resolution of 0.25deg and a maximum field of view of 190deg,\nleading to scans made of 761 ranges. It operates at 16Hz frequency, and the scans are\naveraged every 4 frames during the static data acquisition phase in order to reduce noise. There are a total of 565 scans, 340 of which contains a pallet, while the remaining 225\ndo not.",
      "size": 969,
      "sentences": 8
    },
    {
      "id": 8,
      "content": "he scans are\naveraged every 4 frames during the static data acquisition phase in order to reduce noise. There are a total of 565 scans, 340 of which contains a pallet, while the remaining 225\ndo not. The corresponding 2D images are obtained by converting the range data from\npolar to cartesian coordinates and resizing them to 250×250px. Also, images containing a\npallet come with a pallet Region Of Interest (ROI), defined by its upper-left and lower-right\nvertices. Finally, an additional set of 4 continuous trajectories’ raw range data is also made\navailable, to allow online testing. 2. Experimental design, materials, and methods\n2.1. Equipment and Software\nIn our experiment, the data have been acquired using a commercial 2D laser rangefinder\nfrom SICK, in particular the model S3000 Pro CMS1 pictured in Fig. 3(b). The sensor has\na maximum range of 49m (20m at 20% reflectivity), a resolution of 0.25deg, a 16Hz refresh\nfrequency, and an empirical error of 30mm.",
      "size": 971,
      "sentences": 10
    },
    {
      "id": 9,
      "content": "lar the model S3000 Pro CMS1 pictured in Fig. 3(b). The sensor has\na maximum range of 49m (20m at 20% reflectivity), a resolution of 0.25deg, a 16Hz refresh\nfrequency, and an empirical error of 30mm. The maximum field of view of the rangefinder\nis 190deg, which is sufficient for the detection of objects in front of an eventual AGV. The\nsensor generates an array of 761 distances in polar coordinates, i.e., each value in the array\ncorrespond to the distance to the closest object for every angle in 0.25deg increments. Thechoiceofthissensorwasduetoitswidespreadadoptioninindustrialmobilerobotics,\nwhere it is mostly employed for safety applications and is appreciated for its robustness and\nprecision. It belongs to the class of sensors based on the Time-of-Flight (TOF) principle,\ni.e., sensors which measure the delay between the emission of a signal and the moment it\nhits back a receiver in order to estimate the distance to a surface.",
      "size": 941,
      "sentences": 7
    },
    {
      "id": 10,
      "content": "ed on the Time-of-Flight (TOF) principle,\ni.e., sensors which measure the delay between the emission of a signal and the moment it\nhits back a receiver in order to estimate the distance to a surface. This category of systems\ninvolvessensingdevicessuchasLaserMeasurementSystems(LMS)orLIDARs,radars,TOF\ncameras, and sonar sensors, and they emit either light or sound waves in the environment. Knowing the speed with which the signal propagates and using precise circuitry to measure\nthe exact TOF, the distance can be estimated with high precision. 1https://www.sick.com/ag/en/s3000-professional-cms-sensor-head-with-io-module/s30a-6011db/p/\np31284\nMarch 15, 2019\n=== 페이지 4 ===\nThe laser rangefinder is then connected to a PC through a RS422-USB converter, which\nhas a transmission rate of 500kBaud. The PC used to acquire the data is equipped with an\nIntel® Core i5-4210U 1.70GHz CPU and 6GB of RAM, and runs Ubuntu 16.04 64 bit.",
      "size": 928,
      "sentences": 5
    },
    {
      "id": 11,
      "content": "gh a RS422-USB converter, which\nhas a transmission rate of 500kBaud. The PC used to acquire the data is equipped with an\nIntel® Core i5-4210U 1.70GHz CPU and 6GB of RAM, and runs Ubuntu 16.04 64 bit. Onthesoftwareside, real-worlddataisacquiredonlineusinganad hoc software2 running\nin the Robot Operating System framework3 (ROS). Offline processing (i.e., conversion to\n2D images and manual definition of the regions of interest) has been perfomed in MATLAB. The scripts employed to that purpose and the resulting .mat files are also provided as part\nof this dataset. 2.2. Environment\nWeperformedourexperimentsfordataacquisitionintheindoorenvironmentrepresented\nin Figs. 1-2, with the sensor moving in the 40m2 area highlighted in Fig. 1. Such envi-\nronmennt has been fitted to reproduce a typycal industrial workshop, featuring industrial\npallets, furniture, robots and equipment (e.g., a conveyor belt).",
      "size": 904,
      "sentences": 10
    },
    {
      "id": 12,
      "content": "40m2 area highlighted in Fig. 1. Such envi-\nronmennt has been fitted to reproduce a typycal industrial workshop, featuring industrial\npallets, furniture, robots and equipment (e.g., a conveyor belt). People were also included\nin the scene and allowed to move during data acquisition, which lead to temporary occlu-\nsions of the objects in the environment. Between acquisition sessions, the position of several\nobjects was modified to better simulate a dynamic environment. The 2D laser rangefinder\nwas positioned close to the floor, in a way that was both realistic with real world mounting\nposition and able to perceive a pallet laying directly on the ground. Concerning the type of pallet, we focused on the EUR-pallet standard depicted in\nFig.3(a), which is the European pallet format specified by the European Pallet Association\n(EPAL)4. The size of EUR-pallets is 1200mm×800mm with a height of 144mm. Moreover,\nwe defined as operating face of the pallet the one of narrower width.",
      "size": 985,
      "sentences": 9
    },
    {
      "id": 13,
      "content": "t specified by the European Pallet Association\n(EPAL)4. The size of EUR-pallets is 1200mm×800mm with a height of 144mm. Moreover,\nwe defined as operating face of the pallet the one of narrower width. On that face there are\ntwo slots, each 227.5mm wide. 2.3. Experiments\nIn our experiments, the sensor was moved around the environment. Sensor frames differ\nfrom each other by the position and orientation of the pallet with respect of the sensor, but\nalso due to the dynamic nature of the environment, as described in the previous section. In\nparticular, it is possible that the pallet is heavily occluded and only few points belonging to\nit are visible in the frame. The acquired raw range data R at any time instant i represent the array of measured\ni\ndistances from the rangefinder to surrounding objects in the environment in the direction\ngiven by the angle φ .",
      "size": 865,
      "sentences": 9
    },
    {
      "id": 14,
      "content": "The acquired raw range data R at any time instant i represent the array of measured\ni\ndistances from the rangefinder to surrounding objects in the environment in the direction\ngiven by the angle φ . More formally:\nj\nR = {r ,...,r ,...,r }, (1)\ni 0 j M\nwhere M is the maximum number of range points acquired per frame, which is related to\nthe sensor’s field of view and angular resolution. In our case, M = 761, as the two values\n2https://github.com/RobotnikAutomation/s3000_laser\n3http://www.ros.org/about-ros/\n4https://en.wikipedia.org/wiki/EUR-pallet\nMarch 15, 2019\n=== 페이지 5 ===\nFigure1: Aplanimetryoftheindoorenvironmentwheretheexperimenttookplace. The2Dlaserrangefinder\nhas been moved along several trajectories inside the read area, measuring 40m2. The rest of the environ-\nmentisisstillvisibleinseveralframes. Inthewholeenvironmentsseveralpiecesoffurnitureandequipment,\npallets, robots as well as people were present. are 190deg and 0.25deg respectively.",
      "size": 961,
      "sentences": 7
    },
    {
      "id": 15,
      "content": "t of the environ-\nmentisisstillvisibleinseveralframes. Inthewholeenvironmentsseveralpiecesoffurnitureandequipment,\npallets, robots as well as people were present. are 190deg and 0.25deg respectively. Keep in mind that the sensor employed runs at 16Hz,\nwhich would rapidly lead to a unmanageable amount of data, especially considering the\nmanual labelling steps ahead. For this reason, we decided to effectively reduce the operating\nfrequency to 4Hz in the static data acquisition phase, thus every R is actually the result of\ni\nthe average of 4 raw consecutive frames from the sensor. This also helps reducing noise on\nthe data. An example of such process as well and the structure of the raw range data are\nreported in Table 1. In our experiments, we are focusing on the detection of pallets in the environment,\nhence, the set R of all raw range data readings R , consisting of 565 2D range scans, has\ni\nbeen manually divided into two classes:\n1.",
      "size": 947,
      "sentences": 8
    },
    {
      "id": 16,
      "content": "we are focusing on the detection of pallets in the environment,\nhence, the set R of all raw range data readings R , consisting of 565 2D range scans, has\ni\nbeen manually divided into two classes:\n1. Pallet classrepresentsthecaseofhavingapalletlocatedsomewhereintheenvironment\nwith a free operating face, i.e., it can be eventually be picked up by an AGV as an\nautonomous forklift. It consists of 340 samples. 2. NoPallet class represents the case in which no pallet is present in the environment, or\nthereis, buttheoperatingfaceistooclutteredtoallowanAGVsuchasanautonomous\nforklift to pick up the pallet. It consists of 225 samples. This manual labeling step has been performed with the help of an online ROS visualization\ntool, RViz5. An operator checked the screen of the PC while the sensor was being moved,\nmarking frames where a pallet with a free operating face was present in the sensor’s FOV.",
      "size": 900,
      "sentences": 8
    },
    {
      "id": 17,
      "content": "ine ROS visualization\ntool, RViz5. An operator checked the screen of the PC while the sensor was being moved,\nmarking frames where a pallet with a free operating face was present in the sensor’s FOV. 5http://wiki.ros.org/rviz\nMarch 15, 2019\n=== 페이지 6 ===\nFigure 2: Snaphots of the test environment in different configurations. In the images a number of other\nobjects appear beyond pallets, such as other robots, equipment and furniture. Afterwards, any range data frame R can also be represented as a set S of polar coordi-\ni i\nnates, and consequently converted to Cartesian coordinates using (2) and (3). s = {(r ,φ ),...,(r ,φ ),...,(r ,φ )}. (2)\ni 0 0 j j M−1 M−1\n(cid:26)\nx = r cos(φ ),\nj j j (3)\ny = r sin(φ ). j j j\nThis results in a binary 2D image of the operating area’s floor plan, which is then resized to\n250×250px. An example of the resulting images is given in Fig. 4. Of course, these images are labeled with the same class as the originating frame.",
      "size": 964,
      "sentences": 11
    },
    {
      "id": 18,
      "content": "erating area’s floor plan, which is then resized to\n250×250px. An example of the resulting images is given in Fig. 4. Of course, these images are labeled with the same class as the originating frame. In\npartcular, imagesbelongingtothePallet classcomewiththerespectivepalletROIexpressed\nas its upper-left and lower-right vertices (i.e., (x ,y ) and (x ,y )), as well as a\nmin min max max\ncompanion 250×250px image containing the pallet only. Such ROIs are the results of the\nRegion Proposal Network we employed in the related research paper [1]. The resulting ROIs\nhave been manually labelled to indicate whether they present a pallet or another object. A\nselection of ROIs not including a pallet is also included in the dataset repository.",
      "size": 739,
      "sentences": 8
    },
    {
      "id": 19,
      "content": "[1]. The resulting ROIs\nhave been manually labelled to indicate whether they present a pallet or another object. A\nselection of ROIs not including a pallet is also included in the dataset repository. March 15, 2019\n=== 페이지 7 ===\nEUR-pallet S3000 Professional CMS\nFigure 3: The equipment that has been used to acquire the raw 2D range data: on the left hand side, the\ngeometric characteristics of standard European pallet are shown, whilst on the right hand side the S3000\nProfessional laser scanner (Type: S30A-6011DB) is represented. We will not further delve here into the details of our specific solution to the problem of\npallet localization and tracking, which we present instead in the related research paper [1]. We just point out that the data was indeed employed for pallet localization and tracking and\nthat the proposed architecture was tested using 4 additional continous trajectories, which\nare also made available on the dataset repository.",
      "size": 954,
      "sentences": 6
    },
    {
      "id": 20,
      "content": "indeed employed for pallet localization and tracking and\nthat the proposed architecture was tested using 4 additional continous trajectories, which\nare also made available on the dataset repository. In particular, localization was performed\nusing the aforementioned Region Proposal Network, cascaded with a Faster Recurrent Con-\nvolutional Neural Network classifier that took as input the set of manually labelled ROIs\n[6]. On the other hand, tracking was performed using a Kalman Filter [7]. The filter was\nalso used to implement a Sequantial Classification procedure, i.e., accepting a ROI as an\nactual pallet was deferred till it was detected and tracked for a predefined amount of time,\neventually reaching a sufficient confidence threshold. Finally, note that the dataset can be used for multi-pallet detection, but that was not\npart of our data collection experiment. Indeed, in the related research paper [1] we ran a\npreliminary study on the subject by generating artificial data.",
      "size": 988,
      "sentences": 6
    },
    {
      "id": 21,
      "content": "for multi-pallet detection, but that was not\npart of our data collection experiment. Indeed, in the related research paper [1] we ran a\npreliminary study on the subject by generating artificial data. We want to stress that given\nthat the EUR-Pallet is an official standard with strict tolerances, differences between any\ntwo pallets are not perceivable by the the sensor, due to its characteristics and margin of\nerror. This leads to two major consequences:\n• Itisnotpossiblewiththissensorandwiththedatasetweprovidetounivocallyidentify\na pallet, yet it is possible to distinguish them from each other if appropriate tracking\ntechniques are put in place, like we did in the related research paper [1]. March 15, 2019\n=== 페이지 8 ===\n• Artificial 2D images including two or more pallets in every image are easy to generate.",
      "size": 819,
      "sentences": 5
    },
    {
      "id": 22,
      "content": "king\ntechniques are put in place, like we did in the related research paper [1]. March 15, 2019\n=== 페이지 8 ===\n• Artificial 2D images including two or more pallets in every image are easy to generate. This can be achieved by taking an original image and adding the pallet ROI from\nanother image, possibly changing position, orientation, and/or adding noise, and con-\nsequently deleting any reading in the original image that would now be occluded by\nthe new pallet. Such artificial images are not provided here, but can easily be gener-\nated with the provided materials and tools. Nevertheless, future work on our related\nresearch paper will include real world multi-pallet testing, thus an extended dataset\nwill be made available too. Index Range data R\ni\nIndex Range data R\nFrame #1 (i = 1) i\n0 3.12\n0 3.11\n1 3.11\n1 3.11 Frame #2 (i = 2)\n2 3.06\n2 3.00\n0 3.11 . . . . . . . . 1 3.11 = T = a = k = in = g = a = ve = r = ag ⇒ e . . . .",
      "size": 933,
      "sentences": 17
    },
    {
      "id": 23,
      "content": "Index Range data R\ni\nIndex Range data R\nFrame #1 (i = 1) i\n0 3.12\n0 3.11\n1 3.11\n1 3.11 Frame #2 (i = 2)\n2 3.06\n2 3.00\n0 3.11 . . . . . . . . 1 3.11 = T = a = k = in = g = a = ve = r = ag ⇒ e . . . . Frame #3 (i = 3) over4frames\n100 2.252\n100 2.26 2 3.11\n0 3.13\n101 2.28 . . . . 101 2.28\n. . . . 1 3.11 Frame #4 (i = 4) . . . . . . . . . . 100 2.26 2 3.13\nj 1r j 101 2.28 . . . . . . 0\n1\n3\n3\n. . 1\n1\n3\n1 j\n1rj+·\n4\n··+4rj\n. . . . . . . . . . . . 100 2.26 2 3.00 . . . . . . 757 1.51 j 2r j 101 2.28 . . . . . . 757 1.50\n758 4.05 . . . . . . . . 758 4.07\n. . . . 100 2.23\n759 4.08 759 4.075\n757 1.51 j 3r 101 2.28\n760 4.08 j 760 4.07\n758 4.08 . . . . . . . . ENDofFrame#1 . . . . 759 4.06\n757 1.51 j 4r\nj\n760 4.08\n758 4.08 . . . . . . ENDofFrame#2\n759 4.08\n757 1.48\n760 4.05\n758 4.08\nENDofFrame#3\n759 4.08\n760 4.08\nENDofFrame#4\nTable 1: An example of the raw range data provided by the laser rangefinder.",
      "size": 901,
      "sentences": 93
    },
    {
      "id": 24,
      "content": "760 4.08\n758 4.08 . . . . . . ENDofFrame#2\n759 4.08\n757 1.48\n760 4.05\n758 4.08\nENDofFrame#3\n759 4.08\n760 4.08\nENDofFrame#4\nTable 1: An example of the raw range data provided by the laser rangefinder. As soon as the data is\nvisualized using the standard ROS package rviz, four sequential frames are stored in a text file. Then, the\naverage canbecalculatedinordertoperformthedetectionandtrackingofthepalletusingmachinelearning\ntechniques. 2.4. Dataset inspection\nThe dateset is completely contained in the AllData folder of the provided git repository. The folder is structured as follows:\nMarch 15, 2019\n[표 데이터 감지됨]\n\n=== 페이지 9 ===\nPallet\nNoPallet\nFigure 4: The dataset of real-world 2D scans represented in Cartesian coordinates: the first two rows are\nrelatedtothecasewhereapalletispresentintheenvironmentandtheoperatingfaceisfree,whilstthelast\ntwo rows represent samples of the dataset when no pallet is present or the operating face is not accessible\nby an autonomous forklift.",
      "size": 979,
      "sentences": 12
    },
    {
      "id": 25,
      "content": "ispresentintheenvironmentandtheoperatingfaceisfree,whilstthelast\ntwo rows represent samples of the dataset when no pallet is present or the operating face is not accessible\nby an autonomous forklift. The red box in the first image represent an example of region of interest, i.e.,\nthe part of the image actually where the pallet is located. March 15, 2019\n=== 페이지 10 ===\n• The Class1 and Class2 folders correspond to Pallet and NoPallet classes, respectively. They include 565 raw laser rangefinder scans in .txt format in total, 340 for the former\nclass and 225 for the latter. • DataSet565.mat is a file containing the whole dataset as a 761×565 MATLAB matrix. • PalletImages folder containing all the 250×250px images in various formats, divided\nby class and eventually accompained by the relative pallet’s ROI. In particular, the\nfiles PalletGrayImages.zip and RGBImages.tar.gz contains the images in .jpg and .png\nformat, respectively.",
      "size": 940,
      "sentences": 7
    },
    {
      "id": 26,
      "content": "divided\nby class and eventually accompained by the relative pallet’s ROI. In particular, the\nfiles PalletGrayImages.zip and RGBImages.tar.gz contains the images in .jpg and .png\nformat, respectively. • TrajectoryDataset folder contains 4 additional continous trajectories that we used to\ntest the architecture presented in our related research paper [1]. The trajectories are\nprovided in .mat format. Acknowledgements\nThe work by I. S. Mohamed was supported by a scholarship from the ERASMUS+\nEuropean Master on Advanced Robotics Plus (EMARO+) programme. The authors would\nlike to thank M.Eng. Yusha Kareem for his helping in data collection process. Conflict of interest\nThe authors declare that they have no conflict of interest relevant to this article. Transparency document. Supplementary material\nTransparency data associated with this article can be found in the online version at\nhttps://github.com/EmaroLab/PDT.",
      "size": 920,
      "sentences": 10
    },
    {
      "id": 27,
      "content": "interest relevant to this article. Transparency document. Supplementary material\nTransparency data associated with this article can be found in the online version at\nhttps://github.com/EmaroLab/PDT. References\nReferences\n[1] I. S. Mohamed, A. Capitanelli, F. Mastrogiovanni, S. Rovetta, R. Zaccaria, Detection, localisation and\ntrackingofpalletsusingmachinelearningtechniquesand2Drangedata,arXivpreprintarXiv:1803.11254\n(2018). [2] A.Geiger,P.Lenz,C.Stiller,R.Urtasun,Visionmeetsrobotics: TheKITTIdataset,TheInternational\nJournal of Robotics Research 32 (11) (2013) 1231–1237. [3] W. Maddern, G. Pascoe, C. Linegar, P. Newman, 1 year, 1000 km: The Oxford RobotCar dataset, The\nInternational Journal of Robotics Research 36 (1) (2017) 3–15. [4] J.Jeong,Y.Cho,Y.-S.Shin,H.Roh,A.Kim,Complexurbanlidardataset,in: 2018IEEEInternational\nConference on Robotics and Automation (ICRA), IEEE, 2018, pp. 6344–6351.",
      "size": 903,
      "sentences": 8
    },
    {
      "id": 28,
      "content": "botics Research 36 (1) (2017) 3–15. [4] J.Jeong,Y.Cho,Y.-S.Shin,H.Roh,A.Kim,Complexurbanlidardataset,in: 2018IEEEInternational\nConference on Robotics and Automation (ICRA), IEEE, 2018, pp. 6344–6351. [5] C.Rennie,R.Shome,K.E.Bekris,A.F.DeSouza,Adatasetforimprovedrgbd-basedobjectdetection\nand pose estimation for warehouse pick-and-place, IEEE Robotics and Automation Letters 1 (2) (2016)\n1179–1185. March 15, 2019\n=== 페이지 11 ===\n[6] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time object detection with region\nproposal networks, in: Advances in neural information processing systems, 2015, pp. 91–99. [7] E.V.Cuevas,D.Zaldivar,R.Rojas,Kalmanfilterforvisiontracking,TechnicalReport,FreieUniversität\nBerlin, Inst. Informatik, Berlin, Germany. March 15, 2019",
      "size": 776,
      "sentences": 10
    }
  ]
}