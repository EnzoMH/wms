{
  "source": "ArXiv",
  "filename": "018_Random_Network_Distillation_Based_Deep_Reinforceme.pdf",
  "total_chars": 29989,
  "total_chunks": 44,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nRandom Network Distillation Based Deep Reinforcement Learning for AGV\nPath Planning\nHuilin Yin1, Shengkai Su1, Yinjia Lin1, Pengju Zhen1, Karin Festl2, Daniel Watzenig2\nAbstract—With the flourishing development of intelligent planning environments. With the increasing complexity of\nwarehousing systems, the technology of Automated Guided the environments that agents need to process, Google’s AI\nVehicle (AGV) has experienced rapid growth. Within intel-\nteam DeepMind proposed the innovative concept of com-\nligent warehousing environments, AGV is required to safely\nbining deep learning, which is the processing of perceptual\nand rapidly plan an optimal path in complex and dynamic\nenvironments. Most research has studied deep reinforcement signals, with RL to form Deep Reinforcement Learning\nlearningtoaddressthischallenge.However,intheenvironments (DRL) [15].",
      "size": 878,
      "sentences": 4
    },
    {
      "id": 2,
      "content": "omplex and dynamic\nenvironments. Most research has studied deep reinforcement signals, with RL to form Deep Reinforcement Learning\nlearningtoaddressthischallenge.However,intheenvironments (DRL) [15]. The DeepMind team proposed a new approach\nwith sparse extrinsic rewards, these algorithms often converge to DRL, Deep Q-Learning (DQN) [16]. With the successful\nslowly, learn inefficiently or fail to reach the target. Random\napplication of DRL, much research has begun to explore\nNetwork Distillation (RND), as an exploration enhancement, DRLmethodstosolveproblemsofAGVpathplanning.Yang\ncaneffectivelyimprovetheperformanceofproximalpolicyop-\ntimization,especiallyenhancingtheadditionalintrinsicrewards et al. [17] combined a priori knowledge and the DQN algo-\nof the AGV agent which is in sparse reward environments. rithm to solve the problem of slow convergence of AGVs in\nMoreover, most of the current research continues to use 2D awarehouseenvironment.Panovetal.",
      "size": 966,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "AGV agent which is in sparse reward environments. rithm to solve the problem of slow convergence of AGVs in\nMoreover, most of the current research continues to use 2D awarehouseenvironment.Panovetal. [18]studiedtheDQN\ngrid mazes as experimental environments. These environments\nalgorithm to static grid maps and proved that the algorithm\nhave insufficient complexity and limited action sets. To solve\ncan obtain effective path planning. However, most of the\nthis limitation, we present simulation environments of AGV\npathplanningwithcontinuousactionsandpositionsforAGVs, related research on AGV path planning problems use 2D\nso that it can be close to realistic physical scenarios. Based on gridmapsforexperiments,whicharestillfarfromtheactual\nour experiments and comprehensive analysis of the proposed physical environment.",
      "size": 824,
      "sentences": 7
    },
    {
      "id": 4,
      "content": "so that it can be close to realistic physical scenarios. Based on gridmapsforexperiments,whicharestillfarfromtheactual\nour experiments and comprehensive analysis of the proposed physical environment. Real path planning environments are\nmethod, the results demonstrate that our proposed method\nusually complex and thus current researches haven’t solved\nenablesAGVtomorerapidlycompletepathplanningtaskswith\nthe problem of slow searching of agents in sparse reward\ncontinuousactionsinourenvironments.Avideoofpartofour\nexperiments can be found at https://youtu.be/lwrY9YesGmw. environments. In order to solve the above problems, we\nwill propose a method that can improve intrinsic rewards\nI. INTRODUCTION and conduct experiments in simulation environments that\nWith the development of the industrial digitalisation, in- approximate the real physical environment.",
      "size": 858,
      "sentences": 6
    },
    {
      "id": 5,
      "content": "improve intrinsic rewards\nI. INTRODUCTION and conduct experiments in simulation environments that\nWith the development of the industrial digitalisation, in- approximate the real physical environment. telligent warehousing systems [1] have become an impor- As the Proximal Policy Optimisation (PPO) algorithm\ntant part of industrial production. Nowadays, Automated [19] has been proven to be widely applicable in complex\nGuided Vehicle (AGV) [2] plays a crucial role in intelligent environments, we use PPO as a deep reinforcement learning\nwarehousing systems and its path planning has become the method in this paper. Xiao et al. [20] introduced distributed\nfocus of research. The path planning algorithms [3]–[5] of sample collection training policy and Beta policy for action\nAGVs develop rapidly. Researchers have proposed many sampling, which exhibits stronger robustness in the PPO\nclassical path planning algorithms such as A* algorithm [6], algorithm.",
      "size": 958,
      "sentences": 8
    },
    {
      "id": 6,
      "content": "policy for action\nAGVs develop rapidly. Researchers have proposed many sampling, which exhibits stronger robustness in the PPO\nclassical path planning algorithms such as A* algorithm [6], algorithm. Our team [21] use a curiosity-driven model to\n[7], Rapidly-Exploring Random Tree (RRT) [8], Dynamic enhance the exploration of the AGV agent. Shi et al. [22]\nWindow Approach [9] and Particle Swarm Optimization studied a dynamic hybrid reward mechanism based on the\n[10], which have been widely used in simple environments. PPOalgorithmtosolvetheRLproblemwithsparserewards. However, in realistic scenarios, most of these remain in In DRL, reward shaping [23] can solve the problem of\nsimulation due to the computational complexity as well as sparse reward environment, but constructing suitable reward\nthe limitations of the real environment.",
      "size": 840,
      "sentences": 7
    },
    {
      "id": 7,
      "content": "rd shaping [23] can solve the problem of\nsimulation due to the computational complexity as well as sparse reward environment, but constructing suitable reward\nthe limitations of the real environment. functions is not easy, and in most cases, reward shaping\nAt this time, Reinforcement Learning (RL) [11] has been limits the performance of algorithms such as PPO. In order\nstudiedto solvethepathplanning problem.NAIRet al. [13] to solve the sparse reward problem, we propose to introduce\nproposed a path planning and obstacle avoidance method an exploratory method for the deep reinforcement learning\nModified Temporal Difference Learning for environment algorithm PPO. The basic method of Random Network Dis-\nwhere static obstacles are known. On the basis of Temporal- tillation (RND) [24]–[26] is to increase the intrinsic rewards\nDifference (TD) algorithm, WATKINS et al.",
      "size": 873,
      "sentences": 6
    },
    {
      "id": 8,
      "content": "asic method of Random Network Dis-\nwhere static obstacles are known. On the basis of Temporal- tillation (RND) [24]–[26] is to increase the intrinsic rewards\nDifference (TD) algorithm, WATKINS et al. [14] proposed of agents and assist the extrinsic rewards to enable agents\nQ-Learning algorithm that is widely used in discrete path to better explore the environment. This has not been studied\nfor path planning yet. Combining the PPO algorithm with\n1 Huilin Yin, Shengkai Su, Yinjia Lin and Pengju Zhen are with the intrinsic reward measurements from RND, we augment\nthe School of Electronic and Information Engineering, Tongji University, the extrinsic reward in the environment during AGV path\nShanghai,China. planning. In addition, in order to be able to simulate the\n2KarinFestlandDanielWatzenigarewiththeVirtualVehicleResearch\nGmbH,GrazUniversityofTechnology,8010Graz,Austria.",
      "size": 881,
      "sentences": 7
    },
    {
      "id": 9,
      "content": "ring AGV path\nShanghai,China. planning. In addition, in order to be able to simulate the\n2KarinFestlandDanielWatzenigarewiththeVirtualVehicleResearch\nGmbH,GrazUniversityofTechnology,8010Graz,Austria. path planning in real intelligent warehouses, we set up\n4202\nrpA\n91\n]OR.sc[\n1v49521.4042:viXra\n=== 페이지 2 ===\nexperimental environments for AGVs, and the experimental\nresultsshowthatbyenhancingtheextrinsicrewardsthrough\nthe intrinsic rewards of RND, our proposed method is able\nto explore several sparsely rewarded AGV path-planning\nenvironments more efficiently and stable. In summary, our\ncontributionsofthispaperincludethefollowingtwoaspects. • We propose a novel AGV path planning method RND-\nPPO, which combines the random network distillation\nmechanism with the PPO algorithm. Extrinsic rewards\nfrom environmental feedback are enhanced by addi-\ntional intrinsic rewards, to solve the problem of AGVs\nthat learn hard in sparsely rewarded path planning\nenvironments. Fig.1.",
      "size": 976,
      "sentences": 8
    },
    {
      "id": 10,
      "content": "thm. Extrinsic rewards\nfrom environmental feedback are enhanced by addi-\ntional intrinsic rewards, to solve the problem of AGVs\nthat learn hard in sparsely rewarded path planning\nenvironments. Fig.1. TopviewschematicofAGVpathplanningsimulationscenario. • We design simulation AGV agent path planning en-\nvironments with physical rigid body properties and\nrecords the 3D position of agent, the 3D position of target\ncontinuous motion space. The environments have both\nobject, and the X-axis component and Z-axis component of\nfixed and randomly generated target objects to simulate\nthespeedoftheagentareobservedrespectively.Inaddition,\nthe real environment. the AGV agent is equipped with two 3D ray perception\nThe rest of this paper is organized as follows. Section\nsensors, one for detecting the information around the agent\nII describes the AGV path planning environment model. with 10 rays and horizontal field of view of 360 degrees.",
      "size": 936,
      "sentences": 9
    },
    {
      "id": 11,
      "content": "ganized as follows. Section\nsensors, one for detecting the information around the agent\nII describes the AGV path planning environment model. with 10 rays and horizontal field of view of 360 degrees. In Section III, the framework of our proposed RND-PPO\nThe other one intensively detects information in the forward\nmethod is presented and related algorithms are given in\ndirection of agent, with 7 rays and horizontal field of view\ndetail. The experiments and results are demonstrated in\nof 120 degrees. Each ray can detect 2 targets including\nSection IV. Finally, Section V presents the conclusion and\nwall and target object, and each ray has two dimensions to\nfuture work. detect collision or not, so the total observation dimensions\nII. AGVPATHPLANNINGENVIRONMENTMODEL are (10+7)×(2+2). We design AGV path planning environment model with The computation of reward function within each learning\nreal physical body and action.",
      "size": 927,
      "sentences": 10
    },
    {
      "id": 12,
      "content": "ensions\nII. AGVPATHPLANNINGENVIRONMENTMODEL are (10+7)×(2+2). We design AGV path planning environment model with The computation of reward function within each learning\nreal physical body and action. For the situations that AGV episodeisdividedintotwoparts.Onefortheextrinsicreward\nagentsneedtofaceinrealphysicalenvironments,wedesign r e obtained from the interaction of agent with environment,\nmultiple sets of models based on a simple scene and a and the other for the intrinsic reward r i given to agent by\ncomplex scene, both of which consist of a closed square our proposed RND-PPO model, which will be introduced in\nspace, an AGV agent body, multiple static obstacles and a the next section. Thus the total reward is written as\ntarget object. The complex scene is four times the size of r =re+ri. (1)\nt t\nthesimplescene.Mostoftheresearchisbasedonstudiesof\nstatic environments.",
      "size": 882,
      "sentences": 7
    },
    {
      "id": 13,
      "content": "next section. Thus the total reward is written as\ntarget object. The complex scene is four times the size of r =re+ri. (1)\nt t\nthesimplescene.Mostoftheresearchisbasedonstudiesof\nstatic environments. In order to better test the performance When the agent collids with the target object, the extrinsic\nof RND-PPO in different environments, we added dynamic reward is set to 5. This value is an empirical value obtained\ntarget objects to these scenes. The simulation scenes are from several experiments. In order to make the agent find\nshown in Fig. 1. The simple scene on the left has two the target as soon as possible, we design a tiny single-step\nrandomly generated targets and the complex scene on the negative reward. The extrinsic reward function is defined as\nright has three. Target objects are represented by red blobs (cid:40)\n−1 single-step\nand the agent is a blue blob.",
      "size": 879,
      "sentences": 12
    },
    {
      "id": 14,
      "content": "he complex scene on the negative reward. The extrinsic reward function is defined as\nright has three. Target objects are represented by red blobs (cid:40)\n−1 single-step\nand the agent is a blue blob. re = MaxStep (2)\nt\nThe AGV agent is described by a set of state variables 5 agent collides target object\nand interacts with the environment by performing actions to\nThe MaxStep is the maximum number of steps an agent can\nchangeitsstatevariables.Toreplacethediscreteactionsused\nexplore in a learning episode. inmostresearch,webuildtheagentasrigidbodyandcreate\na continuous action space for it. The continuous actions are III. AGVPATHPLANNINGBASEDONRND-PPO\ngenerated from a neural network and then passed to the It is often impractical to design dense reward functions\naction function, which processes the received action vectors.",
      "size": 828,
      "sentences": 7
    },
    {
      "id": 15,
      "content": "ATHPLANNINGBASEDONRND-PPO\ngenerated from a neural network and then passed to the It is often impractical to design dense reward functions\naction function, which processes the received action vectors. for tasks of RL agents, so agents need to explore the\nIn this paper, the environment contains two consecutive environmentinatargetedmanner.RND[24]wasintroduced\nvectors representing the control forces in the X-axis and as an exploration method for DRL methods, and it has the\nZ-axis, which are transmitted to the physical force to make flexibility to combine intrinsic and extrinsic rewards. agent move. A. Framework of AGV Path Planning with RND-PPO\nDuring training, the sensors provide state information to\nthe agent, such as the position, velocity, colour of other The key to solving the AGV path planning problem\nobjects in the environment and so on.",
      "size": 853,
      "sentences": 5
    },
    {
      "id": 16,
      "content": "ng training, the sensors provide state information to\nthe agent, such as the position, velocity, colour of other The key to solving the AGV path planning problem\nobjects in the environment and so on. In our model, the using RL is how the AGV agent updates its own action\ninternalobservationspacedimensionoftheagentis8,which policybasedonthereceivedrewardstoobtainthemaximum\n=== 페이지 3 ===\nThe prediction network defined as: fˆ : S → Rk, network\nparameter is denoted as φˆ which is trained to minimise the\nprediction error. The parameter φˆ is updated by minimising\nthe expected value of the mean square error (cid:13) (cid:13)fˆ(s\nt\n;φˆ) −\n(cid:13)2\nf(s\nt\n;φ)(cid:13) through gradient descent algorithm. The agent\nwill input the observation s obtained from the environment\nt\ninto the target network f, at which time f(s ) serves as\nt\nthe prediction target of the prediction network fˆ.",
      "size": 884,
      "sentences": 4
    },
    {
      "id": 17,
      "content": "escent algorithm. The agent\nwill input the observation s obtained from the environment\nt\ninto the target network f, at which time f(s ) serves as\nt\nthe prediction target of the prediction network fˆ. When the\nprediction network fˆ(s ) is input with a novel state, due to\nt\nthe large discrepancy between this distribution and inputs\nit has ever received, the agent will receive a large intrinsic\nreward as\nr t i = (cid:13) (cid:13)fˆ(s t ;φˆ)−f(s t ;φ) (cid:13) (cid:13) 2 . (3)\nC. AGV agent path planning with RND-PPO\nIn the sparse reward environment, we propose an explo-\nration mechanism that uses RND based PPO to motivate the\nagenttofindmorenovelstates.First,wegivetheconceptof\nFig.2. StructureoftheproposedRND-PPO. state novelty which can be measured by the prediction error. For AGV agent observing the state s at the current moment,\nthe fewer the number of states similar to state s among all\ncumulative reward value.",
      "size": 924,
      "sentences": 7
    },
    {
      "id": 18,
      "content": "novelty which can be measured by the prediction error. For AGV agent observing the state s at the current moment,\nthe fewer the number of states similar to state s among all\ncumulative reward value. In the real AGV path planning previously visited states, the more novel state s is. environments,therewardsaresparse,inthiscaseweneedto The PPO algorithm is essentially a model-free algorithm,\nuseintrinsicrewardstoguidetheAGVagenttofullyexplore and its core architecture remains an Actor-Critic algorithm. the state space and action space in the environment, so we\ndesign a new exploration mechanism RND-PPO to motivate\nthe agent to explore the environment. Algorithm 1 RND-PPO Pseudocode\nThe structure of our proposed RND-PPO is shown in 1: Input: training epochs per collect E; batch size B;\nFig. 2. The AGV agent training process is divided into two number of learning episodes M; length of learning\nstages.",
      "size": 909,
      "sentences": 8
    },
    {
      "id": 19,
      "content": "roposed RND-PPO is shown in 1: Input: training epochs per collect E; batch size B;\nFig. 2. The AGV agent training process is divided into two number of learning episodes M; length of learning\nstages. The yellow box is the RND stage and the other episodes N; number of predict optimization steps N pre\npart connected to it is PPO training stage. RND defines a 2: Initialize policy network parameters θ\nnew training stage and the RND training alternates with 3: Initialize fixed target network parameters φ and predic-\nthe training of the agent. The model obtained from the tion network parameters φˆ\nRND training is input to PPO and used to generate the 4: for m=1 to M do\ncorresponding intrinsic rewards.",
      "size": 704,
      "sentences": 6
    },
    {
      "id": 20,
      "content": "and predic-\nthe training of the agent. The model obtained from the tion network parameters φˆ\nRND training is input to PPO and used to generate the 4: for m=1 to M do\ncorresponding intrinsic rewards. The next stage is the agent 5: CollectasetoftrajectoriesT m ={s t ,s t+1 ,a t ,r t e}by\ntraining stage, which is a stage of using the trained RND run policy π θm m Timesteps\nmodel, combining the intrinsic rewards predicted by the 6: Update observation normalization parameters by T m\nRND model with the RL algorithm PPO. In the end, the 7: for i=1 to N pre do\nagent completes the learning of the optimal policy by using 8: Sample a t ∼π(a t |s t )\nt e h n e vi o ro b n ta m in e e n d t s e t x a t t r e in s s t i . c rewards r t e, intrinsic rewards r t i and 1 9 0 : : S C a a m lc p u l l e ate s t+ i 1 n , tr r i t e ns ∼ ic p( r s e t w + a 1 r , d r t e r |s t i t , = a t ) (cid:13) (cid:13)fˆ(s t+1 ;φˆ) −\n(cid:13)2\nf(s\nt+1\n;φ)(cid:13)\nB.",
      "size": 950,
      "sentences": 5
    },
    {
      "id": 21,
      "content": "ds r t i and 1 9 0 : : S C a a m lc p u l l e ate s t+ i 1 n , tr r i t e ns ∼ ic p( r s e t w + a 1 r , d r t e r |s t i t , = a t ) (cid:13) (cid:13)fˆ(s t+1 ;φˆ) −\n(cid:13)2\nf(s\nt+1\n;φ)(cid:13)\nB. Random Network Distillation Model 11: Optimize φˆ w.r.t. distillation loss ri using Adam\nt\nToaddressthelackofexplorationofPPOinsparsereward 12: end for\nenvironments, among the intrinsic reward methods used for 13: Normalize r t i, obtain normalized intrinsic reward rˆ t i\nexploration,we invokeRandomNetwork Distillation(RND) 14: Normalize r t e, obtain normalized extrinsic reward rˆ t e\nwhich is a technique based on prediction error. The model 15: Calculate total reward r t =αrˆ t e+βrˆ t i, and obtain the\nfinal trajectories Tˆ ={s ,s ,a ,r }\nis presented in the yellow part of Fig. 2. In RND, the agent m t t+1 t t\nfirst constructs a randomly-fixed target neural network f, 16: C\nva\na\nl\nl\nu\nc\ne\nul\no\na\nn\nte\nTˆ\nadvantageestimatesAˆ θm usingEq.",
      "size": 948,
      "sentences": 6
    },
    {
      "id": 22,
      "content": "s presented in the yellow part of Fig. 2. In RND, the agent m t t+1 t t\nfirst constructs a randomly-fixed target neural network f, 16: C\nva\na\nl\nl\nu\nc\ne\nul\no\na\nn\nte\nTˆ\nadvantageestimatesAˆ θm usingEq. (4)with\nwhere fixed means that it will not be updated throughout m\nthe learning process, and constructs a prediction network 17: for e=1 to E do\nfˆ, whose goal is to predict the output of the randomly-set 18: Sample minibatch b episodes from Tˆ m\ntarget network f. The target network f and the prediction 19: Updatepolicyparametersθ bymaximizingL(θ)in\nnetwork fˆ map the observations S to the reward Rk. The Eq. (6) with Adam, where ratio is used by Eq. (5)\ntarget network defined as: f : S → Rk, network parameter 20: end for\nis denoted as φ and remain fixed after random initialisation.",
      "size": 788,
      "sentences": 7
    },
    {
      "id": 23,
      "content": "the reward Rk. The Eq. (6) with Adam, where ratio is used by Eq. (5)\ntarget network defined as: f : S → Rk, network parameter 20: end for\nis denoted as φ and remain fixed after random initialisation. 21: end for\n=== 페이지 4 ===\nThe critic network fits the state value function and action to prevent the update from being too large and causing the\nvaluefunctionthroughtheenvironmentalstateinformations training to be unsmooth. observedbyanagent, andupdatescriticnetworkparameters In our proposed method shown in Algorithm 1, the first\nbycalculatingadvantagefunctionandusingthemeansquare three lines initialise various parameters of the RND-PPO. errorascriticlossfunction.Theadvantagefunctionisshown After that, the training process of the RND model starts to\nas indicate lines in Algorithm 1.",
      "size": 789,
      "sentences": 7
    },
    {
      "id": 24,
      "content": "lines initialise various parameters of the RND-PPO. errorascriticlossfunction.Theadvantagefunctionisshown After that, the training process of the RND model starts to\nas indicate lines in Algorithm 1. The parameters of the target\nA = (cid:88) βt′−tr −V (s;θ), (4) networkφarefixed,andaccordingtothestochasticgradient\nt t′ π\ndescent method, the expected value of the mean square\nt′>t\nwhere β is a tunable coefficient. Different estimates can be error (cid:13) (cid:13)fˆ(s t ;φˆ) − f(s t ;φ) (cid:13) (cid:13) 2 is minimised, and prediction\nnetwork parameters φˆ are optimised. This RND process\nobtained by adjusting β. When updating the actor network,\ncan be regarded as doing distillation between the target\nPPO uses two networks with the same structure to preserve\nnetwork,whichisrandomlygeneratedwithfixedparameters,\nthe old and new policy.",
      "size": 842,
      "sentences": 6
    },
    {
      "id": 25,
      "content": "etwork,\ncan be regarded as doing distillation between the target\nPPO uses two networks with the same structure to preserve\nnetwork,whichisrandomlygeneratedwithfixedparameters,\nthe old and new policy. The policy ratio is used to measure\nand the prediction network, whose parameters are to be\nthe ratio of the probability of taking a certain state-action\nupdated,sothatthepredictionnetworkisconstantlycloseto\npair (s,a) under the new policy to the probability of taking\nthe target network. Then the training process of the agent\nthe same state-action pair under the old policy. The policy\nusing the PPO algorithm based on intrinsic and extrinsic\nratio is defined as\nrewardsstartstoindicatelinesinAlgorithm1.Theserewards\nr (θ)= π θ (a t |s t ) . (5) are first normalised separately to compute the final set of\nt π training trajectories.",
      "size": 833,
      "sentences": 5
    },
    {
      "id": 26,
      "content": "extrinsic\nratio is defined as\nrewardsstartstoindicatelinesinAlgorithm1.Theserewards\nr (θ)= π θ (a t |s t ) . (5) are first normalised separately to compute the final set of\nt π training trajectories. In the last stages, it combines intrinsic\nθold(at|st)\nmotivationswithextrinsicrewardstocalculatetheadvantage\nPPO introduces a new clip mechanism, which can effec-\nfunction and value function, subsequently refining PPO by\ntivelyreducethenumberofcomputationstepswhilelimiting\nupdating the policy parameters θ.\nthe magnitude of policy update, and it is defined as follows\nIV. EXPERIMENTS\nL(θ)=E [min(r (θ)Aˆ,clip(r (θ),1−ϵ,1+ϵ)Aˆ)], (6)\nt t t t t\nIn this section, we evaluate our method RND-PPO, for\nwhere ϵ is a hyperparameter, Aˆ t is an estimate of the learning AGV agent path planning policy in two groups of\nadvantage function at time step t. The purpose of setting experiments.",
      "size": 880,
      "sentences": 4
    },
    {
      "id": 27,
      "content": "od RND-PPO, for\nwhere ϵ is a hyperparameter, Aˆ t is an estimate of the learning AGV agent path planning policy in two groups of\nadvantage function at time step t. The purpose of setting experiments. First, we introduce details of our implementa-\n1−ϵ,1+ϵ is to specify the magnitude of the policy update tion, including the hyperparameters. Then, we compare our\nFig.3. Behabiorofrewardandepisodelengthinthesimplestaticscenario. Fig. 4. Behavior of reward and episode length in the simple dynamic\n(a)environmentcumulativerewardoftheAGVagentand(b)episodelength scenario. (a) environment cumulative reward of the AGV agent and (b)\noftheAGVagent. episodelengthoftheAGVagent. === 페이지 5 ===\nFig.5. Complexstaticenvironmentpathplanningtrajectories.Fromleftto Fig. 6. Complex dynamic environment path planning trajectories. From\nright,thetrainingepisodesare0.25,0.5,0.75and1.0·106. (a)corresponds left to right, the training episodes are 0.25, 0.5, 0.75 and 1.0·106.",
      "size": 958,
      "sentences": 15
    },
    {
      "id": 28,
      "content": "Complex dynamic environment path planning trajectories. From\nright,thetrainingepisodesare0.25,0.5,0.75and1.0·106. (a)corresponds left to right, the training episodes are 0.25, 0.5, 0.75 and 1.0·106. (a)\ntothePPOand(b)correspondstotheRND-PPO. correspondstothePPOand(b)correspondstotheRND-PPO. proposed method with the baseline PPO in both static and would earn a reward as shown in Fig. 4. Although PPO\ndynamic scenarios. Static and dynamic environments also relies on search by chance to find the first target object\ninclude simple and complex scenarios, respectively. Experi- faster than RND-PPO, it needs to spend a large number\nments show that using RND can improve the efficiency and of training episodes in searching the second target object. stability of AGV agent learning path planning policy. PPO can find the target object with an average of 170\nsteps after requiring 0.52 · 106 training episodes and get\nA. Experimental Settings\nan environmental cumulative reward of 4.8.",
      "size": 982,
      "sentences": 13
    },
    {
      "id": 29,
      "content": "h planning policy. PPO can find the target object with an average of 170\nsteps after requiring 0.52 · 106 training episodes and get\nA. Experimental Settings\nan environmental cumulative reward of 4.8. In contrast, our\nThe AGV agent body and the target object are spheres proposedRND-PPOmethodonlyrequires0.18·106 training\nwith a radiuis of 0.5 per unit length, the size of the simple episodes to get same environmental cumulative reward with\nscene is 20×20, the size of the complex scene is 40×40. an average of 187 steps. The maximum number of steps for each learning episode\nC. Complex Scene Experiments\nof the static and dynamic experiments in the simple scene\nis 2000, and the maximum number of steps for the complex The complex scene is a 40×40 map: (−12.0,0.5,−16.0)\nstaticanddynamicscenesis3000and4000,respectively.The isthestartlocationofagent,and(17.0,0.5,15.0)istheloca-\nnumber of learning episodes in each experiment is 1·106. tion of target object in the static experiment.",
      "size": 984,
      "sentences": 7
    },
    {
      "id": 30,
      "content": "micscenesis3000and4000,respectively.The isthestartlocationofagent,and(17.0,0.5,15.0)istheloca-\nnumber of learning episodes in each experiment is 1·106. tion of target object in the static experiment. In the dynamic\nThe reward function of AGV agent is shown in Eq. (1). All experiment, the target object will be randomly generated\nthe experiments are carried out on an AMD Ryzen 7 5800H in (15.0,0.5,2.0), (15.0,0.5,−17.0) and (−17.0,0.5,15.0). 3.20 GHz PC with 16GB memory. Fig. 5 and Fig. 6 show the trajectory of the AGV agent\nafter training 0.25, 0.5, 0.75 and 1.0·106 episodes in the\nB. Simple Scene Experiments\ncomplex static and dynamic environment respectively.",
      "size": 668,
      "sentences": 10
    },
    {
      "id": 31,
      "content": "ory. Fig. 5 and Fig. 6 show the trajectory of the AGV agent\nafter training 0.25, 0.5, 0.75 and 1.0·106 episodes in the\nB. Simple Scene Experiments\ncomplex static and dynamic environment respectively. The\nThesimplesceneisa20×20map:(−5.0,0.5,−8.0)isthe AGV agent trained using RND-PPO has found the path to\nstartlocation ofagent, and (5.0,0.5,−1.5) isthe locationof reachthetargetobjectafter0.2·106 episodes,whiletheagent\ntargetobjectinstaticexperiment.Indynamicexperiment,the trained only by PPO is still exploring the space around the\ntarget object will be randomly generated in (5.0,0.5,−1.5) starting position. The two metrics used for evaluation can\nand (−8.0,0.5,−1.0). be found in Fig. 7, and our proposed method is the better\nWe test our method in the simple scene and choose three performer on both data.",
      "size": 811,
      "sentences": 9
    },
    {
      "id": 32,
      "content": "The two metrics used for evaluation can\nand (−8.0,0.5,−1.0). be found in Fig. 7, and our proposed method is the better\nWe test our method in the simple scene and choose three performer on both data. In the static environment, after the\nmetrics including training episodes, environment cumulative same training of 0.2·106 episodes, our proposed method is\nreward and episode length, to evaluate our experimental alreadyabletoobtainanenvironmentalcumulativerewardof\nresults. First, we test our proposed method in the simple 4.85 in 492 steps of an episode, while the PPO can hardly\nstatic environment. In the simple static environment, after get an environmental reward value of −1, which means it\n0.18·106 episodes of training, our proposed method RND- still expores the environment. The agent trained by PPO is\nPPOisabletoobtaintheenvironmentalcumulativerewardof able to reach the same environmental reward value of 4.85\n4.8 within 280 steps of an episode.",
      "size": 955,
      "sentences": 7
    },
    {
      "id": 33,
      "content": "expores the environment. The agent trained by PPO is\nPPOisabletoobtaintheenvironmentalcumulativerewardof able to reach the same environmental reward value of 4.85\n4.8 within 280 steps of an episode. In contrast, as shown in only after beeing trained for at least 0.34·106 episodes. Fig. 3, the PPO algorithm without RND performs poorly, In the dynamic experiment, Fig. 6 shows the experimen-\nand the agent is able to obtain the same environmental tal results of the AGV agent after being trained by PPO\nreward value within 238 steps over 0.39·106 episodes of and RND-PPO. It can be seen that our proposed method\ntraining. However, performance of PPO is worse in the RND-PPO can find the optimal path quickly and accurately\nsimple dynamic environment where there are two randomly when the target object randomly appears in three positions. generated target objects, and since there is no intrinsic However, the agent trained only by using PPO can only\nreward for exploration of the environment.",
      "size": 993,
      "sentences": 9
    },
    {
      "id": 34,
      "content": "object randomly appears in three positions. generated target objects, and since there is no intrinsic However, the agent trained only by using PPO can only\nreward for exploration of the environment. It is difficult for find the target object located in the upper left corner due to\nPPO to explore the location of the other target object that the fact that there is no intrinsic reward that can motivate\n=== 페이지 6 ===\nFig. 7. Behavior of reward and episode length in the complex static Fig. 8. Behavior of reward and episode length in the complex dynamic\nscenario. (a) environment cumulative reward of the AGV agent and (b) scenario. (a) environment cumulative reward of the AGV agent and (b)\nepisodelengthoftheAGVagent. episodelengthoftheAGVagent. the AGV agent to explore the whole environment. The approachisefficientwithgoodperformance.TheRND-PPO\nrelationship between the three metrics is shown in Fig.",
      "size": 905,
      "sentences": 12
    },
    {
      "id": 35,
      "content": "ftheAGVagent. episodelengthoftheAGVagent. the AGV agent to explore the whole environment. The approachisefficientwithgoodperformance.TheRND-PPO\nrelationship between the three metrics is shown in Fig. 8. agent makes use of intrinsic rewards, avoids limiting itself\nThe agent trained by RND-PPO found the first target object to a single rewarded target object, and adapts quickly to\nafter 0.07 · 106 episodes of training, while PPO did not changes in the external environment. We adopt the widely\ncomplete this goal until around 0.16·106 episodes. During used PPO algorithm as the basic implementation, which can\nthe 0.08−0.16·106 episodes of training, thecurve of RND- in principle be extended to other RL algorithms (e.g., SAC). PPO fluctuated due to the presence of dynamic objects, and Our future work will focus on statistical analysis of RND-\nfell into a short struggle in exploring the new environment.",
      "size": 907,
      "sentences": 8
    },
    {
      "id": 36,
      "content": "orithms (e.g., SAC). PPO fluctuated due to the presence of dynamic objects, and Our future work will focus on statistical analysis of RND-\nfell into a short struggle in exploring the new environment. PPOinmorecomplexdynamicenvironmentstooptimisethe\nBut soon with the help of the intrinsic rewards of RND, the use of intrinsic rewards. agent learnt the paths to reach the three target objects. The\nACKNOWLEDGMENT\nagent under RND-PPO training is able to reach more than\n4.8 environment cumulative reward after 0.24·106 episodes This work was supported by the National Natural Science\nwith257stepsperepisode.ThePPO,ontheotherhand,still Foundation of China under Grant No. 62133011 and the\nfailed to complete the entire path planning task until the end SpecialFundsoftheTongjiUniversityfor”Sino-GermanCo-\noftraining.Inconclusion,ourproposedmethodexplorestatic operation 2.0 Strategy” No. ZD2023001.",
      "size": 894,
      "sentences": 7
    },
    {
      "id": 37,
      "content": "complete the entire path planning task until the end SpecialFundsoftheTongjiUniversityfor”Sino-GermanCo-\noftraining.Inconclusion,ourproposedmethodexplorestatic operation 2.0 Strategy” No. ZD2023001. The authors would\nand dynamic environment faster in both simple or complex like to thank TU¨V SU¨D for the kind and generous support. scene than the agent trained with PPO only. We are also grateful for the efforts from our colleagues\nin Sino German Center of Intelligent Systems in Tongji\nV. CONCLUSIONS University. In this paper, we propose a novel method RND-PPO\nREFERENCES\nfor AGV path planning, which introduces random network\ndistillation mechanism to give intrinsic rewards to the AGV [1] Zhang,Wenbo,etal.”Real-TimeConflict-FreeTaskAssignmentand\nagent to address the effect of sparse reward environments PathPlanningofMulti-AGVSysteminIntelligentWarehousing.”2018\n37thChineseControlConference(CCC),2018.\nand to improve the speed of training.",
      "size": 948,
      "sentences": 6
    },
    {
      "id": 38,
      "content": "and\nagent to address the effect of sparse reward environments PathPlanningofMulti-AGVSysteminIntelligentWarehousing.”2018\n37thChineseControlConference(CCC),2018.\nand to improve the speed of training. In addition, we have\n[2] Ryck,M.De,M.Versteyhe,andF.Debrouwere.”AutomatedGuided\ndeveloped simulated environments with realistic physical VehicleSystems,State-of-the-artControlAlgorithmsandTechniques.”\nstatescontainingthelocationofstaticobstaclesanddynamic JournalofManufacturingSystems,Vol.54,No.1,2020:152-173. [3] Chun-Ying, Wang, L. Ping, and Q. Hong-Zheng. ”Review on Intel-\ntargets. We evaluate our approach with different scenarios. ligent Path Planning Algorithm of Mobile Robots.” Transducer and\nBoth qualitative and quantitative experiments show that our MicrosystemTechnologies,2024. === 페이지 7 ===\n[4] Guo, H. L., Hao, Y. Y. ”Warehouse AGV path planning based [16] Mnih, Volodymyr, et al.",
      "size": 898,
      "sentences": 8
    },
    {
      "id": 39,
      "content": "and\nBoth qualitative and quantitative experiments show that our MicrosystemTechnologies,2024. === 페이지 7 ===\n[4] Guo, H. L., Hao, Y. Y. ”Warehouse AGV path planning based [16] Mnih, Volodymyr, et al. ”Playing Atari with Deep Reinforcement\non Improved A* algorithm.” Eighth International Conference on Learning.”ComputerScience,2013. ElectromechanicalControlTechnologyandTransportation(ICECTT), [17] Yang,L.Juntao,andP.Lingling.”Multi-robotPathPlanningBasedon\n2023. aDeepReinforcementLearningDQNAlgorithm.”CAAITransactions\n[5] Wen,Tao,andLiSun.”ResearchonOptimizationAlgorithmofAGV onIntelligenceTechnology,Vol.5,No.3,2020:177-183. Path Planning.” 2021 4th International Conference on Information [18] Panov, Aleksandr I., K. S. Yakovlev, and R. Suvorov. ”Grid Path\nSystemsandComputerAidedEducation,2021. Planning with Deep Reinforcement Learning: Preliminary Results.”\n[6] Guruji, Akshay Kumar, H. Agarwal, and D. K. Parsediya.",
      "size": 926,
      "sentences": 8
    },
    {
      "id": 40,
      "content": "Yakovlev, and R. Suvorov. ”Grid Path\nSystemsandComputerAidedEducation,2021. Planning with Deep Reinforcement Learning: Preliminary Results.”\n[6] Guruji, Akshay Kumar, H. Agarwal, and D. K. Parsediya. ”Time- ProcediaComputerScience,Vol.123,2018:347-353.\nefficient A* Algorithm for Robot Path Planning.” International Con- [19] Schulman, John, et al. ”Proximal Policy Optimization Algorithms.”\nferenceonInnovationsinAutomationandMechatronicsEngineering, arXivpreprintarXiv:1707.06347,2017. 2017. [20] Xiao, Qianhao, et al. ”An Improved Distributed Sampling PPO Al-\n[7] Song, Yuanchang Bucknall, Richard. ”Smoothed A* Algorithm for gorithm Based on Beta Policy for Continuous Global Path Planning\nPractical Unmanned Surface Vehicle Path Planning.” Applied Ocean Scheme.”Sensors,Vol.23,No.13,2023:6101. Research,Vol.83,2019:9-20. [21] Yin H, Lin Y, Yan J, et al.",
      "size": 858,
      "sentences": 11
    },
    {
      "id": 41,
      "content": "or Continuous Global Path Planning\nPractical Unmanned Surface Vehicle Path Planning.” Applied Ocean Scheme.”Sensors,Vol.23,No.13,2023:6101. Research,Vol.83,2019:9-20. [21] Yin H, Lin Y, Yan J, et al. ”AGV Path Planning Using Curiosity-\n[8] Wang,Wei,H.Deng,andX.Wu.”PathPlanningofLoadedPin-jointed DrivenDeepReinforcementLearning.”2023IEEE19thInternational\nbar Mechanisms Using Rapidly-exploring Random Tree Method.” Conference on Automation Science and Engineering (CASE), 2023:\nComputers&Structures,Vol.209,2018:65-73. 1-6. [9] Lee,DhongHun,etal.”FiniteDistributionEstimation-BasedDynamic [22] ShiJ,ZhangT,,etal.”EfficientLane-changingBehaviorPlanningvia\nWindowApproachtoReliableObstacleAvoidanceofMobileRobot.” ReinforcementLearningwithImitationLearningInitialization,”2023\nIEEETransactionsonIndustrialElectronics,Vol.68,No.10,2021:998- IEEEIntelligentVehiclesSymposium(IV),2023. 1006.",
      "size": 887,
      "sentences": 7
    },
    {
      "id": 42,
      "content": "AvoidanceofMobileRobot.” ReinforcementLearningwithImitationLearningInitialization,”2023\nIEEETransactionsonIndustrialElectronics,Vol.68,No.10,2021:998- IEEEIntelligentVehiclesSymposium(IV),2023. 1006. [23] Ng,AndrewY.,DaishiHarada,andStuartRussell.”PolicyInvariance\nunder Reward Transformations: Theory and Application to Reward\n[10] Song, Zidong Zou, Lei. ”An Improved PSO Algorithm for Smooth\nShaping.” International Conference on Machine Learning, Vol.99,\nPathPlanningofMobileRobotsUsingContinuousHigh-degreeBezier\n1999. Curve.”AppliedSoftComputing,Vol.100,2021. [24] RaoJ,etal.”AModifiedRandomNetworkDistillationAlgorithmand\n[11] Sutton, Richard S., and A. G. Barto. ”Reinforcement Learning: An\nItsApplicationinUSVsNavalBattleSimulation.”OceanEngineering,\nIntroduction.”AIMagazine,Vol.21,No.1,2000:103. Vol.261,2022:112147. [12] Silver, David, et al.",
      "size": 853,
      "sentences": 9
    },
    {
      "id": 43,
      "content": "nd A. G. Barto. ”Reinforcement Learning: An\nItsApplicationinUSVsNavalBattleSimulation.”OceanEngineering,\nIntroduction.”AIMagazine,Vol.21,No.1,2000:103. Vol.261,2022:112147. [12] Silver, David, et al. ”Mastering the Game of Go with Deep Neural\n[25] Sovrano,Francesco.”CombiningExperienceReplaywithExploration\nNetworksandTreeSearch.”Nature529,2016:484–489. by Random Network Distillation.” 2019 IEEE conference on games\n[13] Nair,DevikaS.,andP.Supriya.”ComparisonofTemporalDifference\n(CoG),2019:1-8. LearningAlgorithmandDijkstra’sAlgorithmforRoboticPathPlan-\n[26] Pan,Lifan,etal.”LearningNavigationPoliciesforMobileRobotsin\nning.”2018SecondInternationalConferenceonIntelligentComputing\nDeep Reinforcement Learning with Random Network Distillation.”\nandControlSystems(ICICCS),2018:1619-1624. 2021 the 5th International Conference on Innovation in Artificial\n[14] Christopher, J. ”Q-learning. Machine Learning.” Machine Learning,\nIntelligence,2021:151-157. Vol.8,1992:279-292.",
      "size": 972,
      "sentences": 11
    },
    {
      "id": 44,
      "content": "2018:1619-1624. 2021 the 5th International Conference on Innovation in Artificial\n[14] Christopher, J. ”Q-learning. Machine Learning.” Machine Learning,\nIntelligence,2021:151-157. Vol.8,1992:279-292. [15] Arulkumaran, Kai, et al. ”A Brief Survey of Deep Reinforcement\nLearning.”IEEESignalProcessingMagazine,Vol.34,No.6,2017:26-\n38.",
      "size": 331,
      "sentences": 7
    }
  ]
}