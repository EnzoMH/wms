{
  "source": "ArXiv",
  "filename": "019_Multi-AGV_Path_Planning_Method_via_Reinforcement_L.pdf",
  "total_chars": 47497,
  "total_chunks": 68,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nMulti-AGV Path Planning Method via Reinforcement\nLearning and Particle Filters\nShuoShao\nWuhanUniversityofTechnology\nshaoshuo@whut.edu.cn\nAbstract\nThankstoitsrobustlearningandsearchstabilities,thereinforcementlearning(RL)\nalgorithm has garnered increasingly significant attention and been exten-sively\nappliedinAutomatedGuidedVehicle(AGV)pathplanning. However,RL-based\nplanningalgorithmshavebeendiscoveredtosufferfromthesubstantialvarianceof\nneuralnetworkscausedbyenvironmentalinstabilityandsignificantfluctua-tions\nin system structure. These challenges manifest in slow convergence speed and\nlow learning efficiency. To tackle this issue, this paper presents a novel multi-\nAGV path planning method named Particle Filters - Double Deep Q-Network\n(PF-DDQN)via leveraging Particle Filters (PF) and RL algorithm. Firstly, the\nproposedmethodleveragestheimpreciseweightvaluesofthenetworkasstate\nvaluestoformulatethestatespaceequation.",
      "size": 943,
      "sentences": 5
    },
    {
      "id": 2,
      "content": "Deep Q-Network\n(PF-DDQN)via leveraging Particle Filters (PF) and RL algorithm. Firstly, the\nproposedmethodleveragestheimpreciseweightvaluesofthenetworkasstate\nvaluestoformulatethestatespaceequation. Subsequently,theDDQNmodelis\noptimizedtoacquiretheoptimaltrueweightvaluesthroughtheiterativefusion\nprocessofneuralnetworksandPFinordertoenhancetheoptimizationefficiencyof\ntheproposedmethod. Lastly,theperformanceoftheproposedmethodisvalidated\nbydifferentnumericalsimulations. Thesimulationresultsdemonstratethatthe\nproposed methoddominates the traditional DDQN algorithm in terms of path\nplanningsuperiorityandtrainingtimeindicatorby92.62%and76.88%,respectively. Therefore,theproposedmethodcouldbeconsideredasavitalalternativeinthe\nfieldofmulti-AGVpathplanning. 1 Introduction\nDuetoitssignificantroleandwideapplicationprospectinginbothmilitaryandciviliandomains,the\nAutonomousGuidedVehicle(AGV)hasgainedconsiderableattentioninthefieldofrobotics[1].",
      "size": 945,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "athplanning. 1 Introduction\nDuetoitssignificantroleandwideapplicationprospectinginbothmilitaryandciviliandomains,the\nAutonomousGuidedVehicle(AGV)hasgainedconsiderableattentioninthefieldofrobotics[1]. As\noneofthekeyfactorstoachievehigh-levelintelligenceofAGV,Pathplanning,whichisessential\nforachievingAGVintelligence,hasalsoattractedextensiveresearchinterest[2]. However,path\nplanningofAGVhasbeendiscoveredandproventobeaNP-hardissue,leadingtofindanoptimal\npathwithinagivencomputationtimetobegreatlydifficultorevenimpossible. Thus,developinghigh\nperformancepathplanningmethodisalwaysheavilydemandedinthefiledofAGVpathplanning. Generally,AGVpathplanningalgorithmscanbecategorizedintotwotypes,thatis,thetraditional\nalgorithms and population-based intelligent optimization algorithms.",
      "size": 779,
      "sentences": 6
    },
    {
      "id": 4,
      "content": "sheavilydemandedinthefiledofAGVpathplanning. Generally,AGVpathplanningalgorithmscanbecategorizedintotwotypes,thatis,thetraditional\nalgorithms and population-based intelligent optimization algorithms. Despite being capable of\nofferingpromisingperformanceregardingtothecomputationtime,traditionalpathplanningmethods,\nsuchasgraphicalmethods[3],artificialpotentialfieldmethods[4],anddynamicwindowapproaches,\nstillsufferfromdeficienciesinplanningstabilityandadaptability[5]. Ontheotherhand,artificial\nintelligenceoptimizationalgorithms,includinggeneticalgorithms[6],neuralnetworks[7],andant\ncolonyalgorithms[8],canachieveformidableplanningstabilityincomplexenvironmentsdueto\ntheirpopulation-basedcharacteristics[9]. Asoneofthemostprominentpopulation-basedintelligentoptimizationalgorithms,Reinforcement\nLearning(RL)exhibitspowerfulmachinelearningcapabilitiesandenablestheintelligentagentto\nPreprint.",
      "size": 894,
      "sentences": 5
    },
    {
      "id": 5,
      "content": "acteristics[9]. Asoneofthemostprominentpopulation-basedintelligentoptimizationalgorithms,Reinforcement\nLearning(RL)exhibitspowerfulmachinelearningcapabilitiesandenablestheintelligentagentto\nPreprint. 4202\nyaM\n32\n]OR.sc[\n4v63281.3042:viXra\n=== 페이지 2 ===\nlearnspecificbehavioralnormsthroughtrialanderror[10]. Moreover,thisalgorithmpossessesthe\ncharacteristicsofreward-basedfeedbackandisindependentontrainingdata,leadingittobenaturally\nadaptableinthefieldofpathplanning[11]. RLisaninfluentialmachinelearningapproachthatenablesintelligentagentstoacquireknowledge\nonhowtoactwithinanenvironmenttoaccomplishpredefinedobjectivesthroughrewardfeedback. It\ndoessowithoutanypriorunderstandingoftheenvironmentortrainingdata[12]. Thischaracteristic\nmakesitespeciallywell-suitedfortasksrelatedtolocalpathmapping. DoubleDeepQ-Network\n(DDQN)representsarenownedRLalgorithmthathasgainedextensiveutilizationinthefieldofpath\nplanning.",
      "size": 913,
      "sentences": 8
    },
    {
      "id": 6,
      "content": ". Thischaracteristic\nmakesitespeciallywell-suitedfortasksrelatedtolocalpathmapping. DoubleDeepQ-Network\n(DDQN)representsarenownedRLalgorithmthathasgainedextensiveutilizationinthefieldofpath\nplanning. However, AGV planning remains challenging for current Deep Reinforcement Learning (DRL)\nmethods,partlyduetotheirneedtothefollowingreasons:(i)partiallyobservableenvironmentsand\n(ii)reasonthroughcomplexobservations,suchasavoidingcollisionsbetweenAGVs. Whilemost\nexistingRLpathplanningalgorithmshaveshownfeasibilityinsingleAGVpathplanningproblems,\ntheyfailtoachievemulti-AGVpathplanninginmorecomplexenvironments. Thiscouldbeprobably\ninterpretedbythefactthattheneuralnetworkshavehighvariance,leadingtoslowconvergenceofthe\nalgorithmandreducingitsabilitytohandlecooperativeplanningamongmultipleagentsincomplex\nenvironments[13].",
      "size": 821,
      "sentences": 6
    },
    {
      "id": 7,
      "content": "robably\ninterpretedbythefactthattheneuralnetworkshavehighvariance,leadingtoslowconvergenceofthe\nalgorithmandreducingitsabilitytohandlecooperativeplanningamongmultipleagentsincomplex\nenvironments[13]. Tohandletheissuenotedabove,somestudieshaveproposedmanydifferentimprovedRL-based\nplanningmethodsbyincreasingthecomplexityofthegridstatespaceandincorporatingperception\nstatespace[14]. However,thistypeofmethodstillreliesontrainingresultsinanidealnoise-free\nenvironment,resultingintheinabilitytoaddresstheissueofweightinaccuracy[15]. Whilefeasible,\novercomingtheseconstraintsincursadditionalcomputationalexpenseswhentrainingdeepneural\nnetworks.Additionally,someresearchhasaimedtoimproveDRLalgorithmsbyintroducingKalman\nFilters(KF)[16]. However,traditionalKFrequirelinearizationofthemodelthroughTaylorseries\nexpansion,whichcompromisestheaccuracyofthemodeltosomeextent,astheyareonlyeffective\nforlinearsystems. Thisstudycommitsitsinterestonmulti-AGVpathplanninginanunfamiliarenvironment.",
      "size": 980,
      "sentences": 6
    },
    {
      "id": 8,
      "content": "elthroughTaylorseries\nexpansion,whichcompromisestheaccuracyofthemodeltosomeextent,astheyareonlyeffective\nforlinearsystems. Thisstudycommitsitsinterestonmulti-AGVpathplanninginanunfamiliarenvironment. Intypical\nscenarios,intricatetasksinvolvingmobilerobots,suchasmapping,itemdelivery,orsurveillance,\nnecessitatepathplanningformultipletargets.OurconjectureisthattheconventionalDDQNapproach\nfails to address the challenge of path planning for multiple targets due to the convergence of Q-\nvaluessolelybasedonthespatialcoordinatesonthemap. Empiricalfindingsillustratethatwhile\nDDQNcansuccessfullylearntoplanapathfortheinitialtarget,itstrugglestoefficientlyplanfor\nsubsequenttargetsduetotheenvironment’scomplexityandtheincapabilitytolearnandregenerate\nthevaluenetworkfortheremainingtargets. Despiteresearchers’effortstomodifyDDQNandreduce\nconvergencetime,thereiscurrentlynoexistingresearchonemployingPFintegrationtotacklethe\nissueofmulti-AGVpathplanningwithinthisenvironment.",
      "size": 970,
      "sentences": 5
    },
    {
      "id": 9,
      "content": "ainingtargets. Despiteresearchers’effortstomodifyDDQNandreduce\nconvergencetime,thereiscurrentlynoexistingresearchonemployingPFintegrationtotacklethe\nissueofmulti-AGVpathplanningwithinthisenvironment. Thesubsequentsectionwillprovidea\nmorecomprehensiveexplanationoftheDDQNalgorithm. Toaddresstheaforementionedissues,thispaperproposesaPF-DDQN-basedmethodformulti-AGV\npathplanning. Firstly,intheproposedmethod,thetrainingnetworkwithenvironmentalnoiseand\nthetargetnetworkwithinaccurateweightsaretreatedasstateandobservationvariables,respectively,\ntoconstructthesystem’sstateequationandobservationequation. Then,throughthefusioniteration\nofneuralnetworksandPF,theweightsoftheneuralnetworkarecontinuouslyupdatedtoimprove\ntheconvergencespeedoftheproposedalgorithm. Finally,theeffectivenessandsuperiorityofthe\nproposedmethodarevalidatedthroughnumericalsimulationsunderdifferentoperatingconditions. The rest of this paper is as follows: Section 2 introduces related work.",
      "size": 961,
      "sentences": 8
    },
    {
      "id": 10,
      "content": "ly,theeffectivenessandsuperiorityofthe\nproposedmethodarevalidatedthroughnumericalsimulationsunderdifferentoperatingconditions. The rest of this paper is as follows: Section 2 introduces related work. Section 3 presents the\ntheoreticalbackgroundofmethods. Section4comparestheexperimentalresultsoftheproposed\nalgorithmwithDDQNandEKF-DDQN.Finally,Section5providesasummaryofthispaper. 2 RelatedWork\nGeneticAlgorithms(GAs)andAntColonyOptimization(ACO)areheuristicmethodsthatshow\npotentialinsolvingoptimizationproblems[17]. GAsgenerateacollectionofpotentialsolutions\nthroughDarwinianevolutionprinciplesandassessthemusingafitnessfunction.Thefittestindividuals\nareselectedforreproductionthroughcrossoveroperations,whilepopulationdiversityismaintained\nthrough mutation operations [18]. Conversely, ACO draws inspiration from the behavior of ants\n2\n=== 페이지 3 ===\nandutilizespheromonetrailstoimprovetheefficiencyoftheshortestpathsbetweeninitialandfinal\npoints[19].",
      "size": 953,
      "sentences": 7
    },
    {
      "id": 11,
      "content": "ation operations [18]. Conversely, ACO draws inspiration from the behavior of ants\n2\n=== 페이지 3 ===\nandutilizespheromonetrailstoimprovetheefficiencyoftheshortestpathsbetweeninitialandfinal\npoints[19]. In recent years, researchers have proposed various optimization techniques for path planning of\nmobilerobotsingridenvironments. Lietal. [20]introducedanenhancedalgorithmcalledImproved\nAntColonyOptimization-ImprovedArtificialBeeColony(IACO-IABC),whicheffectivelyimproves\ntheperformanceofmobilerobotpathplanningbycombiningthestrengthsofbothalgorithmsand\nintroducingnewheuristicsandsearchmechanisms. Lietal. [21]proposedanadvancedAntColony\nOptimization(MACO)algorithmthataddressesissuessuchaslocaloptimaandslowconvergencein\ntrajectoryplanningforUnmannedAerialVehicles(UAVs)byincorporatingthemetropoliscriterion\nanddesigningthreetrajectorycorrectionschemes. Additionally,theyemployedanInscribeCircle\n(IC)smoothingmethodtoenhanceefficiencyandsafetyintrajectoryplanning.",
      "size": 964,
      "sentences": 8
    },
    {
      "id": 12,
      "content": "UAVs)byincorporatingthemetropoliscriterion\nanddesigningthreetrajectorycorrectionschemes. Additionally,theyemployedanInscribeCircle\n(IC)smoothingmethodtoenhanceefficiencyandsafetyintrajectoryplanning. While GAs and ACO have emerged as solutions for path planning problems, their effectiveness\ninreal-timepathplanningislimited. Thesepopulation-basedalgorithmsarecomputationallyde-\nmanding, particularly for large search spaces, making them unsuitable for real-time applications\nthatrequirepromptdecision-making. Furthermore,thepopulation-basedapproachesemployedby\nthesealgorithmscanyieldsuboptimalsolutionsandexhibitslowconvergence,especiallyincomplex\nenvironments[22]. The Double Deep Q-Network (DDQN) algorithm has gained significant traction in the field of\nreinforcementlearningforacquiringoptimalactionsequencesbasedonapredefinedpolicy. This\nalgorithmisrenownedforitssimplicity,speed,andeffectivenessinaddressingchallengingproblems\nincomplexandunknownenvironments.",
      "size": 967,
      "sentences": 7
    },
    {
      "id": 13,
      "content": "mentlearningforacquiringoptimalactionsequencesbasedonapredefinedpolicy. This\nalgorithmisrenownedforitssimplicity,speed,andeffectivenessinaddressingchallengingproblems\nincomplexandunknownenvironments. NumerousresearchershaveappliedtheDDQNalgorithm\ntopathplanningbydiscretizingreal-worldscenariosintoastatespaceandsubsequentlysimulating\nthem. Thesemethodsdifferintermsoftask-specificbehaviordesignandtherepresentationofthe\nstatespace. One of the earliest approaches was the Deep Q-Network (DQN) algorithm proposed by Mnih et\nal. [23],whichcombinesdeepneuralnetworkswithQ-learningtoaddressreinforcementlearning\nproblemsinhigh-dimensionalandcontinuousstatespaces.TheDQNalgorithmresolvestheinstability\nissueinQ-learningbyemployinganexperiencereplaybufferandatargetnetwork. ToaddresstheoverestimationproblemintheDQNalgorithm,Hasseltetal. [24]introducedtheuse\noftwoQ-networks: oneforpolicyactionselectionandtheotherforactionvalueestimation.",
      "size": 933,
      "sentences": 8
    },
    {
      "id": 14,
      "content": "ncereplaybufferandatargetnetwork. ToaddresstheoverestimationproblemintheDQNalgorithm,Hasseltetal. [24]introducedtheuse\noftwoQ-networks: oneforpolicyactionselectionandtheotherforactionvalueestimation. They\nutilizedatargetvaluenetworktoalleviatetheoverestimationproblemandimprovetrainingstability. Theyalsointroduceddistributedreinforcementlearning,modelingthevalueofactionsasprobability\ndistributionsratherthansinglevalues. Theuseofquantileregressiontoestimatethedistributionof\nactionvaluesenhanceslearningefficiencyandstability[25]. Igletal. [26]adoptedtheVariationalSequentialMonteCarlomethod,whileNaessethetal. [27]\nappliedPFforbelieftrackinginreinforcementlearning.Thelattermethodenhancesbelieftrackingca-\npability,butexperimentsrevealthatgenerativemodelslackrobustnessincomplexobservationspaces\nwithhigh-dimensionaluncorrelatedobservations. Toaddressthisissue,morepowerfulgenerative\nmodelssuchasDRAW[28]canbeconsideredtoimproveobservationgenerationmodels.",
      "size": 959,
      "sentences": 10
    },
    {
      "id": 15,
      "content": "ckrobustnessincomplexobservationspaces\nwithhigh-dimensionaluncorrelatedobservations. Toaddressthisissue,morepowerfulgenerative\nmodelssuchasDRAW[28]canbeconsideredtoimproveobservationgenerationmodels. However,\nevaluatingcomplexgenerativemodelsforeachparticlesignificantlyincreasescomputationalcosts\nandoptimizationdifficulty. Recentattentionhasfocusedonembeddingalgorithmsintoneuralnetworksforend-to-enddiscrimi-\nnativetraining. Thisideahasbeenappliedtodifferentiablehistogramfilters,suchasJonschkowski\n&Brock [29], andKF[30]. Maetal. [31]integratedPFwithstandard RNNs(e.g., LSTM)and\nintroducedPF-RNNforsequenceprediction. Murugananthametal. [32]proposedadynamicMulti-ObjectiveEvolutionaryAlgorithm(MOEA)\nbasedonExtendedKalmanFilters(EKF)prediction. Thesepredictionsguidethesearchforchanging\noptima,acceleratingconvergence. AscoringschemewasdesignedtocombineEKFpredictionwith\nrandomreinitializationmethodstoenhancedynamicoptimizationperformance. Gao et al.",
      "size": 955,
      "sentences": 12
    },
    {
      "id": 16,
      "content": "edictionsguidethesearchforchanging\noptima,acceleratingconvergence. AscoringschemewasdesignedtocombineEKFpredictionwith\nrandomreinitializationmethodstoenhancedynamicoptimizationperformance. Gao et al. [33] presented an adaptive KF navigation algorithm, RL-AKF, which employs rein-\nforcementlearningmethodstoadaptivelyestimatetheprocessnoisecovariancematrix. Extensive\nexperimentalresultsdemonstratethatalthoughthisalgorithmaccuratelyestimatestheprocessnoise\ncovariancematrixandimprovesalgorithmrobustness,itistime-consumingandfacesconvergence\nchallenges. 3\n=== 페이지 4 ===\nIn this work, we depart from directly combining PF and RL algorithms. Instead, building upon\nprevious work, we treat the weightsof the neural network as state variables in the PF algorithm. WeleveragethehighlyadaptiveandnonlineariterativecharacteristicsofPFtoupdatethenetwork\nweights,reducingthevarianceofthetargetnetworkintheRLalgorithm.",
      "size": 908,
      "sentences": 8
    },
    {
      "id": 17,
      "content": "al network as state variables in the PF algorithm. WeleveragethehighlyadaptiveandnonlineariterativecharacteristicsofPFtoupdatethenetwork\nweights,reducingthevarianceofthetargetnetworkintheRLalgorithm. Thisapproachenhancesthe\naccuracyofactionguidancefortrajectoriesandimprovestheconvergencespeedofthereinforcement\nlearningalgorithm. 3 Methods\n3.1 ParticleFiltering\nPFisarecursiveBayesianfilteringalgorithmutilizedforstateestimationwithinasystem. Itscore\nprincipleinvolvesapproximatingtheprobabilitydistributionp(x |z )byemployingasetofrandom\nk 1:k\nsamples,knownasparticles,whichrepresentthestates[18]. Here,letx = x ,x ,...,x and\n0:k 0 1 k\nz =z ,z ,...,z representthetargetstatesandobservedvaluesattimek.",
      "size": 702,
      "sentences": 6
    },
    {
      "id": 18,
      "content": "utionp(x |z )byemployingasetofrandom\nk 1:k\nsamples,knownasparticles,whichrepresentthestates[18]. Here,letx = x ,x ,...,x and\n0:k 0 1 k\nz =z ,z ,...,z representthetargetstatesandobservedvaluesattimek. Theparticleweight\n0:k 0 1 k\nupdatefunctiondependsonthetargetstatesandobservationdatax ∼q(x |x ,z ),which\ni,k i,k i,k−1 k\ncanbeexpressedasfollows:\np(z |xi)p(xi|xi )\nwi = k k k k−1 ·wi (1)\nk q(xi|xi ,z ) k−1\nk k−1 k\nConsequently, a collection of particles with weight values\nxi,wiNs\napproximates the posterior\nk ki=1\nprobabilitydensity,wheretheimportancesamplingfunctionisdefinedasfollows:\n(cid:32) (cid:33) (cid:32) (cid:33)\nxi xi\nq k =p k (2)\nxi z xi\nk−1 k k−1\nThePFalgorithmfollowsthesubsequentsteps:\nStep1: Samplex ∼q(x |x ,z )fori=1,...,N . i,k i,k i,k−1 k s\nStep2: Calculatetheimportanceweightw˜i,normalizeit,andupdateparticleweights. k\nStep3: ComparetheeffectivesamplesizeN˜eff withapredefinedthresholdNth. IfN˜eff <Nth,\nperformresamplingof(xi,wi)Ns .",
      "size": 956,
      "sentences": 6
    },
    {
      "id": 19,
      "content": "k s\nStep2: Calculatetheimportanceweightw˜i,normalizeit,andupdateparticleweights. k\nStep3: ComparetheeffectivesamplesizeN˜eff withapredefinedthresholdNth. IfN˜eff <Nth,\nperformresamplingof(xi,wi)Ns . k k i=1\nUnliketheconventionalKF,PFisnotreliantonlinearizationorGaussianassumptions,makingit\nmoresuitablefortheiterativeupdatingofneuralnetworks[19]. 3.2 ExtendedKalmanFiltering\nTheExtendedKalmanFilter(EKF)isanextensionoftheKFthataddressestheestimationofsystem\nstatesinthepresenceofnonlinearsystemdynamicsandmeasurementmodels. Itapproximatesthe\nnonlinearmodelsusinglinearizationtechniquesandappliestheEKFupdateequationstoiteratively\nestimatethestateofthesystem. SimilartotheKF,theEKFmaintainsanestimateofthesystemstate,denotedasxˆ ,andanerror\nk\ncovariancematrix,denotedasP ,whichrepresentstheuncertaintyassociatedwiththeestimate. The\nk\nEKFincorporatesnewmeasurementstoupdatethestateestimateandtheerrorcovariancematrix. TheEKFconsistsoftwomainsteps: thepredictionstepandtheupdatestep.",
      "size": 981,
      "sentences": 9
    },
    {
      "id": 20,
      "content": "sentstheuncertaintyassociatedwiththeestimate. The\nk\nEKFincorporatesnewmeasurementstoupdatethestateestimateandtheerrorcovariancematrix. TheEKFconsistsoftwomainsteps: thepredictionstepandtheupdatestep. PredictionStep: Inthepredictionstep,theEKFpredictsthecurrentstatebasedontheprevious\nestimate and the nonlinear system dynamics. It updates the estimate of the state and the error\ncovariancematrixasfollows:\nxˆ− =f(xˆ ,u ) (3)\nk k−1 k−1\nP− =F P FT +Q (4)\nk k k−1 k k\n4\n=== 페이지 5 ===\nwherexˆ−andP−arethepredictedstateestimateandthepredictederrorcovariancematrix,respec-\nk k\ntively. Thefunctionf(·)representsthenonlinearsystemdynamics,andu isthecontrolinputat\nk−1\ntimestepk−1. ThematrixF istheJacobianmatrixofthesystemdynamicsfunctionevaluatedat\nk\nthepredictedstate,andQ istheprocessnoisecovariancematrixthatrepresentstheuncertaintyin\nk\nthesystemdynamics. UpdateStep: Intheupdatestep,theEKFincorporatesthenewmeasurementtoimprovetheestimate\nofthestate.",
      "size": 947,
      "sentences": 8
    },
    {
      "id": 21,
      "content": "redictedstate,andQ istheprocessnoisecovariancematrixthatrepresentstheuncertaintyin\nk\nthesystemdynamics. UpdateStep: Intheupdatestep,theEKFincorporatesthenewmeasurementtoimprovetheestimate\nofthestate. ItcomputestheKalmangain,whichdeterminestheweightgiventothemeasurementand\nthepredictedstateestimate,andupdatesthestateestimateandtheerrorcovariancematrixasfollows:\nK =P−HT(H P−HT +R )−1 (5)\nk k k k k k k\nxˆ =xˆ−+K (z −h(xˆ−)) (6)\nk k k k k\nP =(I−K H )P− (7)\nk k k k\nwhereK istheKalmangain, H istheJacobianmatrixofthemeasurementfunctionevaluated\nk k\natthepredictedstate,R isthemeasurementnoisecovariancematrix,andz isthemeasurement\nk k\nobtainedattimestepk. Thefunctionh(·)representsthemeasurementmodelthatrelatesthetrue\nstatetothemeasurements. The EKF linearizes the nonlinear system dynamics and measurement models using the Jacobian\nmatrices. Byapproximatingthemaslinear,theEKFcanapplytheKFupdateequationstoestimate\nthe state.",
      "size": 926,
      "sentences": 6
    },
    {
      "id": 22,
      "content": "emeasurements. The EKF linearizes the nonlinear system dynamics and measurement models using the Jacobian\nmatrices. Byapproximatingthemaslinear,theEKFcanapplytheKFupdateequationstoestimate\nthe state. However, the linearization introduces errors, and the accuracy of the EKF depends on\nthe quality of the linearization. In cases where the nonlinearities are significant, more advanced\ntechniquesliketheUnscentedKForPFmaybemoreappropriateforstateestimation. 3.3 DoubleDeepQ-networkAlgorithm\nTheDDQNalgorithmisanenhancediterationoftheDeepQ-Network(DQN)algorithm[21]. In\ncontrasttoconventionalmethodswithsimilarobjectives,DDQNmitigatestheadverseeffectsof\noverestimationbyemployingthecurrentestimationnetworktoapproximatethemaximumQ-value\nof the subsequent state, rather than relying solely on the target network [23].",
      "size": 813,
      "sentences": 7
    },
    {
      "id": 23,
      "content": "ves,DDQNmitigatestheadverseeffectsof\noverestimationbyemployingthecurrentestimationnetworktoapproximatethemaximumQ-value\nof the subsequent state, rather than relying solely on the target network [23]. Within the DDQN\nalgorithm,atargetnetworkarchitectureisestablishedtominimizethelossfunction,whichcanbe\nmathematicallyexpressedasfollows:\nL(θ)=\n1(cid:88)\n(Q(s ,a ;θ)−y )2 (8)\n2 i i i\ni\nwhere y represents the target Q-value, and Q(s ,a ;θ) represents the predicted Q-value. The\ni i i\nnetworkistrainedusingstochasticgradientdescenttoupdatethenetworkweightsθateachtime-step\ni,resultinginimprovedestimatesoftheQ-values. TheupdateexpressionoftheQ-valueasfollows:\nQ(s ,a ;θ)=r +γmaxQ(s ,a |θ˜) (9)\ni i t t+1 t+1\nwhere r represents the reward value obtained from the corresponding action at time t, while γ\nt\ndenotesthediscountfactor.",
      "size": 825,
      "sentences": 4
    },
    {
      "id": 24,
      "content": "ressionoftheQ-valueasfollows:\nQ(s ,a ;θ)=r +γmaxQ(s ,a |θ˜) (9)\ni i t t+1 t+1\nwhere r represents the reward value obtained from the corresponding action at time t, while γ\nt\ndenotesthediscountfactor. Thederivativeofthelossfunctionwithrespecttothenetworkparameters\nθ,intermsofγ,isexpressedasfollows:\n(cid:88)\n∇ L(θ)= (Q(s ,a ,θ)−y )∇ Q(s ,a ;θ) (10)\nθ i i i θ i i\ni\nAtthisjuncture,theupdaterulefortheweightsisdefinedasillustratedinfollowingEquation:\ndQ(s ,a ;θ)\nθ =θ +α(Z −Q(s ,a ;θ) t t ) (11)\nt+1 t t t t dθ\nt\nwhere θ represents the network weights at the current moment, while θ denotes the updated\nt t+1\nnetworkweights. αsignifiesthelearningrate,whichgovernsthestepsizeofweightupdates. Z\nt\nrepresentsthevalueofthetargetnetwork’svaluefunctionforthecurrentstateandaction[25]. 5\n=== 페이지 6 ===\nFigure1: The2DmapofAGVpathplanning\nFigure2: Graphenvironmentimagecoordinatesystem\nTheDDQNalgorithmimprovesconvergencespeedandaddressesoverestimationissuesintraditional\nDQNalgorithms.",
      "size": 975,
      "sentences": 5
    },
    {
      "id": 25,
      "content": "5\n=== 페이지 6 ===\nFigure1: The2DmapofAGVpathplanning\nFigure2: Graphenvironmentimagecoordinatesystem\nTheDDQNalgorithmimprovesconvergencespeedandaddressesoverestimationissuesintraditional\nDQNalgorithms. However,itstillfaceschallengesineffectivelyaddressingweightinaccuracywithin\nthemodel,whichlimitsitssuperiority. Toovercometheselimitations,thisstudyintroducesanovel\nalgorithmcalledPF-DDQN,whichcombinesPFandDDQN. 3.4 SimulationEnvironmentModeling\nTheenvironmentisa2DmapasshowninFigure1,representedasa30×30-pixelimage,whereeach\npixelcorrespondstoa2-metersizeintherealworld. Thetrainingutilizesanentirelyunfamiliarmap,\nwhichisa30×30-pixelimage. Inthisimage,theredandwhiteregionscorrespondtoobstaclesand\nfreespace,respectively. ThetrajectoryoftheAGVandthetargetaredepictedbydifferentcolors,\nwiththeformerrepresentedbycolorsotherthangreen. Sincethemobilerobotlacksknowledgeof\nthereferencemap,itisimperativeforittogenerateitsownmap.",
      "size": 925,
      "sentences": 8
    },
    {
      "id": 26,
      "content": "trajectoryoftheAGVandthetargetaredepictedbydifferentcolors,\nwiththeformerrepresentedbycolorsotherthangreen. Sincethemobilerobotlacksknowledgeof\nthereferencemap,itisimperativeforittogenerateitsownmap. The goal of AGV spatial trajectory planning is to achieve collision-free detection for the AGV. Therefore,duringspatialtrajectoryplanning,optimizationofthetrajectoryisrequiredbasedontwo\naspects: therelationshipbetweentheAGVandobstaclepositionsandtherelativepositionbetween\ntheAGVandobstacles. RealAGVsrelyonvisiontodeterminetheirposition. Inthiswork,thethree-dimensionalposition\nparametersofthetargetareobtainedthroughtrigonometryanddisparity.Giventherelativepositioning\nbetweentwocamerasandtheinternalparametersofthecameras,thespatialcoordinatesordimensions\nofanobjectcanbeobtainedbyknowingthedisparityofitsfeatures. Thecoordinatesystemofthe\nenvironmentalimageisshowninFigure2.",
      "size": 878,
      "sentences": 7
    },
    {
      "id": 27,
      "content": "wocamerasandtheinternalparametersofthecameras,thespatialcoordinatesordimensions\nofanobjectcanbeobtainedbyknowingthedisparityofitsfeatures. Thecoordinatesystemofthe\nenvironmentalimageisshowninFigure2. 6\n=== 페이지 7 ===\nAsdepictedinFigure2,au-v-oCartesiancoordinatesystemisdefinedintheobtainedimage,where\nthepixelcoordinatescorrespondtotherowsandcolumnsinthesequence.Therefore,(u,v)represents\nthepixelcoordinatesystemoftheAGV. Since(u,v)onlyindicatesthenumberofrowsandcolumnswherethepixelislocated,inorderto\ndescribethepositionofthepixelintheimage,anequivalentmeaningneedstobeestablishedusinga\nphysicalcoordinatesystem. Thus,acoordinatesystemparalleltothe(u,v)axisisestablished,with\nthex-axiscorrespondingtotheu-axisandthey-axiscorrespondingtothev-axis. (u,v)corresponds\nto(x,y). Thecoordinatesareexpressedinpixelsandcorrespondtotheimage’scoordinatesystem,whichis\nmeasuredinmillimeters.",
      "size": 882,
      "sentences": 7
    },
    {
      "id": 28,
      "content": "-axiscorrespondingtotheu-axisandthey-axiscorrespondingtothev-axis. (u,v)corresponds\nto(x,y). Thecoordinatesareexpressedinpixelsandcorrespondtotheimage’scoordinatesystem,whichis\nmeasuredinmillimeters. Thus,therelationshipbetweenthepositionofanypixelintheimageand\nthetwocoordinatesystemscanbedescribedasfollows:\n x\n u= dx +u 0\ny (12)\n ν = dy +ν 0\nThe position of the overlap point between the camera optical axis and the image plane in the xy\ncoordinatesystemisdenotedas(u ,v ). Bytransformingtheequationintohomogeneouscoordinates\n0 0\nandmatrixform,wecanobtain:\n 1 0 u \n(cid:34)u(cid:35) dx 0\n \nν 1 =  0 d 1 y ν 0   (13)\nz 0 1\nwherezrepresentsthestatevectorofthematrix. Theinversionisasfollows:\n(cid:34)x(cid:35) (cid:34)dx 0 −u dx(cid:35)(cid:34)u(cid:35)\n0\ny = 0 dy −ν dy v (14)\n0\n1 0 0 1 1\nWhenacameracapturesanobject,thereexistsauniqueprojectionrelationshipbetweentheobject’s\ntrue coordinates and the obtained image coordinates [8].",
      "size": 947,
      "sentences": 6
    },
    {
      "id": 29,
      "content": "5)(cid:34)u(cid:35)\n0\ny = 0 dy −ν dy v (14)\n0\n1 0 0 1 1\nWhenacameracapturesanobject,thereexistsauniqueprojectionrelationshipbetweentheobject’s\ntrue coordinates and the obtained image coordinates [8]. Given the knowledge of the camera’s\ngeometricparameters,theprincipleofsimilartrianglescanbeusedtoderive:\n(cid:40)\nu= f x\n−ze e (15)\nv = f y\n−ze e\nWhere, −Z represents the camera depth, and f/−Z represents the scale factor. Perspective\ne e\nprojectiononlyguaranteesaone-to-onecorrespondencebetweenpointsontheimageandpointson\ntheprojectionline. Inotherwords,thesamepointontheimagecorrespondstoaseriesofpointson\ntheprojectionline. Itisevidentthatduringthisprocess,thedepthinformationofthespatialpointsis\nlost,whichindicatestheneedfortwoormorecamerastodeterminethethree-dimensionalobject.",
      "size": 783,
      "sentences": 5
    },
    {
      "id": 30,
      "content": "ondstoaseriesofpointson\ntheprojectionline. Itisevidentthatduringthisprocess,thedepthinformationofthespatialpointsis\nlost,whichindicatestheneedfortwoormorecamerastodeterminethethree-dimensionalobject. By\ncalculatingtheimagecoordinates,thepositionofathree-dimensionalpointinthecameracoordinate\nsystemcanbeobtained:\n  x e = f+ f w u   x e = − f zeu\ny e = f+ f w v =⇒ y e = − f zev (16)\nz\n= f2\nz\n=z\ne f+w e e\nwherez isrelatedtothetruedistancebetweenthetargetandthecamera. Therefore,thisexperiment\ne\ncanachievetheconversionfrom3Dto2D. Theworkingscenariodesignedinthispaperisasfollows:Inacomplexworkspacewithgreatdanger,\nmultipleAGVscooperatewitheachothertojointlyselectthebestpathaccordingtothecoordinates\n7\n=== 페이지 8 ===\nofobstaclesandtargetpoints.",
      "size": 770,
      "sentences": 5
    },
    {
      "id": 31,
      "content": "riodesignedinthispaperisasfollows:Inacomplexworkspacewithgreatdanger,\nmultipleAGVscooperatewitheachothertojointlyselectthebestpathaccordingtothecoordinates\n7\n=== 페이지 8 ===\nofobstaclesandtargetpoints. InordertoimprovethelikelihoodofanAGVsuccessfullyexecuting\nitstask,therobot-generatedpathneedstomeetthefollowingconditions:\n(a)AftertheAGVisstarted,problemssuchasinsufficientpowerandsuddenfailureareignored;\n(b)ThepathsgeneratedbyeachAGVneedtobecollisionfreefromobstacles;\n(c)ThepathsgeneratedbetweeneachAGVshouldbecollision-free;\n(d)Thegeneratedpathmustensurethattherobotreachesitsdestinationatthesametime. (e)Assumethatthespeedofeachrobotiisboundedby[Vi ,Vi ],whereVi andVi represent\nmin max min max\ntheminimumandmaximumspeedofroboti,respectively.Astherobotnavigatestothetargetlocation,\nthespeedofeachrobotvarieswithinitsvelocityboundary. (f)DuringthemovementofeachAGVtothetargetposition,theyawAngleandtraveldistanceof\neachAGVneedtobekeptwithinitsmaximumyawAngleanddistanceconstraints,respectively.",
      "size": 998,
      "sentences": 4
    },
    {
      "id": 32,
      "content": "chrobotvarieswithinitsvelocityboundary. (f)DuringthemovementofeachAGVtothetargetposition,theyawAngleandtraveldistanceof\neachAGVneedtobekeptwithinitsmaximumyawAngleanddistanceconstraints,respectively. Inthisexperiment,weconsiderthetotalpathlengthofthemulti-AGVsystemastheglobalobjective\nfunctionforthepathplanningproblem. Thetotalpathlengthisdefinedasthesumofthepathlengths\nofeachAGV.Mathematically,theobjectivefunctionisrepresentedasfollows:\nm\n(cid:88)\nF =min{ L } (17)\ni\ni=1\nwherethevariablemrepresentsthetotalnumberofAGVs,andL representstheplannedpathlength\ni\nofthei-thAGV.ThepathlengthofeachAGV,L ,canbecalculatedasfollows:\ni\nns\n(cid:88)\nL = dis(p ,p ) (18)\ni i,k i,k+1\nk=0\nWedefinep andp astheinitialandfinalpositionsofthei-throbot,respectively. Theterm\ni,0 i,ns+1\ndis(p ,p )representstheEuclideandistancebetweenwaypointsp andp .",
      "size": 833,
      "sentences": 5
    },
    {
      "id": 33,
      "content": "id:88)\nL = dis(p ,p ) (18)\ni i,k i,k+1\nk=0\nWedefinep andp astheinitialandfinalpositionsofthei-throbot,respectively. Theterm\ni,0 i,ns+1\ndis(p ,p )representstheEuclideandistancebetweenwaypointsp andp . Thepathfor\ni,k i,k+1 i,k i,k+1\nthei-throbotisdenotedaspath =[p ,p ,...,p ,p ],wherepath (k =0,1,...,n+l)\ni i,0 i,1 i,ns i,ns+1 i,k\nrepresentsthek-thwaypointalongthepathgeneratedbythei-throbot. Theyawangle,θ ,ofthei-throbotatthek-thpathsegmentalongthegeneratedpathiscomputed\ni,k\nasfollows:\n(cid:20) (cid:21)\n(x −x )(x −x )+(y −y )(y −y )\nθ =arccos i,k+1 i,k i,k+2 i,k+1 i,k+1 i,k i,k+2 i,k+1 (19)\ni,k dis(p ,p )·dis(p ,p )\ni,k i,k+1 i,k+2 i,k+1\nThepathplanningproblemissubjecttothefollowingconstraints:\n\nLmin ≤L ≤Lmax\n\nθ\ni\ni\nmin ≤θ\ni\ni\n,k\n≤θ\ni\ni\nmax, 1≤k ≤ns\np p ∩obstacle∈null, 0≤k ≤ns (20)\ni,k i,k+1\nT pat\n∩\nh\ni\nT\n∩\n∩\np\n,\na\n. t\n. h\n.",
      "size": 847,
      "sentences": 6
    },
    {
      "id": 34,
      "content": "athplanningproblemissubjecttothefollowingconstraints:\n\nLmin ≤L ≤Lmax\n\nθ\ni\ni\nmin ≤θ\ni\ni\n,k\n≤θ\ni\ni\nmax, 1≤k ≤ns\np p ∩obstacle∈null, 0≤k ≤ns (20)\ni,k i,k+1\nT pat\n∩\nh\ni\nT\n∩\n∩\np\n,\na\n. t\n. h\n. j\n,∩\n∈\nT\nnul\n∈/\nl,\nno\n∀\nn\ni\n-n\n̸=\null\nj,i,j ∈N\ni j N\nwhere Lmin and Lmax represent the minimum and maximum path length constraints for the i-th\ni i\nrobot,respectively. θmin andθmax representtheminimumandmaximumconstraintsfortheyaw\ni i\nanglerotationofthei-thAGV.T isthearrivaltimeforthei-throbotatthedestinationposition. i\nThevariablesx andy representthevaluesoftheAGVonthexandyaxes,respectively. The\ni,k i,k\nterm\"obstacle\"indicatesthepositioncoordinateoftheobstacle. Theterm\"null\"definesthepath\nintersectionbetweenanytwodifferentrobotsatthesametimeasanemptyset,ensuringcollision-free\npathsbetweenanyAGVandtheobstacle.",
      "size": 817,
      "sentences": 8
    },
    {
      "id": 35,
      "content": "cle\"indicatesthepositioncoordinateoftheobstacle. Theterm\"null\"definesthepath\nintersectionbetweenanytwodifferentrobotsatthesametimeasanemptyset,ensuringcollision-free\npathsbetweenanyAGVandtheobstacle. Theterm\"non-null\"indicatesthattheintersectionofthe\narrivaltimesofanytwodifferentAGVsisanon-emptyset,implyingthatthepathsgeneratedby\ndifferentAGVscanensuresimultaneousarrivalatthedestination. 8\n=== 페이지 9 ===\nFigure3: DDQNandPFcombinedstructure. 3.5 PF-DDQNAlgorithm\nThisarticleproposesthePF-DDQNalgorithm,whichemploysPFtoaddresstheissueofweight\ninaccuracyindeepreinforcementlearningmodels. Thealgorithm’sstructureisdepictedinFigure3.",
      "size": 632,
      "sentences": 6
    },
    {
      "id": 36,
      "content": "structure. 3.5 PF-DDQNAlgorithm\nThisarticleproposesthePF-DDQNalgorithm,whichemploysPFtoaddresstheissueofweight\ninaccuracyindeepreinforcementlearningmodels. Thealgorithm’sstructureisdepictedinFigure3. Intheproposedalgorithm,thestateequationandobservationequation,whichdescribethedynamics\nofthesystem,areassumedtoberepresentedasfollows:\nx =f(x )+w\nt t−1 t−1\n(21)\nz =h(x )+ν\nt t t\nwherex representsthestatevector,z representsthemeasurementvector,f(x )signifiesthestate\nt t t−1\ntransitionfunction,h(x )denotesthetransferfunctionbetweenthestateandobservationvectors,\nt\nw representstheprocessnoise,andv representstheobservationnoise.",
      "size": 627,
      "sentences": 4
    },
    {
      "id": 37,
      "content": "surementvector,f(x )signifiesthestate\nt t t−1\ntransitionfunction,h(x )denotesthetransferfunctionbetweenthestateandobservationvectors,\nt\nw representstheprocessnoise,andv representstheobservationnoise. t−1 t\nSubsequently,theweightparametersθ ,z ,andQ(s ,a ;θ )correspondingtotimetaresubstituted\nt t t t t\nintothePFstateequationandobservationequation,asrepresentedasfollows:\nθ =θ +W\nt t−1 t−1\n(22)\nZ =Q(s ,a ;θ )+ν\nt t t t t\nLett−1timehaveasetofposteriorparticles,expressedas:\n{x (i),ω (i);i=1,2,··· ,N} (23)\nt−1 t−1\nwhere N represents the number of particles, x (i) denotes the i-th particle at time t−1, and\nt−1\nω (i)signifiestheweightofthei-thparticleattimet−1. t−1\nTheentirealgorithmflowisdescribedasfollows:\nParticlesetinitialization,t=0: Randomsamplesaredrawnfromthepriorprobabilitydensity\np(θ ),denotedasθ (1),θ (2),...,θ (N)(whereN representsthenumberofrandomsamples).",
      "size": 873,
      "sentences": 3
    },
    {
      "id": 38,
      "content": "ealgorithmflowisdescribedasfollows:\nParticlesetinitialization,t=0: Randomsamplesaredrawnfromthepriorprobabilitydensity\np(θ ),denotedasθ (1),θ (2),...,θ (N)(whereN representsthenumberofrandomsamples). 0 0 0 0\nWhent=1,2,...,performthefollowingsteps: (a)Stateprediction:\nThepriorparticlesattimestepk aredrawnbasedonthesystem’sstateequation,asshowninthe\nfollowingequation:\n9\n=== 페이지 10 ===\n{θ (i);i=1,2,··· ,N}∼p(θ |θ ) (24)\nt|t−1 k k−1\n(b)Update:\nFirst,theweightupdateisperformed. Afterobtainingthemeasuredvaluesoftheneuralnetwork\nweights, the particle weights, denoted as w(j), are calculated based on the system’s observation\nt\nequationasfollows:\nω(i) =ω(i) p(Z |θ(i)), i=1,...,N (25)\nt t−1 t t\nThen,calculatethenumberofeffectiveparticlesN˜ ,andcompareitwiththesetthresholdN . If\neff th\nN˜ <N ,thenresamplethepriorparticlesettoobtainN particlesofequalweight. Otherwise,\neff th\nproceedtothenextstep.",
      "size": 897,
      "sentences": 5
    },
    {
      "id": 39,
      "content": ",calculatethenumberofeffectiveparticlesN˜ ,andcompareitwiththesetthresholdN . If\neff th\nN˜ <N ,thenresamplethepriorparticlesettoobtainN particlesofequalweight. Otherwise,\neff th\nproceedtothenextstep. (c)Estimation:\nAfteriteratingttimes,thetrueparameterestimationθˆ isobtainedandreturnedtotheestimation\nt\nnetworkforvaluefunctioncomputation. Thespecificformulaisgivenasfollows:\nN\nθˆ = (cid:88) θ (i)ω˜(i) (26)\nt t|t−1 t\ni=1\nThefundamentalPFalgorithmmaintainsthepastsamplesunchangedwhensamplingattimestept,\nandtheimportanceweightsareiterativelycomputed. Insummary,thecombinationofPFanddouble\ndeepQ-networkinvolvesthefollowingsteps:\nStep1: UtilizingtheDDQNmodel,theparametersθ ,Q(s ,a ;θ),andz attimetareemployedas\nt t t t\ninputstoconstructthestateequationandobservationequationforthePF.Furthermore,theparticle\nsetisinitialized. Step2: Iterativelyupdating,performingstateprediction,weightupdating,andresamplingoperations\nateachtimesteptoobtaintheoptimalestimationoftrueweightsθˆ.",
      "size": 975,
      "sentences": 7
    },
    {
      "id": 40,
      "content": "rthePF.Furthermore,theparticle\nsetisinitialized. Step2: Iterativelyupdating,performingstateprediction,weightupdating,andresamplingoperations\nateachtimesteptoobtaintheoptimalestimationoftrueweightsθˆ. t\nStep3: Transmittingtheoptimaltrueparametersθˆ totheestimationnetworktoobtainQ(s ,a ;θ),\nt t t\nwhichisusedtoselecttheactioncorrespondingtothemaximumQ-valueduringexperienceexploita-\ntion,therebyenhancingtheaccuracyoftheneuralnetwork’sapplicationintheDDQNalgorithm. Afterobtainingθ ,Z ,andQ(s ,a ;θ)attimet+1,theaboveprocessisrepeatedtoobtain\nt+1 t+1 t+1 t+1\ntheoptimaldecisionforthenextstep,cyclingthroughthisprocessuntilthemodelconverges. 3.5.1 StatusandRewardMechanisms\nThepathplanningmethodformulti-AGVbasedonPF-DDQNconsiderseachAGVasanintelligent\nagent,whereitspositiononthegridmapisregardedasthecontrolledobject. Theactionsofthe\nAGVcorrespondtoitsmovements,andtheintelligentagentselectsanactionbasedonthecurrent\nstate,executesit,andobservestheresultingstateandreward.",
      "size": 972,
      "sentences": 6
    },
    {
      "id": 41,
      "content": "onthegridmapisregardedasthecontrolledobject. Theactionsofthe\nAGVcorrespondtoitsmovements,andtheintelligentagentselectsanactionbasedonthecurrent\nstate,executesit,andobservestheresultingstateandreward. Theagentcontinuouslyupdatesits\nparameterstomaximizetherewarduntiltheoptimalactionisdetermined. ThealgorithmframeworkisshowninFigure4.Thealgorithmemploysanexperiencepooltostore\nprevioustrainingmemories,whichaidsinthetrainingoftheDDQNnetwork. Intheframework,θ\nrepresentsthenetworkweightsattimet,rrepresentstherewardattimet,(s,a,r,s′)representsthe\nreplaymemoryunitattimet,srepresentsthestateattimet,andarepresentstheactionattimet. 3.5.2 StateSpaceandActionSpace\nInordertoaccuratelyrepresenttheoperatingenvironmentoftheAGVandfacilitatethetraining\nprocess, this study adopts a grid-based approach to depict the entire working area. Each grid is\ndivided into 2x2m sizes, which corresponds to the length of the AGV. The state variables of the\nAGVincludeitsrelativepositionstoobstaclesandtargetpoints.",
      "size": 993,
      "sentences": 8
    },
    {
      "id": 42,
      "content": "depict the entire working area. Each grid is\ndivided into 2x2m sizes, which corresponds to the length of the AGV. The state variables of the\nAGVincludeitsrelativepositionstoobstaclesandtargetpoints. Intermsofplanning,theAGVhas\nninepossibleactions: northward, southward, eastward, westward, northeastward, southeastward,\nnorthwestward, southwestward, orremainingstationary. Figure5illustratestheoverallplanning\nenvironmentandthepathdiagram. 10\n=== 페이지 11 ===\nFigure4: OverallframeworkofthePF-DDQNmodel. Figure5: Overallplanningenvironmentandpathdiagram. 3.5.3 RewardFunction\nTherewardfunctionplaysacriticalroleinreinforcementlearningasitguidesthelearningprocess\noftheintelligentagentandinfluencesactionselection. Therefore,appropriatelydefiningrewardsis\nessentialtoachievedesiredoutcomesandoptimalactionstrategies. Inthecontextofmulti-AGVpath\nplanning,theprimaryobjectiveistominimizethetotaltrajectorypointstraversedbyeachAGVwhile\nsuccessfullyreachingtheirtargetpoints.",
      "size": 968,
      "sentences": 10
    },
    {
      "id": 43,
      "content": "vedesiredoutcomesandoptimalactionstrategies. Inthecontextofmulti-AGVpath\nplanning,theprimaryobjectiveistominimizethetotaltrajectorypointstraversedbyeachAGVwhile\nsuccessfullyreachingtheirtargetpoints. Inthisstudy,therewardisdefinedwithfourcomponents:\nBaselineValue: Todecreasetheoveralltrajectorycount,apenaltyvalueof-4isassignedwhenan\nAGVtakesanactiontochangeitsposition. DistancetoTargetPoint: WhenthesumofdistancesbetweenallAGVsandtheirtargetpoints\ndecreases,indicatingprogress,arewardof5isassigned. Conversely,apenaltyof-5isgivenwhen\ndistancesincrease.Reward200whenitoverlapswiththetargetpoint. DistancetoObstacle: Toensureobstacleavoidance,anAGVispenalizedwithavalueof-20ifit\ncollideswithanobstacle. 11\n=== 페이지 12 ===\nFigure6: DDQNandEKFcombinedstructure. Figure7: OverallframeworkoftheEKF-DDQNmodel. DistancebetweenAGVs: RewardsareassignedbasedonthedistancesbetweenAGVs. Onlywhen\nAGVscollide,apenaltyof-20isimposed,treatingotherAGVsasobstacles.",
      "size": 949,
      "sentences": 10
    },
    {
      "id": 44,
      "content": "tructure. Figure7: OverallframeworkoftheEKF-DDQNmodel. DistancebetweenAGVs: RewardsareassignedbasedonthedistancesbetweenAGVs. Onlywhen\nAGVscollide,apenaltyof-20isimposed,treatingotherAGVsasobstacles. Thisrewardandpenalty\nmechanismisindependentofthedistancesbetweentheAGVs. 3.6 EKF-DDQNAlgorithm\nSimilartoPF-DDQN,wecanalsoreplacePFwithEKF,combineEKFwithneuralnetwork,and\nupdatenetworkweightsaccordingtotheupdatemodeofEKF.Thealgorithmstructureisshownin\nFigure6,ThealgorithmframeworkisshowninFigure7. In the EKF-DDQN algorithm, the state equation and observation equation, which describe the\ndynamicsofthesystem,areassumedtoberepresentedasfollows:\nx =f(x ,u )+w (27)\nt t−1 t−1 t−1\nz =h(x )+ν (28)\nt t t\nwherex representsthestatevector,z representsthemeasurementvector,f(x ,u )signifies\nt t t−1 t−1\nthenonlinearstatetransitionfunction,h(x )denotesthemeasurementfunction,w representsthe\nt t−1\nprocessnoise,andν representsthemeasurementnoise.",
      "size": 936,
      "sentences": 7
    },
    {
      "id": 45,
      "content": "resentsthemeasurementvector,f(x ,u )signifies\nt t t−1 t−1\nthenonlinearstatetransitionfunction,h(x )denotesthemeasurementfunction,w representsthe\nt t−1\nprocessnoise,andν representsthemeasurementnoise. t\nToupdatetheparametervectorθusingtheEKF,thefollowingEKFupdateequationsareemployed:\n12\n=== 페이지 13 ===\nθ− =θ (29)\nt t−1\nP− =F P FT +Q (30)\nt t−1 t−1 t−1 t−1\nK =P−HT(H P−HT +R )−1 (31)\nt t t t t t t\nθ =θ−+K (z −h(θ−)) (32)\nt t t t t\nP =(I−K H )P− (33)\nt t t t\nWhere,θ− andP− representthepredictedparameterestimateandthepredictederrorcovariance\nt t\nmatrix,respectively. F andH aretheJacobianmatricesofthestatetransitionfunctionandthe\nt−1 t\nmeasurement function evaluated at the predicted parameter estimate, while Q and R are the\nt−1 t\nprocessnoisecovariancematrixandthemeasurementnoisecovariancematrix,respectively.",
      "size": 813,
      "sentences": 3
    },
    {
      "id": 46,
      "content": "onfunctionandthe\nt−1 t\nmeasurement function evaluated at the predicted parameter estimate, while Q and R are the\nt−1 t\nprocessnoisecovariancematrixandthemeasurementnoisecovariancematrix,respectively. TheEKF-DDQNalgorithmfollowsasimilarflowtothePF-DDQNalgorithm,withthefollowing\nmodifications:\nEKF-DDQNAlgorithm:\nInitialization,t=0: Setaninitialestimatefortheparametervectorθ basedonpriorknowledge\n0\norassumptions. Whent=1,2,...,performthefollowingsteps: (a)Stateprediction:\nObtainthepredictedparameterestimateattimesteptusingthesystem’sstateequationandtheEKF\npredictionequations:\nθ− =f(θ ,u ) (34)\nt t−1 t−1\nP− =F P FT +Q (35)\nt t−1 t−1 t−1 t−1\n(b)Update:\nIncorporate the new measurement z to improve the parameter estimate using the EKF update\nt\nequations:\nK =P−HT(H P−HT +R )−1 (36)\nt t t t t t t\nθ =θ−+K (z −h(θ−)) (37)\nt t t t t\nP =(I−K H )P− (38)\nt t t t\nTheremainingstepsofthealgorithm,includingweightupdatingandresampling,remainunchanged.",
      "size": 945,
      "sentences": 3
    },
    {
      "id": 47,
      "content": "equations:\nK =P−HT(H P−HT +R )−1 (36)\nt t t t t t t\nθ =θ−+K (z −h(θ−)) (37)\nt t t t t\nP =(I−K H )P− (38)\nt t t t\nTheremainingstepsofthealgorithm,includingweightupdatingandresampling,remainunchanged. The algorithm iteratively performs state prediction and update steps, resulting in an optimized\nestimateofthetrueweightsθ . Theseoptimizedweightsarethenusedtocomputethevaluefunction\nt\nQ(s ,a ;θ),whichisutilizedinthedecision-makingprocesstoselecttheactioncorrespondingto\nt t\nthemaximumQ-valueduringexperienceexploitation. Thealgorithmcontinuestoiterate,updating\nθ ,z ,andQ(s ,a ;θ)ineachiteration. t+1 t+1 t+1 t+1\n4 ExperimentSetupandResults\nThenumericalsimulationsinthisstudywereconductedonasystemcomprisinganIntelCorei7-6500\n3.20GHzprocessor,aGPUGTX3060with6GBofmemory,and8GBofRAM.Thesimulations\nwereperformedonboththeUbuntu20.04andWindows11operatingsystemsusingPython.",
      "size": 869,
      "sentences": 5
    },
    {
      "id": 48,
      "content": "tudywereconductedonasystemcomprisinganIntelCorei7-6500\n3.20GHzprocessor,aGPUGTX3060with6GBofmemory,and8GBofRAM.Thesimulations\nwereperformedonboththeUbuntu20.04andWindows11operatingsystemsusingPython. This section validates the proposed PF-DDQN path planning algorithm’s superior convergence\nspeed and performance compared to the traditional DDQN and EKF-DDQN algorithms through\nexperimentalverification. TwosimulationexperimentsareconductedtotrainPF-DDQN,DDQN,\nandEKF-DDQN.AsillustratedinFigure8,Experiment1focusesonthepathplanningofasingle\n13\n=== 페이지 14 ===\nFigure8: TrainingmapsforA)experiments1andB)experiment2,wheretherobotstartpoint,\ntargets,obstacles,andfreespacearerepresentedasblue,green,magenta,andwhite. AGVtoafixedtarget,wheretheAGVstartsfromtheinitialpositionandaimstoreachafixedtarget\nwithoutcollidingwithobstacles.",
      "size": 828,
      "sentences": 4
    },
    {
      "id": 49,
      "content": "artpoint,\ntargets,obstacles,andfreespacearerepresentedasblue,green,magenta,andwhite. AGVtoafixedtarget,wheretheAGVstartsfromtheinitialpositionandaimstoreachafixedtarget\nwithoutcollidingwithobstacles. Experiment2involvesthepathplanningofmultipleAGVstoa\nfixed target, where multiple AGVs start from dispersed initial positions and aim to reach a fixed\ntargetwithoutcollidingwithobstaclesoreachother. Inthesameunknownenvironmentandunder\nidenticalconditions,quantitativeandqualitativecomparisonsaremadeamongPF-DDQN,DDQN,\nandEKF-DDQNbyevaluatingtheirresultsusingthesamesetofparameters. In the simulation experiments, the parameters of the deep reinforcement learning network are\npresentedinTable1.",
      "size": 692,
      "sentences": 5
    },
    {
      "id": 50,
      "content": "emadeamongPF-DDQN,DDQN,\nandEKF-DDQNbyevaluatingtheirresultsusingthesamesetofparameters. In the simulation experiments, the parameters of the deep reinforcement learning network are\npresentedinTable1. Table1: SystemParameters\nParameter Value\nMemorysize 10000\nBatchsize 500\nDiscountfactor 0.95\nLearningrate 0.0001\nModelupdatefrequency 50\nInitialexplorationrate 1.0\nExplorationratedecay 0.9995\nFinalexplorationrate 0.001\n4.1 Experiment1: PathPlanningComparisonforaSingleAGV\nInthisexperiment,thePF-DDQN,EKF-DDQN,andDDQNalgorithmsunderwent60,000training\niterations. TheAGVstartedfromthebottom-leftcornerofthemapandnavigatedthroughseven\nclustersofobstaclestoreachafixedpositionintheupper-rightcorner. Figure9depictsthepathsplannedbythethreemethods. Itisevidentfromthefigurethatallthree\nalgorithmssuccessfullyguidetheAGVstotheirtargetpoints,withsimilarpathlengths.Thisindicates\nthefeasibilityandstabilityofthealgorithms. The learning curves of the three methods are illustrated in Figure 10.",
      "size": 984,
      "sentences": 7
    },
    {
      "id": 51,
      "content": "ithmssuccessfullyguidetheAGVstotheirtargetpoints,withsimilarpathlengths.Thisindicates\nthefeasibilityandstabilityofthealgorithms. The learning curves of the three methods are illustrated in Figure 10. The experimental results\ndemonstratethatthePF-DDQNandEKF-DDQNmethods,whichincorporatefilters,exhibitsimilar\nconvergencespeedsandyieldconsistenttrajectories. Incomparison,theyoutperformthetraditional\nDDQNalgorithmbyachievingfasterandmoreefficientsolutions. TheDDQNalgorithmprovidestheshortestpathfromthestartingpointtothetargetpoint,assuming\npriorknowledgeofthemaplayout. However,inrealindustrialenvironments,thebehaviorofAGVs\nnear obstacles can be hazardous. As the reward function influences path planning behavior, the\nPF-DDQNmethodprioritizessafetybybetteravoidingobstacles,evenifitresultsinaslightlylonger\npath. 14\n=== 페이지 15 ===\nFigure9: FinalpathresultsoftheexperimentforsingleAGV:A)DDQN,B)EKF-DDQN,and\nC)PF-DDQNafterpathsmoothingfor60,000episodes.",
      "size": 954,
      "sentences": 8
    },
    {
      "id": 52,
      "content": "tteravoidingobstacles,evenifitresultsinaslightlylonger\npath. 14\n=== 페이지 15 ===\nFigure9: FinalpathresultsoftheexperimentforsingleAGV:A)DDQN,B)EKF-DDQN,and\nC)PF-DDQNafterpathsmoothingfor60,000episodes. Figure10: Learningcurvesforsingle-AGVtargetpathplanning: Comparisonoftheproposed\nmethodwithEKF-DDQNandDDQNaftertraining. Table2presentsthespecificnumericalsimulationresultsofthisexperiment. ComparedtotheDDQN\nalgorithm,theproposedmethodinthisstudyreducedtheoveralltrainingtimeby20.69%over60,000\niterations. Additionally,thenumberofiterationsrequiredduringthetrainingprocessdecreasedby\n76.02%comparedtoDDQN.",
      "size": 605,
      "sentences": 6
    },
    {
      "id": 53,
      "content": "N\nalgorithm,theproposedmethodinthisstudyreducedtheoveralltrainingtimeby20.69%over60,000\niterations. Additionally,thenumberofiterationsrequiredduringthetrainingprocessdecreasedby\n76.02%comparedtoDDQN. Table2: ResultsofTargetPathPlanningforaSingleAGV\nDDQN EKF-DDQN PF-DDQN\nTrainingTime 4.59Hours 3.95Hours 3.64Hours\nSolution After 56,568 After 16,482 After 13,564\nEpisode episodes/(4.2Hours) episodes/(2.79 episodes/(2.63\nHours) Hours)\nTarget Hit 7,498 9,698 25,800\nTimes\nObstacle Hit 44,598 26,894 4,632\nTimes\nStep-Timeout 8,579 6,897 4,591\nTimes\nLast Path 23Grids 30Grids 30Grids\nLength\nSincethereisonlyoneAGVinthisexperiment,theconstructednetworkmodelisrelativelysimple. Therefore,thePF-DDQNmethodisnotsignificantlybetterthantheEKF-DDQNmethodintermsof\nfinalrewardvalue. However,comparedwithPF-DDQNmethod,therewardcurveofEKF-DDQN\nmethodhastwobigchanges,whichisabouttofallintothelocaloptimalsolution,indicatingthatits\nstabilityispoor.",
      "size": 933,
      "sentences": 5
    },
    {
      "id": 54,
      "content": "DQNmethodintermsof\nfinalrewardvalue. However,comparedwithPF-DDQNmethod,therewardcurveofEKF-DDQN\nmethodhastwobigchanges,whichisabouttofallintothelocaloptimalsolution,indicatingthatits\nstabilityispoor. 15\n=== 페이지 16 ===\nFigure11: Experiment2finalpathresultsofinA)DDQN,B)EKF-DDQN,andC)PF-DDQN\nafterMultiAGVspathsmoothingfor60000episodes. Figure12: Formulti-AGVtargetpathplanning,theproposedmethodiscomparedwiththe\nlearningcurveafterEKF-DDQNandDDQNtraining. TheproposedmethodminimizesunnecessaryexplorationbytheAGVtoagreatextentbyintroducing\nthePFintheprocessofobtainingtargetnetworkparameters. Thisreduceserrorsresultingfrom\nimpreciseweightestimatesthroughiterativerefinement. UnlikeDDQN,thismethodtreatsimprecise\nweightsasstatevariablesinthestate-spaceequation,allowingittoconvergetothecorrectrangeeven\nifthepreviouslylearnednetworkparametersareaffectedbyerrors. Incomparison,DDQNexhibitsa\nbroaderrangeofrewardvaluesandgreaterdatafluctuations,makingconvergencelesslikely.",
      "size": 969,
      "sentences": 8
    },
    {
      "id": 55,
      "content": "oconvergetothecorrectrangeeven\nifthepreviouslylearnednetworkparametersareaffectedbyerrors. Incomparison,DDQNexhibitsa\nbroaderrangeofrewardvaluesandgreaterdatafluctuations,makingconvergencelesslikely. 4.2 Experiment2: ComparisonofPathPlanningforMultipleAGVs\nInthisexperiment,thePF-DDQN,EKF-DDQN,andDDQNalgorithmsunderwent60,000training\niterations. TenAGVsstartedtheirjourneysfromdifferentfixedpositionsonthemap,navigating\nthroughsevenclustersofobstaclestoreachdesignatedfixedpositionsintheupper-rightcorner. Figure11illustratesthepathsplannedbythethreemethods. Itisevidentfromthefigurethatthe\nalgorithmproposedinthisstudysuccessfullyenablesallAGVstoreachtheirtargetpoints,whereas\ntheEKF-DDQNandDDQNalgorithmsdidnotachievethesamelevelofsuccess. The learning curves of the three methods are depicted in Figure 12. The experimental results\ndemonstratethattheimprovedmethodexhibitsfasterconvergencecomparedtotheothertwomethods,\nwithhighervaluesachievedafterconvergence.",
      "size": 964,
      "sentences": 8
    },
    {
      "id": 56,
      "content": "the three methods are depicted in Figure 12. The experimental results\ndemonstratethattheimprovedmethodexhibitsfasterconvergencecomparedtotheothertwomethods,\nwithhighervaluesachievedafterconvergence. Table3presentsthespecificnumericalsimulationresultsofthisexperiment. Fromthetable,itis\nobservedthattheDDQNandEKF-DDQNmethodsenabledthefirstAGVtoreachthetargetafter\n16,987and27,721trainingiterations,respectively. However,whenmultipleAGVsareinvolved,the\ndisturbancecausedbytheneuralnetwork’shighvarianceincreasesexponentially,leadingtoarapid\ndecreaseinlearningefficiency. Asaresult,theDDQNmethodrequiresasignificantamountoftimetoforgetthestatesassociated\nwithindividualAGVsreceivinghighrewards. Conversely,theEKF-DDQNmethod,duetoitshighly\n16\n=== 페이지 17 ===\nnonlinearnature,experiencesadecreaseinaccuracy,preventingallAGVsfromreachingtheirtarget\npointsasintended.",
      "size": 859,
      "sentences": 7
    },
    {
      "id": 57,
      "content": "vidualAGVsreceivinghighrewards. Conversely,theEKF-DDQNmethod,duetoitshighly\n16\n=== 페이지 17 ===\nnonlinearnature,experiencesadecreaseinaccuracy,preventingallAGVsfromreachingtheirtarget\npointsasintended. Table3: ResultsoftargetpathplanningforasingleAGV\nDDQNAlgorithm(TrainingTime: 5.14Hours)\nAGV Episode TrainingTime Last Path The Ideal\nIndex Length Length\n1 17989 1.3H 87 24\n2 – – – 24\n3 – – – 18\n4 – – – 14\n5 – – – 8\n6 – – – 26\n7 – – – 24\n8 – – – 22\n9 – – – 18\n10 – – – 9\nEKF-DDQNAlgorithm(TrainingTime: 5.74Hours)\nAGV Episode TrainingTime Last Path The Ideal\nIndex Length Length\n1 27721 3.3H 27 24\n2 – – – 24\n3 – – – 18\n4 – – – 14\n5 – – – 8\n6 – – – 26\n7 – – – 24\n8 – – – 22\n9 – – – 18\n10 – – – 9\nPF-DDQNAlgorithm(TrainingTime: 2.98Hours)\nAGV Episode TrainingTime Last Path The Ideal\nIndex Length Length\n1 6584 0.96H 24 24\n2 4783 0.67H 24 24\n3 5869 0.72H 18 18\n4 6202 0.78H 14 14\n5 6563 0.81H 8 8\n6 5905 0.73H 26 26\n7 5968 0.74H 24 24\n8 6330 0.79H 22 22\n9 6097 0.76H 18 18\n10 5780 0.71H 9 9\nIncontrast,thePF-DDQNalgorithmproposedinthisstudysuccessfullyenablesallAGVstoreach\ntheirtargetpoints.",
      "size": 1090,
      "sentences": 3
    },
    {
      "id": 58,
      "content": "8 8\n6 5905 0.73H 26 26\n7 5968 0.74H 24 24\n8 6330 0.79H 22 22\n9 6097 0.76H 18 18\n10 5780 0.71H 9 9\nIncontrast,thePF-DDQNalgorithmproposedinthisstudysuccessfullyenablesallAGVstoreach\ntheirtargetpoints. Thealgorithmconvergesfasterandachievesbetterresultsintermsofthepath\nlengths. ThetrainingtimeforthePF-DDQNalgorithmissignificantlylesscomparedtotheothertwo\nmethods. Inconclusion,thePF-DDQNalgorithmoutperformstheDDQNandEKF-DDQNalgorithmsinterms\nofmulti-AGVpathplanning. Itexhibitsfasterconvergence,higheraccuracy,andbetterperformance\nintermsofpathlengths. 17\n=== 페이지 18 ===\n5 Conclusion\nTosummarize, thisstudyintroducesthePF-DDQNmethodforpathplanninginvolvingmultiple\nAGVs,whichincorporatesPF.ThisapproachaddressesthelimitationsoftheclassicalDDQNlearning\nalgorithminnoisyandcomplexenvironments,whilealsodemonstratingsuperiorfittingaccuracyfor\ncomplexmodelscomparedtotheuseofKF. Themethodutilizesanonlinearmodelwithinaneuralnetworktodescribethesystemandintegrates\nPFtoestimatethesystem’sstate.",
      "size": 990,
      "sentences": 7
    },
    {
      "id": 59,
      "content": ",whilealsodemonstratingsuperiorfittingaccuracyfor\ncomplexmodelscomparedtotheuseofKF. Themethodutilizesanonlinearmodelwithinaneuralnetworktodescribethesystemandintegrates\nPFtoestimatethesystem’sstate. Ateachtimestep,thePFupdatesthestateestimatebasedoncurrent\nmeasurements,whiletheneuralnetworkenhancestheaccuracyoftheestimation. Byleveraging\ntheneuralnetwork’sabilitytolearncomplexpatternsintheenvironmentandcombiningthemwith\nthestateestimationfromPF,theproposedmethodoffersanefficientandeffectivesolutionforpath\nplanning. Simulation experiments were conducted to evaluate the performance of the method, revealing\nsignificantimprovementsintrainingtimeandpathqualitycomparedtotheDDQNmethod,with\nrespective enhancements of 92.62% and 76.88%. This research provides valuable insights into\npathplanningandpresentsanovelandefficientsolutionapplicabletomulti-AGVpathplanningin\ncomplexenvironments.",
      "size": 890,
      "sentences": 6
    },
    {
      "id": 60,
      "content": "h\nrespective enhancements of 92.62% and 76.88%. This research provides valuable insights into\npathplanningandpresentsanovelandefficientsolutionapplicabletomulti-AGVpathplanningin\ncomplexenvironments. Thefindingsholdpromisingpotentialforvariousapplications,including\nrobotics,autonomousvehicles,andunmannedaerialvehicles. References\n[1] Yin,J.,Li,L.,Mourelatos,Z.P.,etal.ReliableGlobalPathPlanningofOff-RoadAutonomous\nGroundVehiclesUnderUncertainTerrainConditions.IEEETransactionsonIntelligentVehicles\n2023,IEEE. [2] Yin,J.,Hu,Z.,Mourelatos,Z.P.,etal.EfficientReliability-BasedPathPlanningofOff-Road\nAutonomousGroundVehiclesThroughtheCouplingofSurrogateModelingandRRT.IEEE\nTransactionsonIntelligentTransportationSystems2023,IEEE. [3] Jones,M.,Djahel,S.,Welsh,K.Path-planningforunmannedaerialvehicleswithenvironment\ncomplexityconsiderations: Asurvey.ACMComputingSurveys2023,55(11),1-39. [4] Wu, Z., Dai, J., Jiang, B., et al.",
      "size": 923,
      "sentences": 7
    },
    {
      "id": 61,
      "content": "EEE. [3] Jones,M.,Djahel,S.,Welsh,K.Path-planningforunmannedaerialvehicleswithenvironment\ncomplexityconsiderations: Asurvey.ACMComputingSurveys2023,55(11),1-39. [4] Wu, Z., Dai, J., Jiang, B., et al. Robot path planning based on artificial potential field with\ndeterministicannealing.ISATransactions2023,138,74-87. [5] Yao, M., Deng, H., Feng, X., et al. Improved dynamic windows approach based on energy\nconsumption management and fuzzy logic control forlocal path planning ofmobile robots. Computers&IndustrialEngineering2024,187,109767. [6] Sun,P.Z.,Yang,Q.,Kuang,W.J.,etal.Limitsongasimpermeabilityofgraphene.Nature\n2020,579(7798),229-232. [7] Aslan,M.F.,Durdu,A.,Sabanci,K.Goaldistance-basedUAVpathplanningapproach,path\noptimizationandlearning-basedpathestimation: GDRRT*,PSO-GDRRT*andBiLSTM-PSO-\nGDRRT.AppliedSoftComputing2023,137,110156. [8] Mumtaz, J., Minhas, K. A., Rauf, M., et al.",
      "size": 892,
      "sentences": 10
    },
    {
      "id": 62,
      "content": "sedUAVpathplanningapproach,path\noptimizationandlearning-basedpathestimation: GDRRT*,PSO-GDRRT*andBiLSTM-PSO-\nGDRRT.AppliedSoftComputing2023,137,110156. [8] Mumtaz, J., Minhas, K. A., Rauf, M., et al. Solving Line Balancing and AGV Scheduling\nProblemsforIntelligentDecisionsusingaGenetic-ArtificialBeeColonyAlgorithm.Computers\n&IndustrialEngineering2024,109976. [9] Xin,B.,Lu,S.,Wang,Q.,etal.SimultaneousSchedulingofProcessingMachinesandAuto-\nmatedGuidedVehiclesviaaMulti-ViewModeling-BasedHybridAlgorithm.IEEETransactions\nonAutomationScienceandEngineering2023. [10] Zhang, L., Yan, Y., Hu, Y. Deep reinforcement learning for dynamic scheduling of energy-\nefficientautomatedguidedvehicles.JournalofIntelligentManufacturing2023,1-14. [11] Zhang,L.,Yang,C.,Yan,Y.,etal.Automatedguidedvehicledispatchingandroutingintegration\nviadigitaltwinwithdeepreinforcementlearning.JournalofManufacturingSystems2024,72,\n492-503.",
      "size": 911,
      "sentences": 7
    },
    {
      "id": 63,
      "content": "facturing2023,1-14. [11] Zhang,L.,Yang,C.,Yan,Y.,etal.Automatedguidedvehicledispatchingandroutingintegration\nviadigitaltwinwithdeepreinforcementlearning.JournalofManufacturingSystems2024,72,\n492-503. 18\n=== 페이지 19 ===\n[12] Liu,Y.,Ping,Y.,Zhang,L.,etal.Schedulingofdecentralizedrobotservicesincloudmanu-\nfacturingwithdeepreinforcementlearning.RoboticsandComputer-IntegratedManufacturing\n2023,80,102454. [13] Chung,J.,Fayyad,J.,Younes,Y.A.,etal.Learningteam-basednavigation: areviewofdeep\nreinforcementlearningtechniquesformulti-agentpathfinding.ArtificialIntelligenceReview\n2024,57(2),41. [14] Aradi,S.Surveyofdeepreinforcementlearningformotionplanningofautonomousvehicles. IEEETransactionsonIntelligentTransportationSystems2020,23(2),740-759. [15] Huang,H.,Savkin,A.V.,Huang,C.Reliablepathplanningfordronedeliveryusingastochastic\ntime-dependentpublictransportationnetwork.IEEETransactionsonIntelligentTransportation\nSystems2020,22(8),4941-4950.",
      "size": 944,
      "sentences": 7
    },
    {
      "id": 64,
      "content": "5] Huang,H.,Savkin,A.V.,Huang,C.Reliablepathplanningfordronedeliveryusingastochastic\ntime-dependentpublictransportationnetwork.IEEETransactionsonIntelligentTransportation\nSystems2020,22(8),4941-4950. [16] Aggarwal, S., Kumar, N.Pathplanningtechniquesforunmannedaerialvehicles: Areview,\nsolutions,andchallenges.ComputerCommunications2020,149,270-299. [17] Li,X.S.,Xu,C.G.,Zhang,Y.,etal.Investigationintogasproductionfromnaturalgashydrate:\nAreview.AppliedEnergy2016,172,286-322. [18] Sekander, S., Tabassum, H., Hossain, E. Multi-tier drone architecture for 5G/B5G cellular\nnetworks: Challenges,trends,andprospects.IEEECommunicationsMagazine2018,56(3),\n96-103. [19] Huang, Z., Zhu, D., Sun, B. A multi-AUV cooperative hunting method in 3-D underwater\nenvironmentwithobstacle.EngineeringApplicationsofArtificialIntelligence2016,50,192-\n200. [20] Li,G.,Liu,C.,Wu,L.,etal.AmixingalgorithmofACOandABCforsolvingpathplanningof\nmobilerobot.AppliedSoftComputing2023,148,110868.",
      "size": 967,
      "sentences": 7
    },
    {
      "id": 65,
      "content": "acle.EngineeringApplicationsofArtificialIntelligence2016,50,192-\n200. [20] Li,G.,Liu,C.,Wu,L.,etal.AmixingalgorithmofACOandABCforsolvingpathplanningof\nmobilerobot.AppliedSoftComputing2023,148,110868. [21] Li,B.,Qi,X.,Yu,B.,etal.TrajectoryplanningforUAVbasedonimprovedACOalgorithm. IEEEAccess2019,8,2995-3006. [22] Patle, B. K., Pandey, A., Parhi, D. R. K., et al. A review: On path planning strategies for\nnavigationofmobilerobot.DefenceTechnology2019,15(4),582-606. [23] Mnih,V.,Kavukcuoglu,K.,Silver,D.,etal.Human-levelcontrolthroughdeepreinforcement\nlearning.Nature2015,518(7540),529-533. [24] VanHasselt,H.,Guez,A.,Silver,D.Deepreinforcementlearningwithdoubleq-learning.In\nProceedingsoftheAAAIconferenceonartificialintelligence2016,30(1). [25] Hessel,M.,Modayil,J.,VanHasselt,H.,etal.Rainbow: Combiningimprovementsindeep\nreinforcementlearning.InProceedingsoftheAAAIconferenceonartificialintelligence2018,\n32(1).",
      "size": 915,
      "sentences": 9
    },
    {
      "id": 66,
      "content": "ialintelligence2016,30(1). [25] Hessel,M.,Modayil,J.,VanHasselt,H.,etal.Rainbow: Combiningimprovementsindeep\nreinforcementlearning.InProceedingsoftheAAAIconferenceonartificialintelligence2018,\n32(1). [26] Igl,M.,Zintgraf,L.,Le,T.A.,etal.DeepvariationalreinforcementlearningforPOMDPs.In\nInternationalConferenceonMachineLearning2018,2117-2126. [27] Naesseth, C., Linderman, S., Ranganath, R., et al. Variational sequential monte carlo. In\nInternationalconferenceonartificialintelligenceandstatistics2018,968-977. [28] Gregor, K., Danihelka, I., Graves, A., et al. DRAW: A recurrent neural network for image\ngeneration.InInternationalconferenceonmachinelearning2015,1462-1471. [29] RicoJonschkowski,DivyamRastogiandOliverBrock. DifferentiableParticleFilters: End-to-\nEndLearningwithAlgorithmicPriors,2018; arXiv:1805.11122. [30] SiqiLiu,GuyLever,ZheWang,JoshMerel,S.M.AliEslami,DanielHennes,WojciechM.",
      "size": 898,
      "sentences": 11
    },
    {
      "id": 67,
      "content": "yamRastogiandOliverBrock. DifferentiableParticleFilters: End-to-\nEndLearningwithAlgorithmicPriors,2018; arXiv:1805.11122. [30] SiqiLiu,GuyLever,ZheWang,JoshMerel,S.M.AliEslami,DanielHennes,WojciechM. Czarnecki,YuvalTassa,ShayeganOmidshafiei,AbbasAbdolmaleki,NoahY.Siegel,Leonard\nHasenclever,LukeMarris,SaranTunyasuvunakool,H.FrancisSong,MarkusWulfmeier,Paul\nMuller,TuomasHaarnoja,BrendanD.Tracey,KarlTuyls,ThoreGraepelandNicolasHeess. FromMotorControltoTeamPlayinSimulatedHumanoidFootball,2021;\n19\n=== 페이지 20 ===\n[31] Ma,M.,Mao,Z.Deep-convolution-basedLSTMnetworkforremainingusefullifeprediction. IEEETransactionsonIndustrialInformatics2020,17(3),1658-1667. [32] Muruganantham,A.,Tan,K.C.,Vadakkepat,P.Evolutionarydynamicmultiobjectiveoptimiza-\ntionviaKalmanfilterprediction.IEEETransactionsonCybernetics2015,46(12),2862-2873. [33] Gao,X.,Luo,H.,Ning,B.,etal.RL-AKF:AnadaptiveKalmanfilternavigationalgorithm\nbasedonreinforcementlearningforgroundvehicles.RemoteSensing2020,12(11),1704.",
      "size": 984,
      "sentences": 8
    },
    {
      "id": 68,
      "content": "ctionsonCybernetics2015,46(12),2862-2873. [33] Gao,X.,Luo,H.,Ning,B.,etal.RL-AKF:AnadaptiveKalmanfilternavigationalgorithm\nbasedonreinforcementlearningforgroundvehicles.RemoteSensing2020,12(11),1704. [34] Zhengxin,J.,Qin,S.,Yujiang,W.,etal.AnImmuneGeneticExtendedKalmanParticleFilter\napproachonstateofchargeestimationforlithium-ionbattery.Energy2021,230,120805. [35] Korkin, R., Oseledets, I., Katrutsa, A. Multiparticle Kalman filter for object localization in\nsymmetricenvironments.ExpertSystemswithApplications2024,237,121408. 20",
      "size": 532,
      "sentences": 5
    }
  ]
}