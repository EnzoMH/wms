{
  "source": "ArXiv",
  "filename": "034_Codesign_of_Edge_Intelligence_and_Automated_Guided.pdf",
  "total_chars": 14353,
  "total_chunks": 21,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nCodesign of Edge Intelligence and Automated\nGuided Vehicle Control\nMalith Gallage, Student Member, IEEE,, Rafaela Scaciota, Member, IEEE,, Sumudu Samarakoon, Member, IEEE,\nand Mehdi Bennis Fellow, IEEE\nCentre for Wireless Communication, University of Oulu, Finland\nemail: {malith.gallage,rafaela.scaciotatimoesdasilva,sumudu.samarakoon,mehdi.bennis}@oulu.fi\nAbstract—This work presents a harmonic design of au-\ntonomous guided vehicle (AGV) control, edge intelligence, and\nhuman input to enable autonomous transportation in industrial\nenvironments. The AGV has the capability to navigate between\na source and destinations and pick/place objects.",
      "size": 659,
      "sentences": 2
    },
    {
      "id": 2,
      "content": "ol, edge intelligence, and\nhuman input to enable autonomous transportation in industrial\nenvironments. The AGV has the capability to navigate between\na source and destinations and pick/place objects. The human\ninputimplicitlyprovidespreferencesofthedestinationandexact\ndroppoint,whicharederivedfromanartificialintelligence(AI)\nmodule at the network edge and shared with the AGV over a\nwirelessnetwork.Thedemonstrationindicatesthattheproposed\nintegrated design of hardware, software, and AI design achieve\na technology readiness level (TRL) of range 4-5. Index Term—Edge AI, Image Processing, Autonomous Navi-\ngation\nI. INTRODUCTION\nTherapidgrowthofcustomerdemandsandincreasingcosts\nof resources, labor, and energy have driven industries to seek\nnew technologies that improve productivity and efficiency.",
      "size": 803,
      "sentences": 5
    },
    {
      "id": 3,
      "content": "avi-\ngation\nI. INTRODUCTION\nTherapidgrowthofcustomerdemandsandincreasingcosts\nof resources, labor, and energy have driven industries to seek\nnew technologies that improve productivity and efficiency. In\nparticular, modern logistics is one of the key players to adopt\nautonomous automation technologies including human-in-the- Figure 1: The robotic platform highlighting the components\nloop, in which, human operators can provide the preferences and wireless connectivity. required for automation [1]. Autonomous guided vehicles\nserver equipped with computing capabilities. The AGV uses\n(AGVs) are one of the pivotal components to enable \"smart\nanonboardcameratosensetheenvironmentanddetermineits\nlogistics\" in manufacturing plants [2]. Realizing full/semi-\ncontrol decisions, i.e., navigation among source and destina-\nautonomy with the fusion of control systems, communication\ntions.",
      "size": 884,
      "sentences": 7
    },
    {
      "id": 4,
      "content": "termineits\nlogistics\" in manufacturing plants [2]. Realizing full/semi-\ncontrol decisions, i.e., navigation among source and destina-\nautonomy with the fusion of control systems, communication\ntions. The destination and precise drop location of the object,\nnetworks, and computation servers at the edge over repetitive\nreferred to as delivery information hereinafter, are determined\ntasks calls for artificial intelligence (AI)-based solutions [3]. by the edge server using camera images and shared with the\nWiththeincreasinginterestsinutilizingAGVsinindustrial\nAGV upon request. The delivery information is derived based\napplications, developing efficient and reliable AI-driven solu-\noncustomdropareasdefinedbyanexternalparty(e.g.,human\ntionsfornavigationtasks[3]isofutmostimportance.Towards\noperator).",
      "size": 804,
      "sentences": 5
    },
    {
      "id": 5,
      "content": "is derived based\napplications, developing efficient and reliable AI-driven solu-\noncustomdropareasdefinedbyanexternalparty(e.g.,human\ntionsfornavigationtasks[3]isofutmostimportance.Towards\noperator). this, line following robots have received significant attention\nCommunicationbetweentheAGVandtheedgeservertakes\ndue to their ease of use and low complexity and robust\nplace through a REST API over a WiFi network. The server\noperations [4]. In this work, we demonstrate the codesign of\nprovides delivery information in JSON format via its exposed\nan intelligent crawler robot, which uses its camera to sense\nendpoint, upon request by the AGV which takes place only\nthe environment and, navigate and accurately deliver objects\nonce when each of the transport job is initiated. To derive\nto their corresponding location with the aid of an edge AI\ndeliveryinformation,theedgeserverobtainsabird’seyeview\nserver to meet preferences set by a human operator.",
      "size": 950,
      "sentences": 5
    },
    {
      "id": 6,
      "content": "sport job is initiated. To derive\nto their corresponding location with the aid of an edge AI\ndeliveryinformation,theedgeserverobtainsabird’seyeview\nserver to meet preferences set by a human operator. of the destinations by accessing the camera through its REST\nII. SYSTEMARCHITECTURE\nAPIoverWiFi.Aftercompletingthedeliveryjobsuccessfully,\nWe consider the robotic platform illustrated in Fig. 1\nthe AGV returns to the source and repeats the procedure. consisting of paths from a source point to multiple destina-\nThe paths between the source and the destinations are\ntions, with an AGV that transports objects from source to\ndefinedbyblacklinesdrawnonthesurfacetoaidAGV’snav-\ndestinations,acameraobservingthedestinations,andanedge\nigation. Here, a single path starting from the source, branches\nThe authors would like to acknowledge the support and contributions out to four paths leading to four adjacent destinations.",
      "size": 918,
      "sentences": 7
    },
    {
      "id": 7,
      "content": ",andanedge\nigation. Here, a single path starting from the source, branches\nThe authors would like to acknowledge the support and contributions out to four paths leading to four adjacent destinations. Each\nof Abdulmomen Ghalka, Dinesh Manimel Wadu, Mithila Amarasena, and destination is a square of width 280mm and the corners\nSurangaPrasadfordevelopingtheimageprocessingsolutions. of the area are marked by cross-hair markers. Hence, four\nThisworkwassupportedbytheprojectsEU-ICTIntellIoT(grantagreement\nNo.957218)andInfotech-R2D2. destinationsaredefinedbytencross-hairmarkersasshownin\n3202\nyaM\n3\n]VC.sc[\n1v88790.5032:viXra\n=== 페이지 2 ===\nFigure 2: The key steps of the intelligent navigation. Fig.1withthefurthestmarkerattheleftdefinedastheorigin morphological operators, erosion and dilation, are applied on\nof the 2D coordinates.",
      "size": 830,
      "sentences": 7
    },
    {
      "id": 8,
      "content": "2 ===\nFigure 2: The key steps of the intelligent navigation. Fig.1withthefurthestmarkerattheleftdefinedastheorigin morphological operators, erosion and dilation, are applied on\nof the 2D coordinates. The custom drop areas are located theimagetoremoveminiatureinconsistenciesonthedetected\ninside each of the destination, which are possibly irregular path caused by to the glare of the surrounding light sources\nshapes.Theprecisedroplocationisdefinedasthecenterpoint whilepreservingthestructureandtheshapeofthepath.Then,\nof the largest circle placed inside the custom drop area. It is the image is segmented by using OTSU method [6], which\nworth highlighting that the drop point coordinates need to be results in a binary image as shown under step 3 in Fig. 2.\ncomputedwithrespecttotheactualcoordinatesusingacamera Therein,thepath(darksurface)andthehorizon(lowlighting)\nimage having measurements in pixels (px).",
      "size": 909,
      "sentences": 5
    },
    {
      "id": 9,
      "content": "a binary image as shown under step 3 in Fig. 2.\ncomputedwithrespecttotheactualcoordinatesusingacamera Therein,thepath(darksurface)andthehorizon(lowlighting)\nimage having measurements in pixels (px). The placement of of the image appear in black color while the rest appears in\ncross-hairmarkersispredefinedandthisknowledgeisutilized white. To neglect the undesired black areas corresponding to\ntotranslatethepxdistancestotheactualmeasurementsduring the horizon, a rectangular window that is likely to contain the\nimage processing at the edge server. pathhasbeendefinedastheregion-of-interest(RoI)(seestep4\nIII. SYSTEMIMPLEMENTATION of Fig. 2). By assigning weights ones and zeros to black and\nThe demo setup is composed of several hardware compo- white px, respectively, the centroid of the RoI is calculated\nnentsandtwokeysoftwaresolutionsprovidingtheintelligence and referred to as the center of the path.",
      "size": 907,
      "sentences": 8
    },
    {
      "id": 10,
      "content": "is composed of several hardware compo- white px, respectively, the centroid of the RoI is calculated\nnentsandtwokeysoftwaresolutionsprovidingtheintelligence and referred to as the center of the path. The displacement of\n(i)fornavigationand(ii)objectplacement.Thedetailsofthese the center of the path from the vertical symmetrical axis of\ncomponents and the implementation are discussed next. the image is denoted as the error. This error value is used in\na propotional-integral-derivative (PID) controller to determine\nA. Hardware\nthecontrolcommands(i.e.,angularvelocitiesofleftandright\nFig. 1 shows all the hardware components used in the wheels) ensuring a smooth navigation [7]. The coefficients\nsystem. AGV is a an off-the-shelf mobile robot known as of the proportional, derivative, and integral are 1, 1, and 0,\n\"Jetank AI kit” which is powered by Nvidia Jetson nano respectively, which are obtained during the tuning phase of\ndevelopermodulewith16GBeMMCand4GBRAM [5].The the AGV.",
      "size": 986,
      "sentences": 8
    },
    {
      "id": 11,
      "content": "and integral are 1, 1, and 0,\n\"Jetank AI kit” which is powered by Nvidia Jetson nano respectively, which are obtained during the tuning phase of\ndevelopermodulewith16GBeMMCand4GBRAM [5].The the AGV. robot is equipped with 4-DoF mechanical arm and a wide- Atjunctions,theAGVswitchestoanalternativecontrolpro-\nangle camera with 160◦ field-of-view. The camera consists cedurethatreliesonthetypeofthejunctionandtheknowledge\nof a Raspberry Pi V2 camera module and connects with a on the direction of moving and the delivery information. The\nRaspberry Pi 4 Model B computer, which hosts a web-server junctionisdetectedanditstypeisdeterminedbyanalyzingthe\nthat serves images of the storage area upon the requests from boundariesoftheregionofinterest.Next,basedonthecurrent\ntheedgeserver.Apowerfulmulti-purpose64-bitWindows10 goal, predefined sequences of control commands are issued to\ncomputer acts as the edge server and it hosts the AI service turn the AGV by 90◦ or 180◦.",
      "size": 968,
      "sentences": 4
    },
    {
      "id": 12,
      "content": "edgeserver.Apowerfulmulti-purpose64-bitWindows10 goal, predefined sequences of control commands are issued to\ncomputer acts as the edge server and it hosts the AI service turn the AGV by 90◦ or 180◦. After turning, the AGV hands\nand share the delivery information with the AGV. over the control to the PID algorithm. The AGV identifies\nthe terminal points, source and destinations, by the absence\nB. Intelligence for The Navigation\nof path segments in the RoI. As the AGV reaches to source\nAvision-basedlinefollowingsystemisimplementedonthe\nor destination, it activates pick/drop procedures accordingly. AGV to successfully navigate around the platform by using\nHere, the robot arm is programmed to move to a given 2D\nthe on-board camera. The camera and the Jetson nano board\ncoordinate along its moving plane using inverse kinematics\nsupport processing images with resolution 300×300 px up\nand grab or release the object. to 30 frames per second.",
      "size": 947,
      "sentences": 9
    },
    {
      "id": 13,
      "content": "a and the Jetson nano board\ncoordinate along its moving plane using inverse kinematics\nsupport processing images with resolution 300×300 px up\nand grab or release the object. to 30 frames per second. During the navigation, the following\nC. Intelligence for The Object Placement\nthreestepsarerepeated:i)acquiringcameraimages,ii)image\npre-processing and line detection, and iii) actuating control The role of the edge server is to provide the intelligence in\ndecisions of the AGV as illustrated in Fig. 2. terms of computing the delivery information, which includes\nTheinitializationistosetboththerobotarmandthecamera thedestinationandthedroplocation.Thedestinationisoneof\norientation to a position providing an obstruction free view the four square areas defined by cross-hair markers. A human\nof the path. During navigation, camera images are obtained operator defines a custom drop area by placing a dark and\nrepeatedly, and converted to gray scale images.",
      "size": 957,
      "sentences": 6
    },
    {
      "id": 14,
      "content": "ned by cross-hair markers. A human\nof the path. During navigation, camera images are obtained operator defines a custom drop area by placing a dark and\nrepeatedly, and converted to gray scale images. Using a 3×3 possibly irregular flat surface made out of paper. The drop\nGaussian kernel, Gaussian blurring is performed to reduce point is the center of the largest circle (not necessarily to\nthe noise in the image (see step 2 in Fig. 2). Next, two be unique) that can be placed inside the user-defined area. === 페이지 3 ===\nFigure 3: The key steps of the edge AI service. Giventhecameraimage,derivationofthedeliveryinformation named poly-label [9].",
      "size": 647,
      "sentences": 9
    },
    {
      "id": 15,
      "content": "be unique) that can be placed inside the user-defined area. === 페이지 3 ===\nFigure 3: The key steps of the edge AI service. Giventhecameraimage,derivationofthedeliveryinformation named poly-label [9]. Since the drop point coordinates are\nrequires several steps including cross-hair marker detection, returned in px with respect to the extracted image segment\nisolate the destination and camera distortion correction, iden- of the destination, they are translated to the actual physical\ntify the custom area within the destination, and compute drop coordinatesbasedonpriorknowledgeofthesetupdimensions\ncoordinates in the px domain and translate it to the actual and then shared with the AGV. coordinates.Theflowoftheedgeserveroperationisillustrated\nD. Demo, Resources, and Future Developments\nin Fig. 3. The software related to this demo is available at github\nA pretrained machine learning (ML) model is used for\nhttps://github.com/ICONgroupCWC/Demo.Percom23. With a\nthe cross-hair marker detection.",
      "size": 997,
      "sentences": 8
    },
    {
      "id": 16,
      "content": "3. The software related to this demo is available at github\nA pretrained machine learning (ML) model is used for\nhttps://github.com/ICONgroupCWC/Demo.Percom23. With a\nthe cross-hair marker detection. For the supervised training,\nsimilar platform developed as per specifications provided\n256×256 px images extracted from the camera is used as the\nin Sec. II and the use of the hardware specified under\ninputs while the labels are images with the same dimensions\nSec. III-A along the aforementioned software, this demo\nhavingwhiteareascorrespondingtothecross-hairmarkersand\ncan be reproduced. This demo in action can be seen from\nblack areas reflecting everything else. To avoid handcrafting\nhttps://youtu.be/DhCSCCZbuHo. a dataset, a data augmentation procedure is adopted. First,\nThe lighting conditions (over/under-exposure and harsh\nusing a single camera image (see step 1 in Fig. 3 as an\nshadows) directly impact the performance of the AGV.",
      "size": 943,
      "sentences": 11
    },
    {
      "id": 17,
      "content": "tation procedure is adopted. First,\nThe lighting conditions (over/under-exposure and harsh\nusing a single camera image (see step 1 in Fig. 3 as an\nshadows) directly impact the performance of the AGV. To\nexample) as the master sample, a master label is generated\nimprove the overall performance, brightness and, exposure\nby filtering out the background (anything except cross-hair\ncorrections methods and deep learning models for denoising\nmarkers). Then, different sizes of rectangles with different\nand filtering will be investigated in future. orientation are extracted randomly from the master sample\nREFERENCES\nand correspondingly from the master label. The extracted\n[1] H. Tang, X. Cheng, W. Jiang, and S. Chen, “Research on equipment\nsamples and labels are then resized to 256×256 px images\nconfigurationoptimizationofAGVunmannedwarehouse,”IEEEAccess,\nto generate the training and testing datasets of sizes 1024 vol.9,pp.47946–47959,2021. and 128, respectively.",
      "size": 968,
      "sentences": 8
    },
    {
      "id": 18,
      "content": "resized to 256×256 px images\nconfigurationoptimizationofAGVunmannedwarehouse,”IEEEAccess,\nto generate the training and testing datasets of sizes 1024 vol.9,pp.47946–47959,2021. and 128, respectively. Next, a ML model based on the U- [2] S. Li, J. Yan, and L. Li, “Automated guided vehicle: the direction of\nintelligent logistics,” in Proc. of IEEE Intl. conf. on SOLI, 2018, pp. NET architecture [8] with the input and output dimensions of\n250–255. 256×256istrainedandtestedwiththeaforementioneddataset. [3] J.-S. Shaw, C. J. Liew, S.-X. Xu, and Z.-M. Zhang, “Development of\nDuring inference, the camera image is partitioned into 10 an AI-enabled AGV with robot manipulator,” in Proc. of IEEE ECICE,\n2019,pp.284–287. segments,resized,andfedintothecross-hairmarkerdetection\n[4] M.BošnakandI.Škrjanc,“Obstacleavoidanceforline-followingAGV\nmodel.Oncethecross-hairmarkersaredetected(seestep2in withlocalmaps,”inProc.ofIEEEIntl.SACI,2021,pp.193–198.",
      "size": 944,
      "sentences": 12
    },
    {
      "id": 19,
      "content": "hecross-hairmarkerdetection\n[4] M.BošnakandI.Škrjanc,“Obstacleavoidanceforline-followingAGV\nmodel.Oncethecross-hairmarkersaredetected(seestep2in withlocalmaps,”inProc.ofIEEEIntl.SACI,2021,pp.193–198. Fig.3),thefourdestinationscanbeisolated.Forthepredefined [5] Waveshare, “Jetank ai kit,” Available at https://www.waveshare.com/\njetank-ai-kit.htm(2022/11/11). destination, using the prior knowledge of markers to define\n[6] N.Otsu,“Athresholdselectionmethodfromgray-levelhistograms,”IEEE\na square, the detected markers are used to remove camera TransactionsonSystems,Man,andCybernetics,vol.9,no.1,pp.62–66,\ndistortions.Themainbenefitofthemarkerdetectioncapability 1979. [7] S.Bennett,“DevelopmentofthePIDcontroller,”IEEEControlSystems\nisthattheserviceprovidedbytheedgeserverisrobustagainst\nMagazine,vol.13,no.6,pp.58–62,1993. camera reposition and variations in lighting conditions up to [8] O.Ronneberger,P.Fischer,andT.Brox,“U-net:Convolutionalnetworks\na certain limit.",
      "size": 971,
      "sentences": 5
    },
    {
      "id": 20,
      "content": "erisrobustagainst\nMagazine,vol.13,no.6,pp.58–62,1993. camera reposition and variations in lighting conditions up to [8] O.Ronneberger,P.Fischer,andT.Brox,“U-net:Convolutionalnetworks\na certain limit. for biomedical image segmentation,” in Proc. of Intl. Conf. on Medical\nimage computing and computer-assisted intervention. Springer, 2015,\nThe isolated destination is extracted as a separate image pp.234–241. [9] V. Agafonkin, “A new algorithm for finding a visual\nsegment and the edge detection procedure based on OTSU\ncenter of a polygon,” Available at https://blog.mapbox.com/\nmethod explained in Sec. III-B is used to identify the edges a-new-algorithm-for-finding-a-visual-center-of-a-polygon-7c77e6492fbc\nof the custom drop area as illustrated under step 3 in Fig. 3. (2022/11/11).",
      "size": 787,
      "sentences": 11
    },
    {
      "id": 21,
      "content": "xplained in Sec. III-B is used to identify the edges a-new-algorithm-for-finding-a-visual-center-of-a-polygon-7c77e6492fbc\nof the custom drop area as illustrated under step 3 in Fig. 3. (2022/11/11). To determine the drop point, which is the center of the largest\ncircle that can be placed inside the drop area (e.g., step 4 in\nFig.3),theedgesarefirstapproximatedintoapolygon.Then,\nthe desired point is the furthest point from all the edges that\nliesinsidethepolygon.Thisgeometricsolutionisprovidedas\nabuilt-infunctionnamedpolylabel()inapythonpackage",
      "size": 550,
      "sentences": 5
    }
  ]
}