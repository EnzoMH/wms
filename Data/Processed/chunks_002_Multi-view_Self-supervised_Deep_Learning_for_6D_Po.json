{
  "source": "ArXiv",
  "filename": "002_Multi-view_Self-supervised_Deep_Learning_for_6D_Po.pdf",
  "total_chars": 38448,
  "total_chunks": 56,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nMulti-view Self-supervised Deep Learning for 6D Pose Estimation\nin the Amazon Picking Challenge\nAndy Zeng1 Kuan-Ting Yu2 Shuran Song1 Daniel Suo1 Ed Walker Jr.3 Alberto Rodriguez2 Jianxiong Xiao4\n1Princeton University 2Massachusetts Institute of Technology 3Google 4AutoX\nAbstract—Robotwarehouseautomationhasattractedsignif-\nicantinterestinrecentyears,perhapsmostvisiblyintheAma-\nzon Picking Challenge (APC) [1]. A fully autonomous ware-\nhousepick-and-placesystemrequiresrobustvisionthatreliably\nrecognizes and locates objects amid cluttered environments,\nself-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-\nview RGB-D data and self-supervised, data-driven learning to\novercomethosedifficulties.TheapproachwaspartoftheMIT-\nPrinceton Team system that took 3rd- and 4th- place in the\nstowing and picking tasks, respectively at APC 2016.",
      "size": 918,
      "sentences": 3
    },
    {
      "id": 2,
      "content": "pervised, data-driven learning to\novercomethosedifficulties.TheapproachwaspartoftheMIT-\nPrinceton Team system that took 3rd- and 4th- place in the\nstowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple\nviews of a scene with a fully convolutional neural network,\nand then fit pre-scanned 3D object models to the resulting\nsegmentationtogetthe6Dobjectpose.Trainingadeepneural\nnetwork for segmentation typically requires a large amount of\ntrainingdata.Weproposeaself-supervisedmethodtogenerate\na large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D\nposeofobjectsunderavarietyofscenarios.Allcode,data,and\nbenchmarks are available at http://apc.cs.princeton.edu/\nI. INTRODUCTION\nThe last two decades have seen a rapid increase in ware-\nhouse automation technologies, satisfying the growing de-\nFig.1.",
      "size": 915,
      "sentences": 4
    },
    {
      "id": 3,
      "content": ",and\nbenchmarks are available at http://apc.cs.princeton.edu/\nI. INTRODUCTION\nThe last two decades have seen a rapid increase in ware-\nhouse automation technologies, satisfying the growing de-\nFig.1. Top:TheMIT-Princetonroboticpickingsystem.Bottom-left:The\nmand of e-commerce and providing faster, cheaper delivery. grippermountedwithanIntelRealSensecamera(outlinedinred).Bottom-\nSome tasks, especially those involving physical interaction, right:Predicted6Dobjectposesfromourvisionsystemduringthestow-task\nfinals of the APC 2016. Each prediction is highlighted with a colored 3D\narestillhardtoautomate.Amazon,incollaborationwiththe\nboundingbox. academic community, has led a recent effort to define two\nsuchtasks:1)pickinganinstanceofagivenaproductIDout\nof a populated shelf and place it into a tote; and 2) stowing ·\nSelf-occlusion: due to limited camera positions, the\na tote full of products into a populated shelf. system only sees a partial view of an object.",
      "size": 965,
      "sentences": 7
    },
    {
      "id": 4,
      "content": "ted shelf and place it into a tote; and 2) stowing ·\nSelf-occlusion: due to limited camera positions, the\na tote full of products into a populated shelf. system only sees a partial view of an object. In this paper we describe the vision system of the MIT- ·\nMissing data: commercial depth sensors are unreliable\nPrinceton Team, that took 3rd place in the stowing task\nat capturing reflective, transparent, or meshed surfaces,\nand 4th in the picking task at the 2016 Amazon Picking\nall common in product packaging. Challenge (APC), and provide experiments to validate our ·\nSmall or deformable objects: small objects provide\ndesign decisions. Our vision algorithm estimates the 6D\nfewerdatapoints,whiledeformableobjectsaredifficult\nposes of objects robustly under challenging scenarios:\nto align to prior models.",
      "size": 811,
      "sentences": 5
    },
    {
      "id": 5,
      "content": "cts provide\ndesign decisions. Our vision algorithm estimates the 6D\nfewerdatapoints,whiledeformableobjectsaredifficult\nposes of objects robustly under challenging scenarios:\nto align to prior models. ·\nCluttered environments: shelves and totes may have ·\nSpeed: the total time dedicated to capturing and pro-\nmultiple objects and could be arranged as to deceive\ncessing visual information is under 20 seconds. vision algorithms (e.g., objects on top of one another). Our approach makes careful use of known constraints\nTheauthorswouldliketothanktheMIT-PrincetonAPCteammembers in the task—the list of possible objects and the expected\nfor their contribution to this project, and ABB Inc. for hardware and background. The algorithm first segments the object from\ntechnical support.",
      "size": 779,
      "sentences": 6
    },
    {
      "id": 6,
      "content": "he task—the list of possible objects and the expected\nfor their contribution to this project, and ABB Inc. for hardware and background. The algorithm first segments the object from\ntechnical support. This project is also supported by the Google Faculty\na scene by feeding multiple-view images to a deep neural\nAwardandIntelGiftFundtoJianxiongXiao.AndyZengandDanielSuo\naresupportedbytheGordonY.S.Wufellowship.ShuranSongissupported networkandthenfitsa3Dmodeltoasegmentedpointcloud\nby the Facebook fellowship. Kuan-Ting Yu is supported by award [NSF- to recover the object’s 6D pose. The deep neural network\nIIS-1427050] through the National Robotics Initiative. Alberto Rodriguez\nprovides speed, and in combination with a multiple-view\nissupportedbytheWalterHenryGale(1929)CareerDevelopmentProfes-\nsorship. approach boosts performance in challenging scenarios. 7102\nyaM\n7\n]VC.sc[\n3v57490.9061:viXra\n=== 페이지 2 ===\nFig.2.",
      "size": 917,
      "sentences": 8
    },
    {
      "id": 7,
      "content": "a multiple-view\nissupportedbytheWalterHenryGale(1929)CareerDevelopmentProfes-\nsorship. approach boosts performance in challenging scenarios. 7102\nyaM\n7\n]VC.sc[\n3v57490.9061:viXra\n=== 페이지 2 ===\nFig.2. Overviewofthevisionalgorithm.Therobotcapturescoloranddepthimagesfrom15to18viewpointsofthescene.Eachcolorimageisfedinto\na fully convolutional network [2] for 2D object segmentation. The result is integrated in 3D. The point cloud will then go through background removal\nandthenalignedwithapre-scanned3Dmodeltoobtainits6Dpose. Training a deep neural network for segmentation requires underperform when confronted with the limited visibility,\nalargeamountoflabeledtrainingdata.Wehavedevelopeda shadows, and clutter imposed by the APC scenario [14]. self-supervised training procedure that automatically gener-\nBenchmark for 6D pose estimation. To properly evalu-\nated130,000imageswithpixel-wisecategorylabelsofthe39\nate our vision system independent from the larger robotic\nobjects in the APC.",
      "size": 990,
      "sentences": 9
    },
    {
      "id": 8,
      "content": "atically gener-\nBenchmark for 6D pose estimation. To properly evalu-\nated130,000imageswithpixel-wisecategorylabelsofthe39\nate our vision system independent from the larger robotic\nobjects in the APC. For evaluation, we constructed a testing\nsystem, we have produced a large benchmark dataset with\ndataset of over 7,000 manually-labeled images. scenarios from APC 2016 with manual labels for objects’\nIn summary, this paper contributes with:\nsegmentation and 6D poses. Previous efforts to construct\n·\nA robust multi-view vision system to estimate the 6D benchmark datasets include Berkeley’s dataset [15] with a\npose of objects; numberofobjectsfromandbeyondAPC2015andRutgers’s\n·\nA self-supervised method that trains deep networks by dataset [16] with semi-automatically labeled data. automatically labeling training data;\n· III. AMAZONPICKINGCHALLENGE2016\nA benchmark dataset for estimating object poses.",
      "size": 903,
      "sentences": 7
    },
    {
      "id": 9,
      "content": "hat trains deep networks by dataset [16] with semi-automatically labeled data. automatically labeling training data;\n· III. AMAZONPICKINGCHALLENGE2016\nA benchmark dataset for estimating object poses. The APC 2016 posed a simplified version of the general\nAll code, data, and benchmarks are publicly available [3]. pickingandstowingtasksinawarehouse.Inthepickingtask,\nrobotssitwithina2x2meterareainfrontofashelfpopulated\nII. RELATEDWORK\nwith objects, and autonomously pick 12 desired items and\nVision algorithms for robotic manipulation typically out- place them in a tote. In the stowing task, robots pick all 12\nput 2D bounding boxes, pixel-level segmentation [4, 5], or items inside a tote and place them in a pre-populated shelf. Before the competition, teams were provided with a list\n6Dposes[6,7]oftheobjects.Thechoicedependsprimarily\nof 39 possible objects along with 3D CAD models of the\non manipulation needs. For example, a suction based picker\nshelf and tote.",
      "size": 969,
      "sentences": 9
    },
    {
      "id": 10,
      "content": "ovided with a list\n6Dposes[6,7]oftheobjects.Thechoicedependsprimarily\nof 39 possible objects along with 3D CAD models of the\non manipulation needs. For example, a suction based picker\nshelf and tote. At run-time, robots were provided with the\nmighthavesufficientinformationwitha2Dboundingboxor\ninitial contents of each bin in the shelf and a work-order\nwithapixel-levelsegmentationoftheobject,whileagrasper\ncontaining which items to pick. After picking and stowing\nmight require its 6D pose. the appropriate objects, the system had to report the final\nObject segmentation. While the 2015 APC winning team contentsofbothshelfandtote.Competitiondetailsarein[1]. used a histogram backprojection method [8] with manually\nIV. SYSTEMDESCRIPTION\ndefined features [5, 4], recent work in computer vision\nOur vision system takes in RGB-D images from multiple\nhas shown that deep learning considerably improves object\nviews, and outputs 6D poses and a segmented point cloud\nsegmentation [2].",
      "size": 980,
      "sentences": 8
    },
    {
      "id": 11,
      "content": "mputer vision\nOur vision system takes in RGB-D images from multiple\nhas shown that deep learning considerably improves object\nviews, and outputs 6D poses and a segmented point cloud\nsegmentation [2]. In this work, we extend the state-of-the-\nfor the robot to complete the picking and stowing tasks. art deep learning architecture used for image segmentation\nThecameraiscompactlyintegratedintheend-effectorofa\nto incorporate depth and multi-view information. 6DOFindustrialmanipulatorABBIRB1600id,andpointsat\nPose estimation. There are two primary approaches for the tip of the fingers (Figure 1). This configuration gives the\nestimating the 6D pose of an object. The first aligns 3D robot full controllability of the camera viewpoint, and pro-\nCAD models to 3D point clouds with algorithms such as vides feedback about grasp or suction success. The camera\niterative closest point [9].",
      "size": 884,
      "sentences": 8
    },
    {
      "id": 12,
      "content": "bot full controllability of the camera viewpoint, and pro-\nCAD models to 3D point clouds with algorithms such as vides feedback about grasp or suction success. The camera\niterative closest point [9]. The second uses more elaborated of choice is the RealSense F200 RGB-D because its depth\nlocal descriptors such as SIFT keypoints [10] for color data range(0.2m–1.2m)isappropriateforclosemanipulation,and\nor3DMatch[11]for3Ddata.Theformerapproachismainly because it is a consumer-level range sensor with a decent\nused with depth-only sensors, in scenarios where lighting amount of flexibility on the data capture process. changes significantly, or on textureless objects. Highly tex- Due to the tight integration of the camera, the gripper\ntured and rigid objects, on the other hand, benefit from local fingers, even when fully open, occupy a small portion of the\ndescriptors.ExistingframeworkssuchasLINEMOD[12]or view frustum.",
      "size": 924,
      "sentences": 5
    },
    {
      "id": 13,
      "content": "the gripper\ntured and rigid objects, on the other hand, benefit from local fingers, even when fully open, occupy a small portion of the\ndescriptors.ExistingframeworkssuchasLINEMOD[12]or view frustum. We overcome this limitation by combining\nMOPED [13] work well under certain assumptions such as different viewpoints, making use of the accurate forward\nobjects sitting on a table top with good illumination, but kinematic reported by the robot controller. === 페이지 3 ===\nFig.4. Poseestimationforobjectswithnodepth.2Dobjectsegmentation\nresults from a fully convolutional network are triangulated between the\ndifferentcameraviewstocreatea3Dconvexhull(green)oftheobject.For\nsimplicity,onlytwocameraviews(yellow)areillustrated.Thecentroidand\naspectratiooftheconvexhullareusedtoestimatethegeometriccenterof\ntheobjectanditsorientation(fromapredefinedsetofexpectedorientations). distribution for each pixel in each RGB-D image. After\nfiltering by the list of expected objects in the scene, we\nFig.3.",
      "size": 991,
      "sentences": 6
    },
    {
      "id": 14,
      "content": "centerof\ntheobjectanditsorientation(fromapredefinedsetofexpectedorientations). distribution for each pixel in each RGB-D image. After\nfiltering by the list of expected objects in the scene, we\nFig.3. CameraviewpointsoftheRGB-Dframescapturedforbinsandtote, threshold the probability maps (three standard deviations\nandcapturedcolorimagesfrom6selectedviewpoints.The15viewpoints\nabove the mean probability across all views) and ignore any\nofashelfbin(upper-left)arearrangedina3x5grid.The18viewpointsof\natote(upper-right)arearrangedina3x6grid. pixels whose probabilities for all classes are below these\nthresholds. We then project the segmented masks for each\nV. 6DOBJECTPOSEESTIMATION\nobject class into 3D space and directly combine them into\nThe algorithm estimates the 6D pose of all objects in\na single segmented point cloud with the forward kinematic\na scene in two phases (Figure 2).",
      "size": 885,
      "sentences": 6
    },
    {
      "id": 15,
      "content": "ct class into 3D space and directly combine them into\nThe algorithm estimates the 6D pose of all objects in\na single segmented point cloud with the forward kinematic\na scene in two phases (Figure 2). First, it segments the\nfeedback from the robot arm (note that segmentation for\nRGB-D point clouds captured from multiple views into\ndifferent object classes can overlap each other). different objects using a deep convolutional neural network. Second, it aligns pre-scanned 3D models of the identified Reduce noise in point cloud. Fitting pre-scanned models\nobjects to the segmented point clouds to estimate the 6D to the segmented point cloud directly often gives poor\npose of each object. Our approach is based on well-known results because of noise from the sensor and noise from the\nmethods. However, our evaluations show that when used segmentation. We address this issue in three steps: First, to\nalone, they are far from sufficient.",
      "size": 938,
      "sentences": 8
    },
    {
      "id": 16,
      "content": "se of noise from the sensor and noise from the\nmethods. However, our evaluations show that when used segmentation. We address this issue in three steps: First, to\nalone, they are far from sufficient. In this section we present reduce sensor noise, we eliminate spatial outliers from the\nbrief descriptions of these methods followed by in-depth segmented point cloud, by removing all point farther than\ndiscussions of how we combine them into a robust vision a threshold from its k-nearest neighbors. Second, to reduce\nsystem. segmentation noise, especially on the object boundaries, we\nremovepointsthatlieoutsidetheshelfbinortote,andthose\nA. Object Segmentation with Fully Convolutional Networks\nthat are close to a pre-scanned background model.",
      "size": 745,
      "sentences": 7
    },
    {
      "id": 17,
      "content": "especially on the object boundaries, we\nremovepointsthatlieoutsidetheshelfbinortote,andthose\nA. Object Segmentation with Fully Convolutional Networks\nthat are close to a pre-scanned background model. Finally,\nInrecentyears,ConvNetshavemadetremendousprogress wefurtherfilteroutlierpointsfromeachsegmentedgroupof\nforcomputervisiontasks[17,2].Weleveragetheseadvance- points by finding the largest contiguous set of points along\nments to segment camera data into the different objects in each principal axis (computed via PCA) and remove any\nthe scene. More explicitly, we train a VGG architecture [18] points that are disjoint from this set. FullyConvolutionalNetwork(FCN)[2]toperform2Dobject\nsegmentation. The FCN takes an RGB image as input and Handle object duplicates. Warehouse shelves commonly\nreturns a set of 40 densely labeled pixel probability maps– contain multiple instances of the same object.",
      "size": 903,
      "sentences": 7
    },
    {
      "id": 18,
      "content": "The FCN takes an RGB image as input and Handle object duplicates. Warehouse shelves commonly\nreturns a set of 40 densely labeled pixel probability maps– contain multiple instances of the same object. Naively seg-\noneforeachofthe39objectsandoneforthebackground–of menting RGB-D data will treat two distinct object with\nthe same dimensions as the input image. the same label as the same object. Since we know the\ninventory list in the warehouse setting, we know the number\nObject segmentation using multiple views. Information of identical objects we expect in the scene. We make use of\nfrom a single camera view and from a given object, is often k-meansclusteringtoseparatethesegmentedandaggregated\nlimited due to clutter, self-occlusions, and bad reflections. point cloud into the appropriate number of objects.",
      "size": 811,
      "sentences": 8
    },
    {
      "id": 19,
      "content": "nd from a given object, is often k-meansclusteringtoseparatethesegmentedandaggregated\nlimited due to clutter, self-occlusions, and bad reflections. point cloud into the appropriate number of objects. Each\nWe address the missing information during the model-fitting clusteristhentreatedindependentlyduringthemodel-fitting\nphasebycombininginformationfrommultipleviewssothat phase of the algorithm. theobjectsurfacesaremoredistinguishable.Inparticular,we\nB. 3D Model-Fitting\nfeed the RGB images captured from each viewpoint (18 for\nstowing from the tote and 15 for picking from the shelf) We use the iterative closest point (ICP) algorithm [19] on\nto the trained FCN, which returns a 40 class probability the segmented point cloud to fit pre-scanned 3D models of\n=== 페이지 4 ===\nThe main assumption is that regions of the point cloud that\nare correctly labeled are denser than regions with incorrect\nlabel.",
      "size": 901,
      "sentences": 5
    },
    {
      "id": 20,
      "content": "the segmented point cloud to fit pre-scanned 3D models of\n=== 페이지 4 ===\nThe main assumption is that regions of the point cloud that\nare correctly labeled are denser than regions with incorrect\nlabel. A first pass with a high inlier threshold (90%) moves\nthepre-scannedcompletemodelclosertothecorrectportion\nofthepartialviewthanthenoisyportion.Startingnowfrom\nacoarsebutrobustinitialization,thesecondpassusesalower\ninlierthreshold(45%)toignorethenoisyportionofthepoint\ncloud and converge to a more accurate pose. C. Handling Objects with Missing Depth. Many objects in the APC, as it is typical in retail ware-\nhouses, have surfaces that challenge infrared-based depth\nsensors, e.g., with plastic wrapping returning noisy or mul-\ntiple reflections, or transparent or meshed materials which\nFig. 5. To automatically obtain pixel-wise object labels, we separate the\nmay not register at all. For these objects the captured point\ntarget objects from the background to create an object mask.",
      "size": 985,
      "sentences": 7
    },
    {
      "id": 21,
      "content": "ch\nFig. 5. To automatically obtain pixel-wise object labels, we separate the\nmay not register at all. For these objects the captured point\ntarget objects from the background to create an object mask. There are a\n2D and a 3D component in this data process. Both use color and depth cloudisnoisyandsparse,andourposeestimationalgorithm\ninformation.The2Dpipelineisrobusttothinobjectsandobjectswithno performs poorly. depth,whilethe3Dpipelineisrobusttoanunstablebackground. Our solution leverages the multi-view segmentation to\nobjects and estimate their poses. The vanilla ICP algorithm, estimateaconvexhulloftheobjectbycarvinga3Dgridded\nhowever, gives nonsensical results in many scenarios. We space of voxels with the segmented RGB images. This\ndescribe here several such pitfalls along with our solutions. process results in a 3D mask that encapsulates the real\nobject. We use the convex hull of that mask to estimate the\nPoint clouds with non-uniform density.",
      "size": 959,
      "sentences": 13
    },
    {
      "id": 22,
      "content": "eral such pitfalls along with our solutions. process results in a 3D mask that encapsulates the real\nobject. We use the convex hull of that mask to estimate the\nPoint clouds with non-uniform density. In a typical RGB- geometriccenteroftheobjectandapproximateitsorientation\nD point cloud, surfaces perpendicular to the sensor’s opti- (assuming that the object is axis-aligned). cal axis have often denser point clouds. The color of the\nsurface changes its reflectivity on the IR spectrum, which VI. SELF-SUPERVISEDTRAINING\nalso affects the effective point cloud density. These non- By bringing deep learning into the approach we gain\nuniformities are detrimental to the ICP algorithm because robustness.This,however,comesattheexpenseofamassing\nit biases convergence toward denser areas.",
      "size": 785,
      "sentences": 8
    },
    {
      "id": 23,
      "content": "inging deep learning into the approach we gain\nuniformities are detrimental to the ICP algorithm because robustness.This,however,comesattheexpenseofamassing\nit biases convergence toward denser areas. By applying a quality training data, which is necessary to learn high-\n3D uniform average grid filter to the point clouds, we are capacitymodelswithmanyparameters.Gatheringandmanu-\nable to give them consistent distributions in 3D space. allylabelingsuchlargeamountsoftrainingdataisexpensive. The existing large-scale datasets used by deep learning (e.g. Poseinitialization.ICPisaniterativelocaloptimizer,andas\nImageNet [20]) are mostly Internet photos, which have very\nsuch, it is sensitive to initialization. The principal directions\ndifferent object and image statistics from our warehouse\nof the segmented point cloud, as estimated by PCA, give us\nsetting.",
      "size": 859,
      "sentences": 6
    },
    {
      "id": 24,
      "content": "ave very\nsuch, it is sensitive to initialization. The principal directions\ndifferent object and image statistics from our warehouse\nof the segmented point cloud, as estimated by PCA, give us\nsetting. a reasonable first approximation to the orientation of objects\nTo automatically capture and pixel-wise label images, we\nwith uneven aspect ratios. We have observed experimentally\npropose a self-supervised method, based on three observa-\nthat the choice of initial orientation for objects with even\ntions:\naspect ratios has little effect on the final result of ICP.",
      "size": 564,
      "sentences": 4
    },
    {
      "id": 25,
      "content": "xperimentally\npropose a self-supervised method, based on three observa-\nthat the choice of initial orientation for objects with even\ntions:\naspect ratios has little effect on the final result of ICP. ·\nAnalogously, one would use the centroid of the point cloud Batch-training on scenes with a single object can yield\nas the initial guess for the geometric center of the object, deep models that perform well on scenes with multiple\nhowever we have observed that since captured point clouds objects [17] (i.e., simultaneous training on cat-only or\nare only partial, those two centers are usually biased from dog-onlyimagesenablessuccessfultestingoncat-with-\neach other.",
      "size": 668,
      "sentences": 2
    },
    {
      "id": 26,
      "content": "aptured point clouds objects [17] (i.e., simultaneous training on cat-only or\nare only partial, those two centers are usually biased from dog-onlyimagesenablessuccessfultestingoncat-with-\neach other. To address this, we push back the initial pose dog images);\n·\nof the pre-scanned object back along the optical axis of the An accurate robot arm and accurate camera calibration,\nRGB-Dcamerabyhalfthesizeoftheobject’sboundingbox, gives us at will control over camera viewpoint;\n·\nunder the naive assumption that we are only seeing “half” For single object scenes, with known background and\nthe object. This initialization has proven more successful in known camera viewpoint, we can automatically obtain\navoiding local optimums. precise segmentation labels by foreground masking. The captured training dataset contains 136,575 RGB-D im-\nCoarse to fine ICP. Even after reducing noise in the\nages of 39 objects, all automatically labeled.",
      "size": 934,
      "sentences": 6
    },
    {
      "id": 27,
      "content": "segmentation labels by foreground masking. The captured training dataset contains 136,575 RGB-D im-\nCoarse to fine ICP. Even after reducing noise in the\nages of 39 objects, all automatically labeled. segmentation step, the resulting point cloud may still have\nnoise (e.g., mislabeled points from adjacent objects). We Semi-automatic data gathering. To semi-autonomously\naddress this with two passes of ICP, acting on different gather large quantities of training data, we place single\nsubsets of the point cloud: we define the inlier threshold of knownobjectsinsidetheshelfbinsortoteinarbitraryposes,\nanICPiterationasthepercentileL2distanceabovewhichwe and configure the robot to move the camera and capture\nignore.ICPwitha90%inlierratiokeepstheclosestpairsof RGB-D images of the objects from a variety of different\npointsbetweenthetwopointcloudsuptothe90thpercentile. viewpoints.",
      "size": 880,
      "sentences": 7
    },
    {
      "id": 28,
      "content": "ove the camera and capture\nignore.ICPwitha90%inlierratiokeepstheclosestpairsof RGB-D images of the objects from a variety of different\npointsbetweenthetwopointcloudsuptothe90thpercentile. viewpoints. The position of the shelf/tote is known to the\n=== 페이지 5 ===\nrobot,asisthecameraviewpoint,whichweusetotransform\nthe collected RGB-D images in shelf/or tote frame. After\ncapturing several hundred RGB-D images, the objects are\nmanually re-arranged to different poses, and the process is\nrepeated several times. Human involvement sums up to re-\narranging the objects and labeling which objects correspond\nto which bin/tote. Selecting and changing the viewpoint,\ncapturing sensor data, and labeling each image by object\nis automated. We collect RGB-D images of the empty shelf\nandtotefromthesameexactcameraviewpointstomodelthe\nbackground, in preparation for the automatic data labeling. Automatic data labeling.",
      "size": 907,
      "sentences": 8
    },
    {
      "id": 29,
      "content": "object\nis automated. We collect RGB-D images of the empty shelf\nandtotefromthesameexactcameraviewpointstomodelthe\nbackground, in preparation for the automatic data labeling. Automatic data labeling. To obtain pixel-wise object seg-\nmentation labels, we create an object mask that separates\nforegroundfrombackground.Theprocessiscomposedof2D\nand 3D pipelines. The 2D pipeline is robust to thin objects\n(objectsnotsufficientvolumetobereliablysegmentedin3D\nwhenplacedtooclosetoawallsorground)andobjectswith\nnodepthinformation,whilethe3Dpipelineisrobusttolarge\nmiss-alignments between the pre-scanned shelf bin and tote. Results from both pipelines are combined to automatically\nlabel an object mask for each training RGB-D image.",
      "size": 725,
      "sentences": 6
    },
    {
      "id": 30,
      "content": "ethe3Dpipelineisrobusttolarge\nmiss-alignments between the pre-scanned shelf bin and tote. Results from both pipelines are combined to automatically\nlabel an object mask for each training RGB-D image. The 2D pipeline starts by fixing minor possible image\nmisalignmentsbyusingmultimodal2Dintensity-basedregis-\ntrationtoalignthetwoRGB-Dimages[21].Wethenconvert\nthe aligned color image from RGB to HSV, and do pixel-\nwisecomparisonsoftheHSVanddepthchannelstoseparate\nand label foreground from background. The3Dpipelineusesmultipleviewsofanemptyshelfbin\nand tote to create their pre-scanned 3D models. We then use Fig. 6. Examples from our benchmark dataset. The dataset contains 477\nscenes with 2,087 unique object poses seen from multiple viewpoints. In\nICP to align all training images to the background model,\ntotal, there are 7,713 images with manually-annotated ground truth 6D\nandremovepointstooclosetothebackgroundtoidentifythe objectposesandsegmentationlabels. foreground mask.",
      "size": 981,
      "sentences": 10
    },
    {
      "id": 31,
      "content": "s to the background model,\ntotal, there are 7,713 images with manually-annotated ground truth 6D\nandremovepointstooclosetothebackgroundtoidentifythe objectposesandsegmentationlabels. foreground mask. Finally, we project the foreground points\nrun-time speeds per component are as follows: 10ms for\nback to 2D to retrieve the object mask. ROS communication overhead, 400ms per forward pass of\nTraining neural network. To leverage features trained VGG-FCN, 1200ms for denoising per scene, and 800ms\nfrom a larger image domain, we use the sizable FCN-VGG on model-fitting per object. On average, pose estimation\nnetwork architecture from [18] and initialize the network time is 3-5 seconds per shelf bin and 8-15 seconds for the\nweightsusingamodelpre-trainedonImageNetfor1000-way tote. Combined with multi-view robot motions, total visual\nobject classification.",
      "size": 857,
      "sentences": 7
    },
    {
      "id": 32,
      "content": "network time is 3-5 seconds per shelf bin and 8-15 seconds for the\nweightsusingamodelpre-trainedonImageNetfor1000-way tote. Combined with multi-view robot motions, total visual\nobject classification. We fine-tune the network over the 40- perception time is 10-15 seconds per shelf bin and 15-20\nclass output classifier (39 classes for each APC object and 1 seconds for the tote. class for background) using stochastic gradient descent with\nmomentum.Duetoilluminationandobjectviewpointbiases,\nVIII. EVALUATION\nwemaximizeperformancebytrainingtwosuchsegmentation We evaluate variants of our method in different scenarios\nnetworks: one for shelf bins and one for the tote. The inthebenchmarkdatasettounderstand(1)howsegmentation\nsegmentation labels automatically generated for the training performsunderdifferentinputmodalitiesandtrainingdataset\ndata can be noisy. However, we find that the networks are sizes and (2) how the full vision system performs.",
      "size": 950,
      "sentences": 7
    },
    {
      "id": 33,
      "content": "tically generated for the training performsunderdifferentinputmodalitiesandtrainingdataset\ndata can be noisy. However, we find that the networks are sizes and (2) how the full vision system performs. stillcapableofworkingwellduringtesttimeduetothesheer\nA. Benchmark Dataset\nsize of available training data. Ourbenchmarkdataset,‘Shelf&Tote’,containsover7,000\nVII. IMPLEMENTATION RGB-D images spanning 477 (Figure 6) scenes at 640 ×\nAllcomponentsofthevisionsystemaremodularizedinto 480 resolution. We collected the data during practice runs\nreusableROSpackages,withCUDAGPUacceleration.Deep andcompetitionfinalsfortheAPCandmanuallylabeled6D\nmodels are trained and tested with Marvin [22], a ROS- object poses and segmentations using our online annotator\ncompatible and lightweight deep learning framework. Train- (Figure 7). The data reflects various challenges found in the\ning our models takes up to 16 hours prior to convergence.",
      "size": 929,
      "sentences": 9
    },
    {
      "id": 34,
      "content": "our online annotator\ncompatible and lightweight deep learning framework. Train- (Figure 7). The data reflects various challenges found in the\ning our models takes up to 16 hours prior to convergence. warehouse setting: reflective materials, variation in lighting\nOur robot is controlled by a computer with an Intel conditions, partial views, and sensor limitations (noisy and\nE3-1241 CPU 3.5 GHz and an NVIDIA GTX 1080. The missing depth) over cluttered environments. === 페이지 6 ===\ntraining data. However in our scenario—instance-level ob-\nject segmentation on few object categories—it is not clear\nwhether such a large dataset is necessary. We create two\nnew datasets by randomly sampling 1% and 10% of the\noriginal and use them to train two VGG FCNs (Table I). We confirm marked improvements in F-score across all\nbenchmark categories going from 1% to 10% to 100% of\ntraining data.",
      "size": 883,
      "sentences": 9
    },
    {
      "id": 35,
      "content": "pling 1% and 10% of the\noriginal and use them to train two VGG FCNs (Table I). We confirm marked improvements in F-score across all\nbenchmark categories going from 1% to 10% to 100% of\ntraining data. C. Evaluating Pose Estimation\nWe evaluate several key components of our vision system\ntodeterminewhethertheyincreaseperformanceinisolation. Fig. 7. The 3D online annotation tool used to label the benchmark. The\ndrag-and-dropUIallowsannotatorstonavigatein3Dspaceandmanipulate Metrics.Wereportthepercentageofobjectposepredictions\npoint clouds with ease. Annotators are instructed to move and rotate a witherrorinorientationsmallerthan15◦,andthepercentage\npre-scanned object model to its ground truth location in a 3D point cloud\nwith error in translation smaller than 5cm. The metric also\ngeneratedfromRGB-Ddata.Labelingoneobjecttakesabout1minute.",
      "size": 845,
      "sentences": 9
    },
    {
      "id": 36,
      "content": "entage\npre-scanned object model to its ground truth location in a 3D point cloud\nwith error in translation smaller than 5cm. The metric also\ngeneratedfromRGB-Ddata.Labelingoneobjecttakesabout1minute. recognizes the structural invariance of several objects, some\nTables I and II summarize our experimental results and ofwhichareaxially-symmetric(cuboids),radially-symmetric\nhighlight the differences in performance over different over- (bottles, cylinders), or deformable (see web page [3] for\nlapping scene categories: further details). We have observed experimentally that these\n· cptn: during competition at the APC finals. bondsof15◦ and5cmaresufficientforpickingwithsensor-\n· environment:inanoffice(off);intheAPCcompetition guarded motions. warehouse (whs). · Multi-view information. With multiple views the system\ntask: picking from a shelf bin or stowing from a tote. · overcomesmissinginformationduetoself-occlusions,other-\nclutter: with multiple objects. · object occlusions, or clutter.",
      "size": 995,
      "sentences": 10
    },
    {
      "id": 37,
      "content": "tiple views the system\ntask: picking from a shelf bin or stowing from a tote. · overcomesmissinginformationduetoself-occlusions,other-\nclutter: with multiple objects. · object occlusions, or clutter. Multi-view information also\nocclusion:with%ofobjectoccludedbyanotherobject,\nalleviates problems with illumination on reflective surfaces. computed from ground truth. · Toquantifytheeffectofthemultiple-viewsystem,wetest\nobject properties: with objects that are deformable,\nthe full vision system on the benchmark with three different\nthin,orhavenodepthfromtheRealSenseF200camera. subsets of camera views:\n·\n[Full]All15viewsforshelfbinsa ={0...14}and\nB. Evaluating Object Segmentation 1shelf\nall 18 views for the tote a = {0 ...17}. ·\n1tote\nWe test several variants of our FCN on object segmenta- [5v-10v]5viewsforshelfa ={0,4,7,10,14}and10\n2shelf\ntiontoanswertwoquestions:(1)canweleveragebothcolor views for tote a ={0,2,4,6,8,9,11,13,15,17}, with a\n2tote\nand depth segmentation?",
      "size": 978,
      "sentences": 9
    },
    {
      "id": 38,
      "content": "bject segmenta- [5v-10v]5viewsforshelfa ={0,4,7,10,14}and10\n2shelf\ntiontoanswertwoquestions:(1)canweleveragebothcolor views for tote a ={0,2,4,6,8,9,11,13,15,17}, with a\n2tote\nand depth segmentation? (2) is more training data useful? sparse arrangement and a preference for wide-baseline\nview angles. Metrics.Wecomparethepredictedobjectsegmentationfrom ·\n[1v-2v] 1 view for shelf bins a = {7} and 2 views\nour trained FCNs against the ground truth segmentation\n3shelf\nfor the tote a ={7,13}. labels of the benchmark dataset using pixel-wise precision\n3tote\nThe viewpoint ids are zero-indexed in row-major order as\nand recall. Table I displays the mean average F-scores (F =\ndepicted in Figure 3. Our results show that multiple views\n2· precision·recall ). precision+recall robustlyaddressocclusionandheavyclutterinthewarehouse\nDepth for segmentation. We use HHA features [23] to setting (Table II [clutter] and [occlusion]).",
      "size": 923,
      "sentences": 9
    },
    {
      "id": 39,
      "content": "e views\n2· precision·recall ). precision+recall robustlyaddressocclusionandheavyclutterinthewarehouse\nDepth for segmentation. We use HHA features [23] to setting (Table II [clutter] and [occlusion]). They also present\nencode depth information into three channels: horizontal a clear contrast between the performance of our algorithm\ndisparity, height above ground, and angle of local surface using a single view of the scene, versus multiple views of\nnormal with the inferred direction of gravity. We compare the scene (Table II [Full] v.s [1v-2v]). AlexNet trained on this encoding, VGG on RGB data, and\nDenoising.ThedenoisingstepdescribedinSectionVproves\nboth networks concatenated in Table I.\nimportantforachievinggoodresults.Withthisturnedoff,the\nWe find that adding depth does not yield any notable\naccuracy in estimating the translation and rotation decreases\nimprovements in segmentation performance, which could be\nby 6.0% and 4.4% respectively (Table II).",
      "size": 964,
      "sentences": 6
    },
    {
      "id": 40,
      "content": "dding depth does not yield any notable\naccuracy in estimating the translation and rotation decreases\nimprovements in segmentation performance, which could be\nby 6.0% and 4.4% respectively (Table II). in part due to the noisiness of the depth information from\nour sensor. On the other hand, we observe that the FCN ICP improvements. Without the pre-processing steps to\nperformssignificantlybetterwhentrainedoncolordata,with ICP, we observe a drop in prediction accuracy of 0.9% in\nthe largest disparity for deformable objects and thin objects, translation and 3.1% in rotation (Table II). whosetexturesprovidemorediscriminativepowerthantheir\nPerformance upper bound. We also evaluated how well\ngeometric structure. the model-fitting part of our algorithm alone performs on\nSize of training data. Deep learning models have seen the benchmark by using ground truth segmentation labels\nsignificant success, especially if given large amounts of from the benchmark as the performance upper bound.",
      "size": 990,
      "sentences": 8
    },
    {
      "id": 41,
      "content": "ta. Deep learning models have seen the benchmark by using ground truth segmentation labels\nsignificant success, especially if given large amounts of from the benchmark as the performance upper bound. === 페이지 7 ===\nFig.8. Exampleresultsfromourvisionsystem.6Dposepredictionsarehighlightedwitha3Dboundingbox.Fordeformableobjects(clothina,c,i)we\noutputthecenterofmass.Weadditionallyillustratesuccessfulposepredictionsforobjectswithmissingdepth(waterbottle,blackbin,greensippycup,\ngreenbowl)\nFig. 9. Several common failure cases. These include low-confidence predictions due to severe occlusion (missing object labels in m,o,p), completely\nincorrectposepredictionsduetoconfusionintexture(m,p,r)orbadinitialization(n,q),andmodel-fittingerrors(o). D. Common Failure Modes IX.",
      "size": 768,
      "sentences": 8
    },
    {
      "id": 42,
      "content": "to severe occlusion (missing object labels in m,o,p), completely\nincorrectposepredictionsduetoconfusionintexture(m,p,r)orbadinitialization(n,q),andmodel-fittingerrors(o). D. Common Failure Modes IX. DISCUSSION\nHere we summarize the most common failure modes of Despite tremendous advances in computer vision, many\nour vision system, which are illustrated in Figure 9: state-of-the-art well-known approaches are often insufficient\n· The FCN segmentation for objects under heavy occlu- for relatively common scenarios. We describe here two\nsion or clutter are likely to be incomplete resulting in observations that can lead to improvements in real systems:\npoor pose estimation (Fig. 8.e), or undetected (Fig. 9.m\nMakethemostoutofeveryconstraint.Externalconstraints\nand p). This happens with more frequency at back of\nlimit what systems can do. Indirectly they also limit the\nthe bin with poor illumination.",
      "size": 905,
      "sentences": 8
    },
    {
      "id": 43,
      "content": ". 9.m\nMakethemostoutofeveryconstraint.Externalconstraints\nand p). This happens with more frequency at back of\nlimit what systems can do. Indirectly they also limit the\nthe bin with poor illumination. · set of states in which the system can be, which can lead\nObjects color textures are confused with each other. to opportunities for simplifications and robustness in the\nFigure 9.r shows a Dove bar (white box) on top of a\nperception system. In the picking task, each team received\nyellow Scotch mail envelope, which combined have a\na list of items, their bin assignments, and a model of the\nsimilar appearance to the outlet plugs. · shelf. All teams used the bin assignments to rule out objects\nModel fitting for cuboid objects often confuses corner\nfrom consideration and the model of the shelf to calibrate\nalignments (marker boxes in Fig. 9.o). This inaccuracy,\ntheir robots. These optimizations are straightforward and\nhowever, is still within the range of tolerance that the\nuseful.",
      "size": 988,
      "sentences": 12
    },
    {
      "id": 44,
      "content": "e shelf to calibrate\nalignments (marker boxes in Fig. 9.o). This inaccuracy,\ntheir robots. These optimizations are straightforward and\nhowever, is still within the range of tolerance that the\nuseful. However, further investigation yields more opportu-\nrobot can tolerate thanks to sensor-guarded motions. nity. By using these same constraints, we constructed a self-\nsupervising mechanism to train a deep neural network with\nFiltering failure modes by confidence score. We compute\nsignificantlymoredata.Asourevaluationsshow,thevolume\naconfidencescoreperobjectposepredictionthatfavorshigh\nof training data is strongly correlated with performance. precisionforlowrecall.Specifically,theconfidencescoreof\naposepredictionequalsthemeanvalueofconfidencescores Designing robotic and vision systems hand-in-hand. Vi-\nover all points belonging to the segmentation of the object.",
      "size": 869,
      "sentences": 10
    },
    {
      "id": 45,
      "content": "ecifically,theconfidencescoreof\naposepredictionequalsthemeanvalueofconfidencescores Designing robotic and vision systems hand-in-hand. Vi-\nover all points belonging to the segmentation of the object. sionalgorithmsaretoooftendesignedinisolation.However,\nWe observe that erroneous poses (especially those due to vision is one component of a larger robotic system with\npartial occlusions) more often have low confidence scores. needs and opportunities. Typical computer vision algorithms\nThe robot system uses this value to target only predictions operate on single images for segmentation and recognition. with high scores. Robotic arms free us from that constraint, allowing us to\nWe evaluate the usefulness of the confidence scores by precisely fuse multiple views and improve performance in\nrecallingtheoutputoftheperceptionsystemtoonlyconsider cluttered environments.",
      "size": 870,
      "sentences": 7
    },
    {
      "id": 46,
      "content": "ing us to\nWe evaluate the usefulness of the confidence scores by precisely fuse multiple views and improve performance in\nrecallingtheoutputoftheperceptionsystemtoonlyconsider cluttered environments. Computer vision systems also tend\npredictionswithconfidencescoreslargerthan10%and70% to have fixed outputs (e.g., bounding boxes or 2D segmenta-\nrespectively (see Table II). These confidence percentages tion maps), but robotic systems with multiple manipulation\nare important thresholds, because the full robot system, strategies can benefit from variety in output. For example,\npredictions with <10% confidence (conf-10, at 78% recall) suction cups and grippers might have different perceptual\nare ignored during planning, and prediction with > 70% requirements. While the former might work more robustly\nconfidence (conf-70, at 23% recall) trigger a pick attempt.",
      "size": 865,
      "sentences": 5
    },
    {
      "id": 47,
      "content": "ht have different perceptual\nare ignored during planning, and prediction with > 70% requirements. While the former might work more robustly\nconfidence (conf-70, at 23% recall) trigger a pick attempt. with a segmented point cloud, the latter often requires\n=== 페이지 8 ===\nTABLEI\n2DOBJECTSEGMENTATIONEVALUATION(PIXEL-LEVELOBJECTCLASSIFICATIONAVERAGE%F-SCORES). environment task clutter(#ofobjects) occlusion(%) object-specificproperties\nnetwork all cptn off whs shelf tote 1-3 4-5 6+ <5 5-30 30+ dfrm.",
      "size": 498,
      "sentences": 4
    },
    {
      "id": 48,
      "content": "ALUATION(PIXEL-LEVELOBJECTCLASSIFICATIONAVERAGE%F-SCORES). environment task clutter(#ofobjects) occlusion(%) object-specificproperties\nnetwork all cptn off whs shelf tote 1-3 4-5 6+ <5 5-30 30+ dfrm. nodepth thin\ncolor 45.5 42.7 46.8 44.2 47.7 43.7 53.0 46.0 42.2 49.9 41.4 33.3 54.0 47.9 41.7\ncolor+depth 43.8 41.5 44.8 42.6 45.8 41.9 52.2 43.5 40.0 47.5 39.1 32.6 51.1 47.7 37.2\ndepth 37.1 35.0 38.6 35.5 39.8 34.9 45.5 37.0 33.5 40.8 33.2 26.3 44.1 42.3 29.1\n10%data 20.4 18.8 19.5 21.3 21.7 20.3 36.0 21.6 18.0 21.2 25.5 0.0 41.9 17.2 33.3\n1%data 8.0 9.2 7.2 8.8 15.8 6.5 17.3 7.5 6.0 7.7 8.3 7.8 10.1 5.7 3.5\nTABLEII\nFULLVISIONSYSTEMEVALUATION(AVERAGE%CORRECTROTATIONANDTRANSLATIONPREDICTIONSFOROBJECTPOSE)\nenvironment task clutter(#ofobjects) occlusion(%) object-specificproperties\nalgorithm all cptn off whs shelf tote 1-3 4-5 6+ <5 5-30 30+ dfrm. nodepth thin\nFull(rot.) 49.8 62.9 52.5 47.1 50.4 49.3 56.1 54.6 45.4 56.9 43.2 33.9 - 55.6 54.7\nFull(trans.)",
      "size": 963,
      "sentences": 5
    },
    {
      "id": 49,
      "content": "ject-specificproperties\nalgorithm all cptn off whs shelf tote 1-3 4-5 6+ <5 5-30 30+ dfrm. nodepth thin\nFull(rot.) 49.8 62.9 52.5 47.1 50.4 49.3 56.1 54.6 45.4 56.9 43.2 33.9 - 55.6 54.7\nFull(trans.) 66.1 71.0 66.3 65.9 63.4 68.1 76.7 66.7 61.9 79.4 57.4 27.3 75.4 63.3 58.1\n5v-10v(rot.) 44.0 48.6 50.9 35.9 50.9 38.9 53.9 53.1 34.4 47.6 40.0 26.7 - 47.4 42.4\n5v-10v(trans.) 58.4 50.0 63.7 52.1 61.0 56.5 69.4 63.0 50.3 66.2 49.8 21.3 54.7 67.3 35.4\n1v-2v(rot.) 38.9 60.0 41.1 36.5 45.0 35.3 45.7 45.2 32.7 43.6 33.9 14.8 - 40.9 35.4\n1v-2v(trans.) 52.5 50.0 56.3 48.2 53.8 51.8 60.4 56.5 46.7 58.2 47.8 16.7 52.9 55.9 33.3\nconf-70(rot.) 58.3 77.3 65.0 49.0 64.2 53.2 63.8 69.3 49.0 63.7 43.1 36.4 - 64.5 81.6\nconf-70(trans.) 84.5 95.5 84.7 84.2 82.6 86.1 86.2 84.1 83.2 87.1 77.1 72.7 83.1 77.4 85.7\nconf-10(rot.) 55.0 70.8 57.0 52.7 54.9 55.0 58.6 59.3 51.0 59.8 50.0 34.2 - 53.1 60.2\nconf-10(trans.) 76.5 81.2 76.7 76.3 73.4 79.1 80.8 74.4 75.4 84.0 70.0 40.0 78.1 72.0 70.1\nnodenoise(rot.)",
      "size": 992,
      "sentences": 12
    },
    {
      "id": 50,
      "content": "4 85.7\nconf-10(rot.) 55.0 70.8 57.0 52.7 54.9 55.0 58.6 59.3 51.0 59.8 50.0 34.2 - 53.1 60.2\nconf-10(trans.) 76.5 81.2 76.7 76.3 73.4 79.1 80.8 74.4 75.4 84.0 70.0 40.0 78.1 72.0 70.1\nnodenoise(rot.) 43.8 45.6 46.9 40.6 45.3 42.7 52.0 46.7 39.5 51.1 37.3 28.1 - 48.8 54.1\nnodenoise(trans.) 61.7 66.4 61.9 61.5 60.4 62.6 74.8 62.7 56.4 76.5 52.9 19.9 75.0 62.3 53.8\nnoICP+(rot.) 48.9 60.8 51.2 46.7 49.1 48.8 55.4 54.1 44.4 55.8 41.9 36.2 - 53.6 52.5\nnoICP+(trans.) 63.0 67.2 63.2 62.9 59.7 65.4 72.1 64.4 59.1 75.2 57.0 24.6 67.3 62.8 53.2\ngtsegrot. 63.4 74.4 65.8 60.9 68.1 60.1 69.1 68.8 59.1 67.6 60.0 53.5 - 58.0 74.1\ngtsegtrans. 88.1 90.4 85.7 90.4 86.9 88.9 88.3 88.0 88.0 90.7 90.3 71.4 90.5 71.5 79.8\nknowledge of the object pose and geometry. [10] A. Dias, C. Brites, J. Ascenso, and F. Pereira, “Sift-based homogra-\nphiesforefficientmultiviewdistributedvisualsensing,”IEEESensors\nX. CONCLUSION Journal,vol.15,no.5,pp.2643–2656,May2015.",
      "size": 945,
      "sentences": 11
    },
    {
      "id": 51,
      "content": "etry. [10] A. Dias, C. Brites, J. Ascenso, and F. Pereira, “Sift-based homogra-\nphiesforefficientmultiviewdistributedvisualsensing,”IEEESensors\nX. CONCLUSION Journal,vol.15,no.5,pp.2643–2656,May2015. [11] A. Zeng, S. Song, M. Nießner, M. Fisher, and J. Xiao, “3dmatch:\nIn this paper, we present the vision system of Team MIT-\nLearning the matching of local 3d geometry in range scans,” arXiv:\nPrinceton’s 3rd- and 4th-place entry in the 2016 Amazon 1603.08182,2016. Picking Challenge.",
      "size": 484,
      "sentences": 5
    },
    {
      "id": 52,
      "content": "e present the vision system of Team MIT-\nLearning the matching of local 3d geometry in range scans,” arXiv:\nPrinceton’s 3rd- and 4th-place entry in the 2016 Amazon 1603.08182,2016. Picking Challenge. To address the challenges posed by [12] S. Hinterstoisser, S. Holzer, C. Cagniart, S. Ilic, K. Konolige,\nN. Navab, and V. Lepetit, “Multimodal templates for real-time de-\nthe warehouse setting, our framework leverages multi-view\ntection of texture-less objects in heavily cluttered scenes,” in ICCV,\nRGB-D data and data-driven, self-supervised deep learning 2011.\ntoreliablyestimatethe6Dposesofobjectsunderavarietyof [13] A.Collet,M.Martinez,andS.S.Srinivasa,“Themopedframework:\nObject recognition and pose estimation for manipulation,” IJRR,\nscenarios.Wealsoprovideawell-labeledbenchmarkdataset\nvol.30,no.10,pp.1284–1306,2011. ofAPC2016containingover7,000imagesfrom477scenes.",
      "size": 876,
      "sentences": 4
    },
    {
      "id": 53,
      "content": ":\nObject recognition and pose estimation for manipulation,” IJRR,\nscenarios.Wealsoprovideawell-labeledbenchmarkdataset\nvol.30,no.10,pp.1284–1306,2011. ofAPC2016containingover7,000imagesfrom477scenes. [14] N.Correll,K.Bekris,D.Berenson,O.Brock,A.Causo,K.Hauser,\nK.Okada,A.Rodriguez,J.Romano,andP.Wurman,“Analysisand\nREFERENCES ObservationsfromtheFirstAmazonPickingChallenge,”T-ASE,2016. [1] Official website of amazon picking challenge. [Online]. Available: [15] A.Singh,J.Sha,K.S.Narayan,T.Achim,andP.Abbeel,“Bigbird:\nhttp://amazonpickingchallenge.org Alarge-scale3ddatabaseofobjectinstances,”inICRA,2014. [2] J.Long,E.Shelhamer,andT.Darrell,“Fullyconvolutionalnetworks [16] C. Rennie, R. Shome, K. E. Bekris, and A. F. De Souza, “A dataset\nforsemanticsegmentation,”inCVPR,2015,pp.3431–3440. for improved rgbd-based object detection and pose estimation for\n[3] Websiteforcodeanddata.[Online].Available:http://apc.cs.princeton. warehousepick-and-place,”RoboticsandAutomationLetters,2016.",
      "size": 986,
      "sentences": 9
    },
    {
      "id": 54,
      "content": "440. for improved rgbd-based object detection and pose estimation for\n[3] Websiteforcodeanddata.[Online].Available:http://apc.cs.princeton. warehousepick-and-place,”RoboticsandAutomationLetters,2016. edu/ [17] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\n[4] R. Jonschkowski, C. Eppner, S. Ho¨fer, R. Mart´ın-Mart´ın, and hierarchiesforaccurateobjectdetectionandsemanticsegmentation,”\nO.Brock,“Probabilisticmulti-classsegmentationfortheamazonpick- inCVPR,2014. ingchallenge,”http://dx.doi.org/10.14279/depositonce-5051,2016. [18] K. Simonyan and A. Zisserman, “Very deep convolutional networks\n[5] C.Eppner,S.Ho¨fer,R.Jonschkowski,R.Martın-Martın,A.Sieverling, forlarge-scaleimagerecognition,”arXiv:1409.1556,2014. V.Wall,andO.Brock,“Lessonsfromtheamazonpickingchallenge: [19] N.Gelfand,L.Ikemoto,S.Rusinkiewicz,andM.Levoy,“Geometri-\nFouraspectsofbuildingroboticsystems,”inRSS,2016. callystablesamplingfortheicpalgorithm,”in3DIM,2003.",
      "size": 954,
      "sentences": 7
    },
    {
      "id": 55,
      "content": "essonsfromtheamazonpickingchallenge: [19] N.Gelfand,L.Ikemoto,S.Rusinkiewicz,andM.Levoy,“Geometri-\nFouraspectsofbuildingroboticsystems,”inRSS,2016. callystablesamplingfortheicpalgorithm,”in3DIM,2003. [6] H. Zhang, P. Long, D. Zhou, Z. Qian, Z. Wang, W. Wan, [20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nD. Manocha, C. Park, T. Hu, C. Cao, Y. Chen, M. Chow, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and\nand J. Pan, “Dorapicker: An autonomous picking system for L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”\ngeneral objects,” arXiv: 1603.06317, 2016. [Online]. Available: IJCV,2015. http://arxiv.org/abs/1603.06317 [21] M.Styner,C.Brechbuhler,G.Szckely,andG.Gerig,“Parametricesti-\n[7] K.-T.Yu,N.Fazeli,N.C.Dafle,O.Taylor,E.Donlon,G.D.Lankenau, mateofintensityinhomogeneitiesappliedtomri,”IEEETransactions\nandA.Rodriguez,“Asummaryofteammit’sapproachtotheamazon onMedicalImaging,2000. pickingchallenge2015,”arXiv:1604.03639,2016.",
      "size": 986,
      "sentences": 7
    },
    {
      "id": 56,
      "content": "n,G.D.Lankenau, mateofintensityinhomogeneitiesappliedtomri,”IEEETransactions\nandA.Rodriguez,“Asummaryofteammit’sapproachtotheamazon onMedicalImaging,2000. pickingchallenge2015,”arXiv:1604.03639,2016. [22] J. Xiao, S. Song, D. Suo, and F. Yu. Marvin: A minimalist\n[8] M.-Y.Liu,O.Tuzel,A.Veeraraghavan,Y.Taguchi,T.K.Marks,and GPU-only N-dimensional ConvNet framework. [Online]. Available:\nR. Chellappa, “Fast object localization and pose estimation in heavy http://marvin.is\nclutterforroboticbinpicking,”IJRR,2016. [23] S. Gupta, R. Girshick, P. Arbela´ez, and J. Malik, “Learning rich\n[9] P.J.BeslandN.D.McKay,“Amethodforregistrationof3-dshapes,” features from rgb-d images for object detection and segmentation,”\ninPAMI,1992. inECCV,2014. [표 데이터 감지됨]",
      "size": 750,
      "sentences": 9
    }
  ]
}