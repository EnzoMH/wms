{
  "source": "ArXiv",
  "filename": "047_Decentralized_Multi-AGV_Task_Allocation_based_on_M.pdf",
  "total_chars": 36733,
  "total_chunks": 51,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\n1202\nguA\n61\n]OR.sc[\n1v68860.8012:viXra\nDecentralized Multi-AGV Task Allocation based on\nMulti-Agent Reinforcement Learning with\nInformation Potential Field Rewards\nMengyuan Li, Bin Guo*, Jiangshan Zhang, Zhetao Li Liyao Xiang\nJiaqi Liu, Sicong Liu, Zhiwen Yu College of Computer Science John Hopcroft Center for Computer Science\nSchool of Computer Science Xiangtan University ShanghaiJiao Tong University\nNorthwestern Polytechnical University Xiangtan 411105, China Shanghai 200240, China\nXi’an 710072, China liztchina@hotmail.com xiangliyao08@sjtu.edu.cn\nguob@nwpu.edu.cn\nAbstract—Automated Guided Vehicles (AGVs) have been multiple AGVs collaborate to perform material transportation\nwidely used for material handling in flexible shop floors. Each tasks remainsa significant topic in intelligentstorage systems\nproductrequiresvariousrawmaterialstocompletetheassembly\n[5], [6]. The traditional approaches are mostly centralized\nin production process.",
      "size": 965,
      "sentences": 3
    },
    {
      "id": 2,
      "content": "sks remainsa significant topic in intelligentstorage systems\nproductrequiresvariousrawmaterialstocompletetheassembly\n[5], [6]. The traditional approaches are mostly centralized\nin production process. AGVs are used to realize the automatic\ncontrol methods (Fig.1 (a)) and consider task assignment as\nhandling of raw materials in different locations. Efficient AGVs\ntask allocation strategy can reduce transportation costs and im- a path planning problem for single or multiple robots [7],\nprovedistributionefficiency.However,thetraditionalcentralized [8]. On one hand, it places extremely high demands on the\napproaches make high demandson thecontrol center’s comput- controlcenter’scomputingpowerandreal-timecapability.On\ning power and real-time capability. In this paper, we present\nthe other hand, the complexity and dynamic obstacles of the\ndecentralized solutions to achieve flexible and self-organized\nenvironment can impair the system’s stability and scalability. AGVs task allocation.",
      "size": 991,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "r hand, the complexity and dynamic obstacles of the\ndecentralized solutions to achieve flexible and self-organized\nenvironment can impair the system’s stability and scalability. AGVs task allocation. In particular, we propose two improved\nmulti-agent reinforcement learning algorithms, MADDPG-IPF In comparison to centralized solutions, agent-level decentral-\n(Information Potential Field) and BiCNet-IPF, to realize the ized task allocation strategies (Fig.1 (b)) evenly distribute\ncoordination among AGVs adapting to different scenarios. To computing load and make advantage of agents’ autonomous\naddress the reward-sparsity issue, we propose a reward shaping\ndecision-making ability.",
      "size": 686,
      "sentences": 4
    },
    {
      "id": 4,
      "content": "dination among AGVs adapting to different scenarios. To computing load and make advantage of agents’ autonomous\naddress the reward-sparsity issue, we propose a reward shaping\ndecision-making ability. strategy based on information potential field, which provides\nstepwise rewards and implicitly guides the AGVs to different\nmaterialtargets.Weconductexperimentsunderdifferentsettings\n(3 AGVs and 6 AGVs), and the experiment results indicate\nthat, compared with baseline methods, our work obtains up to\n47% task response improvement and 22% training iterations\nreduction. Index Terms—Multi-agent reinforcement learning, AGVs, de-\ncentralized task allocation, information potential field\nI. INTRODUCTION\nDriven by the recent advancements in industry 4.0 and\nindustrialartificialintelligence,theuseofautonomoussystems\nin manufacturing enterprises has become inevitable [1], [2].",
      "size": 873,
      "sentences": 5
    },
    {
      "id": 5,
      "content": "ial field\nI. INTRODUCTION\nDriven by the recent advancements in industry 4.0 and\nindustrialartificialintelligence,theuseofautonomoussystems\nin manufacturing enterprises has become inevitable [1], [2]. Automated Guided Vehicles (AGVs), as a type of flexible\nintelligentlogisticsequipment,haveagreatdegreeoffreedom\nandplayanessentialroleinflexiblytransportingmaterialsand\nproducts.AGVshavebeenhailedasoneofthemostpromising\ntechnologies and have been implemented in a variety of shop\nFig. 1. Centralized control methods and decentralized control methods of\nfloors and warehouse logistics operations for material supply\nAGVs. [3], [4]. WiththecontinuousdevelopmentofMulti-AgentReinforce-\nThe multi-variety, small-batch, and customized production\nment Learning (MARL) [9], Reinforcement Learning (RL)\nmode results in more logistics tasks and higher real-time\nhas developed the capabilities of autonomous learning and\ndemands. Using AGVs for cooperativetransportationcan sig-\ndistributed computing.",
      "size": 991,
      "sentences": 8
    },
    {
      "id": 6,
      "content": "L)\nmode results in more logistics tasks and higher real-time\nhas developed the capabilities of autonomous learning and\ndemands. Using AGVs for cooperativetransportationcan sig-\ndistributed computing. Agents generate their own behaviors,\nnificantly improve efficiency and cut expenses. How to make\nmodify their own state information, and accomplish the goal\n*Corresponding author. efficiently throughcooperationwith others[10]. For example,\n=== 페이지 2 ===\nLowe et al. [11] propose the Multi-Agent Deep Determinis- ferential preference for each target in MADDPG, while the\ntic Policy Gradient (MADDPG), which extends the DDPG agents prefer the closest target in BiCNet. method to MARL by observing the opponent’s behavior. Thepaperisorganizedasfollows:InSectionII,wediscuss\nMeanwhile, a global critic function is constructed to evaluate related literature on collaborative task allocation and multi-\nglobal state action. The Alibaba team proposes the Bidirec- agent reinforcement learning.",
      "size": 986,
      "sentences": 10
    },
    {
      "id": 7,
      "content": "bal critic function is constructed to evaluate related literature on collaborative task allocation and multi-\nglobal state action. The Alibaba team proposes the Bidirec- agent reinforcement learning. Section III formulates the task\ntionally Coordinated Network (BiCNet) algorithm [12] in the allocationproblem.InSectionIV,wemodelthetaskallocation\npysc2multi-agentscenario[13].UsingBidirectionalRecurrent problem as a partially observable Markov decision process,\nNeural Networks (BRNN) [14] for implicit communication, andtheproposedalgorithmisdemonstrated.Theeffectiveness\nBiCNethasdemonstratedsuperiorperformanceincomplicated of the methodis verifiedby experimentsin section V. Section\nenvironments. VI gives the conclusions of this study and envisages some\nHowever, existing MARL approaches have a number of future work. drawbacksthat make them unsuitable for decentralizedmulti-\nII. RELATEDWORKS\nAGV task allocation directly, such as environmental non-\nstationarity and partial observability.",
      "size": 996,
      "sentences": 6
    },
    {
      "id": 8,
      "content": "e a number of future work. drawbacksthat make them unsuitable for decentralizedmulti-\nII. RELATEDWORKS\nAGV task allocation directly, such as environmental non-\nstationarity and partial observability. Additionally, the reward A. Multi-AGV Task Allocation\nmechanism in multi-agent system is more sophisticated than Multi-AGVtaskallocationisacriticalpartofAGVcontrol,\nit is in single-agentsystem, and the reward-sparsity issue fre- as it seeks to determine the appropriate transit time and\nquentlymakestrainingprogressdifficulttoconverge.Acritical equipmentforeachtask. ThetraditionalAGVstask allocation\nquestion is how to design an effective reward mechanism that approach is to apply classical optimization algorithms to\nwillboostperformanceandexpediteconvergence.Information the production scheduling field, such as genetic algorithm,\nPotential Field (IPF) [15] is often utilized to tackle the particle swarm algorithm, ant colony algorithm. Wang et al.",
      "size": 953,
      "sentences": 6
    },
    {
      "id": 9,
      "content": "vergence.Information the production scheduling field, such as genetic algorithm,\nPotential Field (IPF) [15] is often utilized to tackle the particle swarm algorithm, ant colony algorithm. Wang et al. path planningproblem.Using the virtualinformationgradient [16] optimize the path selection problem using an improved\ndiffusion of the target position data, the robot can advance micro-genetic algorithm that takes into account running time,\nto the target position along a specific gradient direction. By stopping time, and turning time. Zhang et al. [17] employ\nincluding IPF into reward function, the agents’ status can be the makespan of jobs as the goal function and the machine\nassessedmorecomprehensively,guidingtheagentstowardthe and AGV utilization ratios as the comprehensive evaluation\ntarget positions. function. An improvedparticle swarm optimization algorithm\nTo solve the above challenges, this paper proposes a novel is developed to solve a reasonable scheduling scheme.",
      "size": 983,
      "sentences": 8
    },
    {
      "id": 10,
      "content": "valuation\ntarget positions. function. An improvedparticle swarm optimization algorithm\nTo solve the above challenges, this paper proposes a novel is developed to solve a reasonable scheduling scheme. Liu\nmulti-agent reinforcement learning algorithm based on infor- et al. [18] develop a multi-objective mathematical model and\nmation potential field rewards. We model the decentralized integrate with two adaptive genetic algorithms to optimize\nmulti AGV task allocation as a Partially Observable Markov the task scheduling of AGVs while taking into account the\nDecision Process (POMDP). To address reward-sparsity is- charging task and the AGV’s variable speed. Saidi et al. [19]\nsue, we propose a reward shaping mechanism based on IPF address the conflict-free AGV path planning problem for job\nthat provides AGV collaboration with stepwise and implicit shop scheduling and solve it using a two-stage ant colony\ndirection. Additionally, we apply IPF to the state-of-the-art algorithm.",
      "size": 985,
      "sentences": 10
    },
    {
      "id": 11,
      "content": "m for job\nthat provides AGV collaboration with stepwise and implicit shop scheduling and solve it using a two-stage ant colony\ndirection. Additionally, we apply IPF to the state-of-the-art algorithm. These algorithms require knowledge of the global\nMADDPG and BiCNet algorithms to prove the superiority environmentinordertocalculatetheoptimalpolicies,andthe\nof this mechanism. Extensive experiments demonstrate that decision-making capability of a single agent is insufficient in\nour methodology can result in considerable performance and real-worldscenarios.Themulti-agentsystemcancompletenot\nconvergence improvements. The main contributions of this only a single agent’s goal, but also exceed the efficiency of\nwork are summarized as follows. the single agent, which means that many agents can increase\n(1)Thetraditionalcentralizedtaskallocationmethodsplace its strength.",
      "size": 873,
      "sentences": 6
    },
    {
      "id": 12,
      "content": "oal, but also exceed the efficiency of\nwork are summarized as follows. the single agent, which means that many agents can increase\n(1)Thetraditionalcentralizedtaskallocationmethodsplace its strength. extraordinarily high demands on control center’s computing\nB. Multi-Agent Reinforcement Learning\npower and real-time capability. We innovatively formulate\nthe decentralized multi-AGV task allocation problem as a In multi-agent system, traditional independent Q-learning\npartially observable Markov decision process, and propose [20] or DQN based on experience replay [21] cannot be\ntwo improved multi-agent reinforcement learning algorithms applied to a multi-agent environment directly. Because the\nto achieve coordination among AGVs adapting to different experience pool’s samples become old when the environment\nscenarios. changes, the method produced from outdated sample train-\n(2) We introduce information potential field to address ing is frequently not ideal. Therefore, Foerster et al.",
      "size": 994,
      "sentences": 7
    },
    {
      "id": 13,
      "content": "hen the environment\nscenarios. changes, the method produced from outdated sample train-\n(2) We introduce information potential field to address ing is frequently not ideal. Therefore, Foerster et al. [22]\nthe reward sparsity issue in decentralized multi-AGV task propose two strategies for maintaining the DQN experience\nallocation. It can provide implicit direction for autonomous replay pool’s stability. The central idea is to augment the\ndecision-makingand improve the AGV system’s cooperation. experience buffer with additional information and to under-\n(3)Weconductexperimentsunderdifferentsettings,andthe take importance sampling in order to mitigate the influence\nexperiment results show that our strategy obtains up to 47% of unstable surroundings on multi-agent training. Lowe et\ntask response improvement compared with baseline methods. al.",
      "size": 851,
      "sentences": 9
    },
    {
      "id": 14,
      "content": "igate the influence\nexperiment results show that our strategy obtains up to 47% of unstable surroundings on multi-agent training. Lowe et\ntask response improvement compared with baseline methods. al. [11] propose MADDPG to train a centralized critic for\nAdditionally, we demonstrate the cooperation mechanism of each agent using all agents’ policies during training in order\nMADDPG-IPF and BiCNet-IPF. The agents establish a dif- to reduce variance by eliminating the non-stationarity. The\n=== 페이지 3 ===\nactor only has local information and the experience buffer ordertocoordinatetransportationtasks. We definethelogistic\nrecords the experiences of all agents. Foerster et al. [23] network using G = (T,V,L), where T, V and L denote the\npropose an actor-critic counterfactual multi-agent (COMA) set of material targets, vehicles and trajectories, respectively. policygradientmethod.COMAisintendedforuseinboththe More specifically,\nfully centralized and multiagent credit assignment problems.",
      "size": 991,
      "sentences": 10
    },
    {
      "id": 15,
      "content": "A) set of material targets, vehicles and trajectories, respectively. policygradientmethod.COMAisintendedforuseinboththe More specifically,\nfully centralized and multiagent credit assignment problems. TargetsetT:Eachcooperativetransportationtaskentailsthe\nBy comparing the current Q value to the counterfactual, an movementof N differentmaterials. The material targetsT ∈\ni\nadvantagefunctioncan be constructed.In contrastto previous T(1≤i≤N)arerandomlydispersedindifferentplaces,and\napproaches,in BidirectionallyCoordinatedNetwork(BiCNet) the position of the target T is represented by (xT,yT). i i i\n[12], communication takes place in the latent space, and it Vehicle set V: we assume that all of N AGVs are modeled\nalso uses parameter sharing. Note that in BiCNet, agents do as discs with the same radius D, i.e., all AGVs are homoge-\nnotexplicitlyshareamessage,itmightbeconsideredamethod neous. At each timestep t, utilize the vector G ={pt,vt,r }\ni i i i\nfor learning cooperation.",
      "size": 983,
      "sentences": 7
    },
    {
      "id": 16,
      "content": "with the same radius D, i.e., all AGVs are homoge-\nnotexplicitlyshareamessage,itmightbeconsideredamethod neous. At each timestep t, utilize the vector G ={pt,vt,r }\ni i i i\nfor learning cooperation. to describe the state of the AGV i (1 ≤ i ≤ N), including\nMulti-agent reinforcement learning technology provides its position pt = (x,y), velocity vt = (v ,v ), and sensing\ni i x y\nnew ideas for implementing autonomous decision-making of distance r . The AGV i obtains an observation ot within the\ni i\nmultiple AGVs. Our proposed method utilizes the powerful sensing range r , and then compute the action command at\ni i\ndata representation and decision-making capabilities of deep according to the policy π , where θ denotes the policy pa-\nθ\nreinforcement learning to enable self-organizing task assign- rameters. The calculated action at is a velocity vt that directs\ni i\nment of multi-AGV systems. theAGV towardthetask targetwhile avoidingcollisionswith\nother robots.",
      "size": 968,
      "sentences": 7
    },
    {
      "id": 17,
      "content": "able self-organizing task assign- rameters. The calculated action at is a velocity vt that directs\ni i\nment of multi-AGV systems. theAGV towardthetask targetwhile avoidingcollisionswith\nother robots. C. Information Potential Field\nTrajectorysetL:To wrapuptheprecedingformulation,we\nInformation Potential Field (IPF) is an effective path plan- define L = {l ,i = 1,...,N} as the set of trajectories of all\ni\nning method. The robot can accomplish the global objective AGVs, which are subject to the AGV’s kinematic constraints,\nby employing a greedy strategy based on the information i.e. :\ngradient. Liu et al. [15] propose two effective algorithms for\nconstructing IPF: the hierarchical skeleton-based construction v i t ∽π θ (at i |ot i )\nalgorithm and the value estimation replacement algorithm, kvtk≤vmax\ni i\n(1)\nbothofwhichachieveatrade-offbetweenenergyconsumption pt =pt−1+∆t·vt\nand convergencespeed.Wei et al.",
      "size": 915,
      "sentences": 8
    },
    {
      "id": 18,
      "content": "i t ∽π θ (at i |ot i )\nalgorithm and the value estimation replacement algorithm, kvtk≤vmax\ni i\n(1)\nbothofwhichachieveatrade-offbetweenenergyconsumption pt =pt−1+∆t·vt\nand convergencespeed.Wei et al. [24] proposeefficientpark- i i i\n∀j ∈[1,N],j 6=i, pt−pt >2D\ning navigation via a continuous information ascent method. i j\nIn the first step, a partial differential equation is used to Tofindanoptimalpolicy,wes(cid:13)etanobj(cid:13)ectivebyminimizing\n(cid:13) (cid:13)\nestablisha globalpotentialfield.Inthesecondstep,a Poisson the expectation of the mean arrival time of all AGVs in the\nequation is employed to construct the local potential field in same scenario, which is defined as:\nthe navigation process. Lin et al. [25] propose an artificial\nN\n1\ninformation gradient that is robust and has no local extrema. argmin E t |π (2)\nThey use a harmonic function to establish IPF, representing πθ \" N i θ #\ni=1\nX\nthe diffusion of a specific type of event of interest (EoI).",
      "size": 971,
      "sentences": 6
    },
    {
      "id": 19,
      "content": "that is robust and has no local extrema. argmin E t |π (2)\nThey use a harmonic function to establish IPF, representing πθ \" N i θ #\ni=1\nX\nthe diffusion of a specific type of event of interest (EoI). Wei\nWheret isthetraveltimeofthetrajectoryl inLcontrolled\ni i\net al. [26] offer a novel heat diffusion equation to efficiently\nby policy π . θ\nand quickly complete the navigation procedure. The strategy\nDecentralized multi-AGV task allocation can be viewed as\nassures that a local information field is sufficiently large to\na special mobile robot moving path planning problem. AGV\nencompass many appropriate targets, and that competition\ndecides its target and plans a collision-free course based on\nconflicts can be addressed concurrently. The majority of cur-\nits surroundings cognition. rent research directly addresses the path planning problem\nusing the information potential field method. In this paper, B.",
      "size": 910,
      "sentences": 10
    },
    {
      "id": 20,
      "content": "be addressed concurrently. The majority of cur-\nits surroundings cognition. rent research directly addresses the path planning problem\nusing the information potential field method. In this paper, B. System Architecture\nthe informationpotential field is utilized to design the reward We propose improved multi-agent reinforcement learning\nfunction of multi-agent reinforcement learning. The reward algorithms to solve this problem, the architecture of which is\nis evaluated in relation to the information potential value of shown as Fig.2. In real world situations, agents make noisy\nthe AGV location to implicitly steer the AGV to the target observations of the true environment state to inform their\nposition. action selection, typically modeled as a POMDP. Formally, a\nPOMDPcanbedescribedasatuple:M =(N,S,A,P,R,O),\nIII. PROBLEM FORMULATION AND SYSTEMOVERVIEW\nwhere N denotes the number of agents, S represents the\nA.",
      "size": 918,
      "sentences": 10
    },
    {
      "id": 21,
      "content": "election, typically modeled as a POMDP. Formally, a\nPOMDPcanbedescribedasatuple:M =(N,S,A,P,R,O),\nIII. PROBLEM FORMULATION AND SYSTEMOVERVIEW\nwhere N denotes the number of agents, S represents the\nA. Problem Formulation\nsystem state space, A represents the joint action space of all\nIn this section, we will formally define the multi-AGV agents,P isthetransitionprobabilityfunction,Risthereward\ncollaborative task allocation problem. In the manufacturing function, and O is the observation probability distribution\nworkshop, processing products typically require various raw given the system state (o ∽ O(s)). Specific to the problem\nmaterials, which are stored in different locations across the scenario of AGV collaborativetask allocation, the state space\nwarehouse.",
      "size": 768,
      "sentences": 6
    },
    {
      "id": 22,
      "content": "s raw given the system state (o ∽ O(s)). Specific to the problem\nmaterials, which are stored in different locations across the scenario of AGV collaborativetask allocation, the state space\nwarehouse. AGVs must travel to multiple destinations in S and action space A are specifically designed as follows:\n=== 페이지 4 ===\nState space S: For the AGV task assignment problem, the\nselection of the state space should not only characterize the\nattributesoftheagentsandtargets,butalsonotbringtoomuch\ncomputational burden. Therefore, we set the state space as\n{v,p,D ,D }, where {v,p}is the speed andposition of the\nA B\nagent itself, and {D ,D } is the relative distance from the\nA B\ntargets and other agents. Action space A: We set the AGV’s action space as a one-\ndimensional vector {x,y}, the value is (−1,1), representing\nthe acceleration in the left and right directions and the front\nand back directions. Combined with the weight and damping\nof the AGV itself, the velocity of the AGV is computed.",
      "size": 993,
      "sentences": 6
    },
    {
      "id": 23,
      "content": "(−1,1), representing\nthe acceleration in the left and right directions and the front\nand back directions. Combined with the weight and damping\nof the AGV itself, the velocity of the AGV is computed. Reward R: Our objective is each AGV avoids collisions Fig.3. InformationPotential Field. and self-organizes to different targets as quickly as possible. A reward function is designed to guide a team of AGVs\nvalueof-3.Additionally,wesettheinformationvalueofsome\nto achieve this objective. we design a target reward when\nothernodesto0,oftennodesonthenetworkboundary,inorder\nreaching the target position and a collision penalty when a\nto enforce a gradient throughout the network. The remaining\ncollision occurs. nodes compute the information potential field using Jacobi\nWhen the new tasks arrive,state informationis inputto the\niterations. Each non-boundarynode iterates:\nnetwork to determine the action. Following that, the chosen\naction will be used to route the AGVs to various task targets.",
      "size": 992,
      "sentences": 11
    },
    {
      "id": 24,
      "content": ",state informationis inputto the\niterations. Each non-boundarynode iterates:\nnetwork to determine the action. Following that, the chosen\naction will be used to route the AGVs to various task targets. Φk+1(u)← 1 Φk(u) (3)\nThe reward function is used to direct model training in this d(u)\nprocess, allowing the model to learn the ideal strategy. v∈\nX\nN(u)\nWhere Φk(u) is the valueof node u in the k−th iteration. N(u) signifies the set of u’s neighbors, while d(u) denotes\nthe degree of u. Each position will have a corresponding\ninformation potential value after iteration. The AGV obtains\nthe reward value r according to the information potential\nIPF\nvalue of the position at the time step t. As illustrated in\nFig.4,theIPFvaluearoundthetargetlocationishigh,andthe\ngravitational range grows more vast when several targets are\ngathered.When anotherAGV is alreadyin close proximityto\nthetarget,therewardisreduced,essentially avoidingmultiple\nAGVs competing for the same target. Fig.2.",
      "size": 982,
      "sentences": 9
    },
    {
      "id": 25,
      "content": "e grows more vast when several targets are\ngathered.When anotherAGV is alreadyin close proximityto\nthetarget,therewardisreduced,essentially avoidingmultiple\nAGVs competing for the same target. Fig.2. Architecture ofAGVstaskallocation approach. IV. METHODS\nA. Reward Shaping with IPF\nA well-designed reward function can enhance robustness\nand promote agent collaboration. In the previous section,\nwe discuss a general AGV task allocation framework. In\nthis section, we propose a reward shaping strategy based\non information potential field to address the issue of reward Fig.4. IPFprovides implicit guidance foragent’s decision-making. sparsity. Along with r for implicit guidance, we design a target\nInformation Potential Field (IPF) is introduced to design IPF\nrewardr andacollisionpenaltyr forexplicitguidance.The\nthe rewardfunctionr , as shownin Fig.3.",
      "size": 855,
      "sentences": 11
    },
    {
      "id": 26,
      "content": "r for implicit guidance, we design a target\nInformation Potential Field (IPF) is introduced to design IPF\nrewardr andacollisionpenaltyr forexplicitguidance.The\nthe rewardfunctionr , as shownin Fig.3. We partitionthe g c\nIPF\ntarget reward r and the collision penalty r are specified as\nscenario into a bounded grid map, assign a positive informa- g c\nfollows:\ntion potential value for the location of the target target, and\nr =− min (d ) (4)\nassignanegativeinformationpotentialvalueforthelocationof g j ij\notherAGVs,whichcanimplicitlyguidetheAGVstodifferent X i\ntargets.Thetargetsare setto amaximumpotentialvalueof5, −1 ifkpt−ptk≤2R\nr = i j (5)\nwhiletheotherAGV’spositionsaresettoaminimumpotential c 0 otherwise\n(cid:26)\n=== 페이지 5 ===\nWhere d is the distance between task target j and AGV Among them, o represents the observation of the agent\nij i\ni. Additionally, when the AGV collides with other AGVs in i, and x = [o ,...,o ] represents the observation vector.",
      "size": 962,
      "sentences": 2
    },
    {
      "id": 27,
      "content": "en task target j and AGV Among them, o represents the observation of the agent\nij i\ni. Additionally, when the AGV collides with other AGVs in i, and x = [o ,...,o ] represents the observation vector. 1 n\nthe environment, it incurs a r penalty. Qµ(x,a ,...,a )representsthecentralizedstate-actionfunc-\nc i 1 n\nIn general, we hope that when a new handling task arrives, tion of the agent i. The experience replay buffer D contains\nthe AGV system can self-organize and complete it in the (x,x,,a ,...,a ,r ,...,r ) these tuples, which acts as the\n1 n 1 n\nshortest time possible. Based on the observed information, knowledge base of the agent, storing the experience of all\nAGVs must plan a collision-free path to different material agents. targets.",
      "size": 745,
      "sentences": 6
    },
    {
      "id": 28,
      "content": "n 1 n\nshortest time possible. Based on the observed information, knowledge base of the agent, storing the experience of all\nAGVs must plan a collision-free path to different material agents. targets. We use the sum of r , r and r to represent The action-value function Qµ is updated based on:\nIPF g c i\n( th 6 e ), r d e i w re a c r t d ing r t a h c e qu A i G re V d s b y y st A em GV to i ac a h t ie ti v m e e se s l t f e - p org t, an a i s zi s n e g en tas i k n y =r i (s,a)+λQµ i ′ max θ (x′,a′ 1 ,...,a′ n )| a′ j =µ′ j (oj) (8)\nassignment.r\ng\nincentivizesthepresenceofpreciselyoneagent L(θ\ni\n)=E x,a,r,x′[(Qµ\ni\n(x,a\n1\n,...,a\nn\n)−y)2]\nnear each target. r wishes for the fewest potential collisions. c Among them, Qµ′ represents the target network, and µ′ =\nr IPF providesan implicitshoveto the AGV, guidingit to the [µ′,µ′,...,µ′ ]is th i e parameter θ′ of the target network that\ntarget place in a distributed fashion. 1 2 n j\nhas a lagging update.",
      "size": 963,
      "sentences": 7
    },
    {
      "id": 29,
      "content": "d µ′ =\nr IPF providesan implicitshoveto the AGV, guidingit to the [µ′,µ′,...,µ′ ]is th i e parameter θ′ of the target network that\ntarget place in a distributed fashion. 1 2 n j\nhas a lagging update. rt =(r )t+(r )t+(r )t (6) 2) BiCNet-IPF: BiCNet [12] is still based on the actor-\ni IPF i g i c i\ncritic framework, and the network structure as illustrated in\nB. The Algorithm Design\nFig.6. The actor and the critic are both constructed using\nIn multi-agent training, we focus on two algorithms based a bidirectional recurrent neural network. Through implicit\non the actor-critic framework, MADDPG and BiCNet. These communication,theactorsharesobservationandreturnsaction\ntwo algorithms offer the following advantages over other for each agent. Each agent has the ability to retain its own\nMADL algorithms. MADDPG does not require explicit com- internal state and communicate with other agents.",
      "size": 894,
      "sentences": 9
    },
    {
      "id": 30,
      "content": "e following advantages over other for each agent. Each agent has the ability to retain its own\nMADL algorithms. MADDPG does not require explicit com- internal state and communicate with other agents. munication rules, is applicable to a wide variety of contexts,\nincluding cooperative, competitive, and mixed environments,\nand is capable of solving the non-stationary problem asso-\nciated with multi-agent environments. All agents in BiCNet\nshare models and parameters and build communication chan-\nnels in the hidden layer, enabling any number of agents to\ncooperate. These two algorithms approach issues differently,\nand there are clear distinctions in the model structure, loss\nfunction, and other factors. 1) MADDPG-IPF: MADDPG [11] adopts centralized\ntraining with distributed execution method.",
      "size": 799,
      "sentences": 7
    },
    {
      "id": 31,
      "content": "ssues differently,\nand there are clear distinctions in the model structure, loss\nfunction, and other factors. 1) MADDPG-IPF: MADDPG [11] adopts centralized\ntraining with distributed execution method. Each agent trains\na critic network that requires global information and an actor\nnetworkthatonlyrequireslocalknowledge.Theactorchooses\nthe best action for a given state by optimizing the neural\nnetworkparametersθ.Thecriticevaluatestheactiongenerated\nby the actor by computing the temporal difference error. The\nMADDPG algorithm network structure is shown in Fig.5. Fig.6. Thestructure ofBiCNet-IPF. We denote the objective of a single agent i by J (θ), that\ni\nis to maximize its expected cumulative individual reward r\ni\nFig.5. ThestructureofMADDPG-IPF. as J i (θ) = E s∽ρτ [r i (s,a θ (s))]. Therefore, we can get the\naθ\nobjective of N agents denoted by J(θ) as follows:\nThe policy gradient is calculated as:\nN\n∇ θi J(µ i )=E ∇ x,a Q ∽ µ D ( [ x ∇ , θ a i µ , i . ( . a .",
      "size": 972,
      "sentences": 12
    },
    {
      "id": 32,
      "content": "[r i (s,a θ (s))]. Therefore, we can get the\naθ\nobjective of N agents denoted by J(θ) as follows:\nThe policy gradient is calculated as:\nN\n∇ θi J(µ i )=E ∇ x,a Q ∽ µ D ( [ x ∇ , θ a i µ , i . ( . a . i , |o a i ) ) · | ] (7) J(θ)=E s∽ρτ aθ [ r i (s,a θ (s))] (9)\nai i 1 n ai=µi(oi)\nX\ni=1\n=== 페이지 6 ===\nCombined with the deterministic policy gradient, we have\nthe policy gradient as follows:\nN N\n∇ θ J(θ)=E s∽ρτ aθ (s) [ ∇ θ a j,θ ·∇ aj Qa i θ(s,a θ (s))]\ni=1j=1\nXX\n(10)\nIn training the critic network, using the sum of square loss,\nthegradientcanbewrittenasin(11),whereξ istheparameter\nof the Q-network:\nN\n∇ ξ L(ξ)=E s∽ρτ aθ (s) [ (r i (s,a θ (s))+λQξ i (s′,a θ (s′ )) Fig.7. Cooperative taskallocation. i=1\nX\n−Qξ(s,a (s)))·∇ Qξ(s,a (s))] TABLEI\ni θ ∂ξ i θ MODELPERFORMANCEIN3V3SCENARIO\n(11)\nIn different agents, the parameters are shared, hence the\nAveragetask\nnumberofparametersisindependentofthenumberofagents.",
      "size": 912,
      "sentences": 7
    },
    {
      "id": 33,
      "content": "s,a (s)))·∇ Qξ(s,a (s))] TABLEI\ni θ ∂ξ i θ MODELPERFORMANCEIN3V3SCENARIO\n(11)\nIn different agents, the parameters are shared, hence the\nAveragetask\nnumberofparametersisindependentofthenumberofagents. Averagereward Averagetime\nresponserate\nParameter sharing leads to a compact model that speeds up MADDPG-MiniDist 88.64% -101.1 14.3\nthe learning process. MADDPG-Greedy 88.67% -116.61 11.8\nMADDPG-IPF 95.00% -85.8 11.1\nV. EVALUATION BiCNet-MiniDist 93.03% -71.5 10.4\nBiCNet-Greedy 73.56% -143.8 10.2\nA. Experimental Settings BiCNet-IPF 97.58% -65.8 9.7\nIn order to conduct experiments, we build an AGV task\nallocation simulator based on a multi-agentenvironment[11],\nwhich comprises of N AGVs and N tasks inhabiting a two- agent.Theshorterthedistancebetweentwo targets,thelarger\ndimensional world with continuous space and discrete time the reward. (see Fig.7). For MARL algorithms, as the number of agents MADDPG-Greedy: The Greedy is an individual reward.",
      "size": 955,
      "sentences": 6
    },
    {
      "id": 34,
      "content": "wo targets,thelarger\ndimensional world with continuous space and discrete time the reward. (see Fig.7). For MARL algorithms, as the number of agents MADDPG-Greedy: The Greedy is an individual reward. increases, the joint state-action space increases exponentially, Whenanagentapproachesthetasktarget,itreceivesapositive\nwhich makes the task intractable. Therefore we verify the reward, which rises as the distance between the agent and the\nrobustness of the proposed methods under two scenarios: a task target decreases. 3 AGVs and 3 tasks simple scenario and a 6 AGVs and MADDPG-IPF: The IPF as we discussed in Section 4. 6 tasks complex scenario, referred to as 3V3 scenario and Additionally, BiCNet-MiniDist, BiCNet-Greedy and\n6V6 scenario. In each scenario, the position of the AGV BiCNet-IPF are similar to the above. The Q-network and\nand the position of the task are randomly generated.",
      "size": 893,
      "sentences": 9
    },
    {
      "id": 35,
      "content": "BiCNet-MiniDist, BiCNet-Greedy and\n6V6 scenario. In each scenario, the position of the AGV BiCNet-IPF are similar to the above. The Q-network and\nand the position of the task are randomly generated. Taking policy network in MADDPG are parameterized by three\ninto accountthe actual scenario, we define boundariesaround fully connected layers. The Q-network and policy network\nthe simulator, within which the agent can only move. We in BiCNet are based on the bi-directional RNN structure. hope that the AGV can learn to disperse to different task Both the input and output modules are made up of four fully\ntargets in the shortest time and avoid collisions as much as connected layers. feasible. Performance is measured by average task response Each model is trained for 30k epochs in the 3V3 scenario.",
      "size": 801,
      "sentences": 9
    },
    {
      "id": 36,
      "content": "lly\ntargets in the shortest time and avoid collisions as much as connected layers. feasible. Performance is measured by average task response Each model is trained for 30k epochs in the 3V3 scenario. rate, average reward, and average time: Forthe 6V6scenario,the action space andstate space dimen-\nAverage task response rate: the number of tasks completed sions are greatly increased, necessitating the use of additional\nby N AGVs in the entire test epochs divided by the total rounds.Asaresult,eachmodelbasedonMADDPGistrained\nnumber of tasks generated. for 50k epochs, and each model based on BiCNet, a more\nAverage reward: the rewards obtained by N AGVs complicated network structure, is trained for 90k epochs. at each time step, calculated using the formula R = Finally, we execute 300 epochs for testing on each model in\n− min (d )−C. the two scenarios,and the resultsare presentedin Table I and\ni j ij\nAveragetime:thetotaltimerequiredforN AGVstoexecute Table II.",
      "size": 968,
      "sentences": 7
    },
    {
      "id": 37,
      "content": "ally, we execute 300 epochs for testing on each model in\n− min (d )−C. the two scenarios,and the resultsare presentedin Table I and\ni j ij\nAveragetime:thetotaltimerequiredforN AGVstoexecute Table II. all P tasks (for example, in the 3V3 scenario, the three AGVs MADDPG-IPF achieves a task response rate of 95% in\nhave reached the three task targets correctly). the 3V3 scenario, an increase of approximately 6% over the\notherMADDPGmodels.ComparingtheresultsofMADDPG-\nB. Performance Comparison\nIPFandBiCNet-IPF,theBiCNet-IPFconsistentlyoutperforms\nInthissubsection,theperformanceoffollowingmethodsis MADDPG-IPF, possibly because of implicit communication,\nextensively evaluated by the simulation. which enables better decision-makingwith more information.",
      "size": 754,
      "sentences": 6
    },
    {
      "id": 38,
      "content": "bsection,theperformanceoffollowingmethodsis MADDPG-IPF, possibly because of implicit communication,\nextensively evaluated by the simulation. which enables better decision-makingwith more information. MADDPG-MiniDist: The MiniDist is a global reward that In the more complex 6V6 scenario, BiCNet-IPF achieves a\nsums the distance between each task target and its nearest task response rate of 91.61%, a significant advantage over all\n=== 페이지 7 ===\nTABLEII are unable to acquire vital knowledge in the first 60k epochs,\nMODELPERFORMANCEIN6V6SCENARIO butthereperformanceimprovessignificantlyafter70kepochs. Average task\nAveragereward Averagetime\nresponserate\nMADDPG-MiniDist 69.22% -438.5 17.7\nMADDPG-Greedy 46.06% -675.0 17.1\nMADDPG-IPF 80.22% -371.5 16.2\nBiCNet-MiniDist 80.44% -249.8 16.0\nBiCNet-Greedy 44.56% -664.5 17.5\nBiCNet-IPF 91.61% -241.1 15.6\nother models.",
      "size": 864,
      "sentences": 4
    },
    {
      "id": 39,
      "content": "iDist 69.22% -438.5 17.7\nMADDPG-Greedy 46.06% -675.0 17.1\nMADDPG-IPF 80.22% -371.5 16.2\nBiCNet-MiniDist 80.44% -249.8 16.0\nBiCNet-Greedy 44.56% -664.5 17.5\nBiCNet-IPF 91.61% -241.1 15.6\nother models. Although MADDPG-IPF is not as good as the\nbestapproach,itstillachievesan80.22%taskresponserate.In\ngeneral,theglobalreward(MiniDist)assignsthesamereward Fig.9. Convergence comparison.Averagetaskresponserateunderdifferent\nrewardmechanisms duringthetraining phase. to all agents without regardof their contributions,which may\nencourage slothful agents. In comparison, the local reward\n(Greedy) only provides different local rewards to each agent D. Implicit Cooperation Mechanism Analysis\nbased on individual behavior, leading to selfish agents.",
      "size": 742,
      "sentences": 5
    },
    {
      "id": 40,
      "content": "gents. In comparison, the local reward\n(Greedy) only provides different local rewards to each agent D. Implicit Cooperation Mechanism Analysis\nbased on individual behavior, leading to selfish agents. IPF\nInthe6V6scenario,bynumberingeachAGVandeachtask,\nreward incorporates global and local information and gives\nweobserveaninterestingphenomenon:the2-thAGVand6-th\nongoing rewards at each step, allowing the agent to improve\nAGV directed by MADDPG always arrive at the identical 3-\nits performance on various task targets. th task, resulting in no AGV reachingthe 1-th task. However,\nthis will notoccur in BiCNet. Thus, we countthe task targets\nC. The Effectiveness of IPF\nachieved by each agent of MADDPG-IPF and BiCNet-IPF\nAlongwiththeperformancecomparisonsmentionedabove,\nin the 3V3 scenario and 6V6 scenario, and investigate the\nwe examinethe taskcompletionofeachroundofthreeAGVs\ncooperation mechanism of the two methods MADDPG and\nunder different reward designs in 3V3 scenario.",
      "size": 980,
      "sentences": 6
    },
    {
      "id": 41,
      "content": "3 scenario and 6V6 scenario, and investigate the\nwe examinethe taskcompletionofeachroundofthreeAGVs\ncooperation mechanism of the two methods MADDPG and\nunder different reward designs in 3V3 scenario. As shown\nBiCNet, as shown in Fig. 10.\nin the Fig.8, after applying the IPF reward mechanism, the\nagents can complete all tasks mostly in a distributed manner. IPF can significantly reduce the likelihood of multiple AGVs\ncompeting for the same target by offering implicit guidance. Global rewards may lead to laziness, so the agents inspired\nby MiniDist sometimes reach the target nearby but stagnate,\nresulting in worse task response than IPF. The Greedy reward\nfrequently motivates agents to fight for a single task target,\nresulting in suboptimal performance. Fig. 10. Thecooperation mechanism ofMADDPG and BiCNet. In (a), the\nagents develop a differential preference for each task, e.g. the 1-th agent\npreferstask1.In(b),theagents tendtocomplete thenearesttask.",
      "size": 964,
      "sentences": 11
    },
    {
      "id": 42,
      "content": ". 10. Thecooperation mechanism ofMADDPG and BiCNet. In (a), the\nagents develop a differential preference for each task, e.g. the 1-th agent\npreferstask1.In(b),theagents tendtocomplete thenearesttask. We discover that what MADDPG learned is each agent’s\npreference for a certain fixed task. As illustrated in Fig.11,\nwhile training 30k epochs in the 3V3 scenario, 97% of the\nepochs of 1-th AGV chooses the 1-th task. What BiCNet\nlearnedisthe choiceofeachagentforthe closesttasktargets. As shown in Fig.12, the reach rate of 1-th AGV for the three\ntasks in 30k epochs is approximately 30%, and it does not\nshow exceptional performance for a particular task. Fig. 8. Task response rate per round. For 3 response, all three tasks are\nIn terms of this phenomenon, we argue that under MAD-\ncompleted. DPG framework, each agent has an independent network\nConvergence is assessed by examining the average task structure and takes decisions based on local observations.",
      "size": 960,
      "sentences": 14
    },
    {
      "id": 43,
      "content": "argue that under MAD-\ncompleted. DPG framework, each agent has an independent network\nConvergence is assessed by examining the average task structure and takes decisions based on local observations. completionrateofBiCNetduringthetrainingphaseunderthe Therefore,bycontinuouslystrengtheningtherewardsobtained\nchallenging6V6scenario.AsillustratedinFig.9,theapproach at a particular task target during initial training, the agent\nusing IPF can achieve a 40% task response rate after 40k will prefer it. While all agents in BiCNet share parameters\nepochs and 60% task response rate after 60k epochs. Due to and communicate implicitly via the bi-directional RNN, each\nthe fact that BiCNet-Greedy is an individual reward network, agent coordinates with others and moves toward the nearest\nitsconvergencerateisslower.TheagentsinspiredbyMiniDist task target. === 페이지 8 ===\n[2] WangJ,Zhang Y, LiuY, et al.",
      "size": 896,
      "sentences": 6
    },
    {
      "id": 44,
      "content": "individual reward network, agent coordinates with others and moves toward the nearest\nitsconvergencerateisslower.TheagentsinspiredbyMiniDist task target. === 페이지 8 ===\n[2] WangJ,Zhang Y, LiuY, et al. Multiagent andbargaining-game-based\nreal-time scheduling forinternet ofthings-enabled flexible jobshop[J]. IEEEInternet ofThingsJournal, 2018,6(2):2518-2531. [3] DemesureG,DefoortM,BekrarA,etal.Decentralizedmotionplanning\nandschedulingofAGVsinanFMS[J].IEEETransactionsonIndustrial\nInformatics, 2017,14(4):1744-1752. [4] WangW,ZhangY, ZhongRY. A proactive material handling method\nforCPSenabledshop-floor[J].RoboticsandComputer-Integrated Man-\nufacturing, 2020,61:101849. [5] KhamisA,HusseinA,ElmogyA.Multi-robottaskallocation:Areview\nofthestate-of-the-art[J].CooperativeRobotsandSensorNetworks2015,\n2015:31-51. [6] Wu G, Sun X. AGV Task Distribution Study[C]//Journal of Physics:\nConference Series.",
      "size": 898,
      "sentences": 9
    },
    {
      "id": 45,
      "content": ".Multi-robottaskallocation:Areview\nofthestate-of-the-art[J].CooperativeRobotsandSensorNetworks2015,\n2015:31-51. [6] Wu G, Sun X. AGV Task Distribution Study[C]//Journal of Physics:\nConference Series. IOPPublishing, 2020,1486(7):072016\n[7] Zhu Z, Tang B, Yuan J. Multirobot task allocation based on an\nimprovedparticleswarmoptimizationapproach[J].internationalJournal\nFig. 11. The1-thagent’s preference inMADDPG.Afterfully training the\nofAdvancedrobotic systems,2017,14(3):1729881417710312.\nmodel,the1-thagenttendstocompletetask1,butrarelychoosestask2and\n[8] MousaviM,YapHJ,MusaSN,etal.Multi-objectiveAGVscheduling\ntask3. in an FMS using a hybrid of genetic algorithm and particle swarm\noptimization[J]. PloSone,2017,12(3):e0169817. [9] OroojlooyJadid A, Hajinezhad D. A review ofcooperative multi-agent\ndeepreinforcementlearning[J].arXivpreprintarXiv:1908.03963,2019.",
      "size": 867,
      "sentences": 8
    },
    {
      "id": 46,
      "content": "nd particle swarm\noptimization[J]. PloSone,2017,12(3):e0169817. [9] OroojlooyJadid A, Hajinezhad D. A review ofcooperative multi-agent\ndeepreinforcementlearning[J].arXivpreprintarXiv:1908.03963,2019. [10] Nguyen TT,Nguyen ND,Nahavandi S. Deepreinforcement learning\nformultiagent systems:Areview ofchallenges, solutions,andapplica-\ntions[J].IEEEtransactions oncybernetics, 2020,50(9):3826-3839. [11] Lowe R, Wu Y, Tamar A, et al. Multi-agent actor-critic for\nmixed cooperative-competitive environments[J]. arXiv preprint\narXiv:1706.02275,2017. [12] Peng P, Wen Y, Yang Y, et al. Multiagent bidirectionally-coordinated\nnets:Emergenceofhuman-levelcoordinationinlearningtoplaystarcraft\ncombatgames[J].arXivpreprintarXiv:1703.10069,2017. [13] Vinyals O, Ewalds T, Bartunov S, et al. Starcraft ii: A new challenge\nforreinforcement learning[J]. arXivpreprintarXiv:1708.04782,2017. [14] Schuster M, Paliwal K K. Bidirectional recurrent neural networks[J]. Fig.12.",
      "size": 955,
      "sentences": 14
    },
    {
      "id": 47,
      "content": "T, Bartunov S, et al. Starcraft ii: A new challenge\nforreinforcement learning[J]. arXivpreprintarXiv:1708.04782,2017. [14] Schuster M, Paliwal K K. Bidirectional recurrent neural networks[J]. Fig.12. The1-thagent’spreferenceinBiCNet.Afterfullytrainingthemodel, IEEEtransactions onSignalProcessing, 1997,45(11):2673-2681.\ntheprobabilities ofthe1-thagentchoosingthreetasksaresimilar. [15] Liu S, Du J, Liu H, et al. Energy-efficient algorithm to construct the\ninformation potential field in WSNs[J]. IEEE Sensors Journal, 2017,\n17(12):3822-3831. [16] Guo Z, Sun F. Research on integrated navigation method for AUV[J]. VI. CONCLUSION JournalofMarineScience andApplication, 2005,4(2):34-38. [17] ZhangF,LiJ.Animproved particle swarmoptimization algorithm for\nIn this paper, we first formulated the AGVs task alloca-\nintegrated scheduling model in AGV-served manufacturing systems[J].",
      "size": 879,
      "sentences": 13
    },
    {
      "id": 48,
      "content": "34-38. [17] ZhangF,LiJ.Animproved particle swarmoptimization algorithm for\nIn this paper, we first formulated the AGVs task alloca-\nintegrated scheduling model in AGV-served manufacturing systems[J]. tion problem in logistics networks as a partially observable JournalofAdvanced Manufacturing Systems,2018,17(03):375-390. Markov decision process. Given this setting, we introduced [18] LiuY,JiS,SuZ,etal.Multi-objectiveAGVschedulinginanautomatic\nsorting system of an unmanned (intelligent) warehouse by using two\ntheinformationpotentialfieldoptimizationrewardmechanism\nadaptive genetic algorithms and a multi-adaptive genetic algorithm[J]. andproposedtwocooperativemulti-agentreinforcementlearn- PloSone,2019,14(12):e0226161. ing algorithms to solve the problem. Extensive experiments [19] Saidi-Mehrabad M, Dehnavi-Arani S, Evazabadian F, et al.",
      "size": 846,
      "sentences": 8
    },
    {
      "id": 49,
      "content": "osedtwocooperativemulti-agentreinforcementlearn- PloSone,2019,14(12):e0226161. ing algorithms to solve the problem. Extensive experiments [19] Saidi-Mehrabad M, Dehnavi-Arani S, Evazabadian F, et al. An Ant\nColony Algorithm (ACA) for solving the new integrated model of job\ndemonstrate that our new approach can stimulate cooperation\nshop scheduling and conflict-free routing of AGVs[J]. Computers &\namong agents and give rise to a significant improvement Industrial Engineering, 2015,86:2-13.\nin both performance and convergence. For future work, we [20] TanM.Multi-agentreinforcementlearning:Independentvs.cooperative\nagents[C]//Proceedings ofthetenthinternationalconferenceonmachine\nwillcreatemoremulti-agentcoordinationandcommunications\nlearning. 1993:330-337.\nscenariosconsideringcomplexoperationsituationsanduncer- [21] MnihV, Kavukcuoglu K,Silver D,et al. Human-level control through\ntainties. Anotherinteresting and practicaldirection to develop deepreinforcement learning[J].",
      "size": 984,
      "sentences": 9
    },
    {
      "id": 50,
      "content": "complexoperationsituationsanduncer- [21] MnihV, Kavukcuoglu K,Silver D,et al. Human-level control through\ntainties. Anotherinteresting and practicaldirection to develop deepreinforcement learning[J]. nature, 2015,518(7540): 529-533. [22] Foerster J,Nardelli N,Farquhar G,et al. Stabilising experience replay\nistouseaheterogeneousagentsettingwithindividualspecific\nfordeepmulti-agentreinforcementlearning[C]//Proceedings ofthe34th\nfeature to improve collaboration. International ConferenceonMachineLearning-Volume70.JMLR.org,\n2017:1146-1155. ACKNOWLEDGMENT [23] Foerster J, Farquhar G, Afouras T, et al. Counterfactual multi-agent\npolicy gradients[C]//Proceedings oftheAAAIConference onArtificial\nThis work was partially supported by the National Science Intelligence. 2018,32(1). Fund for Distinguished Young Scholars(62025205), National [24] Wei W, Song H, Li W, et al.",
      "size": 870,
      "sentences": 11
    },
    {
      "id": 51,
      "content": "IConference onArtificial\nThis work was partially supported by the National Science Intelligence. 2018,32(1). Fund for Distinguished Young Scholars(62025205), National [24] Wei W, Song H, Li W, et al. Gradient-driven parking navigation\nusingacontinuous informationpotential fieldbasedonwirelesssensor\nKey R&D Program of China(2019YFB1703901), and the\nnetwork[J]. InformationSciences, 2017,408:100-114. NationalNaturalScienceFoundationofChina(No.62032020, [25] Lin H, Lu M, Milosavljevic N, et al. Composable information gradi-\n61960206008,61725205). ents in wireless sensor networks[C]//2008 International Conference on\nInformation Processing in Sensor Networks (ipsn 2008). IEEE, 2008:\n121-132. REFERENCES\n[26] WeiW,QiY.InformationpotentialfieldsnavigationinwirelessAd-Hoc\n[1] ZhangY,MaS,YangH,etal.Abigdatadrivenanalytical framework sensornetworks[J]. Sensors,2011,11(5):4794-4807.\nfor energy-intensive manufacturing industries[J]. Journal of Cleaner\nProduction, 2018,197:57-72.",
      "size": 979,
      "sentences": 12
    }
  ]
}