#!/usr/bin/env python3
"""
ChromaDB â†’ Faiss ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
================================

WMS ChromaDBì˜ ë²¡í„° ë°ì´í„°ë¥¼ Faiss í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬
ë³¸ í”„ë¡œì íŠ¸ì˜ Langchain Applicationì— í†µí•©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

ì‘ì„±ì: WMS ì—°êµ¬íŒ€
ë‚ ì§œ: 2024ë…„ 1ì›” 15ì¼
ë²„ì „: 1.0.0
"""

import os
import json
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
from datetime import datetime
import argparse

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    import chromadb
    import faiss
    import pandas as pd
    from sentence_transformers import SentenceTransformer
    from tqdm import tqdm
except ImportError as e:
    print(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("ì„¤ì¹˜: pip install chromadb faiss-cpu sentence-transformers pandas tqdm")
    exit(1)


class ChromaToFaissMigrator:
    """ChromaDB ë°ì´í„°ë¥¼ Faissë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 chroma_db_path: str = "../VectorDB/chroma_storage",
                 output_dir: str = "./faiss_output",
                 collection_name: str = "wms_research_papers"):
        """
        ë§ˆì´ê·¸ë ˆì´í„° ì´ˆê¸°í™”
        
        Args:
            chroma_db_path: ChromaDB ì €ì¥ ê²½ë¡œ
            output_dir: Faiss íŒŒì¼ ì¶œë ¥ ë””ë ‰í† ë¦¬
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
        """
        self.chroma_db_path = Path(chroma_db_path)
        self.output_dir = Path(output_dir)
        self.collection_name = collection_name
        
        self.setup_logging()
        self.setup_directories()
        self.setup_chromadb()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('chroma_to_faiss_migration.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_directories(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„: {self.output_dir}")
        
    def setup_chromadb(self):
        """ChromaDB ì—°ê²°"""
        self.logger.info("ChromaDB ì—°ê²° ì¤‘...")
        
        if not self.chroma_db_path.exists():
            raise FileNotFoundError(f"ChromaDBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.chroma_db_path}")
            
        try:
            self.chroma_client = chromadb.PersistentClient(path=str(self.chroma_db_path))
            self.collection = self.chroma_client.get_collection(self.collection_name)
            
            count = self.collection.count()
            self.logger.info(f"âœ… ChromaDB ì—°ê²° ì™„ë£Œ: {count}ê°œ ë¬¸ì„œ ë°œê²¬")
            
            if count == 0:
                raise ValueError("ChromaDBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                
        except Exception as e:
            self.logger.error(f"âŒ ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
            
    def extract_all_data(self) -> Tuple[np.ndarray, List[str], List[Dict]]:
        """ChromaDBì—ì„œ ëª¨ë“  ë²¡í„°, ë¬¸ì„œ, ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        self.logger.info("ChromaDBì—ì„œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        
        # ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        total_count = self.collection.count()
        batch_size = 1000  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
        
        all_embeddings = []
        all_documents = []
        all_metadatas = []
        all_ids = []
        
        self.logger.info(f"ì´ {total_count}ê°œ ë¬¸ì„œë¥¼ {batch_size} ë°°ì¹˜ë¡œ ì²˜ë¦¬...")
        
        # ë°°ì¹˜ë³„ë¡œ ë°ì´í„° ì¶”ì¶œ
        for offset in tqdm(range(0, total_count, batch_size), desc="ë°ì´í„° ì¶”ì¶œ"):
            limit = min(batch_size, total_count - offset)
            
            # ChromaDBì—ì„œ ë°°ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            batch_data = self.collection.get(
                limit=limit,
                offset=offset,
                include=['embeddings', 'documents', 'metadatas']
            )
            
            # ë°ì´í„° ëˆ„ì 
            if batch_data['embeddings']:
                all_embeddings.extend(batch_data['embeddings'])
            if batch_data['documents']:
                all_documents.extend(batch_data['documents'])
            if batch_data['metadatas']:
                all_metadatas.extend(batch_data['metadatas'])
            if batch_data['ids']:
                all_ids.extend(batch_data['ids'])
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        embeddings_array = np.array(all_embeddings, dtype=np.float32)
        
        self.logger.info(f"âœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ:")
        self.logger.info(f"   - ë²¡í„° ìˆ˜: {len(all_embeddings)}")
        self.logger.info(f"   - ë²¡í„° ì°¨ì›: {embeddings_array.shape[1] if len(all_embeddings) > 0 else 0}")
        self.logger.info(f"   - ë¬¸ì„œ ìˆ˜: {len(all_documents)}")
        self.logger.info(f"   - ë©”íƒ€ë°ì´í„° ìˆ˜: {len(all_metadatas)}")
        
        return embeddings_array, all_documents, all_metadatas, all_ids
    
    def create_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        """Faiss ì¸ë±ìŠ¤ ìƒì„±"""
        self.logger.info("Faiss ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        dimension = embeddings.shape[1]
        n_vectors = embeddings.shape[0]
        
        self.logger.info(f"ë²¡í„° ì°¨ì›: {dimension}, ë²¡í„° ìˆ˜: {n_vectors}")
        
        # ì¸ë±ìŠ¤ íƒ€ì… ê²°ì • (ë²¡í„° ìˆ˜ì— ë”°ë¼)
        if n_vectors < 10000:
            # ì‘ì€ ë°ì´í„°ì…‹: Flat (ì •í™•í•œ ê²€ìƒ‰)
            index = faiss.IndexFlatIP(dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            self.logger.info("Faiss IndexFlatIP ì‚¬ìš© (ì •í™•í•œ ê²€ìƒ‰)")
        else:
            # í° ë°ì´í„°ì…‹: IVF (ë¹ ë¥¸ ê·¼ì‚¬ ê²€ìƒ‰)
            nlist = min(100, n_vectors // 100)  # í´ëŸ¬ìŠ¤í„° ìˆ˜
            quantizer = faiss.IndexFlatIP(dimension)
            index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            self.logger.info(f"Faiss IndexIVFFlat ì‚¬ìš© (nlist={nlist})")
            
            # ì¸ë±ìŠ¤ í›ˆë ¨
            self.logger.info("ì¸ë±ìŠ¤ í›ˆë ¨ ì¤‘...")
            index.train(embeddings)
        
        # ë²¡í„° ì¶”ê°€
        self.logger.info("ë²¡í„°ë¥¼ ì¸ë±ìŠ¤ì— ì¶”ê°€ ì¤‘...")
        index.add(embeddings)
        
        self.logger.info(f"âœ… Faiss ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        return index
    
    def save_faiss_data(self, 
                       index: faiss.Index, 
                       documents: List[str], 
                       metadatas: List[Dict],
                       ids: List[str]):
        """Faiss ì¸ë±ìŠ¤ì™€ ê´€ë ¨ ë°ì´í„° ì €ì¥"""
        self.logger.info("Faiss ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # 1. Faiss ì¸ë±ìŠ¤ ì €ì¥
        index_file = self.output_dir / "wms_knowledge.index"
        faiss.write_index(index, str(index_file))
        self.logger.info(f"âœ… Faiss ì¸ë±ìŠ¤ ì €ì¥: {index_file}")
        
        # 2. ë¬¸ì„œ í…ìŠ¤íŠ¸ ì €ì¥
        documents_file = self.output_dir / "documents.json"
        with open(documents_file, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… ë¬¸ì„œ í…ìŠ¤íŠ¸ ì €ì¥: {documents_file}")
        
        # 3. ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_file = self.output_dir / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadatas, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_file}")
        
        # 4. ID ë§¤í•‘ ì €ì¥
        ids_file = self.output_dir / "ids.json"
        with open(ids_file, 'w', encoding='utf-8') as f:
            json.dump(ids, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… ID ë§¤í•‘ ì €ì¥: {ids_file}")
        
        # 5. í†µí•© ì •ë³´ ì €ì¥
        info = {
            "migration_date": datetime.now().isoformat(),
            "source_collection": self.collection_name,
            "total_vectors": index.ntotal,
            "vector_dimension": index.d,
            "index_type": type(index).__name__,
            "files": {
                "index": "wms_knowledge.index",
                "documents": "documents.json", 
                "metadata": "metadata.json",
                "ids": "ids.json"
            },
            "usage_example": {
                "python": """
# Faiss ì¸ë±ìŠ¤ ë¡œë“œ ì˜ˆì‹œ
import faiss
import json

# ì¸ë±ìŠ¤ ë¡œë“œ
index = faiss.read_index('wms_knowledge.index')

# ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ
with open('documents.json', 'r', encoding='utf-8') as f:
    documents = json.load(f)
with open('metadata.json', 'r', encoding='utf-8') as f:
    metadata = json.load(f)

# ê²€ìƒ‰ ì˜ˆì‹œ
query_vector = ...  # ì¿¼ë¦¬ ë²¡í„°
k = 5  # ìƒìœ„ 5ê°œ ê²°ê³¼
distances, indices = index.search(query_vector, k)
"""
            }
        }
        
        info_file = self.output_dir / "migration_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì •ë³´ ì €ì¥: {info_file}")
        
    def test_faiss_search(self, index: faiss.Index, documents: List[str], test_query: str = "warehouse automation"):
        """Faiss ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"ğŸ” Faiss ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{test_query}'")
        
        try:
            # ì„ë² ë”© ëª¨ë¸ë¡œ ì¿¼ë¦¬ ë²¡í„°í™” (ChromaDBì™€ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©)
            model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            query_vector = model.encode([test_query], convert_to_numpy=True).astype(np.float32)
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            k = 5
            distances, indices = index.search(query_vector, k)
            
            self.logger.info(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {k}ê°œ):")
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx < len(documents):
                    doc_preview = documents[idx][:200] + "..." if len(documents[idx]) > 200 else documents[idx]
                    self.logger.info(f"\n{i+1}. ìœ ì‚¬ë„: {distance:.4f}")
                    self.logger.info(f"   ë¬¸ì„œ: {doc_preview}")
                    
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def run_migration(self, test_search: bool = True):
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.logger.info("ğŸš€ ChromaDB â†’ Faiss ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        try:
            # 1. ë°ì´í„° ì¶”ì¶œ
            embeddings, documents, metadatas, ids = self.extract_all_data()
            
            # 2. Faiss ì¸ë±ìŠ¤ ìƒì„±
            index = self.create_faiss_index(embeddings)
            
            # 3. ë°ì´í„° ì €ì¥
            self.save_faiss_data(index, documents, metadatas, ids)
            
            # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            if test_search:
                self.test_faiss_search(index, documents)
            
            # 5. ë§ˆì´ê·¸ë ˆì´ì…˜ ë³´ê³ ì„œ ìƒì„±
            self.generate_migration_report(len(embeddings), embeddings.shape[1] if len(embeddings) > 0 else 0)
            
            self.logger.info("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
            
        except Exception as e:
            self.logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            raise
    
    def generate_migration_report(self, vector_count: int, dimension: int):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë³´ê³ ì„œ ìƒì„±"""
        report = f"""
WMS ChromaDB â†’ Faiss ë§ˆì´ê·¸ë ˆì´ì…˜ ë³´ê³ ì„œ
=====================================

ë§ˆì´ê·¸ë ˆì´ì…˜ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ì†ŒìŠ¤: ChromaDB ({self.collection_name})
ëŒ€ìƒ: Faiss Index

ë°ì´í„° í†µê³„:
- ì´ ë²¡í„° ìˆ˜: {vector_count:,}
- ë²¡í„° ì°¨ì›: {dimension}
- ì†ŒìŠ¤ ê²½ë¡œ: {self.chroma_db_path}
- ì¶œë ¥ ê²½ë¡œ: {self.output_dir}

ìƒì„±ëœ íŒŒì¼:
- wms_knowledge.index: Faiss ë²¡í„° ì¸ë±ìŠ¤
- documents.json: ì›ë³¸ ë¬¸ì„œ í…ìŠ¤íŠ¸
- metadata.json: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
- ids.json: ë¬¸ì„œ ID ë§¤í•‘
- migration_info.json: ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„¸ ì •ë³´

ë³¸ í”„ë¡œì íŠ¸ í†µí•© ë°©ë²•:
1. wms_knowledge.index íŒŒì¼ì„ ë³¸ í”„ë¡œì íŠ¸ì˜ ë²¡í„°DB ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
2. documents.json, metadata.jsonë„ í•¨ê»˜ ë³µì‚¬
3. Langchain Applicationì—ì„œ Faiss ì¸ë±ìŠ¤ ë¡œë“œ ì½”ë“œ ìˆ˜ì •
4. ê¸°ì¡´ Faiss ì¸ë±ìŠ¤ë¥¼ WMS ì „ë¬¸ì§€ì‹ ì¸ë±ìŠ¤ë¡œ êµì²´

ë‹¤ìŒ ë‹¨ê³„:
1. ë³¸ í”„ë¡œì íŠ¸ì˜ Langchain ì½”ë“œì—ì„œ Faiss ë¡œë“œ ë¶€ë¶„ í™•ì¸
2. ë²¡í„° ê²€ìƒ‰ ë¡œì§ì´ WMS ë„ë©”ì¸ì— ë§ê²Œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë¬¼ë¥˜/WMS ì „ë¬¸ ìš©ì–´ì— ë§ê²Œ ì¡°ì •

ì£¼ì˜ì‚¬í•­:
- ì„ë² ë”© ëª¨ë¸ì´ ë³¸ í”„ë¡œì íŠ¸ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”
- ë²¡í„° ì°¨ì›ì´ í˜¸í™˜ë˜ëŠ”ì§€ ê²€ì¦ í•„ìš”
- ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ íŠœë‹ ê³ ë ¤
"""
        
        report_file = self.output_dir / "migration_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(report)
        self.logger.info(f"ğŸ“„ ë§ˆì´ê·¸ë ˆì´ì…˜ ë³´ê³ ì„œ ì €ì¥: {report_file}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ChromaDB â†’ Faiss ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬")
    parser.add_argument("--chroma-db", default="../VectorDB/chroma_storage",
                       help="ChromaDB ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--output-dir", default="./faiss_output",
                       help="Faiss íŒŒì¼ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--collection", default="wms_research_papers",
                       help="ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„")
    parser.add_argument("--no-test", action="store_true",
                       help="ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°")
    
    args = parser.parse_args()
    
    # ë§ˆì´ê·¸ë ˆì´í„° ì‹¤í–‰
    migrator = ChromaToFaissMigrator(
        chroma_db_path=args.chroma_db,
        output_dir=args.output_dir,
        collection_name=args.collection
    )
    
    migrator.run_migration(test_search=not args.no_test)


if __name__ == "__main__":
    main()
