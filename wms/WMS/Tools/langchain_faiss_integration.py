#!/usr/bin/env python3
"""
Langchain Faiss í†µí•© ì½”ë“œ
========================

ë§ˆì´ê·¸ë ˆì´ì…˜ëœ WMS ì „ë¬¸ì§€ì‹ Faiss ì¸ë±ìŠ¤ë¥¼ 
ë³¸ í”„ë¡œì íŠ¸ì˜ Langchain Applicationì— í†µí•©í•˜ê¸° ìœ„í•œ ì½”ë“œ

ì‘ì„±ì: WMS ì—°êµ¬íŒ€
ë‚ ì§œ: 2024ë…„ 1ì›” 15ì¼
"""

import os
import json
import faiss
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging

# Langchain ê´€ë ¨ ì„í¬íŠ¸
try:
    from langchain.vectorstores import FAISS
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.schema import Document
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
    from langchain_community.llms import Ollama
    from sentence_transformers import SentenceTransformer
except ImportError as e:
    print(f"Langchain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: {e}")
    print("ì„¤ì¹˜: pip install langchain langchain-community sentence-transformers")
    exit(1)


class WMSFaissVectorStore:
    """WMS ì „ë¬¸ì§€ì‹ Faiss ë²¡í„°ìŠ¤í† ì–´ ë˜í¼"""
    
    def __init__(self, 
                 faiss_index_path: str,
                 documents_path: str,
                 metadata_path: str,
                 embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        WMS Faiss ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        
        Args:
            faiss_index_path: Faiss ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
            documents_path: ë¬¸ì„œ JSON íŒŒì¼ ê²½ë¡œ
            metadata_path: ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ
            embedding_model_name: ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.faiss_index_path = Path(faiss_index_path)
        self.documents_path = Path(documents_path)
        self.metadata_path = Path(metadata_path)
        self.embedding_model_name = embedding_model_name
        
        self.setup_logging()
        self.load_data()
        self.setup_embeddings()
        self.create_langchain_vectorstore()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def load_data(self):
        """Faiss ì¸ë±ìŠ¤ì™€ ê´€ë ¨ ë°ì´í„° ë¡œë“œ"""
        self.logger.info("WMS Faiss ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # Faiss ì¸ë±ìŠ¤ ë¡œë“œ
        if not self.faiss_index_path.exists():
            raise FileNotFoundError(f"Faiss ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.faiss_index_path}")
        
        self.index = faiss.read_index(str(self.faiss_index_path))
        self.logger.info(f"âœ… Faiss ì¸ë±ìŠ¤ ë¡œë“œ: {self.index.ntotal}ê°œ ë²¡í„°")
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¡œë“œ
        with open(self.documents_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)
        self.logger.info(f"âœ… ë¬¸ì„œ ë¡œë“œ: {len(self.documents)}ê°œ")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        self.logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ: {len(self.metadata)}ê°œ")
        
    def setup_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì„¤ì •"""
        self.logger.info(f"ì„ë² ë”© ëª¨ë¸ ì„¤ì •: {self.embedding_model_name}")
        
        # HuggingFace ì„ë² ë”© (Langchain í˜¸í™˜)
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.embedding_model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # SentenceTransformer (ì§ì ‘ ì‚¬ìš©)
        self.sentence_model = SentenceTransformer(self.embedding_model_name)
        
    def create_langchain_vectorstore(self):
        """Langchain FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
        self.logger.info("Langchain FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        
        # Document ê°ì²´ ìƒì„±
        docs = []
        for i, (doc_text, meta) in enumerate(zip(self.documents, self.metadata)):
            doc = Document(
                page_content=doc_text,
                metadata={
                    'paper_filename': meta.get('paper_filename', 'unknown'),
                    'paper_source': meta.get('paper_source', 'unknown'),
                    'chunk_id': meta.get('chunk_id', i),
                    'chunk_size': meta.get('chunk_size', len(doc_text)),
                    'sentences': meta.get('sentences', 0)
                }
            )
            docs.append(doc)
        
        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš©)
        self.vectorstore = FAISS.from_documents(
            documents=docs,
            embedding=self.embeddings
        )
        
        # ê¸°ì¡´ Faiss ì¸ë±ìŠ¤ë¡œ êµì²´
        self.vectorstore.index = self.index
        
        self.logger.info("âœ… Langchain FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        
    def similarity_search(self, query: str, k: int = 5) -> List[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        return self.vectorstore.similarity_search(query, k=k)
    
    def similarity_search_with_score(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """ì ìˆ˜ì™€ í•¨ê»˜ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        return self.vectorstore.similarity_search_with_score(query, k=k)
    
    def get_retriever(self, search_kwargs: Dict = None):
        """Langchain Retriever ë°˜í™˜"""
        if search_kwargs is None:
            search_kwargs = {"k": 5}
        return self.vectorstore.as_retriever(search_kwargs=search_kwargs)


class WMSRAGChain:
    """WMS ì „ë¬¸ì§€ì‹ ê¸°ë°˜ RAG ì²´ì¸"""
    
    def __init__(self, 
                 vectorstore: WMSFaissVectorStore,
                 llm_model: str = "llama3.1",
                 llm_base_url: str = "http://localhost:11434"):
        """
        WMS RAG ì²´ì¸ ì´ˆê¸°í™”
        
        Args:
            vectorstore: WMS Faiss ë²¡í„°ìŠ¤í† ì–´
            llm_model: LLM ëª¨ë¸ëª…
            llm_base_url: LLM ì„œë²„ URL
        """
        self.vectorstore = vectorstore
        self.llm_model = llm_model
        self.llm_base_url = llm_base_url
        
        self.setup_llm()
        self.setup_prompt()
        self.create_rag_chain()
        
    def setup_llm(self):
        """LLM ì„¤ì •"""
        self.llm = Ollama(
            model=self.llm_model,
            base_url=self.llm_base_url,
            temperature=0.1
        )
        
    def setup_prompt(self):
        """WMS ì „ë¬¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        template = """
ë‹¹ì‹ ì€ WMS(ì°½ê³ ê´€ë¦¬ì‹œìŠ¤í…œ) ë° ë¬¼ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ WMS ì—°êµ¬ ë…¼ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì œê³µëœ ì—°êµ¬ ìë£Œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ë…¼ë¬¸ëª…ê³¼ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”
3. WMS/ë¬¼ë¥˜ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
4. ì‹¤ë¬´ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”
5. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "ì œê³µëœ ìë£Œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”

ë‹µë³€:
"""
        
        self.prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
    def create_rag_chain(self):
        """RAG ì²´ì¸ ìƒì„±"""
        retriever = self.vectorstore.get_retriever(
            search_kwargs={"k": 5}
        )
        
        self.rag_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": self.prompt},
            return_source_documents=True
        )
        
    def ask(self, question: str) -> Dict:
        """ì§ˆë¬¸í•˜ê¸°"""
        result = self.rag_chain({"query": question})
        
        return {
            "answer": result["result"],
            "source_documents": [
                {
                    "content": doc.page_content[:300] + "...",
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ]
        }


def create_wms_rag_system(faiss_data_dir: str) -> WMSRAGChain:
    """WMS RAG ì‹œìŠ¤í…œ ìƒì„± (ì›ìŠ¤í†± í•¨ìˆ˜)"""
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    faiss_data_path = Path(faiss_data_dir)
    index_path = faiss_data_path / "wms_knowledge.index"
    documents_path = faiss_data_path / "documents.json"
    metadata_path = faiss_data_path / "metadata.json"
    
    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = WMSFaissVectorStore(
        faiss_index_path=str(index_path),
        documents_path=str(documents_path),
        metadata_path=str(metadata_path)
    )
    
    # RAG ì²´ì¸ ìƒì„±
    rag_chain = WMSRAGChain(vectorstore)
    
    return rag_chain


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    print("ğŸš€ WMS Langchain Faiss í†µí•© í…ŒìŠ¤íŠ¸")
    
    # WMS RAG ì‹œìŠ¤í…œ ìƒì„±
    try:
        rag_system = create_wms_rag_system("./faiss_output")
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_questions = [
            "WMSì—ì„œ ê°€ì¥ íš¨ìœ¨ì ì¸ ì°½ê³  ìë™í™” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "AGVì™€ ë¡œë´‡ì„ í™œìš©í•œ ë¬¼ë¥˜ ìµœì í™” ì „ëµì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ì°½ê³  ê´€ë¦¬ì—ì„œ IoT ê¸°ìˆ ì˜ ROIëŠ” ì–´ë–»ê²Œ ì¸¡ì •í•˜ë‚˜ìš”?",
            "Amazonì˜ ì°½ê³  ë¡œë´‡ ì‹œìŠ¤í…œê³¼ ë¹„êµí–ˆì„ ë•Œ ìš°ë¦¬ê°€ ì§‘ì¤‘í•´ì•¼ í•  ê¸°ìˆ ì€?"
        ]
        
        for question in test_questions:
            print(f"\nğŸ’¬ ì§ˆë¬¸: {question}")
            print("-" * 80)
            
            result = rag_system.ask(question)
            
            print(f"ğŸ¤– ë‹µë³€:\n{result['answer']}")
            print(f"\nğŸ“š ì°¸ê³  ë…¼ë¬¸:")
            for i, source in enumerate(result['source_documents'], 1):
                print(f"{i}. {source['metadata']['paper_filename']}")
                print(f"   ì²­í¬ #{source['metadata']['chunk_id']}")
                print(f"   ë‚´ìš©: {source['content']}")
            print("=" * 80)
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        print("ë¨¼ì € chroma_to_faiss_migrator.pyë¥¼ ì‹¤í–‰í•˜ì—¬ Faiss ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
