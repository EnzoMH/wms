#!/usr/bin/env python3
"""
WMS ChatOllama RAG ì‹œìŠ¤í…œ
========================

ChromaDB + ChatOllamaë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ
EXAONE, gpt-oss ë“± ë¡œì»¬ ëª¨ë¸ ì§€ì›

ì‘ì„±ì: WMS ì—°êµ¬íŒ€  
ë‚ ì§œ: 2024ë…„ 1ì›” 15ì¼
"""

import os
import sys
from pathlib import Path

# ChromaDBì™€ LangChain ì„í¬íŠ¸
try:
    import chromadb
    from langchain_community.llms import Ollama
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    print("âœ… í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    print(f"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì˜¤ë¥˜: {e}")
    print("ì„¤ì¹˜: pip install chromadb langchain-community")
    exit(1)


class WMSChatOllamaRAG:
    """ChromaDB + ChatOllama RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_name="hf.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-GGUF:BF16"):
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  Ollama ëª¨ë¸ëª…
        """
        self.model_name = model_name
        print(f"ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {model_name}")
        
        # ChromaDB ì—°ê²°
        self.setup_chromadb()
        
        # ChatOllama LLM ì„¤ì •
        self.setup_llm()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        self.setup_prompt()
        
    def setup_chromadb(self):
        """ChromaDB ì—°ê²°"""
        print("ğŸ“Š ChromaDB ì—°ê²° ì¤‘...")
        
        # ChromaDB ê²½ë¡œ (WMS/VectorDBì—ì„œ ì°¾ê¸°)
        vector_db_path = Path("../WMS/VectorDB/chroma_storage")
        
        if not vector_db_path.exists():
            print("âŒ ChromaDBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("ë¨¼ì € WMS/Tools/chromadb_builder.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            exit(1)
        
        try:
            self.chroma_client = chromadb.PersistentClient(path=str(vector_db_path))
            self.collection = self.chroma_client.get_collection("wms_research_papers")
            
            count = self.collection.count()
            print(f"âœ… ChromaDB ì—°ê²° ì™„ë£Œ: {count:,}ê°œ ì²­í¬ ë¡œë“œë¨")
            
        except Exception as e:
            print(f"âŒ ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
            exit(1)
    
    def setup_llm(self):
        """ChatOllama LLM ì„¤ì •"""
        print("ğŸ§  ChatOllama ì„¤ì • ì¤‘...")
        
        try:
            self.llm = Ollama(
                model=self.model_name,
                base_url="http://localhost:11434",
                temperature=0.1,  # ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
                num_predict=2048   # ê¸´ ë‹µë³€ í—ˆìš©
            )
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            test_response = self.llm.invoke("ì•ˆë…•í•˜ì„¸ìš”")
            print(f"âœ… ChatOllama ì—°ê²° ì„±ê³µ!")
            
        except Exception as e:
            print(f"âŒ ChatOllama ì—°ê²° ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")
            exit(1)
    
    def setup_prompt(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        template = """
ë‹¹ì‹ ì€ WMS(ì°½ê³ ê´€ë¦¬ì‹œìŠ¤í…œ) ì „ë¬¸ ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ì—°êµ¬ ë…¼ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ê´€ë ¨ ì—°êµ¬ ìë£Œ:
{context}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì œê³µëœ ì—°êµ¬ ìë£Œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ë…¼ë¬¸ëª…ì„ ì¸ìš©í•˜ì„¸ìš”
3. ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”  
4. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
5. í™•ì‹ ì´ ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ìë£Œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”

ë‹µë³€:
"""
        
        self.prompt = PromptTemplate(
            input_variables=["question", "context"],
            template=template
        )
        
        # LangChain ì²´ì¸ ìƒì„±
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)
    
    def search_relevant_docs(self, question, top_k=5):
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰"""
        print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘... (ìƒìœ„ {top_k}ê°œ)")
        
        try:
            results = self.collection.query(
                query_texts=[question],
                n_results=top_k
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
            docs = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0], 
                results['distances'][0]
            )):
                docs.append({
                    'content': doc,
                    'paper': metadata['paper_filename'],
                    'chunk_id': metadata['chunk_id'],
                    'similarity': 1 - distance
                })
            
            print(f"âœ… {len(docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
            return docs
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def ask(self, question):
        """ì§ˆë¬¸í•˜ê¸°"""
        print(f"\nğŸ’¬ ì§ˆë¬¸: {question}")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.search_relevant_docs(question, top_k=5)
        
        if not relevant_docs:
            return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = "\n\n".join([
            f"ğŸ“„ ë…¼ë¬¸: {doc['paper']}\n"
            f"ìœ ì‚¬ë„: {doc['similarity']:.3f}\n"
            f"ë‚´ìš©: {doc['content'][:500]}..."
            for doc in relevant_docs
        ])
        
        print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        
        # 3ë‹¨ê³„: LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        try:
            response = self.chain.run(question=question, context=context)
            
            print(f"ğŸ’¡ ë‹µë³€:\n{response}")
            
            print(f"\nğŸ“š ì°¸ê³  ë…¼ë¬¸:")
            for doc in relevant_docs:
                print(f"- {doc['paper']} (ìœ ì‚¬ë„: {doc['similarity']:.3f})")
            
            return response
            
        except Exception as e:
            error_msg = f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}"
            print(error_msg)
            return error_msg
    
    def chat_interactive(self):
        """ëŒ€í™”í˜• ì±„íŒ…"""
        print("\nğŸš€ WMS ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ ì‹œì‘!")
        print(f"ğŸ“Š ë¡œë“œëœ ë°ì´í„°: {self.collection.count():,}ê°œ ì²­í¬")
        print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {self.model_name}")
        print("\nğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:")
        print("- WMSì—ì„œ ê°€ì¥ íš¨ìœ¨ì ì¸ ì°½ê³  ìë™í™” ë°©ë²•ì€?")
        print("- Amazon ë¡œë´‡ ì‹œìŠ¤í…œê³¼ ë¹„êµí•œ ìš°ë¦¬ì˜ ê²½ìŸ ìš°ìœ„ëŠ”?")
        print("- 2024ë…„ WMS ê¸°ìˆ  íŠ¸ë Œë“œëŠ”?")
        print("- ì°½ê³  ë¡œë´‡ì˜ ROI ë¶„ì„ ê²°ê³¼ëŠ”?")
        print("\nì¢…ë£Œ: 'quit', 'exit', 'q' ì…ë ¥")
        print("=" * 80)
        
        while True:
            try:
                question = input("\nğŸ™‹â€â™‚ï¸ ì§ˆë¬¸: ").strip()
                
                if question.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                    print("ğŸ‘‹ WMS ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                
                if not question:
                    continue
                
                # ì§ˆë¬¸ ì²˜ë¦¬
                self.ask(question)
                print("=" * 80)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ WMS ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def quick_test(self):
        """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
        test_questions = [
            "WMS ì‹œìŠ¤í…œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê¸°ìˆ ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì°½ê³  ìë™í™”ì˜ ìµœì‹  íŠ¸ë Œë“œëŠ”?",
            "ë¡œë´‡ì„ í™œìš©í•œ ë¬¼ë¥˜ ìµœì í™” ë°©ë²•ì€?"
        ]
        
        print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n--- í…ŒìŠ¤íŠ¸ {i}/3 ---")
            self.ask(question)
            
        print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤
    available_models = [
        "hf.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-GGUF:BF16",  # ì¶”ì²œ
        "hf.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF:BF16",
        "gpt-oss:20b"
    ]
    
    print("ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
    for i, model in enumerate(available_models, 1):
        print(f"{i}. {model}")
    
    try:
        choice = input(f"\nëª¨ë¸ ì„ íƒ (1-{len(available_models)}, ê¸°ë³¸ê°’: 1): ").strip()
        if not choice:
            choice = "1"
        
        model_index = int(choice) - 1
        selected_model = available_models[model_index]
        
    except (ValueError, IndexError):
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        selected_model = available_models[0]
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = WMSChatOllamaRAG(model_name=selected_model)
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    mode = input("\nì‹¤í–‰ ëª¨ë“œ ì„ íƒ (1: ëŒ€í™”í˜•, 2: í…ŒìŠ¤íŠ¸, ê¸°ë³¸ê°’: 1): ").strip()
    
    if mode == "2":
        rag.quick_test()
    else:
        rag.chat_interactive()


if __name__ == "__main__":
    main()
