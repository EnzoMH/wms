#!/usr/bin/env python3
"""
WMS Faiss Repository
===================

VSS-AI-API-devì—ì„œ ì‚¬ìš©í•  WMS Faiss ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ í´ëž˜ìŠ¤
"""

import faiss
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class WMSFaissRepository:
    """WMS Faiss ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ í´ëž˜ìŠ¤"""
    
    def __init__(self, faiss_storage_path: str):
        """
        WMS Faiss Repository ì´ˆê¸°í™”
        
        Args:
            faiss_storage_path: Faiss ì €ìž¥ì†Œ ê²½ë¡œ (faiss_storage í´ë”)
        """
        self.storage_path = Path(faiss_storage_path)
        self.index = None
        self.documents = []
        self.metadatas = []
        self.embedding_model = None
        
        self._load_index()
        self._load_documents()
        self._setup_embedding_model()
        
        logger.info(f"âœ… WMS Faiss Repository ì´ˆê¸°í™” ì™„ë£Œ: {len(self.documents)} ë¬¸ì„œ")
    
    def _load_index(self):
        """Faiss ì¸ë±ìŠ¤ ë¡œë“œ"""
        index_path = self.storage_path / "wms_knowledge.index"
        if not index_path.exists():
            raise FileNotFoundError(f"Faiss ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_path}")
        
        self.index = faiss.read_index(str(index_path))
        logger.info(f"ðŸ“Š Faiss ì¸ë±ìŠ¤ ë¡œë“œ: {self.index.ntotal} ë²¡í„°, {self.index.d} ì°¨ì›")
    
    def _load_documents(self):
        """ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        # ë¬¸ì„œ ë¡œë“œ
        documents_path = self.storage_path / "documents.json"
        with open(documents_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = self.storage_path / "metadata.json"
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadatas = json.load(f)
        
        logger.info(f"ðŸ“š ë¬¸ì„œ ë¡œë“œ: {len(self.documents)} ê°œ")
    
    def _setup_embedding_model(self):
        """ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from langchain_huggingface import HuggingFaceEmbeddings
            import torch
            
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            self.embedding_model = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            logger.info(f"ðŸ¤– ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
            
        except Exception as e:
            logger.error(f"âŒ ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì¿¼ë¦¬ ìž„ë² ë”©
            query_embedding = self.embedding_model.embed_query(query)
            query_vector = np.array(query_embedding, dtype=np.float32).reshape(1, -1)
            
            # Faiss ê²€ìƒ‰
            scores, indices = self.index.search(query_vector, top_k)
            
            # ê²°ê³¼ êµ¬ì„±
            results = []
            for i, (idx, score) in enumerate(zip(indices[0], scores[0])):
                if idx < len(self.documents):
                    similarity = float(1 / (1 + score))  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    
                    results.append({
                        'rank': i + 1,
                        'document': self.documents[idx],
                        'metadata': self.metadatas[idx],
                        'score': float(score),
                        'similarity': similarity
                    })
            
            logger.info(f"ðŸ” ê²€ìƒ‰ ì™„ë£Œ: '{query}' -> {len(results)} ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_document_by_id(self, doc_id: str) -> Dict[str, Any]:
        """ë¬¸ì„œ IDë¡œ íŠ¹ì • ë¬¸ì„œ ì¡°íšŒ"""
        for i, metadata in enumerate(self.metadatas):
            if metadata.get('id') == doc_id:
                return {
                    'document': self.documents[i],
                    'metadata': metadata
                }
        return None
    
    def get_papers_by_source(self, source: str) -> List[str]:
        """ì†ŒìŠ¤ë³„ ë…¼ë¬¸ ëª©ë¡ ì¡°íšŒ"""
        papers = set()
        for metadata in self.metadatas:
            if metadata.get('paper_source', '').lower() == source.lower():
                papers.add(metadata.get('paper_filename', ''))
        return list(papers)
    
    def get_database_stats(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´"""
        # ë…¼ë¬¸ë³„ í†µê³„
        papers = {}
        sources = {}
        
        for metadata in self.metadatas:
            # ë…¼ë¬¸ë³„ ì²­í¬ ìˆ˜
            filename = metadata.get('paper_filename', 'unknown')
            if filename not in papers:
                papers[filename] = 0
            papers[filename] += 1
            
            # ì†ŒìŠ¤ë³„ ë…¼ë¬¸ ìˆ˜
            source = metadata.get('paper_source', 'unknown')
            if source not in sources:
                sources[source] = set()
            sources[source].add(filename)
        
        # ì†ŒìŠ¤ë³„ ë…¼ë¬¸ ìˆ˜ ê³„ì‚°
        source_stats = {source: len(papers) for source, papers in sources.items()}
        
        return {
            'total_documents': len(self.documents),
            'total_papers': len(papers),
            'vector_dimension': self.index.d,
            'embedding_model': 'jhgan/ko-sroberta-multitask',
            'papers_by_source': source_stats,
            'top_papers': sorted(papers.items(), key=lambda x: x[1], reverse=True)[:10]
        }
    
    def search_by_paper(self, paper_filename: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë…¼ë¬¸ì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ"""
        results = []
        for i, metadata in enumerate(self.metadatas):
            if metadata.get('paper_filename') == paper_filename:
                results.append({
                    'chunk_id': metadata.get('chunk_id'),
                    'document': self.documents[i],
                    'metadata': metadata
                })
        
        # ì²­í¬ ID ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['chunk_id'])
        return results
    
    def get_similar_papers(self, paper_filename: str, top_k: int = 5) -> List[str]:
        """íŠ¹ì • ë…¼ë¬¸ê³¼ ìœ ì‚¬í•œ ë…¼ë¬¸ë“¤ ì°¾ê¸°"""
        # í•´ë‹¹ ë…¼ë¬¸ì˜ ì²« ë²ˆì§¸ ì²­í¬ë¡œ ê²€ìƒ‰
        paper_chunks = self.search_by_paper(paper_filename)
        if not paper_chunks:
            return []
        
        # ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš©ìœ¼ë¡œ ê²€ìƒ‰
        first_chunk = paper_chunks[0]['document']
        search_results = self.search(first_chunk, top_k * 2)  # ì—¬ìœ ìžˆê²Œ ê²€ìƒ‰
        
        # ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ë§Œ ì¶”ì¶œ
        similar_papers = []
        for result in search_results:
            result_paper = result['metadata']['paper_filename']
            if result_paper != paper_filename and result_paper not in similar_papers:
                similar_papers.append(result_paper)
                if len(similar_papers) >= top_k:
                    break
        
        return similar_papers


