{
  "source": "ArXiv",
  "filename": "041_Deep_Reinforcement_Learning_for_Single-Shot_Diagno.pdf",
  "total_chars": 37930,
  "total_chunks": 56,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nDeep Reinforcement Learning for Single-Shot Diagnosis and\nAdaptation in Damaged Robots\nSHRESTHVERMA,ABV-IndianInstituteofInformationTechnologyandManagement,Gwalior\nHARITHAS.NAIR,ABV-IndianInstituteofInformationTechnologyandManagement,Gwalior\nGAURAVAGARWAL,ABV-IndianInstituteofInformationTechnologyandManagement,Gwalior\nJOYDIPDHAR,ABV-IndianInstituteofInformationTechnologyandManagement,Gwalior\nANUPAMSHUKLA,ABV-IndianInstituteofInformationTechnologyandManagement,Gwalior\nRoboticshasprovedtobeanindispensabletoolinmanyindustrialaswell todamages,offlinelearningwouldmeantrainingarobustpolicybe-\nassocialapplications,suchaswarehouseautomation,manufacturing,dis- foretherobotisdeployedwhileonlinecapabilitiessuggestlearning\nasterrobotics,etc.Inmostofthesescenarios,damagetotheagentwhile toadaptatthetimeofdamage.Buttheenvironmentsandagents\naccomplishingmission-criticaltaskscanresultinfailure.Toenablerobotic inthesesituationsareverycomplexinnature,asaresultofwhich,\nadaptationinsuchsituations,theagentneedstoadoptpolicieswhichare\nretrainingtheRLpolicyeverytimeachangeoccursineitherof\nrobusttoadiversesetofdamagesandmustdosowithminimumcompu-\nthemishighlyimpractical.Thispointstothenecessityofhaving\ntationalcomplexity.Wethusproposeadamageawarecontrolarchitecture\nanefficientcontrolarchitecturewhichcanhelptheagentadaptin\nwhichdiagnosesthedamagepriortogaitselectionwhilealsoincorporating\nvaryingadversarialconditions.",
      "size": 1427,
      "sentences": 1
    },
    {
      "id": 2,
      "content": ".Wethusproposeadamageawarecontrolarchitecture\nanefficientcontrolarchitecturewhichcanhelptheagentadaptin\nwhichdiagnosesthedamagepriortogaitselectionwhilealsoincorporating\nvaryingadversarialconditions. domainrandomizationinthedamagespaceforlearningarobustpolicy.To\nimplementdamageawareness,wehaveusedaLongShortTermMemory Toimplementthis,severalapproacheshavetriedtolearnmultiple\nbasedsupervisedlearningnetworkwhichdiagnosesthedamageandpre- policiesattrainingtimeandthenchoosingfromthematthetime\ndictsthetypeofdamage.Themainnoveltyofthisapproachisthatonlya of damage. However, models which have made progress in this\nsinglepolicyistrainedtoadaptagainstawidevarietyofdamagesandthe domainrequireresetoftheagenttoinitialstate[Cullyetal.2015],\ndiagnosisisdoneinasingletrialatthetimeofdamage.",
      "size": 784,
      "sentences": 3
    },
    {
      "id": 3,
      "content": "h have made progress in this\nsinglepolicyistrainedtoadaptagainstawidevarietyofdamagesandthe domainrequireresetoftheagenttoinitialstate[Cullyetal.2015],\ndiagnosisisdoneinasingletrialatthetimeofdamage. ormultiplehardwaretrialsaretobeperformedtohelptheagent\nrecoveroradapt[BongardandLipson2004;Cullyetal.2015;Koos\nAdditionalKeyWordsandPhrases:ReinforcementLearning,DomainAdap-\netal.2013].Althoughthisisintuitive,itisinefficientconsideringthe\ntation,Damagerecovery,GaitSelection,LSTM\noverheadofchoosingfromasetofhighperforminggaits.Tomake\n1 INTRODUCTION asmartrecoverydecision,analternativemethodcanbeforagent\ntounderstandthedamagefirstandthenusethatdamageawareness\nOneofthemotivesofintroducingwastoprovideasafemethod toactoptimally.",
      "size": 729,
      "sentences": 2
    },
    {
      "id": 4,
      "content": "make\n1 INTRODUCTION asmartrecoverydecision,analternativemethodcanbeforagent\ntounderstandthedamagefirstandthenusethatdamageawareness\nOneofthemotivesofintroducingwastoprovideasafemethod toactoptimally. ofaccessandoperationinenvironmentsthatarehazardousand WethusproposeDamageAware-ProximalPolicyOptimization\nunreachabletohumans.Butveryoften,theseenvironmentsdesta- (DA-PPO),combiningdamagediagnosiswithdeepreinforcement\nbilizeordamagetherobotpartially,oftenimpairingthem,andthus learning.Thecontrolarchitecturefirstperformsdamagediagnosis\nleadingtoamissionfailureorsignificantdropinperformance.This onmultipledamagecasesusingaLongShortTermMemory(LSTM)\nisespeciallycriticalforrobotsdeployedinmanufacturingindustries [HochreiterandSchmidhuber1997],basedsupervisedlearningnet-\nandwarehouses[Khatib2005],searchandrescuemissions[Mur- work.Itusesthedifferencebetweenthegaitsofa(simulated)healthy\nphy2004]anddisasterresponse[Nagatanietal.2013].Although andadamagedrobotasinputandclassifiesthedamagethathas\nthissituationofpartialdamageistackledinhumansoranimalsby occurred,ifany.Thedataofthediagnoseddamageiscombined\ntheirlearningofalternatewaystoperformtheaction,thiskindof alongwiththecurrentobservationvectortocreateanaugmented\nlearninginrobotsrequires,whatwecall,intelligence.Hence,the observationspace,whichcontainsinformationofbothstatespace\nobjectivewhiledesigningroboticdevicesisnotjustrestrictedto observationaswellasdamage.Thisaugmentedobservationisused\navoidingortacklingobstacles,italsoincludestheadaptationof totrainourRLmodel,whichisoptimizedusingProximalPolicy\ntheagentinpresenceofadversaries,bothintheformofinternal Optimization(PPO)[Schulmanetal.2017].Thetrainedmodelshall\ndamagesaswellasexternaleffects.",
      "size": 1711,
      "sentences": 2
    },
    {
      "id": 5,
      "content": "totrainourRLmodel,whichisoptimizedusingProximalPolicy\ntheagentinpresenceofadversaries,bothintheformofinternal Optimization(PPO)[Schulmanetal.2017].Thetrainedmodelshall\ndamagesaswellasexternaleffects. beabletounderstandthedamagethathasoccurredandchooseits\nDeepReinforcementlearning(DeepRL)hasbeenshowntobe gaitaccordingly.Since,onlyasinglepolicyislearnt,thereisno\neffectiveinmodelingsuchnavigationproblemsbecauseofbothits overheadofstoringandchoosingbetweenmultiplepolicies,making\nonlineandofflinelearningcapabilitiesinhighdimensionalsearch ouralgorithmeffectiveinrealtime.",
      "size": 572,
      "sentences": 2
    },
    {
      "id": 6,
      "content": "nmodelingsuchnavigationproblemsbecauseofbothits overheadofstoringandchoosingbetweenmultiplepolicies,making\nonlineandofflinelearningcapabilitiesinhighdimensionalsearch ouralgorithmeffectiveinrealtime. spaces[Chatzilygeroudisetal.2018;Hwangboetal.2017;Lobos- Themajorobjectivesofourworkare:\nTsunekawaetal.2018;Pintoetal.2017a].Inthecontextofadapting\nAuthors’addresses:ShresthVerma,ABV-IndianInstituteofInformationTechnology (1) Tocreateadeepreinforcementlearningbasedcontrolarchi-\nandManagement,Gwalior,vermashresth@gmail.com;HarithaS.Nair,ABV-IndianIn- tectureforenablinglocomotoryagentstoaccomplishmission-\nstituteofInformationTechnologyandManagement,Gwalior,haritha1313@gmail.com;\nGauravAgarwal,ABV-IndianInstituteofInformationTechnologyandManagement, criticaltaskseveninthepresenceofsingleormultipleinter-\nGwalior,gaurava05@gmail.com;JoydipDhar,ABV-IndianInstituteofInformation naldamages.",
      "size": 891,
      "sentences": 2
    },
    {
      "id": 7,
      "content": "wal,ABV-IndianInstituteofInformationTechnologyandManagement, criticaltaskseveninthepresenceofsingleormultipleinter-\nGwalior,gaurava05@gmail.com;JoydipDhar,ABV-IndianInstituteofInformation naldamages. TechnologyandManagement,Gwalior,jdhar@iiitm.ac.in;AnupamShukla,ABV-Indian\n(2) Tooptimizethecontrolarchitecturesothattheagentadapts\nInstituteofInformationTechnologyandManagement,Gwalior,anupamshukla@iiitm. ac.in. itsgaitinasinglehardwaretrial.",
      "size": 442,
      "sentences": 4
    },
    {
      "id": 8,
      "content": ".ac.in;AnupamShukla,ABV-Indian\n(2) Tooptimizethecontrolarchitecturesothattheagentadapts\nInstituteofInformationTechnologyandManagement,Gwalior,anupamshukla@iiitm. ac.in. itsgaitinasinglehardwaretrial. 9102\ntcO\n2\n]GL.sc[\n1v04210.0191:viXra\n=== 페이지 2 ===\n2 • ShresthVerma,HarithaS.Nair,GauravAgarwal,JoydipDhar,andAnupamShukla\n2 RELATEDWORK\nonadamagedrobot,whichistruemainlyincomplexroboticsys-\n2.1 AutomatedRecoveryinRobotics temslikehumanoidsormulti-leggedrobots.SimilartoITE,RTE\npre-computesandgeneratesabehaviorperformancemapusing\nPreliminary work on automated recovery in robots were based\nMAP-ELITES[MouretandClune2015].Itlearnstherobot’smodel,\nonevolutionaryalgorithmsandgenerallydividedtheprocessinto\nespeciallywhenitisdamagedandusesMonteCarloTreeSearch\ntwophases-damageestimationandrecovery.Thisnecessitated\n[Chaslotetal.2008]tocomputethenextbestactionforthecurrent\ntheneedforahealthyrobot’ssimulationtoalwaysbeavailableto\nstateofrobot.Also,themethodusesaprobabilisticmodeltoincor-\nthephysicalrobot,soastoestimatethedamage.Thisestimation\nporateuncertaintyofpredictionsandusesthisdatatocorrectthe\nwouldthenhelpcreateaneuralcontroller,duringtheexplorationor\noutcomeofeachactiononthedamagedrobot[Silveretal.2016;T\nrecoveryphase,whichcanhandlethedamage.Theneuralcontroller\n2013].Thisculminationofalgorithmsmakessurethatthereisno\nispassedtotherobotandthususedforadaptation.Thealgorithm\nresetrequiredwhenadamageoccurs.",
      "size": 1418,
      "sentences": 4
    },
    {
      "id": 9,
      "content": "coveryphase,whichcanhandlethedamage.Theneuralcontroller\n2013].Thisculminationofalgorithmsmakessurethatthereisno\nispassedtotherobotandthususedforadaptation.Thealgorithm\nresetrequiredwhenadamageoccurs. introducedin[BongardandLipson2004],wasoneofthefirstto\nAsignificantdrawbackintheprevioustwomethodsisthehuge\nproposeanautomaticandcontinuousinformationflowbetween\ncomplexityoverheadduetotheuseofgaussianprocessandalso\naphysicalrobotanditssimulationwhereintherobotprovidesits\ntheinabilitytoworkondynamicunknownterrains. currentstateinformation.Thesimulatorupdatesitsownstateusing\nthis information and provides the robot with neural controllers 2.3 HandlingEnvironmentalAdversaries\ntohandleitsstateordamage.Themajoradvantagewasthatthe\nAdversarialforcesonrobotsarenotlimitedtophysicaldamages.",
      "size": 786,
      "sentences": 3
    },
    {
      "id": 10,
      "content": "ation and provides the robot with neural controllers 2.3 HandlingEnvironmentalAdversaries\ntohandleitsstateordamage.Themajoradvantagewasthatthe\nAdversarialforcesonrobotsarenotlimitedtophysicaldamages. creationofrecoverymethoddidn’thavetobeperformeddirectlyon\nTherecouldalsobeenvironmentalfactorswhichhindernormal\nthephysicalrobotandthusthenumberoftrialsrequiredtorecover\nroboticlocomotion.Severalmethodshavebeenproposedtodeal\nwasdrasticallyreduced. with these kind of damages. Robust Adversarial Reinforcement\nExtendingthiswork,Koosetal. [Koosetal.2013],alsocreateda\nLearning (RARL) [Pinto et al.",
      "size": 595,
      "sentences": 5
    },
    {
      "id": 11,
      "content": "ethodshavebeenproposedtodeal\nwasdrasticallyreduced. with these kind of damages. Robust Adversarial Reinforcement\nExtendingthiswork,Koosetal. [Koosetal.2013],alsocreateda\nLearning (RARL) [Pinto et al. 2017b], concentrates on ensuring\nselfdiagnosismodel.Themaindifferencebetweenthetwoworksis\nstabilityofanagentinthepresenceofanadversary,whichistrying\ntheuseofanundamagedself-modeloftherobottofindoutbehav-\ntodestabilizeit.Itisbasedontheassumptionthatenvironmental\niorsratherthanconstantlyupdatingitbasedondiagnosis.Although\nchanges,suchaschangeincoefficientoffrictionoffloor,between\ntheintuitionbehindthesearecorrectandapplicableeventoday,the\ntrainingandtestingcanalsobemodelledasanadversaryactingon\nuseofevolutionaryalgorithmsmakethesemethodsinefficient. somepartoftheagent’sbody. Thealgorithmisbasicallyreducedtoamin-maxgamewherethe\n2.2 Map-basedAlgorithmsforAdaptation adversarytriestominimizetherewardoftheconcernedMarkov\nDecisionProcess(MDP)andtheprotagonisttriestomaximizeit.",
      "size": 979,
      "sentences": 7
    },
    {
      "id": 12,
      "content": "Thealgorithmisbasicallyreducedtoamin-maxgamewherethe\n2.2 Map-basedAlgorithmsforAdaptation adversarytriestominimizetherewardoftheconcernedMarkov\nDecisionProcess(MDP)andtheprotagonisttriestomaximizeit. Algorithmsbasedonbehaviorperformancemaps[Chatzilygeroudis Themethodofachievingthis,asproposed,istoalternatebetween\netal.2018;Cullyetal.2015]relyontheassumptionthatknowledge trainingofpoliciesforbothadversaryandprotagonistforafixed\nofthecauseofdamagei.e.,aproperdiagnosisreportisnotnecessary numberofiterationsuntilconvergence. torecoverfromthedamage.Ratherthanconsideringtwoseparate Anotherapproach,introducedin[Kumeetal.2017],isbasedon\nphasesfordamagediagnosisandrecoveryalgorithmgeneration, enablingadaptationtobothenvironmentaladversariesaswellas\nCullyetal.",
      "size": 760,
      "sentences": 3
    },
    {
      "id": 13,
      "content": "onsideringtwoseparate Anotherapproach,introducedin[Kumeetal.2017],isbasedon\nphasesfordamagediagnosisandrecoveryalgorithmgeneration, enablingadaptationtobothenvironmentaladversariesaswellas\nCullyetal. [Cullyetal.2015],proposedamethodinspiredfrom physicalorinternaldamageofrobot.Themajordifferencebetween\nanimals,whoperformtrialanderrortodeterminetheleastpainful theirworkandpreviousworkslikeITEandRTEistheexistenceof\nalternategaitinthepresenceofinjury.Theapproachputforward amulti-policymappingforasinglebehaviorinplaceofasinglepol-\ninthiswork,IntelligentTrialandError(ITE),reliesonabehavior- icy.Map-basedMulti-PolicyReinforcementLearning(MMPRL),pro-\nperformancemapspace.Thismapenablestherobottotrymultiple posedinthiswork,trainsmanydifferentpoliciesbycollaboratinga\nbehaviorswhicharepredictedtoperformwell.Basedonthetrials behavior-performancemapandtheconceptsofdeepreinforcement\nconductedandtheirresults,theestimatedperformancevaluesare learning.Itaimstosearchandstorethesemultiplepolicieswhile\nalso updated in the map.",
      "size": 1021,
      "sentences": 2
    },
    {
      "id": 14,
      "content": "s behavior-performancemapandtheconceptsofdeepreinforcement\nconductedandtheirresults,theestimatedperformancevaluesare learning.Itaimstosearchandstorethesemultiplepolicieswhile\nalso updated in the map. The process converges when the best maximizingexpectedreward.MMPRLsavesallpossiblepolicies\nbehaviorpossiblehasbeenestimated.Evenwhenthedamageis with different behavioral features, making it extremely fast and\nabsent,thehighperformingbehaviorsareexpectedtobeuseful. adaptable.",
      "size": 475,
      "sentences": 3
    },
    {
      "id": 15,
      "content": "ssiblepolicies\nbehaviorpossiblehasbeenestimated.Evenwhenthedamageis with different behavioral features, making it extremely fast and\nabsent,thehighperformingbehaviorsareexpectedtobeuseful. adaptable. The implementation of this idea uses gaussian process [Ras-\n2.4 DomainRandomization\nmussenandWilliams2005],andbayesianoptimizationprocedure\n[BorjiandItti2013;Mockus1989],tochoosewhichgaitsorbe- Somerecentworkshavealsoexperimentedwithrandomizationin\nhaviorstotryatthetimeofdamagebymaximizingperformance simulationenvironmentsthroughdomainanddynamicsrandom-\nfunctionfromthebehavior-performancespace.Theselectedgait ization[Pengetal.2017;Tobinetal.2017],soastobridgethegap\nistestedontherobotanditsperformanceisrecordedwhichthen betweensimulationandrealworld.Theideaistocreatenumerous\nhelpsupdatethevalueofexpectedperformanceofthatgait.This variationsinthesimulationenvironmentsothatrealworldappears\nselect-test-updateloopcontinuestilltherightbehaviorisobtained.",
      "size": 958,
      "sentences": 3
    },
    {
      "id": 16,
      "content": "deaistocreatenumerous\nhelpsupdatethevalueofexpectedperformanceofthatgait.This variationsinthesimulationenvironmentsothatrealworldappears\nselect-test-updateloopcontinuestilltherightbehaviorisobtained. asjustanothersamplefromarichdistributionoftrainingsamples. InspiredbyITE,Chatzilygeroudisetal.[Chatzilygeroudisetal.",
      "size": 316,
      "sentences": 4
    },
    {
      "id": 17,
      "content": "hatrealworldappears\nselect-test-updateloopcontinuestilltherightbehaviorisobtained. asjustanothersamplefromarichdistributionoftrainingsamples. InspiredbyITE,Chatzilygeroudisetal.[Chatzilygeroudisetal. In[Tobinetal.2017],theauthorshaveexperimentedonobjectlocal-\n2018],proposedamoreoptimizedversionofthealgorithm.Reset izationforthepurposeofgraspinginclutteredenvironment.They\nfreetrialanderror(RTE)focusesonthefactthatsomeofthehigh haveshownimpressiveresults,randomizinginthevisualdomain\nperformingpolicieswhichworkonanintactrobotshouldalsowork totransferlearningfromsimulationtorealworldwithoutrequiring\n=== 페이지 3 ===\nDeepReinforcementLearningforSingle-ShotDiagnosisandAdaptationinDamagedRobots • 3\nrealworldtrainingimages.Ontheotherhand,in[Pengetal.2017], Result:Anarraywithcollectedsamples\ntheauthorshaverandomizedthedynamicsoftheenvironmentsuch Initialize:\nasmass,dampingfactor,frictioncoefficientandhaveshownthat Loadanexpertpolicytrainedonhealthyrobot\nthepolicylearnedinsuchadynamicenvironmentisquiterobust Runparallelthreads\ntocalibrationerrorsintherealworld.",
      "size": 1064,
      "sentences": 4
    },
    {
      "id": 18,
      "content": ",dampingfactor,frictioncoefficientandhaveshownthat Loadanexpertpolicytrainedonhealthyrobot\nthepolicylearnedinsuchadynamicenvironmentisquiterobust Runparallelthreads\ntocalibrationerrorsintherealworld. fori ←0ton_rolloutsdo\nSetarandomseed\nWhilemostmap-basedmethodsareabletoadaptoverawide\nInitializeenvironmentsenv ,env forhealthyand\nrangeofdamages,theircomputationaloverheadincreatingthe h d\ndamagedrobotswiththesameseedvalue\nbehaviour-performancemapisasignificantdrawback.InITEand\nfordamaдe_class ←0ton_damaдe_classesdo\nRTE,thecomplexityisfurtherincreasedbythegaussianprocess\nenv .applyDamage(damage_class)\ncomputations.Moreover,alltheseapproachesrequiremultiplehard- d\nforn←0ton_timestepsdo\nwaretrialsforadaptingtoadamage.Wetrytoincorporatedomain\ngetactionfrompredefinedpolicy\nrandomizationapproachinthecontextofdamagessothatdam-\na =policy_fn(obs )\nagesintherealworldarejustanothervariationoftrainingsamples.",
      "size": 908,
      "sentences": 2
    },
    {
      "id": 19,
      "content": "ingtoadamage.Wetrytoincorporatedomain\ngetactionfrompredefinedpolicy\nrandomizationapproachinthecontextofdamagessothatdam-\na =policy_fn(obs )\nagesintherealworldarejustanothervariationoftrainingsamples. h h\na =policy_fn(obs )\nMoreover,wefurtherimprovethisapproachbypresentingasingle d d\ndosimulationstepinbothenvironments\nhardwaretrialcontrolloopfordiagnosingthedamage. obs ,rew =env .step(a )\nd d d d\n3 APPROACH obs h ,rew h =env h .step(a h )\nend\n3.1 Overview\ncollect(o -o )\nh d\nWe consider the following scenario: A robot has been damaged end\nwhileinaremoteandhazardousenvironment.Werequiretherobot end\nto reach the destination by adapting its gait so as to overcome Concatenatecollectedsamples\nthe damage. Rather than making the agent dependent on a pre- Algorithm1:Samplecollection\ncomputedsetofhighperforminggaits,itshouldbeabletoidentify\nandadapttoitsdamageautonomously.",
      "size": 874,
      "sentences": 4
    },
    {
      "id": 20,
      "content": "atecollectedsamples\nthe damage. Rather than making the agent dependent on a pre- Algorithm1:Samplecollection\ncomputedsetofhighperforminggaits,itshouldbeabletoidentify\nandadapttoitsdamageautonomously. Thusweproposeaself-diagnosenetworkwhichcanpredictthe Sincethistimeseriesismultivariateandhighdimensional,we\ntypeofdamagethathasoccurredinthestructureoftherobot.With useLSTMhiddenunitswhicharepowerfulandincreasinglypop-\nthisdamageawareness,weuseanaugmentedobservationspace ularmodelsforlearningfromsequencialdata[Greffetal.2017].",
      "size": 528,
      "sentences": 3
    },
    {
      "id": 21,
      "content": "curredinthestructureoftherobot.With useLSTMhiddenunitswhicharepowerfulandincreasinglypop-\nthisdamageawareness,weuseanaugmentedobservationspace ularmodelsforlearningfromsequencialdata[Greffetal.2017]. forlearningawell-performingpolicythroughamodifiedversionof Algorithm1describesindetailthesamplecollectionprocess.The\nProximalPolicyOptimization(PPO)whichwecallDamageAware- healthyanddamagedrobotenvironmentsarerepresentedbyenv\nh\nProximalPolicyOptimization(DA-PPO).Inourwork,weassume andenv respectively.Boththeenvironmentsarerunfromthesame\nd\nthatinternaldamages,unlikeenvironmentaladversaries,donot initialstateandthedifferencebetweentheirobservationspacesis\nkeepchangingconstantly.Thus,weonlyneedtoperformtheself- collectedcontinuouslyforafixednumberoftimestepsT (repre-\ndiagnosisstepfordeterminingdamageclasswheneverthereward sentedhereasn_timesteps).Foranyenvironment,thisresultsina\ndrasticallydropsbelowacertainthreshold,indicatingthatdamage matrixofsizeO∗T (whereOistheobservationspacesizeforthat\nhasoccurred.",
      "size": 1013,
      "sentences": 2
    },
    {
      "id": 22,
      "content": "verthereward sentedhereasn_timesteps).Foranyenvironment,thisresultsina\ndrasticallydropsbelowacertainthreshold,indicatingthatdamage matrixofsizeO∗T (whereOistheobservationspacesizeforthat\nhasoccurred. environment)andthisrepresentsasingledatapoint.Thesedata\npointsactastrainingdata,forwhichlabelsarethecorresponding\n3.2 Self-DiagnoseNetwork\ndamage classes upon which thesimulation was run. The whole\nInthemin-maxbasedgameapproachputforwardinRARL[Pinto processisrepeatedn_rolloutsnumberoftimestogetmultipledata\netal.2017b],thetechniquefailstogeneralizeoverchangingdam- points.Notethatpolicy_fnrepresentsanexpertpolicywhichhas\nagesfromadversaryateverytimestep.Thisisactuallyatough beenpretrainedonahealthyrobot.",
      "size": 707,
      "sentences": 3
    },
    {
      "id": 23,
      "content": "etal.2017b],thetechniquefailstogeneralizeoverchangingdam- points.Notethatpolicy_fnrepresentsanexpertpolicywhichhas\nagesfromadversaryateverytimestep.Thisisactuallyatough beenpretrainedonahealthyrobot. problemsincethepolicyhasnofeedbackmechanismtojudgethe Thenetworkistrainedusingdataobtainedthroughthesample\nperformanceofactiontakeninthelasttimestep.Thuswepropose collectionstepexplainedinAlgorithm1.Thisstepisalsoparallelized\naself-diagnosenetwork,anLSTM[HochreiterandSchmidhuber andthusdoesn’tactasabottleneckfortheentirealgorithm.The\n1997]basedpredictivemodel,whichtriestoclassifythetypeof self-diagnosenetwork,representedbyΘ,canbeaccessedondemand\ndamagethathasoccurredintherobotusingcontinuousfeedback todeterminedamageclassµwithinasingletrialasshowninFig.1. fromitsgait.In[BongardandLipson2004],theauthorshaveused\n3.3 EncodingofDamageIndicators\nthedifferencebetweenthebehavioursofsimulatedrobotandthe\nphysicalrobotintermsofforwarddisplacementtoclassifydamages.",
      "size": 964,
      "sentences": 3
    },
    {
      "id": 24,
      "content": "mitsgait.In[BongardandLipson2004],theauthorshaveused\n3.3 EncodingofDamageIndicators\nthedifferencebetweenthebehavioursofsimulatedrobotandthe\nphysicalrobotintermsofforwarddisplacementtoclassifydamages. Theself-diagnosenetworkpredictsthedamageclassoftherobot\nWeextendthisideabymeasuringthedifferenceinsensorvalues whichcanactasanadditionalstateinformationabouttheenvi-\nbetweenthetwoforafixednumberoftimesteps.Thisresultsina ronment.Wethusconcatenateitwiththeobservationspaceofthe\ntimeseriesandourproblemisreducedtoclassifyingdamagefrom originalrobottoform,whatwecall,anaugmentedobservation\nthisdata.Morespecifically,theon-boardcomputeroftherobotcan space. runasimulationofahealthyrobotandcompareitsgaitwiththe Thisposesanecessitytoencodetheoutputoftheclassifierso\nactualstepstaken.Basedonthedifferencebetweenthetwo,the thatthepolicyefficientlylearnsvariousgaitsinaccordancewith\nnetworkcandiagnosetheclassofdamage(seeFig.1).",
      "size": 920,
      "sentences": 3
    },
    {
      "id": 25,
      "content": "ssitytoencodetheoutputoftheclassifierso\nactualstepstaken.Basedonthedifferencebetweenthetwo,the thatthepolicyefficientlylearnsvariousgaitsinaccordancewith\nnetworkcandiagnosetheclassofdamage(seeFig.1). thedamage.Ifarandomencodingschemeisusedforcreatingthe\n=== 페이지 4 ===\n4 • ShresthVerma,HarithaS.Nair,GauravAgarwal,JoydipDhar,andAnupamShukla\n3.4 ProximalPolicyOptimization\naugmentedobservationspace,itresultsinthealgorithmperceiving\ntheencodingasnoise,andcompletelyignoringitduringpolicy Sinceourtaskisthatofcontinuousactioncontrol,weformulateit\nlearning. We have thus used partial one hot encoding and it is asareinforcementlearningproblem,startingfrominitialstates0,\nobservedtoworkwellinpracticeasthedamageinformationisnot choosingaseriesofactionaandobtainingstatesi andrewardri\nlostduringtraining.",
      "size": 798,
      "sentences": 3
    },
    {
      "id": 26,
      "content": "d it is asareinforcementlearningproblem,startingfrominitialstates0,\nobservedtoworkwellinpracticeasthedamageinformationisnot choosingaseriesofactionaandobtainingstatesi andrewardri\nlostduringtraining. attheith timestepwhilemaximizingtheexpectedsumofrewards\nInourexperiments,wehavelimitedthenumberofdamagesthat\nbychangingtheparameterθ oftheparameterizedstochasticpolicy\ncanoccursimultaneouslytotwoandhavetakentheassumption\nπ .Buttheuseoflargescaleoptimizationislesswidespreadin\nθ\nthatonlyonedamagecanoccuronalimbatatime.Thenumber\ncontinuousactionspaces.Anattractiveoptionforsuchproblems\nofdamageclassescanthusbecalculatedasthesumofnodamage\nistousepolicygradientalgorithms[Silveretal.2014].Proximal\ncase,singledamagecasesandmultipledamagecasesoccurringat\nPolicyOptimizationisasimplifiedversionofTrustRegionPolicy\nvariouslimbs.Thisisgivenby:\nOptimization(TRPO)[Schulmanetal.2015a].Itimprovesuponthe\n(cid:18) n (cid:19) (cid:18) n (cid:19) (cid:18) n (cid:19) stabilityofpolicygradientmethodsbyallowingmultipleupdates\nD=k0 +k1 +k2 , (1)\n0 1 2 onminibatchofon-policydata.Thisisimplementedbylimitingthe\nKLdivergencebetweenupdatedpolicyandthepolicyfromwhich\nwherenrepresentsthenumberoflimbsintheagentandkrepresents thedatawassampled.TRPOusesahardoptimizationconstraintfor\nthenumberofdifferentdamagetypesconsidered.",
      "size": 1306,
      "sentences": 2
    },
    {
      "id": 27,
      "content": "ebetweenupdatedpolicyandthepolicyfromwhich\nwherenrepresentsthenumberoflimbsintheagentandkrepresents thedatawassampled.TRPOusesahardoptimizationconstraintfor\nthenumberofdifferentdamagetypesconsidered. achievingthesamebutiscomputationallyexpensivetocompute. Theencodedvectorisoflength2nwherethedamageofith limb PPOapproximatesTRPObyusingasoftconstraint.Theoriginalpa-\nisrepresentedbythevaluesatindices2iand2i+1intheencoded per[Schulmanetal.2017]proposestwomethodsforimplementing\nvector.Thus,wehaveatupleofsize2associatedwitheachlimb thissoftconstraint:anadaptiveKLlosspenaltyandusingaclipped\nwhere[0,0]representsnodamage,[1,0]representsdamagetype1 surrogatelossfunction. and[0,1]representsdamagetype2atthelimb.Notethatthetuple PPOrepresentstheratiobetweennewpolicyandoldpolicyas:\n[ d\ntu\n1 a ,\np\nm 1\nle\n] ag c\nsi\na e\nz\nn s\ne\nc b\nc\na\na\ne n\nn\nu ’t\nb\nse\ne\no d c\nin\nc i u f\ncr\nr w\ne\nt\na\ne o\ns\ng r\ne\ne e\nd\nm th\nto\no e v r\nm\ne a\no\nt t\nd\nh a\ne\ne\nl\ns a\nm\ni s n s\no\ng u\nr\nl\ne\nm e\nt\np l\ny\ni t m\np\nio\ne\nb n\ns\n.",
      "size": 996,
      "sentences": 4
    },
    {
      "id": 28,
      "content": "le\n] ag c\nsi\na e\nz\nn s\ne\nc b\nc\na\na\ne n\nn\nu ’t\nb\nse\ne\no d c\nin\nc i u f\ncr\nr w\ne\nt\na\ne o\ns\ng r\ne\ne e\nd\nm th\nto\no e v r\nm\ne a\no\nt t\nd\nh a\ne\ne\nl\ns a\nm\ni s n s\no\ng u\nr\nl\ne\nm e\nt\np l\ny\ni t m\np\nio\ne\nb n\ns\n. o\nt F h\nf\nu a\nd\nr t t\na\nh\nm\ntw e\na\nr o m\nge\nt o\ns\ny r\n. p e e , s th o e f rt (θ)=\nπ θ\nπ\no\nθ\nl\n(\nd\na\n(\nt\na\n|\nt\ns\n|\nt\ns\n)\nt )\n. (2)\nTheobjectivefunctionscanbe[Schulmanetal.2017]:\nLCLIP(θ)=Eˆ t [min(rt (θ)Aˆ t,clip(rt (θ),1−ϵ,1+ϵ)Aˆ t )], (3)\nLKL(θ)=rt (θ)Aˆ t −βKL[π θold ,π θ ] (4)\nBoththeseobjectivefunctionsstabilizetrainingbyconstraining\nthepolicychangesateachstep,thusapproximatingthegradient\ntoalocalvalue,sothatlargestepsarenottakenbetweenitera-\ntions.Additionally,weuseGeneralizedAdvantageEstimation(GAE)\n[Schulmanetal.2015b]forcomputingtheadvantagefunctionAˆ.In\nourimplementationofPPO,wehaveusedacombinationofboth\nclippinglossandadaptiveKLpenaltyforlocomotiontasks.The\nhyperparametersforthesamearementionedinSection4.3.",
      "size": 927,
      "sentences": 4
    },
    {
      "id": 29,
      "content": "15b]forcomputingtheadvantagefunctionAˆ.In\nourimplementationofPPO,wehaveusedacombinationofboth\nclippinglossandadaptiveKLpenaltyforlocomotiontasks.The\nhyperparametersforthesamearementionedinSection4.3. 3.5 DamageAwareProximalPolicyOptimization\nWiththeself-diagnosenetworkinplace,wecannowusethepolicy\nlearningalgorithmonaugmentedobservationspacewhichencap-\nsulatesbothenvironmentstate(throughobservationvector)and\ndamageawareness(throughdamageencodingvector).Weusethe\nPPOalgorithmforpolicylearningfromtheaugmentedobservation\nspacewherext istheobservationattimestept,uistheactiontaken\naccordingtopolicyΠandfµ istheenvironmentinwhichdamage\nµhasoccurred(seeFig.1).Notethatweonlyrunself-diagnosenet-\nworkwhenrewardduringarunfallsbelowacertainthreshold.At\nothertimes,thedamageisconsideredtobethesameasdiagnosed\ninthelastrun. 4 EXPERIMENTALSETUP\n4.1 SimulationSetup\nToevaluateourapproach,wehaveconductedexperimentsontwo\nFig.1.",
      "size": 917,
      "sentences": 3
    },
    {
      "id": 30,
      "content": "allsbelowacertainthreshold.At\nothertimes,thedamageisconsideredtobethesameasdiagnosed\ninthelastrun. 4 EXPERIMENTALSETUP\n4.1 SimulationSetup\nToevaluateourapproach,wehaveconductedexperimentsontwo\nFig.1. ControlArchitecture\nenvironments,Ant,aquadrupedallocomotoryrobotandHexapod,\nasix-leggedlocomotoryrobot.WehaveusedOpenAIgymtoolkit\n=== 페이지 5 ===\nDeepReinforcementLearningforSingle-ShotDiagnosisandAdaptationinDamagedRobots • 5\n[Brockmanetal.2016],forperformingsimulationsincombination\nwithMuJoCophysicsengine[Todorovetal.2012].TheAntisan\nalreadyimplementedenvironmentinOpenAIGymwhiletheHexa-\npodisimplementedusingtheconfigurationandmodeldescribedin\nITE[Cullyetal.2015]. Thetwoenvironmentsusedinourexperimentsarediscussed\nbelow:\nAnt(Quadrupedalbot):Antisasimplequadrupedalrobotwith\n(a)Antdamagescenario1 (b)Antdamagescenario2\n12degreesoffreedom(DoF)and8torqueactuatedjoints.Thejoint\nhasmaximumflexandextensionof30degreesfromtheiroriginal\nsetting and also has a force and torque sensor.",
      "size": 982,
      "sentences": 4
    },
    {
      "id": 31,
      "content": "ntdamagescenario1 (b)Antdamagescenario2\n12degreesoffreedom(DoF)and8torqueactuatedjoints.Thejoint\nhasmaximumflexandextensionof30degreesfromtheiroriginal\nsetting and also has a force and torque sensor. The observation\nincludesfeaturescontainingjointangles,angularvelocity,thepo-\nsitionofallstructuralelementswithrespecttothecenterofmass\nandforceandtorquesensoroutputsofeachjointforminga111-\ndimensionvector.Thetargetactionvaluesarethemotortorque\nvalueswhicharelimitedintherange-1.0to1.0.Welimitanepisode\ntoatmost1000timestepsandtheepisodewillendwheneverit\ncrossesthislimitorrobotfallsdownonitslegsorjumpsabovea\ncertainheight.Therewardfunctionisdefinedasfollows: (c)Hexapoddamagescenario1 (d)Hexapoddamagescenario2\nRt =∆xt +st −w0Ct −(w1 ||ϕt || 2 )2, (5) Fig.2. SomeofthedamagescenariosinAntandHexapod.Yellowandred\nwhere∆xt isthecovereddistanceoftherobotinthecurrenttime circlesrepresentjammedjointandmissinglimbdamagetypesrespectively.",
      "size": 934,
      "sentences": 3
    },
    {
      "id": 32,
      "content": "||ϕt || 2 )2, (5) Fig.2. SomeofthedamagescenariosinAntandHexapod.Yellowandred\nwhere∆xt isthecovereddistanceoftherobotinthecurrenttime circlesrepresentjammedjointandmissinglimbdamagetypesrespectively. stepsincetheprevioustimestep,st isthesurvivalreward,whichis1\nonsurvivaland0iftheepisodeisterminatedbytheaforementioned\nthexmlfilesofthe3Dmodels.Thiscanbedoneontheflywith-\nconditions.ThevariableCt isthenumberoflegsmakingcontact\nwiththeground,ϕt ∈R8arethetargetjointangles(theactions), outaffectingparallelyrunningexperiments.Inourwork,wehave\nsimulatedbroadlytwokindsofinternaldamageswhichare\nandwn istheweightofeachcomponentwithw0=0.5,w1=0.5. (1) Jammingofjointsuchthatitcan’tmoveirrespectiveofthe\nHexapod:TherearethreeactuatorsoneachlegoftheHexapod. amountoftorsionalforceappliedbythemotoratthatjoint. In the neutral position, the height of the robot is 0.2 meters. In (2) Missingtoe,i.e.,lowerlimboftherobotbreaksoff.",
      "size": 918,
      "sentences": 7
    },
    {
      "id": 33,
      "content": "ctuatorsoneachlegoftheHexapod. amountoftorsionalforceappliedbythemotoratthatjoint. In the neutral position, the height of the robot is 0.2 meters. In (2) Missingtoe,i.e.,lowerlimboftherobotbreaksoff. additiontothis,theactionsaretakentobethejointanglepositions In MuJoCo environments, these damages are implemented as\nofall18joints,whichrangesfrom-0.785to0.785radians.Asthe follows:\nobservationspaceoftheagent,a53-dimensionvectorisgivenas AntEnvironment\ninputwhichconsistsofthepositionandvelocityofallthejointsas • Jammingofjointismodelledbyrestrictingtheanglerangeof\nwellashecenterofmass.Alongwiththis,theobservationspace concernedjointto-0.1to0.1degreesfromthedefaultvalue\ncontainsbooleanvaluesfromtouchsensorswhichindicatewhether of-30to30degrees. alegismakingcontactwiththegroundornot.Again,welimit • Missingtoeismodelledbyshrinkingthelowerlimbsizeto\nanepisodetobeatmost1000timestepsandtheepisodewillend 0.01fromtheoriginalvalueof0.8.",
      "size": 937,
      "sentences": 6
    },
    {
      "id": 34,
      "content": "o30degrees. alegismakingcontactwiththegroundornot.Again,welimit • Missingtoeismodelledbyshrinkingthelowerlimbsizeto\nanepisodetobeatmost1000timestepsandtheepisodewillend 0.01fromtheoriginalvalueof0.8. whenevertherobotfallsdownonitslegsorjumpsaboveacertain\nHexapodEnvironment\nheightorcrossesthetimelimit. • Theoriginalanglerangeofhexapodis-45to45degrees.This\nTherewardfunctionRisdefinedasfollows:\nisrestrictedto-0.1to0.1whenjammingofjointismodelled. Rt =∆xt +st −w0Ct −(w1 ||τt || 2 )2−(w2 ||ϕt || 2 )2, (6) • Missingofanyofthe6toesinhexapodismodelledbyreduc-\nwhere∆xt isthecovereddistanceoftherobotinthecurrenttime ingthethelowerlimbsizeto0.01insteadof0.07inhealthy\nstepsincetheprevioustimestep,st isthesurvivalreward,whichis robot. 0.1onsurvivaland0iftheepisodeisterminatedbytheaforemen- • Therearetouchsensorsoneachlowerlimbofthehexapod.",
      "size": 838,
      "sentences": 6
    },
    {
      "id": 35,
      "content": ".01insteadof0.07inhealthy\nstepsincetheprevioustimestep,st isthesurvivalreward,whichis robot. 0.1onsurvivaland0iftheepisodeisterminatedbytheaforemen- • Therearetouchsensorsoneachlowerlimbofthehexapod. tionedconditions.ThevariableCt representsthenumberoflegs Thuswheneveralowerlimbbreaksoffweconsiderthatthe\nmakingcontactwiththeground,τt ∈R18isthevectorofsquared touchsensorcorrespondingtoitstopsgivinganysignaland\nsum of external forces and torques on each joint,ϕt ∈ R18 are itsoutputisconsideredtobe0. thetargetjointangles(theactions),andwn istheweightofeach\n4.3 HyperparameterDetails\ncomponentwithw0=0.03,w1=0.0005,andw2=0.05.",
      "size": 628,
      "sentences": 4
    },
    {
      "id": 36,
      "content": "forces and torques on each joint,ϕt ∈ R18 are itsoutputisconsideredtobe0. thetargetjointangles(theactions),andwn istheweightofeach\n4.3 HyperparameterDetails\ncomponentwithw0=0.03,w1=0.0005,andw2=0.05. Fortheself-diagnosenetwork,wetakeasinputamatrixofsize\n4.2 DamageSimulation\nbatch_size∗O∗T andthisisfollowedbyanembeddinglayerwith\nSinceboththeenvironmentsconsideredinourexperimentsaresim- embeddingsize512andanLSTMlayerwith32hiddenunits.After\nulatedinOpenAIgym,thedamagesareimplementedbychanging this,westackthreedenselayersofsize256,128,64alongwith\n=== 페이지 6 ===\n6 • ShresthVerma,HarithaS.Nair,GauravAgarwal,JoydipDhar,andAnupamShukla\n(a)AntEnvironment (b)HexapodEnvironment\nFig.3.",
      "size": 681,
      "sentences": 3
    },
    {
      "id": 37,
      "content": "edbychanging this,westackthreedenselayersofsize256,128,64alongwith\n=== 페이지 6 ===\n6 • ShresthVerma,HarithaS.Nair,GauravAgarwal,JoydipDhar,andAnupamShukla\n(a)AntEnvironment (b)HexapodEnvironment\nFig.3. TrainingcurvecomparisionbetweenDA-PPOandPPOUnawareinbothAntandHexapodEnvironments\ndropouts,soastoreduceoverfitting.Theoutputlayerusessoftmax outperformstheuseofobservationsfromdamagedrunonlyinall\nasactivationsothatitoutputsclassprobabilities.Thelossfunction thecases.However,ifthereisaconstraintoncomputationpower\nandoptimizerusedarecategoricalcrossentropyandadamrespec- oftheon-boardcomputeroftherobot,thelatterapproachcanbe\ntively. For Ant and Hexapod environments, the possible classes preferredovertheformerone. rangefrom0to32and0to72respectivelyascalculatedfromEqua-\ntion1. Table1.",
      "size": 786,
      "sentences": 5
    },
    {
      "id": 38,
      "content": "puteroftherobot,thelatterapproachcanbe\ntively. For Ant and Hexapod environments, the possible classes preferredovertheformerone. rangefrom0to32and0to72respectivelyascalculatedfromEqua-\ntion1. Table1. ClassificationaccuracyinpredictingdamageclassinAntandHexa-\nAsforthepolicylearningusingPPO,weusetheimplementation podenvironmentwithvaryingnumberoftimestepsandrollouts.Method\nfrom[Guadarramaetal.2018].Forbothvaluefunctionandpolicy Arepresentsusingobservationsofdamagedrunonlyastimeseriesand\nfunction,weusethesamenetworkconfigurationhavinghidden methodBrepresentsusingdifferenceofobservationsbetweenhealthyrobot\nanddamagedrobotastimeseries. layer sizes as 100, 200, 100.",
      "size": 668,
      "sentences": 6
    },
    {
      "id": 39,
      "content": "nonlyastimeseriesand\nfunction,weusethesamenetworkconfigurationhavinghidden methodBrepresentsusingdifferenceofobservationsbetweenhealthyrobot\nanddamagedrobotastimeseries. layer sizes as 100, 200, 100. Adam optimizer was used for both\ntheneuralnetworks.TheGAEgammavalueistakenas0.995and\nlambdaas0.98.Theclippingrangeiskeptat0.2andadaptiveKL ClassificationAccuracyinAntEnvironment\ntargetisinitializedwith0.01.AdamlearningrateandKLtarget NumberofRollouts\nTimesteps Method\nvalueareadjusteddynamicallyduringthetraining.Moreover,we 1000 2000 7000\ntrainedthevaluefunctiononthecombinationofcurrentbatchand A 78.2±1.11 81.4±0.6 82.4±0.87\n10\npreviousbatchtostabilizetraining.",
      "size": 664,
      "sentences": 3
    },
    {
      "id": 40,
      "content": "od\nvalueareadjusteddynamicallyduringthetraining.Moreover,we 1000 2000 7000\ntrainedthevaluefunctiononthecombinationofcurrentbatchand A 78.2±1.11 81.4±0.6 82.4±0.87\n10\npreviousbatchtostabilizetraining. B 81.24±2.88 85.2±1.2 84.33±0.72\nA 82.17±1.7 87.1±1.8 88.17±1.3\n30\n5 RESULTSANDDISCUSSION B 83.62±2.03 90.8±0.9 91.5±1.067\nA 83.11±0.8 90.17±1.2 92.83±1.8\nWeevaluatetheperformanceofourapproachwithinthetwoele- 50\nB 84.29±1.21 92.6±1.83 96.8±1.48\nmentsinvolved:(1)Self-Diagnosenetworkforpredictingclassof\nClassificationAccuracyinHexapodEnvironment\ndamage(2)DA-PPO,whichlearnstoadoptapolicygiventhata\nNumberofRollouts\nparticulardamagehasoccurred.",
      "size": 643,
      "sentences": 2
    },
    {
      "id": 41,
      "content": "mentsinvolved:(1)Self-Diagnosenetworkforpredictingclassof\nClassificationAccuracyinHexapodEnvironment\ndamage(2)DA-PPO,whichlearnstoadoptapolicygiventhata\nNumberofRollouts\nparticulardamagehasoccurred. Timesteps Method\n1000 2000 7000\nA 22.2±0.6 33.1±1.23 44.6±0.9\n5.1 Self-DiagnoseNetwork 10\nB 32.6±0.8 38.5±1.1 47.8±1.13\nForthecomparisonofperformance,weconsiderdifferentnumber A 60.5±1.9 62.9±1.8 79.67±1.02\nofrollouts(amountofdatatotrainon),lengthofhistorytolook 30 B 65.45±1.2 69.17±1.11 82.6±1.28\nbackinto(timesteps)andwhattogiveasobservationdata,i.e.,our A 65.23±1.3 69.7±1.1 82.2±1.8\nproposed approach of using difference of observations between 50 B 68.83±1.8 72.17±1.29 87.6±0.86\nhealthyanddamagedrunortheobservationsofonlydamagedrun. Table1summarizesthevalidationaccuracyacrosstheseparameters.",
      "size": 799,
      "sentences": 3
    },
    {
      "id": 42,
      "content": "oach of using difference of observations between 50 B 68.83±1.8 72.17±1.29 87.6±0.86\nhealthyanddamagedrunortheobservationsofonlydamagedrun. Table1summarizesthevalidationaccuracyacrosstheseparameters. 5.2 DamageAware-ProximalPolicyOptimization\nWecanobservethatclassifyingusingfewertimestepsresultsin\nfasterdiagnosisbutattheexpenseofaccuracy.Moreover,classifi- Westartbycreatingabaselinemodelforcomparisonofperformance. cationusingthedifferencebetweenobservationvectorsasinput WedefineamodelusingPPOpolicywhichistrainedonexperiments\n[표 데이터 감지됨]\n\n=== 페이지 7 ===\nDeepReinforcementLearningforSingle-ShotDiagnosisandAdaptationinDamagedRobots • 7\nFig.4.",
      "size": 645,
      "sentences": 4
    },
    {
      "id": 43,
      "content": "eenobservationvectorsasinput WedefineamodelusingPPOpolicywhichistrainedonexperiments\n[표 데이터 감지됨]\n\n=== 페이지 7 ===\nDeepReinforcementLearningforSingle-ShotDiagnosisandAdaptationinDamagedRobots • 7\nFig.4. ForwardrewardcomparisonbetweenDA-PPOandPPO-UnawareacrossdifferentdamageclassesinHexapod\nFortheHexapodenvironment,wealsousetheconceptofcur-\nriculumlearning[Bengioetal.2009],byprogressivelytrainingon\ncaseswhicharemoredifficult.Weimplementthisbyincreasing\nthepercentageofdamageclassesintrainingexamplesandalsopro-\ngressivelyincreasingtheseverityofdamages(byincludingmultiple\ndamages).InFig.3b,eachpiece-wisecurverepresentsastage(I,\nII,IIIorIV)inthecurriculumlearningprocess.Ihas100%healthy\ncases,IIhas60%healthyand40%singledamagecases,IIIhas70%\nhealthyandsingledamagecasesand30%multipledamagecases\nandIVhasalldamagesequallylikely.Inthisway,wewereableto\nencourageafasterlearningprogress.",
      "size": 883,
      "sentences": 2
    },
    {
      "id": 44,
      "content": "hy\ncases,IIhas60%healthyand40%singledamagecases,IIIhas70%\nhealthyandsingledamagecasesand30%multipledamagecases\nandIVhasalldamagesequallylikely.Inthisway,wewereableto\nencourageafasterlearningprogress. Wealsodoaperclassperformanceanalysisofthetwoapproaches\ndiscussedacrossvariousdamageclassesinbothAntandHexapod\n(seeFig.4,5).IntheAntenvironment,DA-PPOperformsbetterin\n82.84%ofdamageclasseswhencomparedtoPPO-Unaware.Com-\nparingbetweenvariousdamageclasses,DA-PPOisseentoadapt\nreallywell(intermsofrewardimprovementoverPPO-Unaware)\nwhendamagesoccuronoppositelimbsascomparedtodamages\nFig.5. ForwardrewardComparisonbetweenPPO-UnawareandDA-PPO\nacrossdifferentgroupeddamageclassesinAnt.D1andD2referstosingle occurringonadjacentlimbs.IntheHexapodenvironment,DA-PPO\njammedjointandsinglemissingtoedamages.Dij AandOrepresentsthat performsbetterin72.6%ofdamageclasseswhencomparedtoPPO-\ndamagetypeiandjarepresentinadjacent(A)oropposite(O)limbs.",
      "size": 928,
      "sentences": 3
    },
    {
      "id": 45,
      "content": "exapodenvironment,DA-PPO\njammedjointandsinglemissingtoedamages.Dij AandOrepresentsthat performsbetterin72.6%ofdamageclasseswhencomparedtoPPO-\ndamagetypeiandjarepresentinadjacent(A)oropposite(O)limbs. Unaware(seeFig.4).Thisshowsthatbeingdamageawareresultsin\nsignificantimprovementinperformanceinpresenceofadversaries. havingdamagedrobotbutwithoutaugmentedobservationspace\n(i.e.,withoutexplicitknowledgeofdamageclass),andcallitPPO-\n6 CONCLUSIONS\nUnaware.Thisisanalogoustohavingapolicyimplementingdomain\nrandomizationindamagespacebutwithouthavingafeedbackloop. Wehaveproposedandimplementedatwo-partcontrolarchitec-\nOur proposed model, which uses Damage Aware PPO policy, is tureforroboticdamageadaptation.Thisisparticularlyusefulwhen\ncalledDA-PPO.Theperformancemetricusedistheforwardreward robotsareusedinhazardousenvironments,wherehumaninterven-\noftheagent,averagedacrossallthedamageclasses.Fig.3showsthe tionisnearlyimpossible.",
      "size": 926,
      "sentences": 4
    },
    {
      "id": 46,
      "content": "hen\ncalledDA-PPO.Theperformancemetricusedistheforwardreward robotsareusedinhazardousenvironments,wherehumaninterven-\noftheagent,averagedacrossallthedamageclasses.Fig.3showsthe tionisnearlyimpossible. trainingcurvecomparisonbetweenPPO-UnawareandDA-PPOin Ourapproachenablestheagenttoautonomouslyidentifyand\nAntandHexapodenvironments(seeFig.3a,3b).DA-PPOshowsa understandthedamagethathasoccurredinitsphysicalstructure\n60.7%improvementinaverageforwardrewardinAntenvironment andadaptitsgaitaccordingly.Sincetheultimategoalisthecreation\nwhileinHexapodenvironment,thereisa31.5%rewardgainover ofintelligentmachines,understandingthedamageisasimportant\nPPO-Unaware. asadaptingfromit,whichhasoftenbeenoverlookedinpastworks. === 페이지 8 ===\n8 • ShresthVerma,HarithaS.Nair,GauravAgarwal,JoydipDhar,andAnupamShukla\nOncomparisonwithmap-basedapproaches,DA-PPOdoesn’t AyakaKume,EiichiMatsumoto,KuniyukiTakahashi,WilsonKo,andJethroTan. requireanymapgenerationphaseandthustheinitialtrainingtime 2017.",
      "size": 979,
      "sentences": 5
    },
    {
      "id": 47,
      "content": "ar,andAnupamShukla\nOncomparisonwithmap-basedapproaches,DA-PPOdoesn’t AyakaKume,EiichiMatsumoto,KuniyukiTakahashi,WilsonKo,andJethroTan. requireanymapgenerationphaseandthustheinitialtrainingtime 2017. Map-basedMulti-PolicyReinforcementLearning:EnhancingAdaptability\nofRobotsbyDeepReinforcementLearning. CoRRabs/1710.06117(2017). http:\nismuchless.Thisisalsoenhancedbythefactthatourapproach\n//arxiv.org/abs/1710.06117\nadaptstothedamageinasingletrialitself,withouttryingmultiple K.Lobos-Tsunekawa,F.Leiva,andJ.Ruiz-del-Solar.2018.VisualNavigationforBiped\nwell-performinggaitsorwithouthavingtoberesettotheinitial HumanoidRobotsUsingDeepReinforcementLearning.IEEERoboticsandAutoma-\ntionLetters3,4(Oct2018),3247–3254. https://doi.org/10.1109/LRA.2018.2851148\nstatetoperformthetrial. JonasMockus.1989.BayesianApproachtoGlobalOptimization.SpringerNetherlands. Ourworkcanalsobeeasilyscaledtoalargernumberofdam- Jean-BaptisteMouretandJeffClune.2015.Illuminatingsearchspacesbymappingelites.",
      "size": 978,
      "sentences": 8
    },
    {
      "id": 48,
      "content": "asMockus.1989.BayesianApproachtoGlobalOptimization.SpringerNetherlands. Ourworkcanalsobeeasilyscaledtoalargernumberofdam- Jean-BaptisteMouretandJeffClune.2015.Illuminatingsearchspacesbymappingelites. CoRRabs/1504.04909(2015).arXiv:1504.04909 http://arxiv.org/abs/1504.04909\nageclasses.Sincenodifferentiationismadebetweenthecauseof\nR.R.Murphy.2004.Trialbyfire[rescuerobots].IEEERoboticsAutomationMagazine\ndamage,adaptationispossibleincaseofbothmorphologicalandex- 11,3(Sep.2004),50–61. https://doi.org/10.1109/MRA.2004.1337826\nternaldamages.Also,inthecaseofunknowndamages,thenetwork KeijiNagatani,SeigaKiribayashi,YoshitoOkada,KazukiOtake,KazuyaYoshida,Satoshi\nTadokoro,TakeshiNishimura,TomoakiYoshida,EijiKoyanagi,MineoFukushima,\nisexpectedtopredictadamageclasswhichresemblestheactual\nandShinjiKawatsuma.2013.EmergencyResponsetotheNuclearAccidentatthe\ndamagethemostandtrytochooseagaitaccordingly.Thisimplies FukushimaDaiichiNuclearPowerPlantsUsingMobileRescueRobots.J.FieldRobot.",
      "size": 979,
      "sentences": 4
    },
    {
      "id": 49,
      "content": "tual\nandShinjiKawatsuma.2013.EmergencyResponsetotheNuclearAccidentatthe\ndamagethemostandtrytochooseagaitaccordingly.Thisimplies FukushimaDaiichiNuclearPowerPlantsUsingMobileRescueRobots.J.FieldRobot. averylowrateofcompletefailure.Weintendtostudymoreonthis 30,1(Jan.2013),44–63. https://doi.org/10.1002/rob.21439\nXueBinPeng,MarcinAndrychowicz,WojciechZaremba,andPieterAbbeel.2017. inafuturework. Sim-to-RealTransferofRoboticControlwithDynamicsRandomization. CoRR\nFutureworkshallbefocusedonextendingthealgorithmtohandle abs/1710.06537(2017).arXiv:1710.06537 http://arxiv.org/abs/1710.06537\nLerrelPinto,JamesDavidson,RahulSukthankar,andAbhinavGupta.2017a.Robust\nenvironmentaladversaries,whichismuchdesirablesincereal-world\nAdversarialReinforcementLearning.ICML(2017).",
      "size": 764,
      "sentences": 6
    },
    {
      "id": 50,
      "content": "xiv.org/abs/1710.06537\nLerrelPinto,JamesDavidson,RahulSukthankar,andAbhinavGupta.2017a.Robust\nenvironmentaladversaries,whichismuchdesirablesincereal-world\nAdversarialReinforcementLearning.ICML(2017). https://arxiv.org/abs/1703.02702\nenvironmentsarenotpredictable.WealsointendtoworkonDA- LerrelPinto,JamesDavidson,RahulSukthankar,andAbhinavGupta.2017b.Robust\nPPOforcomplexanddynamicenvironments,usingSLAM[Durrant- AdversarialReinforcementLearning.InProceedingsofthe34thInternationalConfer-\nenceonMachineLearning(ProceedingsofMachineLearningResearch),Vol.70.PMLR,\nWhyteandBailey2006].Finally,weplantoextendourmethodand\n2817–2826. http://proceedings.mlr.press/v70/pinto17a.html\nproveitseffectivenessbyapplyingitonaphysicalrobot. CarlEdwardRasmussenandChristopherK.I.Williams.2005.GaussianProcessesfor\nMachineLearning(AdaptiveComputationandMachineLearning).TheMITPress. JohnSchulman,SergeyLevine,PieterAbbeel,MichaelJordan,andPhilippMoritz.2015a.",
      "size": 942,
      "sentences": 5
    },
    {
      "id": 51,
      "content": "enandChristopherK.I.Williams.2005.GaussianProcessesfor\nMachineLearning(AdaptiveComputationandMachineLearning).TheMITPress. JohnSchulman,SergeyLevine,PieterAbbeel,MichaelJordan,andPhilippMoritz.2015a. TrustRegionPolicyOptimization.InProceedingsofthe32ndInternationalConference\nREFERENCES onMachineLearning(ProceedingsofMachineLearningResearch),FrancisBachand\nDavidBlei(Eds.),Vol.37.PMLR,Lille,France,1889–1897. http://proceedings.mlr. YoshuaBengio,JérômeLouradour,RonanCollobert,andJasonWeston.2009.Curricu- press/v37/schulman15.html\nlumLearning.InProceedingsofthe26thAnnualInternationalConferenceonMachine\nJohnSchulman,PhilippMoritz,SergeyLevine,MichaelJordan,andPieterAbbeel.2015b. Learning(ICML’09).ACM,NewYork,NY,USA,41–48. https://doi.org/10.1145/ High-dimensionalcontinuouscontrolusinggeneralizedadvantageestimation.arXiv\n1553374.1553380 preprintarXiv:1506.02438(2015). J.C.BongardandH.Lipson.2004.",
      "size": 903,
      "sentences": 8
    },
    {
      "id": 52,
      "content": ",NewYork,NY,USA,41–48. https://doi.org/10.1145/ High-dimensionalcontinuouscontrolusinggeneralizedadvantageestimation.arXiv\n1553374.1553380 preprintarXiv:1506.02438(2015). J.C.BongardandH.Lipson.2004. Automateddamagediagnosisandrecoveryfor JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov.2017. remoterobotics.InIEEEInternationalConferenceonRoboticsandAutomation,2004. ProximalPolicyOptimizationAlgorithms.(072017). Proceedings.ICRA’04.2004,Vol.4.3545–3550Vol.4. https://doi.org/10.1109/ROBOT. D.Silver,A.Huang,C.J.Maddison,A.Guez,L.Sifre,G.VanDeDriessche,J.Schrittwieser,\n2004.1308802 I.Antonoglou,V.Panneershelvam,M.Lanctot,S.Dieleman,D.Grewe,J.Nham,N. AliBorjiandLaurentItti.2013.Bayesianoptimizationexplainshumanactivesearch. Kalchbrenner,I.Sutskever,T.Lillicrap,M.Leach,K.Kavukcuoglu,T.Graepel,and\nInAdvancesinNeuralInformationProcessingSystems26.CurranAssociates,Inc.,\nD.Hassabis.2016.MasteringthegameofGowithdeepneuralnetworksandtree\n55–63. search.Nature(2016),484–489.",
      "size": 994,
      "sentences": 12
    },
    {
      "id": 53,
      "content": "Kavukcuoglu,T.Graepel,and\nInAdvancesinNeuralInformationProcessingSystems26.CurranAssociates,Inc.,\nD.Hassabis.2016.MasteringthegameofGowithdeepneuralnetworksandtree\n55–63. search.Nature(2016),484–489. GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman, DavidSilver,GuyLever,NicolasHeess,ThomasDegris,DaanWierstra,andMartin\nJieTang,andWojciechZaremba.2016.OpenAIGym.CoRRabs/1606.01540(2016). Riedmiller.2014.DeterministicPolicyGradientAlgorithms.InProceedingsofthe31st\narXiv:1606.01540 http://arxiv.org/abs/1606.01540 InternationalConferenceonInternationalConferenceonMachineLearning-Volume\nGuillaumeChaslot,SanderBakkes,IstvánSzita,andPieterSpronck.2008.Monte-Carlo 32(ICML’14).JMLR.org,I–387–I–395. http://dl.acm.org/citation.cfm?id=3044805. TreeSearch:ANewFrameworkforGameAI.InAIIDE. 3044850\nKonstantinosChatzilygeroudis,VassilisVassiliades,andJean-BaptisteMouret.2018. HesterT.2013.TheTEXPLOREAlgorithm.Springer,Heidelberg.",
      "size": 944,
      "sentences": 8
    },
    {
      "id": 54,
      "content": "cfm?id=3044805. TreeSearch:ANewFrameworkforGameAI.InAIIDE. 3044850\nKonstantinosChatzilygeroudis,VassilisVassiliades,andJean-BaptisteMouret.2018. HesterT.2013.TheTEXPLOREAlgorithm.Springer,Heidelberg. Reset-freeTrial-and-ErrorLearningforRobotDamageRecovery. Roboticsand JoshTobin,RachelFong,AlexRay,JonasSchneider,WojciechZaremba,andPieter\nAutonomousSystems100(2018),236–250. https://www.sciencedirect.com/science/ Abbeel.2017.Domainrandomizationfortransferringdeepneuralnetworksfrom\narticle/pii/S0921889017302440 simulationtotherealworld.In2017IEEE/RSJInternationalConferenceonIntelligent\nAntoineCully,JeffClune,DaneshTarapore,andJean-BaptisteMouret.2015.Robots RobotsandSystems(IROS).IEEE,23–30. thatcanadaptlikeanimals. Nature521,7553(28May2015),503–507. https: EmanuelTodorov,TomErez,andYuvalTassa.2012.",
      "size": 806,
      "sentences": 10
    },
    {
      "id": 55,
      "content": "lune,DaneshTarapore,andJean-BaptisteMouret.2015.Robots RobotsandSystems(IROS).IEEE,23–30. thatcanadaptlikeanimals. Nature521,7553(28May2015),503–507. https: EmanuelTodorov,TomErez,andYuvalTassa.2012. MuJoCo:Aphysicsenginefor\n//doi.org/10.1038/nature14422 model-basedcontrol.2012IEEE/RSJInternationalConferenceonIntelligentRobots\nHughDurrant-WhyteandTimBailey.2006.SimultaneousLocalisationandMapping andSystems(2012),5026–5033. (SLAM):PartITheEssentialAlgorithms.IEEERoboticsandAutomationMagazine2\n(2006),2006. KlausGreff,RupeshKSrivastava,JanKoutník,BasRSteunebrink,andJürgenSchmid-\nhuber.2017.LSTM:Asearchspaceodyssey.IEEEtransactionsonneuralnetworks\nandlearningsystems28,10(2017),2222–2232. SergioGuadarrama,AnoopKorattikara,OscarRamirez,PabloCastro,EthanHolly,Sam\nFishman,KeWang,EkaterinaGonina,NealWu,ChrisHarris,VincentVanhoucke,\nandEugeneBrevdo.2018. TF-Agents:AlibraryforReinforcementLearningin\nTensorFlow.https://github.com/tensorflow/agents.",
      "size": 950,
      "sentences": 9
    },
    {
      "id": 56,
      "content": ",EthanHolly,Sam\nFishman,KeWang,EkaterinaGonina,NealWu,ChrisHarris,VincentVanhoucke,\nandEugeneBrevdo.2018. TF-Agents:AlibraryforReinforcementLearningin\nTensorFlow.https://github.com/tensorflow/agents. https://github.com/tensorflow/\nagents[Online;accessed25-June-2019]. SeppHochreiterandJürgenSchmidhuber.1997. LongShort-TermMemory. Neural\nComput.9,8(Nov.1997),1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735\nJ.Hwangbo,I.Sa,R.Siegwart,andM.Hutter.2017. ControlofaQuadrotorWith\nReinforcementLearning. IEEERoboticsandAutomationLetters2,4(Oct2017),\n2096–2103. https://doi.org/10.1109/LRA.2017.2720851\nBrunoSicilianoOussamaKhatib.2005.GaussianProcessesforMachineLearning(Adap-\ntiveComputationandMachineLearning).TheMITPress. SylvainKoos,AntoineCully,andJean-BaptisteMouret.2013.Fastdamagerecovery\ninroboticswiththeT-resiliencealgorithm. TheInternationalJournalofRobot-\nicsResearch32,14(2013),1700–1723. https://doi.org/10.1177/0278364913499192\narXiv:https://doi.org/10.1177/0278364913499192",
      "size": 990,
      "sentences": 13
    }
  ]
}