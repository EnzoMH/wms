{
  "source": "ArXiv",
  "filename": "005_Efficiently_Manipulating_Clutter_via_Learning_and_.pdf",
  "total_chars": 223828,
  "total_chunks": 311,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\n5202\nyaM\n31\n]OR.sc[\n1v35880.5052:viXra\n©2025\nBaichuanHuang\nALLRIGHTSRESERVED\n=== 페이지 2 ===\nEFFICIENTLYMANIPULATINGCLUTTERVIALEARNINGAND\nSEARCH-BASEDREASONING\nBy\nBAICHUANHUANG\nAdissertationsubmittedtothe\nSchoolofGraduateStudies\nRutgers,TheStateUniversityofNewJersey\nInpartialfulfillmentoftherequirements\nForthedegreeof\nDoctorofPhilosophy\nGraduatePrograminComputerScience\nWrittenunderthedirectionof\nJingjinYu\nAndapprovedby\nNewBrunswick,NewJersey\nMay2025\n=== 페이지 3 ===\nABSTRACTOFTHEDISSERTATION\nEfficientlyManipulatingClutterviaLearningandSearch-BasedReasoning\nbyBAICHUANHUANG\nDissertationDirector: JingjinYu\nObject rearrangement is a fundamental and highly challenging problem in robotic\nmanipulation,encompassingadiversesetoftaskssuchasclutterremovalandobjectretrieval. These tasks require robots to intelligently plan and execute sequences of manipulation\nactions to reorganize objects or extract specific targets from cluttered environments.",
      "size": 956,
      "sentences": 2
    },
    {
      "id": 2,
      "content": "emovalandobjectretrieval. These tasks require robots to intelligently plan and execute sequences of manipulation\nactions to reorganize objects or extract specific targets from cluttered environments. The\nsignificanceofobjectrearrangementextendstonumerousreal-worldapplications,including\nwarehouse automation, household assistance, and industrial manufacturing. However,\nsolvingthisproblemefficientlyremainsdifficultduetothehigh-dimensionalconfiguration\nspaces, intricate object interactions, and long planning horizons involved. The ability to\ndevelopmoreadvancedsolutionstothesechallengesiscriticalforenablingrobotstooperate\nautonomouslyinunstructuredreal-worldsettings. This dissertation is motivated by the need for more efficient and robust manipulation\nplanning approaches that can be adapted to dynamic and complex environments. To\naddressthesechallenges,weproposeasetofnovelalgorithmsthatleveragethestrengthsof\nsearch-basedplanning,deeplearning,andparallelizedcomputation.",
      "size": 979,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "at can be adapted to dynamic and complex environments. To\naddressthesechallenges,weproposeasetofnovelalgorithmsthatleveragethestrengthsof\nsearch-basedplanning,deeplearning,andparallelizedcomputation. Ourworkfocuseson\nimprovingthepredictionofobjectinteractions,integratingthesepredictionsintotreesearch\nalgorithms,andutilizinghigh-performanceparallelcomputingtosignificantlyacceleratethe\nplanningprocess. Our research beginswith the development of theDeep Interaction PredictionNetwork\nii\n=== 페이지 4 ===\n(DIPN),whichenablesaccuratepredictionsofobjectmotionswhensubjectedtopushing\nactions. DIPN is trained to model object interactions with high precision, achieving\nover90%accuracyinpredictingthefinalposesofobjectsafterapush. Thissignificantly\nsurpassesexistingbaselinemethodsandallowsformorereliabledecision-makingincluttered\nenvironments. Buildingonthiscapability,weintegrateDIPNwithMonteCarloTreeSearch\n(MCTS)tooptimizetheplanningofnon-prehensileactionsforobjectretrievaltasks.",
      "size": 978,
      "sentences": 7
    },
    {
      "id": 4,
      "content": "lowsformorereliabledecision-makingincluttered\nenvironments. Buildingonthiscapability,weintegrateDIPNwithMonteCarloTreeSearch\n(MCTS)tooptimizetheplanningofnon-prehensileactionsforobjectretrievaltasks. This\nintegrated approach enables robots to autonomously determine effective sequences of push\nactions,leadingtoa100%completionrateinspecific,well-definedchallengingscenarios\nwhereheuristic-basedsolutionspreviouslystruggled. To further improve computational efficiency, we introduce the Parallel Monte Carlo\nTree Search with Batched Simulations (PMBS) framework. This framework leverages\nGPU-acceleratedphysicssimulationsto parallelizeplanningcomputations, achieving more\nthan a 30× speed-up compared to traditional serial implementations. Importantly, this\nacceleration does not compromise solution quality; PMBS maintains or even improves it,\ndemonstrating its effectiveness for real-time robotic planning.",
      "size": 907,
      "sentences": 6
    },
    {
      "id": 5,
      "content": "tional serial implementations. Importantly, this\nacceleration does not compromise solution quality; PMBS maintains or even improves it,\ndemonstrating its effectiveness for real-time robotic planning. Additionally, we combine\ndifferentmanipulationtechniques,suchaspick-and-placeandpush,tomakeourapproach\nmoreflexibleandadaptabletovarioustasks. Byintegratingdiversemanipulationtechniques,\noursystemcantackleawiderrangeofobjectrearrangementchallengesmoreeffectively. Extensiveexperimentsconductedinbothsimulatedenvironmentsandreal-worldrobotic\nsystems validate theefficacy of our proposed methods. Our findings demonstrate state-of-\nthe-artperformanceintermsofsuccessrates,solutionquality,andcomputationalefficiency\nacross a variety of complex rearrangement tasks.",
      "size": 761,
      "sentences": 6
    },
    {
      "id": 6,
      "content": "fficacy of our proposed methods. Our findings demonstrate state-of-\nthe-artperformanceintermsofsuccessrates,solutionquality,andcomputationalefficiency\nacross a variety of complex rearrangement tasks. By pushing the boundaries of robotic\nmanipulationcapabilities,thisworkcontributestotheadvancementofautonomousrobotic\nsystems, bringing us closer to deploying intelligent robots capable of handling complex\nobjectrearrangementtasksinreal-world,unstructuredenvironments. iii\n=== 페이지 5 ===\nACKNOWLEDGMENTS\nI would like to express my deepest gratitude to my advisor, Prof. Jingjin Yu, for his\ninspirationandguidancethroughoutmyresearchjourney. Yourexpertiseandmentorship\nwere instrumental in shaping every aspect of my work during my five-year Ph.D. study,\nnot only in academic matters but also in practical aspects such as building robust robotic\nsystemsandinguidingmycareerplanning. Iamalsoprofoundlythankfultomydissertation\ncommittee members.",
      "size": 940,
      "sentences": 6
    },
    {
      "id": 7,
      "content": ".D. study,\nnot only in academic matters but also in practical aspects such as building robust robotic\nsystemsandinguidingmycareerplanning. Iamalsoprofoundlythankfultomydissertation\ncommittee members. In particular, I would like to thank Prof. Abdeslam Boularias, who\ncontributed significantly to much of my works and provided insightful ideas that greatly\nenhancedmyresearch. Mysincereappreciationgoestoallmycommitteemembers,notonly\nfortheirvaluablecommentsandsuggestionsonthisdissertationbutalsofortheirbroader\nimpact on my academic growth. I am especially honored to have served as a teaching\nassistantforProf. KostasBekris,fromwhomIlearnedinvaluablelessonsabouteffective\nteaching. I extend myheartfeltgratitude toDr. Bowen Wen, amember of my committee,\nwhoseresearchfindingshavedeeplyinfluencedmyownworkandthisdissertation. I have been fortunate to collaborate, both formally and informally, with several of my\nlabmembers.",
      "size": 925,
      "sentences": 10
    },
    {
      "id": 8,
      "content": "Wen, amember of my committee,\nwhoseresearchfindingshavedeeplyinfluencedmyownworkandthisdissertation. I have been fortunate to collaborate, both formally and informally, with several of my\nlabmembers. IwouldparticularlyliketothankShuaiHanforhiscontributionstomyfirst\npaperatRutgers. Toallmylabmates: thankyouforyourcompanionshipandunwavering\nsupport during times of need. I am grateful to the funding agencies that supported my\nwork, as well as to my colleagues during my internships. Special thanks to Dr. Siddarth\nJainandtheteamatMitsubishiElectricResearchLaboratories,whereIgainedinvaluable\nindustrialexperience. IalsoappreciatetheopportunitiesprovidedbyCoupang,whichfurther\nbroadenedmyprofessionalhorizons. Lastbutcertainlynotleast,Iwanttoexpressmyheartfeltthankstomyparentsfortheir\nunconditionalloveandsupportthroughoutthisjourney. Yourencouragementhasbeenmy\nfoundationandinspiration. iv\n=== 페이지 6 ===\nTABLEOFCONTENTS\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 1000,
      "sentences": 44
    },
    {
      "id": 9,
      "content": "veandsupportthroughoutthisjourney. Yourencouragementhasbeenmy\nfoundationandinspiration. iv\n=== 페이지 6 ===\nTABLEOFCONTENTS\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv\nListofTables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi\nListofFigures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\nChapter1: Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1 DIPN: Deep Interaction Prediction Network with Application to Clutter\nRemoval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 VisualForesightTreesforObjectRetrievalfromClutterwithNonprehensile\nRearrangement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Interleaving Monte Carlo Tree Search and Self-Supervised Learning for\nObjectRetrievalinClutter . . . . . . . .",
      "size": 999,
      "sentences": 261
    },
    {
      "id": 10,
      "content": "sile\nRearrangement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Interleaving Monte Carlo Tree Search and Self-Supervised Learning for\nObjectRetrievalinClutter . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.4 ParallelMonteCarloTreeSearchwithBatchedRigid-bodySimulationsfor\nSpeedingupLong-HorizonEpisodicRobotPlanning . . . . . . . . . . . . 5\n1.5 TowardOptimalTabletopRearrangementwithMultipleManipulationPrimi-\ntives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nChapter2: RelatedWorks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1 Prehensilevs. Non-PrehensileManipulation . . . . . . . . . . . . . . . . . 7\n2.2 ObjectGrasping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nv\n=== 페이지 7 ===\n2.2.1 Analyticalvs. Data-DrivenApproaches . . . . . . . . . . . . . . . 8\n2.2.2 GraspinginDenseEnvironments . . . . . . . . . . . . . . . . . . . 8\n2.3 Pushing . . . . . . . . . . . . . . . . . . . . . .",
      "size": 999,
      "sentences": 245
    },
    {
      "id": 11,
      "content": "1 Analyticalvs. Data-DrivenApproaches . . . . . . . . . . . . . . . 8\n2.2.2 GraspinginDenseEnvironments . . . . . . . . . . . . . . . . . . . 8\n2.3 Pushing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.4 Push-Grasping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.5 Singulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.5.1 DefinitionandTechniques . . . . . . . . . . . . . . . . . . . . . . 10\n2.5.2 LimitationsandExtensions . . . . . . . . . . . . . . . . . . . . . . 10\n2.6 ObjectRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.6.1 ProblemSettingandChallenges . . . . . . . . . . . . . . . . . . . 11\n2.6.2 Model-Freevs. Model-BasedApproaches . . . . . . . . . . . . . . 11\n2.7 RearrangementPlanning . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.8 TaskandMotionPlanning(TAMP) . . . . . . . . . . . . . . . . . . . . . .",
      "size": 951,
      "sentences": 297
    },
    {
      "id": 12,
      "content": "pproaches . . . . . . . . . . . . . . 11\n2.7 RearrangementPlanning . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.8 TaskandMotionPlanning(TAMP) . . . . . . . . . . . . . . . . . . . . . . 12\n2.9 MonteCarloTreeSearch(MCTS)forManipulation . . . . . . . . . . . . . 12\nChapter3: DIPN: Deep Interaction Prediction Network with Application to\nClutterRemoval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.2 ProblemFormulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.3.1 DeepInteractionPredictionNetwork(DIPN) . . . . . . . . . . . . 17\n3.3.2 TheGraspNetwork(GN) . . . . . . . . . . . . . . . . . . . . . . . 21\n3.3.3 TheCompleteAlgorithmicPipeline . . . . . . . . . . . . . . . . . 22\n3.4 ExperimentalEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 984,
      "sentences": 280
    },
    {
      "id": 13,
      "content": ". . . . . . . . . . . . . . . . . . . . . 21\n3.3.3 TheCompleteAlgorithmicPipeline . . . . . . . . . . . . . . . . . 22\n3.4 ExperimentalEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.4.1 DeepInteractionPredictionNetwork(DIPN) . . . . . . . . . . . . 23\nvi\n=== 페이지 8 ===\n3.4.2 GraspNetwork(GN) . . . . . . . . . . . . . . . . . . . . . . . . . 25\n3.4.3 EvaluationoftheCompletePipeline . . . . . . . . . . . . . . . . . 26\n3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nChapter4: VisualForesightTreesforObjectRetrievalfromClutterwithNon-\nprehensileRearrangement . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.2 ProblemFormulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4.2.1 ProblemStatement . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4.2.2 ManipulationMotionPrimitives . . . . . . . . . . . . . . . . . . .",
      "size": 993,
      "sentences": 286
    },
    {
      "id": 14,
      "content": ". . . . . . . . . . . . . . . . . . . . . . . 32\n4.2.1 ProblemStatement . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4.2.2 ManipulationMotionPrimitives . . . . . . . . . . . . . . . . . . . 32\n4.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.3.1 OverviewoftheProposedApproach . . . . . . . . . . . . . . . . . 34\n4.3.2 VisualForesightTrees . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.3.3 GraspNetwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.3.4 PushPredictionNetwork . . . . . . . . . . . . . . . . . . . . . . . 36\n4.3.5 VisualForesightTreeSearch(VFT) . . . . . . . . . . . . . . . . . 37\n4.4 ExperimentalEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4.4.1 ExperimentSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.4.2 NetworkTrainingProcess . . . . . . . . . . . . . . . . . . . . . . 43\n4.4.3 ComparedMethodsandEvaluationMetrics . . . . . . . . . . . . .",
      "size": 974,
      "sentences": 299
    },
    {
      "id": 15,
      "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.4.2 NetworkTrainingProcess . . . . . . . . . . . . . . . . . . . . . . 43\n4.4.3 ComparedMethodsandEvaluationMetrics . . . . . . . . . . . . . 44\n4.4.4 SimulationStudies . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.4.5 EvaluationonaRealSystem . . . . . . . . . . . . . . . . . . . . . 46\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\nvii\n=== 페이지 9 ===\nChapter5: InterleavingMonteCarloTreeSearchandSelf-SupervisedLearning\nforObjectRetrievalinClutter . . . . . . . . . . . . . . . . . . . . . . 50\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n5.2 ProblemFormulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n5.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n5.3.1 Monte-CarloTreeSearch . . . . . . . . . . . . . . . . . . . . . . . 54\n5.3.2 PushPredictionNetwork(PPN) . . . . . . . . . . . . . . .",
      "size": 999,
      "sentences": 299
    },
    {
      "id": 16,
      "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n5.3.1 Monte-CarloTreeSearch . . . . . . . . . . . . . . . . . . . . . . . 54\n5.3.2 PushPredictionNetwork(PPN) . . . . . . . . . . . . . . . . . . . 56\n5.3.3 GuidedMonte-CarloTreeSearch . . . . . . . . . . . . . . . . . . . 58\n5.4 ExperimentalEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n5.4.1 Simulationexperiments . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.4.2 RobotExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\nChapter6: Parallel Monte Carlo Tree Search with Batched Rigid-body Simula-\ntionsforSpeedingupLong-HorizonEpisodicRobotPlanning . . . . 67\n6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n6.2 ProblemFormulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 998,
      "sentences": 299
    },
    {
      "id": 17,
      "content": ". . . . . . . . . . . . . . . . . 67\n6.2 ProblemFormulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.3.1 SerialMCTSforObjectRetrievalfromClutter . . . . . . . . . . . 71\n6.3.2 AdaptionsforGPU . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n6.3.3 ParallelMCTSwithBatchedSimulation . . . . . . . . . . . . . . . 74\n6.4 ExperimentalEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n6.4.1 SimulationStudies . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n6.4.2 RealRobotExperiments . . . . . . . . . . . . . . . . . . . . . . . 83\nviii\n=== 페이지 10 ===\n6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\nChapter7: TowardOptimalTabletopRearrangementwithMultipleManipula-\ntionPrimitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 999,
      "sentences": 306
    },
    {
      "id": 18,
      "content": "opRearrangementwithMultipleManipula-\ntionPrimitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n7.2 ProblemFormulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n7.2.1 RearrangementwithMultipleManipulationPrimitives . . . . . . . 87\n7.2.2 MonteCarloTreeSearch . . . . . . . . . . . . . . . . . . . . . . . 88\n7.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n7.3.1 ActionSpaceDesign . . . . . . . . . . . . . . . . . . . . . . . . . 89\n7.3.2 HierarchicalBest-FirstSearch . . . . . . . . . . . . . . . . . . . . 90\n7.3.3 SpeedingupMCTSwithParallelism . . . . . . . . . . . . . . . . . 91\n7.3.4 AdaptingMCTSfor REMP . . . . . . . . . . . . . . . . . . . . . . 92\n7.4 ExperimentalEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n7.4.1 SimulationStudies . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 980,
      "sentences": 293
    },
    {
      "id": 19,
      "content": ". . . . . . . . . . . . . . . . . . 92\n7.4 ExperimentalEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n7.4.1 SimulationStudies . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n7.4.2 AblationStudies . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n7.4.3 RealRobotExperiments . . . . . . . . . . . . . . . . . . . . . . . 98\n7.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\nChapter8: Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nAppendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\nAppendixA: Chapter6-PMBSSupplementary . . . . . . . . . . . . . . . . . . 104\nAppendixB: Chapter7-REMPSupplementary . . . . . . . . . . . . . . . . . 107\nix\n=== 페이지 11 ===\nAcknowledgmentofPreviousPublications . . . . . . . . . . . . . . . . . . . . . . 108\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 972,
      "sentences": 324
    },
    {
      "id": 20,
      "content": ". . 107\nix\n=== 페이지 11 ===\nAcknowledgmentofPreviousPublications . . . . . . . . . . . . . . . . . . . . . . 108\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\nx\n=== 페이지 12 ===\nLISTOFTABLES\n3.1 Simulation,randomandhardinstances(mean%) . . . . . . . . . . . . . . 28\n3.2 Realsystem,randomandhardinstances(mean%) . . . . . . . . . . . . . . 28\n4.1 Simulationresultsforthe10testcasesfrom[48]. . . . . . . . . . . . . . . 46\n4.2 Simulationresultforthe22testcasesin Figure4.4. . . . . . . . . . . . . . 46\n4.3 Realexperimentresultsforthe22TestcasesinFigure4.4. . . . . . . . . . 48\n4.4 Realexperimentresultsforcases19to22inFigure4.4. . . . . . . . . . . . 48\n5.1 Simulateexperiment resultsfor 22casesChapter 4. Budgetsof MCTSand\nMOREarelimitedupto50iterations. . . . . . . . . . . . . . . . . . . . . 62\n5.2 Simulateexperimentresultsfor10cases[48]. Budgetsof MCTSandMORE\narelimitedupto50iterations. . . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 985,
      "sentences": 190
    },
    {
      "id": 21,
      "content": "50iterations. . . . . . . . . . . . . . . . . . . . . 62\n5.2 Simulateexperimentresultsfor10cases[48]. Budgetsof MCTSandMORE\narelimitedupto50iterations. . . . . . . . . . . . . . . . . . . . . . . . . 62\n5.3 RealexperimentresultsforsixcasesasshowninFigure5.8. Thebudgetof\nMCTSandMOREislimitedto10iterations. Forgo-PGN,onlythefirstfour\ncasesapply,andresultsarefrom[48]. Onlyplanningtimeisrecorded(robot\nexecutionwasintentionallysloweddownforsafety). Thecomputationtime\nfor PPNtosolveataskis3secondsonaverage(estimated). . . . . . . . . 65\n6.1 Simulationexperimentresultsfor20cases. Timebudgetsarelimitedupto\n60seconds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n6.2 Realrobotexperimentresultsonthesixmostdifficultcases. Timebudgets\narelimitedto60secondspercase. . . . . . . . . . . . . . . . . . . . . . . 84\n7.1 Summary of simulation results (25 cases) and real-robot experiments (15\ncases)for HBFSandPMMR-40. . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 982,
      "sentences": 144
    },
    {
      "id": 22,
      "content": "ase. . . . . . . . . . . . . . . . . . . . . . . 84\n7.1 Summary of simulation results (25 cases) and real-robot experiments (15\ncases)for HBFSandPMMR-40. . . . . . . . . . . . . . . . . . . . . . . . 96\nxi\n=== 페이지 13 ===\n7.2 Ablationstudyresults(averagedover40cases),forcomparisonwithTable7.1. 97\n7.3 Experiment results of real robot trials across 15 cases, with time budgets\nconstrainedtoamaximum of40secondsforasingleMCTS run. Therobot\ntimeisonlyconsideredincaseswhere bothmethodssucceedatleastonce. Additionally, benchmarks from simulationscovering 15 casesare included\nforsim-to-realgapcomparisons. TherobottimeforPMMR-40andHBFS,\ndenotedwithanasterisk,isrecordedonlyforsuccessfulcases. . . . . . . . 99\nxii\n=== 페이지 14 ===\nLISTOFFIGURES\n1.1 Examples of robot manipulation tasks. (a) Grasping objects from clutter:\nThis can involve grasping all objects or targeting a specific item. The\nchallengeliesincreatingsufficientspaceforthegrippertoaccessobjects.",
      "size": 956,
      "sentences": 62
    },
    {
      "id": 23,
      "content": "obot manipulation tasks. (a) Grasping objects from clutter:\nThis can involve grasping all objects or targeting a specific item. The\nchallengeliesincreatingsufficientspaceforthegrippertoaccessobjects. (b)Dynamicgrasping: Manipulatingmovingobjects,suchasreceivingan\nitemfromahumanhand. (c)Objectrearrangement: Reorganizingobjectsto\nachieve a desired layout, similar to housekeeping. This task requires both\nhigh-levelplanningandprecisemotioncontrol. . . . . . . . . . . . . . . . 2\n1.2 Structure of the dissertation. Chapter 3 introduces the Deep Interaction\nPredictionNetworkforone-steppushpredictioninclutterremoval. Chapter4\nextendstomulti-stepplanningforefficientobjectretrievalusingpushactions. Chapters5and6exploreGPU-acceleratedMonteCarlotreesearch: Chapter5\nfocusesonlearningastrategicnetworktoguidetreesearch,whileChapter6\nutilizesIsaacGymforparallelsimulationsinrealrobotexecution. Chapter7\napplies similar concepts to object rearrangement in constrained spaces,\nincorporatingmotionplanning.",
      "size": 999,
      "sentences": 26
    },
    {
      "id": 24,
      "content": "guidetreesearch,whileChapter6\nutilizesIsaacGymforparallelsimulationsinrealrobotexecution. Chapter7\napplies similar concepts to object rearrangement in constrained spaces,\nincorporatingmotionplanning. . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.1 (a)Thesystemsetupincludesaworkspacewithobjectstoremove,aUniversal\nRobots UR-5e manipulator with a Robotiq 2F-85 two-finger gripper, and\nan Intel RealSense D435 RGB-D camera. (b) An example push action\nand superimposed images of scenes before and after the push. (c) System\narchitectureofourpipeline,andonepredictedimagethatDIPNcangenerate\nfor the push shown in (b). Notice the similarity between the predicted\nsyntheticimageandtherealimageresultingfromthepushaction. . . . . . 15\n3.2 DIPNflowwithanexample. Thenetworkcomponentsdedicatedtoanobject\narecolor-codedtomatchtheobject. Weonlyshowthefullnetworkforthe\nblue triangle object; the instance-specific structures for the other objects\nshare the same weights and are simplified as dashed lines.",
      "size": 999,
      "sentences": 38
    },
    {
      "id": 25,
      "content": "arecolor-codedtomatchtheobject. Weonlyshowthefullnetworkforthe\nblue triangle object; the instance-specific structures for the other objects\nshare the same weights and are simplified as dashed lines. Components\ninsidetheorangedottedlinearethecoreoftheDIPN.Theoutputimageis\nsynthesizedbyapplyingthepredictedtransformationstotheobjectsegments. 17\n3.3 Sampledactioninpurplearrowsaroundeachobject. . . . . . . . . . . . . . 18\nxiii\n=== 페이지 15 ===\n3.4 Architectureof GN.Pink,blue,andgreentextareusedforchannelcount,\nimagesize,andkernelsize,respectively. . . . . . . . . . . . . . . . . . . 21\n3.5 DIPNlearningcurvewithstandarddeviationshownasshadedregions. The\nx-axisisthenumberofpushesfortrainingDIPN.They-axisistheprediction\nerror: 1−IoU. Thedottedanddashedlinesarebaselines. . . . . . . . . . 24\n3.6 Typical DIPN results. The figures from left to right are: original and\npredicted images in simulation, and original and predicted images in a\nreal experiment.",
      "size": 955,
      "sentences": 50
    },
    {
      "id": 26,
      "content": "esarebaselines. . . . . . . . . . 24\n3.6 Typical DIPN results. The figures from left to right are: original and\npredicted images in simulation, and original and predicted images in a\nreal experiment. The ground truth images after a push are overlaid on the\npredictedimageswithtransparency. Thearrowsvisualizethepushactions. 25\n3.7 Manuallygeneratedhardinstanceslargelysimilartotheonesin[27]. The\ncasesareusedinbothsimulationandrealexperiment. . . . . . . . . . . . 25\n3.8 GrasplearningcurvesofalgorithmsforPaGinsimulation. Thex-axisisthe\ntotalnumber oftrainingsteps, i.e.,number ofactions taken, includingpush\nandgrasp. They-axisisthegraspsuccessrate. Thedashedlinesdenotethe\nsuccessrateforagrasprightafterapushaction. . . . . . . . . . . . . . . 26\n3.9 Grasplearningcurvesfor PaGinrealexperiment. Solidlinesindicategrasp\nsuccess rate and dotted lines indicate push-then-grasp success rates over\ntrainingsteps. TheGNistrainedinagrasponlymanner. . . . . . . . . . .",
      "size": 964,
      "sentences": 58
    },
    {
      "id": 27,
      "content": "ngcurvesfor PaGinrealexperiment. Solidlinesindicategrasp\nsuccess rate and dotted lines indicate push-then-grasp success rates over\ntrainingsteps. TheGNistrainedinagrasponlymanner. . . . . . . . . . . 28\n4.1 (a)ThehardwaresetupforobjectretrievalinaclutterincludesaUniversal\nRobotsUR-5emanipulatorwithaRobotiq2F-85two-fingergripper,andan\nIntel RealSense D435 RGB-D camera. The objects are placed in a square\nworkspace. (b),(c), (d) Threesequential push actions(green arrows)create\nspace to access the target (purple) object. The push directions are toward\ntop-left, top-right, and bottom-right, respectively. (e) The target object is\nsuccessfullygraspedandretrieved. . . . . . . . . . . . . . . . . . . . . . 31\n4.2 Overview of the proposed technique for object retrieval from clutter with\nnonprehensilerearrangement. Theproblemisiterativelysolvedbyobserving\nthe environment at each time step, taking the current state as input, and\nreturningthebestaction. Itisrepeateduntiltheobjectisretrieved. . . .",
      "size": 999,
      "sentences": 45
    },
    {
      "id": 28,
      "content": "erearrangement. Theproblemisiterativelysolvedbyobserving\nthe environment at each time step, taking the current state as input, and\nreturningthebestaction. Itisrepeateduntiltheobjectisretrieved. . . . . . 33\n4.3 Exampleof4consecutivepushesshowingthat DIPNcanaccuratelypredict\npushoutcomes overalong horizon. Weusepurplearrowstoillustratepush\nactions. Thefirstandsecondcolumnsarethepredictionsandgroundtruth\n(objects’ positionsafterexecutingthe pushes)insimulation. Thethirdand\nfourth columns show results on a real system. The last column is the side\nview of the push result. Each row represents the push outcome with the\npreviousrowastheinputobservation. . . . . . . . . . . . . . . . . . . . . 38\nxiv\n=== 페이지 16 ===\n4.4 22Testcasesusedinbothsimulationandrealworldexperiments. Thetarget\nobjectsareblue. Imagesarezoomedinforbettervisualization. . . . . . . 42\n4.5 Simulationresultspertestcaseforthe10problemsfrom[48].",
      "size": 916,
      "sentences": 44
    },
    {
      "id": 29,
      "content": "4.4 22Testcasesusedinbothsimulationandrealworldexperiments. Thetarget\nobjectsareblue. Imagesarezoomedinforbettervisualization. . . . . . . 42\n4.5 Simulationresultspertestcaseforthe10problemsfrom[48]. Thehorizontal\naxisshowstheaveragenumberofactionsusedtosolveaprobleminstance:\nthelower,thebetter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.6 Simulationresultpertestcaseforthe22harderproblems(Figure4.4). The\nhorizontalaxisshowstheaveragenumberofactionsusedtosolveaproblem\ninstance: thelower,thebetter. . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.7 Realexperimentresultspertestcaseforthe22harderproblems(Figure4.4). The horizontal axis shows the average number of actions used to solve a\nprobleminstance: thelower,thebetter. . . . . . . . . . . . . . . . . . . . 47\n4.8 Test scenario with soap boxes and masked (in purple) 3D printed vehicle. Twopushactionsandonegraspaction. . . . . . . . . . . . . . . . . . . .",
      "size": 941,
      "sentences": 108
    },
    {
      "id": 30,
      "content": "thebetter. . . . . . . . . . . . . . . . . . . . 47\n4.8 Test scenario with soap boxes and masked (in purple) 3D printed vehicle. Twopushactionsandonegraspaction. . . . . . . . . . . . . . . . . . . . 48\n5.1 (a)Thehardwaresetupforobject-retrieval-from-clutterincludesaUniversal\nRobotsUR5emanipulatorwithaRobotiq2F-85two-fingergripper,andan\nIntel RealSense D455 RGB-D camera. The objects are placed in a square\nworkspaceandthetargetobjectismaskedinpurple. (b)(c)Twopushactions\n(shown with green arrows) are used to enable the grasping of the target\n(purple)object. (d)Thetargetobjectissuccessfullygraspedandretrieved. (e)Theoverviewofouroverallsystem. . . . . . . . . . . . . . . . . . . . 51\n5.2 Sampledpushactions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n5.3 The left two figures are the input to PPN. The first is a segmentation of\nobjects;thesecondisthemaskofthetargetobject.",
      "size": 898,
      "sentences": 97
    },
    {
      "id": 31,
      "content": "ledpushactions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n5.3 The left two figures are the input to PPN. The first is a segmentation of\nobjects;thesecondisthemaskofthetargetobject. Theimageontherightis\ntheoutputfromthePPN.WeuseJetcolormaptorepresenttherewardvalue,\nwhere the value ranges from red (high) to blue (low). The pixel with the\nhighest Q-value is plotted with a circle and attached with an arrow on the\nrightimage,representingpushingactionstartingatthecircleandmovingto\ntherightwithadistanceof10cm. . . . . . . . . . . . . . . . . . . . . . . 57\n5.4 AnexampleoftheguidedMCTSwithabudgetof10iterations. Statewith\nlargerimagehavehigherestimatedQ-values. Allexpandednodesareplotted. The numbersin thefirst levelsrepresent theestimatedQ-value returnedby\nPPNforcorrespondingpushaction. Thesevalues,togetherwiththereward\nreturnedfromsimulation,guidethetreesearch. . . . . . . . . . . . . . . . 59\n5.5 Theaveragenumber(outof5trials)ofactionusedtosolveonecasefor22\ncases. . . . .",
      "size": 999,
      "sentences": 81
    },
    {
      "id": 32,
      "content": "haction. Thesevalues,togetherwiththereward\nreturnedfromsimulation,guidethetreesearch. . . . . . . . . . . . . . . . 59\n5.5 Theaveragenumber(outof5trials)ofactionusedtosolveonecasefor22\ncases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nxv\n=== 페이지 17 ===\n5.6 Theaveragetime(of5trials)usedtosolveonecasefor22cases. . . . . . . 63\n5.7 DifferentamountsoftrainingdataareusedtotrainPPN,whichareevaluated\nonMOREwithdifferentbudgets(iteration). Thisistheevaluationofthe22\ncases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n5.8 Manually generated casessimilar to[48]and Chapter4. Thetargetobjectis\nmasked inpurple. These cases areused alsoin simulationexperimentsas\nshowninFigure4.4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n5.9 Thenumberofactionandtimeusedonsolvingsixcases. Thebudgetisup\nto10iterationsfor MCTSandMORE. . . . . . . . . . . . . . . . . . . . .",
      "size": 937,
      "sentences": 155
    },
    {
      "id": 33,
      "content": "4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n5.9 Thenumberofactionandtimeusedonsolvingsixcases. Thebudgetisup\nto10iterationsfor MCTSandMORE. . . . . . . . . . . . . . . . . . . . . 65\n6.1 (a)The hardwaresetup includesa Universal RobotsUR-5ewith aRobotiq\n2F-85two-fingergripper andanIntelRealSense D455RGB-Dcamera. (b)\nPlanningandsimulationcarriedoutinphysicssimulatorwherethousands\nofvirtualrobotsoperateinparallel. (c)Overviewofoursystem;thesmall\nbluecylinderatthecenteristhetargetobjecttoberetrieved. . . . . . . . . 68\n6.2 Sampledpushactions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n6.3 Examples of using the grasp classifier to produce probabilities to grasp\nthe object at center (blue in this case). Here we used an RGB image for\nillustrationpurpose(inputshouldbeadepthimage). . . . . . . . . . . . . . 74\n6.4 StepsinPMBS,ourparallelMCTSwithbatchedoperation. . . . . . . . .",
      "size": 919,
      "sentences": 116
    },
    {
      "id": 34,
      "content": "ter (blue in this case). Here we used an RGB image for\nillustrationpurpose(inputshouldbeadepthimage). . . . . . . . . . . . . . 74\n6.4 StepsinPMBS,ourparallelMCTSwithbatchedoperation. . . . . . . . . 76\n6.5 20casesfromChapter4usedinsimulationexperiments,wherethetarget\nobjecthasabluemask. Noobjectshouldexceedtheboundary(redlines). . 80\n6.6 Theaveragenumber(overfiveindependenttrials)ofactionspercaseneeded\nforsolvingthetwentycases,givenatimebudgetof60seconds. . . . . . . 81\n6.7 Theaveragetime(overfiveindependenttrials)percaseneededforsolving\nthetwentycases,givenatimebudgetof60seconds. . . . . . . . . . . . . 81\n6.8 PMBSandserialMCTSevaluatedwithdifferenttimebudgets. Thereported\nvaluesareaveragesoverall20cases. . . . . . . . . . . . . . . . . . . . . 83\n6.9 The number of actions and time used for solving the six most challenging\ncasesonthephysicalrobot. Thetimebudgetis60seconds. . . . . . . . .",
      "size": 903,
      "sentences": 79
    },
    {
      "id": 35,
      "content": "rall20cases. . . . . . . . . . . . . . . . . . . . . 83\n6.9 The number of actions and time used for solving the six most challenging\ncasesonthephysicalrobot. Thetimebudgetis60seconds. . . . . . . . . 84\nxvi\n=== 페이지 18 ===\n7.1 (a)Overviewofsystemsetup,acameraismountedontheend-effectorfor\nperception. (b)-(d) An example caseand anintermediate step insolvingit. (e)Exampleobjectsrequiringapush. (f)pick-n-placemaybreakthebook. (g)pick-n-placewillseparateabox,failingtopickitup. . . . . . . . . . . 86\n7.2 Consideractionsamplingforlabeled3tobemanipulatedusingpush(there\nare a total of four objects). The absence of sampled actions in the right\nregionisattributedtoobstructionsposedbyobjects0,1,and2,preventing\nthemovementofobject3tothatarea. . . . . . . . . . . . . . . . . . . . . 90\n7.3 Examplecases. Thetoprowshowsthestartstatesandthebottomgoalstates. Lightly shaded objects can be pick-n-placed; heavily shaded objects must\nbe manipulated using push.",
      "size": 951,
      "sentences": 71
    },
    {
      "id": 36,
      "content": ". . . . . . . . . . . . . 90\n7.3 Examplecases. Thetoprowshowsthestartstatesandthebottomgoalstates. Lightly shaded objects can be pick-n-placed; heavily shaded objects must\nbe manipulated using push. Cases 4.1, r.7.3, and r.8.3 are evaluated and\npresentedinFigure7.4. Objectsaredistinguishedbycolor. . . . . . . . . . 94\n7.4 AsanexpandedillustrationofTable7.1,theupperplotliststhenumberof\nactionstherobotexecutestoresolveindividualcases. Thelowerplotlists\nthe robot’s execution times in solving the individual cases following the\ncomputed plan. For thelabels onthe horizontal axis,the firstdigit indicates\nthe number of objects contained within each case, while the second digit\nrepresentstheindex ofthecases. Casesbeginning withtheprefix’r’arethe\nonesthatareconstructedforandexecutedbythereal-robotsetup. . . . . . 95\n7.5 PMMR is evaluated with different time budgets. The reported values are\naveragedover40cases. . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "size": 969,
      "sentences": 66
    },
    {
      "id": 37,
      "content": "orandexecutedbythereal-robotsetup. . . . . . 95\n7.5 PMMR is evaluated with different time budgets. The reported values are\naveragedover40cases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n7.6 Thefullsetofobjectsusedinourreal-robotexperiments. . . . . . . . . . 98\n7.7 AsanexpandedillustrationofTable7.3,thisplotillustratesthenumberof\nactionstherobotexecutestoresolveindividualcases. . . . . . . . . . . . . 99\nA.1 Casestudyoneofrealtosimtorealgap. Simulatorprovidesaccuratephysics\nsimulations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\nA.2 Case study two of real to sim to real gap. Simulator provide non-accurate\nbutreasonablephysicssimulations. . . . . . . . . . . . . . . . . . . . . . 106\nB.1 Somecasesinrealworldsetup. . . . . . . . . . . . . . . . . . . . . . . . 107\nxvii\n=== 페이지 19 ===\n1\nCHAPTER1\nINTRODUCTION\nRoboticmanipulation[1]isafundamentalcapabilitythatenablesvariousapplicationsacross\nmany industries.",
      "size": 962,
      "sentences": 143
    },
    {
      "id": 38,
      "content": "etup. . . . . . . . . . . . . . . . . . . . . . . . 107\nxvii\n=== 페이지 19 ===\n1\nCHAPTER1\nINTRODUCTION\nRoboticmanipulation[1]isafundamentalcapabilitythatenablesvariousapplicationsacross\nmany industries. In warehouse automation, robots are revolutionizing order fulfillment\nand inventory management [2]. In healthcare, robotic systems assist in surgeries, patient\ncare, and laboratory tasks [3]. Human-robot collaboration is enhancing productivity in\nmanufacturing and assembly lines [4]. In disaster response scenarios, search and rescue\nrobots navigate hazardous environments to locate and assist survivors [5]. These diverse\napplicationshighlightthecriticalroleofroboticmanipulation. Among theseapplications, clutter removal isan importanttask involving the grasping\nand extracting of objects from cluttered environments such as bins or conveyor belts [6]. Object retrieval is a closely related but distinct challenge, where the goal is to extract a\nspecifictargetobjectfrom aclutteredscene.",
      "size": 990,
      "sentences": 32
    },
    {
      "id": 39,
      "content": "from cluttered environments such as bins or conveyor belts [6]. Object retrieval is a closely related but distinct challenge, where the goal is to extract a\nspecifictargetobjectfrom aclutteredscene. Thiscapabilityiscrucialforhouseholdrobots\ntasked with retrieving items from a pile, where specific items need to be identified and\nretrieved [7]. Beyond removal and retrieval, object rearrangement [8] presents its own\nchallenges, requiring robots to reorganize objects within a given space. Furthermore, the\nability to manipulate objects in dynamic environments, where items may be moving or\nrolling, adds another layer of complexity to these tasks [9]. These challenges, illustrated\ninFigure1.1,allfallunderthebroadercategoryofrobotmanipulationtasksandrepresent\nactiveareasofresearch. The field of robotic manipulation has a rich history of research and development. Approaches to solving these problems have generally fallen into two main categories:\nlearning-basedmethodsandanalyticalalgorithms.",
      "size": 997,
      "sentences": 8
    },
    {
      "id": 40,
      "content": "robotic manipulation has a rich history of research and development. Approaches to solving these problems have generally fallen into two main categories:\nlearning-basedmethodsandanalyticalalgorithms. Learning-basedmethods,whichrelyon\nneural networks to determine the next action, show promise in performing specific tasks. However, they currently face limitations regarding stability and generalization due to the\n=== 페이지 20 ===\n2\n(a)GraspingallfromClutter (b)GraspingtargetfromClutter\n(c)ObjectRearrangement: Fromstart(left)togoal(right)\nFigure1.1: Examplesofrobotmanipulationtasks. (a)Graspingobjectsfromclutter: This\ncaninvolve grasping allobjectsortargetingaspecificitem. Thechallenge liesincreating\nsufficient space for the gripper to access objects. (b) Dynamic grasping: Manipulating\nmovingobjects,suchasreceivinganitemfromahumanhand. (c)Objectrearrangement:\nReorganizingobjectstoachieveadesiredlayout,similartohousekeeping. Thistaskrequires\nbothhigh-levelplanningandprecisemotioncontrol.",
      "size": 995,
      "sentences": 9
    },
    {
      "id": 41,
      "content": "gobjects,suchasreceivinganitemfromahumanhand. (c)Objectrearrangement:\nReorganizingobjectstoachieveadesiredlayout,similartohousekeeping. Thistaskrequires\nbothhigh-levelplanningandprecisemotioncontrol. scarcityoftrainingdataintheroboticsdomain[10]. Ontheotherhand,analyticalalgorithms,\nwhile often more stable, struggle with defining metrics and rules from visual inputs, face\nchallengesinexploringvastsolutionspaces,andfrequentlylacktheflexibilitytoadaptto\nvariedproblemsorchangesinthescene. Thesemethodsareoftensensitivetoenvironmental\nvariationsandstruggletogeneralizeacrossdifferentscenarios[11]. Giventhesechallenges,developingrobustandversatilealgorithmsforreal-worldrobotic\nmanipulation applications is paramount. Such algorithms must be capable of planning\nsolutions,reasoningaboutphysicalinteractionsbetweenobjects,andcontrollingtherobot\nto complete tasks successfully.",
      "size": 876,
      "sentences": 8
    },
    {
      "id": 42,
      "content": "c\nmanipulation applications is paramount. Such algorithms must be capable of planning\nsolutions,reasoningaboutphysicalinteractionsbetweenobjects,andcontrollingtherobot\nto complete tasks successfully. We aim to create a general framework supporting similar\nbut distinct applications, balancing solution quality and computational efficiency. This\n=== 페이지 21 ===\n3\ndissertation aims to address these challenges by proposing approaches combining the\nstrengths of learning-based methods and analytical algorithms in robotics. We explore\nusing deep learning to predict object interactions, integrate these predictions with tree\nsearch algorithms for efficient planning, and leverage parallel computing to accelerate\ndecision-making processes. Our work spans various aspects of robotic manipulation, from\nobject retrieval in cluttered environments to dynamic grasping of moving objects, with\ntheoverarchinggoalofadvancingthecapabilitiesofroboticsystemsinhandlingcomplex,\nreal-worldmanipulationtasks.",
      "size": 992,
      "sentences": 6
    },
    {
      "id": 43,
      "content": "from\nobject retrieval in cluttered environments to dynamic grasping of moving objects, with\ntheoverarchinggoalofadvancingthecapabilitiesofroboticsystemsinhandlingcomplex,\nreal-worldmanipulationtasks. Throughaseriesofinterconnectedstudies,wedemonstrate\nhow our proposed methods achieve state-of-the-art performance in various challenging\nmanipulation scenarios. By focusing on both the theoretical foundations and practical\nimplementationsofthesealgorithms,wecontributetothebroadergoalofdeployingversatile\nandefficientroboticsystemsinunstructuredreal-worldenvironments. Thisdissertationisstructuredtoalignwiththeplannedchapters,eachcorrespondingto\npublishedresearchwork[12]–[23]. 1.1 DIPN:DeepInteractionPredictionNetworkwithApplicationtoClutterRemoval\nChapter 3 introduces the Deep Interaction Prediction Network (DIPN), a neural model\ndesignedtopredicttheeffectsofpushactionsinclutteredenvironments.",
      "size": 900,
      "sentences": 5
    },
    {
      "id": 44,
      "content": "onPredictionNetworkwithApplicationtoClutterRemoval\nChapter 3 introduces the Deep Interaction Prediction Network (DIPN), a neural model\ndesignedtopredicttheeffectsofpushactionsinclutteredenvironments. DIPNleverages\ndeeplearningtoestimateobjectinteractionswhenarobotmanipulatorexecutespushactions,\ngenerating accurate synthetic images of potential outcomes. This predictive capability\nenables thesystemto make intelligent push-versus-graspdecisions, ultimatelyfacilitating\nefficientclutterremoval. Byintegrating DIPNwithagrasppredictionnetwork, thesystem\nachievesrobustself-supervisedlearning,significantlyoutperformingpreviousstate-of-the-\nart methods. Remarkably, DIPN demonstrates superior generalization on real hardware\ncomparedtosimulation,underscoringitspracticalutilityinroboticmanipulationtasks. === 페이지 22 ===\n4\nChapter 3: Deep Interaction Prediction Chapter 4: Visual Foresight Tree –Object\nNetwork –Clutter Removal Retrieval\nDirectly Yes Perform the best\ngraspable?",
      "size": 975,
      "sentences": 6
    },
    {
      "id": 45,
      "content": "nroboticmanipulationtasks. === 페이지 22 ===\n4\nChapter 3: Deep Interaction Prediction Chapter 4: Visual Foresight Tree –Object\nNetwork –Clutter Removal Retrieval\nDirectly Yes Perform the best\ngraspable? grasp\nMask R-CNN No\nPlan multiple Reward calcula�on MonteCarloTreeSearch\nPush action DIPN steps ahead Expansion\nPerform the best\nGrasp Network Deep Interac�on Predic�on push\nNetwork\nInput image Output image\nBefore push A�erpush\nMaskR-CNN Graspable Ground Truth\nChapter 5: Learning guided search –Object Chapter 6: Parallel MCTS with Batched\nRetrieval Simulations –Object Retrieval\nPlanning with a\nA deepnettells\nfast physics\nwheretoplan\nsimulator and a\nfirstly\ntiny deep net\nPPN\nChapter 7: Parallel MCTS with Motion Planning –Object Rearrangement\nMultiple manipulation primitives: pick-n-placeand pushing\nLong horizon planning: Sampling based methods\nStart Push Goal\nFigure1.2: Structureofthedissertation.",
      "size": 905,
      "sentences": 3
    },
    {
      "id": 46,
      "content": "th Motion Planning –Object Rearrangement\nMultiple manipulation primitives: pick-n-placeand pushing\nLong horizon planning: Sampling based methods\nStart Push Goal\nFigure1.2: Structureofthedissertation. Chapter3introducestheDeepInteractionPrediction\nNetwork for one-step push prediction in clutter removal. Chapter 4 extends to multi-step\nplanning for efficient object retrieval using push actions. Chapters 5 and 6 explore GPU-\nacceleratedMonteCarlotreesearch: Chapter5focusesonlearningastrategicnetworkto\nguidetreesearch, whileChapter6utilizes IsaacGymforparallel simulationsinrealrobot\nexecution. Chapter7appliessimilarconceptstoobjectrearrangementinconstrainedspaces,\nincorporatingmotionplanning. 1.2 Visual Foresight Trees for Object Retrieval from Clutter with Nonprehensile\nRearrangement\nBuildingonsingle-steppushpredictions,Chapter4extendsthescopetomulti-stepplanning\nfor object retrieval in clutter.",
      "size": 905,
      "sentences": 6
    },
    {
      "id": 47,
      "content": "al Foresight Trees for Object Retrieval from Clutter with Nonprehensile\nRearrangement\nBuildingonsingle-steppushpredictions,Chapter4extendsthescopetomulti-stepplanning\nfor object retrieval in clutter. This chapter presents the Visual Foresight Trees (VFT)\nframework,whichemploysadeeppredictivemodel(DIPN)toanticipateobjectmovements\nresulting from sequential pushing actions. By integrating a tree search algorithm, VFT\nevaluatesvariouspushsequencestodeterminetheoptimalstrategyforrearrangingtheenvi-\nronmentbeforegraspingatargetobject. Themethodsignificantlyimprovesretrievalsuccess\nratesandreducesthenumberofrequiredactionscomparedtothebaseline. Experimentsin\n[표 데이터 감지됨]\n\n=== 페이지 23 ===\n5\nboth simulation and real-world settings confirm that VFT effectively balances prediction\naccuracywithcomputationalefficiency,pavingthewayformoreintelligentroboticplanning.",
      "size": 861,
      "sentences": 5
    },
    {
      "id": 48,
      "content": "데이터 감지됨]\n\n=== 페이지 23 ===\n5\nboth simulation and real-world settings confirm that VFT effectively balances prediction\naccuracywithcomputationalefficiency,pavingthewayformoreintelligentroboticplanning. 1.3 InterleavingMonteCarloTreeSearchandSelf-SupervisedLearningforObject\nRetrievalinClutter\nChapter 5 explores a deeper integration of Monte Carlo Tree Search with learning-based\nstrategies for object retrieval in cluttered environments. The Monte Carlo tree search\nandlearningforObjectREtrieval(MORE)frameworkfollowsaself-supervisedapproach\ninspiredbyKahneman’sSystem2→System1learningparadigm. Initially,MCTSenablesa\ndeepneuralnetworktounderstandobjectinteractionsandpredictoptimalpushactions. Once\ntrained,thenetworkisincorporatedintoMCTStoacceleratedecision-making,significantly\nreducingcomputationaloverheadwhilemaintainingorimprovingsolutionquality. MORE\nrepresentsakeystep inclosingtheloopbetweenclassicalplanninganddeeplearningfor\nefficientroboticmanipulation.",
      "size": 965,
      "sentences": 6
    },
    {
      "id": 49,
      "content": "significantly\nreducingcomputationaloverheadwhilemaintainingorimprovingsolutionquality. MORE\nrepresentsakeystep inclosingtheloopbetweenclassicalplanninganddeeplearningfor\nefficientroboticmanipulation. 1.4 Parallel Monte Carlo Tree Search with Batched Rigid-body Simulations for\nSpeedingupLong-HorizonEpisodicRobotPlanning\nUnlike Chapter 5, which focuses on learning-guided Monte Carlo Tree Search (MCTS),\nPMBS takes a differentapproach byintroducing large-scale parallel simulationsto enhance\nplanning efficiency. The Parallel Monte Carlo Tree Search with Batched Simulations\n(PMBS)methodleveragesGPU-basedparallelismtoevaluatemultipleactiontrajectories\nsimultaneously using Isaac Gym. By executing a vast number of physics simulations in\nparallel with strategic sampling, PMBS significantly accelerates long-horizon planning\ntasks, such as object retrieval from clutter, achieving over 30× speedups compared to\nconventional MCTS.",
      "size": 929,
      "sentences": 5
    },
    {
      "id": 50,
      "content": "ions in\nparallel with strategic sampling, PMBS significantly accelerates long-horizon planning\ntasks, such as object retrieval from clutter, achieving over 30× speedups compared to\nconventional MCTS. PMBS achieves significant computational efficiency and maintains\nhigh solution quality while minimizing planning latency, thus making real-time robotic\ndecision-makingmoreviable. ExperimentalresultsfurtherdemonstratethatPMBScanbe\n=== 페이지 24 ===\n6\nseamlesslydeployedonrealhardwarewithminimalsim-to-realdiscrepancies. 1.5 TowardOptimalTabletopRearrangementwithMultipleManipulationPrimitives\nFinally,Chapter7appliestheseprinciplestoabroaderclassofroboticmanipulationtasks. ThischapterpresentstheRearrangementwithMultipleManipulationPrimitives(REMP)\nproblem,whichinvolvescoordinatingpick-and-placeandpushactionstooptimallyorganize\nobjectsinconstrainedspaces.",
      "size": 854,
      "sentences": 5
    },
    {
      "id": 51,
      "content": "pulationtasks. ThischapterpresentstheRearrangementwithMultipleManipulationPrimitives(REMP)\nproblem,whichinvolvescoordinatingpick-and-placeandpushactionstooptimallyorganize\nobjectsinconstrainedspaces. Twocomplementaryalgorithmsaredeveloped: hierarchical\nbest-first search(HBFS)forfast heuristicplanningand parallelMCTSformulti-primitive\nrearrangement(PMMR) forhigh-quality, human-like solutions. The integrationofpush and\npick-and-placestrategiesallows robotstoefficientlysolve complexrearrangement tasksthat\nrequirediversemanipulationskills. Extensiveevaluationsinbothsimulationandreal-world\nenvironmentshighlighttheeffectivenessoftheseapproachesinoptimizingobjectplacement\nwhileensuringtasksuccess. === 페이지 25 ===\n7\nCHAPTER2\nRELATEDWORKS\nRoboticmanipulationencompassesabroadrangeofmethodsandtasks,includinggrasping,\nsingulation, retrieval, rearrangement, pushing, andcombined push-graspingstrategies.",
      "size": 901,
      "sentences": 6
    },
    {
      "id": 52,
      "content": "= 페이지 25 ===\n7\nCHAPTER2\nRELATEDWORKS\nRoboticmanipulationencompassesabroadrangeofmethodsandtasks,includinggrasping,\nsingulation, retrieval, rearrangement, pushing, andcombined push-graspingstrategies. The\nfollowing sections review the relevant literature alongthese dimensions, highlighting both\nclassical(analytical)approachesandmorerecentlearning-basedmethods. Wealsohighlight\nhow Task and Motion Planning (TAMP), Monte Carlo Tree Search (MCTS), and other\nadvanced approaches have been used to tackle long-horizon challenges such as clutter\nremoval,objectretrieval,andobjectrearrangement. 2.1 Prehensilevs. Non-PrehensileManipulation\nManipulationactionscanbebroadlyclassifiedintoprehensileandnon-prehensileactions. Prehensile (or “grasping”) actions involve lifting or holding objects using a gripper or\nsuction,whilenon-prehensileactions(e.g.,pushing,dragging,toppling)manipulateobjects\nbyapplyingforceswithoutgraspingthem. Often,thesetwofamiliesofactionsarestudied\nseparately [24]–[26].",
      "size": 989,
      "sentences": 7
    },
    {
      "id": 53,
      "content": "g a gripper or\nsuction,whilenon-prehensileactions(e.g.,pushing,dragging,toppling)manipulateobjects\nbyapplyingforceswithoutgraspingthem. Often,thesetwofamiliesofactionsarestudied\nseparately [24]–[26]. However, there is an increasing interest in leveraging both types\nto tackle challenging tasks more effectively [27]–[29]. Despite being limited in variety,\nprehensile actions allow a robot to secure an object and move it in tandem with the end-\neffector. Non-prehensile actions, on the other hand, use contact with the environment to\ncontrolorguideobjectsonasurface[1],[30],[31]. Aswewilldiscussbelow,combining\nthestrengthsofprehensileandnon-prehensileactions(e.g.,pushingtocreatespaceororient\nanobjectpriortograsping)cansignificantlyimproveperformanceinclutteredorcomplex\nenvironments. === 페이지 26 ===\n8\n2.2 ObjectGrasping\n2.2.1 Analyticalvs. Data-DrivenApproaches\nRoboticgraspingmethodscangenerallybedividedintoanalyticalanddata-drivencategories.",
      "size": 947,
      "sentences": 8
    },
    {
      "id": 54,
      "content": "anceinclutteredorcomplex\nenvironments. === 페이지 26 ===\n8\n2.2 ObjectGrasping\n2.2.1 Analyticalvs. Data-DrivenApproaches\nRoboticgraspingmethodscangenerallybedividedintoanalyticalanddata-drivencategories. Analyticalapproachesrelyonpreciseobjectmodels(often3D)andmechanicalproperties\n(e.g.,frictioncoefficientsandmassdistributions)toevaluateforce-closureorform-closure\nconditions for grasp stability [11], [24]. However, building these exact models can be\nprohibitively difficult in real-world settings due to incomplete scans of objects, unknown\nfrictioncoefficients,andinaccuratemassestimates. Toaddressthesechallenges,data-drivenmethods [32]learngraspsuccessprobabilities\ndirectly from observations. Early data-driven techniques often focused on isolated single\nobjects [33]–[37]. More recent work has shifted to grasping in clutter, exploring how\nlearned models can effectively handle occlusions, unexpected contacts, and multi-object\ninteractions[38]–[44].",
      "size": 955,
      "sentences": 8
    },
    {
      "id": 55,
      "content": "le\nobjects [33]–[37]. More recent work has shifted to grasping in clutter, exploring how\nlearned models can effectively handle occlusions, unexpected contacts, and multi-object\ninteractions[38]–[44]. Acommonapproachtrainsconvolutionalneuralnetworks(CNNs)to\npropose candidate grasp actions, sometimes producing 6-DoF grasp poses in point clouds,\nsuchasDex-Net[45],[46]. Dex-Netandotherapproachesalsocombinesuctiongrippers\nwithfingeredjawstomaximizegraspstability[6],[47]. 2.2.2 GraspinginDenseEnvironments\nWhen objects are densely packed, direct grasping may fail due to collisions or partial\nocclusions. Recenttechniquesincorporateadditionalmanipulationprimitives(e.g.,pushing,\npoking, or top-sliding) to move obstacles or improve grasp accessibility [27], [48]. This\nsynergymotivatesustocombinegraspingwithauxiliaryactions. 2.3 Pushing\nPushing is an example of non-prehensile actions that robots can apply on objects.",
      "size": 918,
      "sentences": 8
    },
    {
      "id": 56,
      "content": "or improve grasp accessibility [27], [48]. This\nsynergymotivatesustocombinegraspingwithauxiliaryactions. 2.3 Pushing\nPushing is an example of non-prehensile actions that robots can apply on objects. As\nwith grasping, there are two main categories of methods that predict the effect of a push\n=== 페이지 27 ===\n9\naction[49]. Analyticalmethodsrelyonmechanicalandgeometricmodelsoftheobjectsand\nutilizephysicssimulationstopredictthemotionofanobject[50]–[55]. Notably,Mason[51]\nderivedthevotingtheoremtopredicttherotationandtranslationofanobjectpushedbya\npointcontact. Astablepushingtechniquewhenobjectsremainincontactwasalsoproposed\nin [52]. These methods often make strong assumptions such as quasi-static motion and\nuniformfrictioncoefficientsandmassdistributions. Todealwithnon-uniformfrictions,a\nregressionmethodwasproposedin[56]foridentifyingthesupportpointsofapushedobject\nbydividingthesupportsurfaceintoagrid. The limit surface plays a crucial role in the mechanical models of pushing.",
      "size": 985,
      "sentences": 10
    },
    {
      "id": 57,
      "content": "ions,a\nregressionmethodwasproposedin[56]foridentifyingthesupportpointsofapushedobject\nbydividingthesupportsurfaceintoagrid. The limit surface plays a crucial role in the mechanical models of pushing. It is a\nconvexsetofallfrictionforcesandtorquesthatcanbeappliedtoanobjectinquasi-static\npushing. The limit surface is often approximated as an ellipsoid [57], or a higher-order\nconvex polynomial [58]–[60]. An ellipsoid approximation was also used to simulate the\nmotionofapushedobjecttoperformapush-grasp[61]. Toovercometherigidassumptions\nofanalyticalmethods, statisticallearningtechniquespredicthownewobjectsbehaveunder\nvariouspushingforcesbygeneralizingobservedmotionsintrainingexamples. Forexample,\naGaussianprocesswasusedtosolvethisproblemin[25],butwaslimitedtoisolatedsingle\nobjects. Most recent push prediction techniques rely on deep learning [62]–[64], which\ncan capturea widerrange of physicalinteractions fromvision. DeepRL was alsoused for\nlearningpushingstrategiesfromimages[65]–[68].",
      "size": 996,
      "sentences": 9
    },
    {
      "id": 58,
      "content": "ent push prediction techniques rely on deep learning [62]–[64], which\ncan capturea widerrange of physicalinteractions fromvision. DeepRL was alsoused for\nlearningpushingstrategiesfromimages[65]–[68]. 2.4 Push-Grasping\nCombiningpushingandgraspinginasingleframeworkhasledtomorerobustperformance\nin cluttered settings. Two common paradigms are pre-grasp push, where non-prehensile\nactionsrepositionoruncoveratargetobjectforanensuinggrasp,andpush-assistedgrasping,\nwhere the push directly aids in achieving a stable grasp. Several techniques implement\na push-then-grasp sequence to reduce clutter around a target object [28], [29], [69]–[71]. Others focuson ’push-grasp’ actions that simultaneouslyslide and lift the object[61]. The\n=== 페이지 28 ===\n10\nVisualPushingandGrasping(VPG)framework[27]isawell-knownexamplethatusesa\nmodel-freeQ-learningapproachtoselectpushorgraspactions.",
      "size": 874,
      "sentences": 7
    },
    {
      "id": 59,
      "content": "that simultaneouslyslide and lift the object[61]. The\n=== 페이지 28 ===\n10\nVisualPushingandGrasping(VPG)framework[27]isawell-knownexamplethatusesa\nmodel-freeQ-learningapproachtoselectpushorgraspactions. ExtensionstoVPGfurther\nincorporatepredictivemodelsofhowobjectsmoveunderpush,allowingfor’look-ahead’\nplanningratherthanpurelyreactivedecisions. Insuchmodel-basedsetups,therobotcan\nsimulatepushingoutcomesbeforedecidingwhetherandwheretopush,therebyavoiding\nunnecessaryactionsinclutteredenvironments. 2.5 Singulation\n2.5.1 DefinitionandTechniques\nSingulationreferstoisolatingoneormorespecificobjectsfromaclutteredcollection[72]. Byclearingthetarget’simmediatesurroundings,singulationcanmakegraspingorretrieval\nmorestraightforward. Acommon approach istouse acombinationof pushingand grasping\nactionstomoveobjectsthatareobstructingthemove. Themethodsin[73]–[75]oftenrely\nonmodel-free reinforcementlearning (RL)policiesthat reactivelypush objectsaway until\nthetargetisexposed.",
      "size": 969,
      "sentences": 8
    },
    {
      "id": 60,
      "content": "hingand grasping\nactionstomoveobjectsthatareobstructingthemove. Themethodsin[73]–[75]oftenrely\nonmodel-free reinforcementlearning (RL)policiesthat reactivelypush objectsaway until\nthetargetisexposed. 2.5.2 LimitationsandExtensions\nThese reactive techniques can be effective in lightly cluttered or moderate-density scenes,\nwhere a single push often suffices to create sufficient clearance [72]. However, for more\ndenselypackedscenariosor targetsthat require repeated,strategicallychosenpushes,short-\nsightedorpurelyreactivemethodsmayunderperform. Thislimitationhasspurredresearch\non longer-horizon or model-based push planning to enable more intelligent singulation\nstrategies. === 페이지 29 ===\n11\n2.6 ObjectRetrieval\n2.6.1 ProblemSettingandChallenges\nObjectretrievaltasksaimtolocateandextractatargetobjectfromclutter.",
      "size": 816,
      "sentences": 6
    },
    {
      "id": 61,
      "content": "planning to enable more intelligent singulation\nstrategies. === 페이지 29 ===\n11\n2.6 ObjectRetrieval\n2.6.1 ProblemSettingandChallenges\nObjectretrievaltasksaimtolocateandextractatargetobjectfromclutter. Inmanyreal-world\napplications(e.g.,warehouseorderfulfillment,householdassistance),objectsarestackedor\npartiallyoccluded,necessitatingasequenceofdeliberateactionstouncoverandgraspthe\ntarget. This process often involves pushing, rearranging, or even removing other objects\nfromtheworkspace. 2.6.2 Model-Freevs. Model-BasedApproaches\nEarlymethodstreatobjectretrievalasanonlineplanningchallengeunderpartialobservabil-\nity[61],sometimesrelyingonsearchheuristicstoreducethesolutionspace. Othersexplore\nmodel-freeRLtoselectamongpush,poke,orgraspactions[76]–[78]. Whilethesemethods\ncanlearneffectivestrategiesformoderateclutter,long-horizonreasoningisoftenlimited.",
      "size": 855,
      "sentences": 8
    },
    {
      "id": 62,
      "content": "oreducethesolutionspace. Othersexplore\nmodel-freeRLtoselectamongpush,poke,orgraspactions[76]–[78]. Whilethesemethods\ncanlearneffectivestrategiesformoderateclutter,long-horizonreasoningisoftenlimited. More recentworkintegrates predictive modelsto anticipate how objectswill move under\ncertainpushes[12],orusesMCTStosystematicallyexploremulti-stepactionsequences[79]. Explicitly modeling future states is especially beneficial in tightly packed scenes, where\nsmallchangescansubstantiallyaffectthefeasibilityofextractingthetarget. 2.7 RearrangementPlanning\nRearrangementplanning extends objectretrieval to moregeneral problems of reconfiguring\nmultiple objects from an initial to a goal arrangement [79]–[87]. In tabletop scenarios,\nrearranging objects often requires carefully allocating free space, deciding which objects\nto move first, and determining how to move them. Many methods rely on pick-and-place\nonly, treating pushing as secondary or ignoring it entirely [18], [88], [89].",
      "size": 983,
      "sentences": 8
    },
    {
      "id": 63,
      "content": "g free space, deciding which objects\nto move first, and determining how to move them. Many methods rely on pick-and-place\nonly, treating pushing as secondary or ignoring it entirely [18], [88], [89]. However,\npick-and-placecanbeinefficientorinfeasiblewhenobjectsareheavy,large,orextremely\ncluttered. Some approaches reduce complexity by removing objects from the workspace\n=== 페이지 30 ===\n12\naltogether[86]orbytemporarilyusingexternalspacetoholdobjects. Whendealingwith\ntightlypackedconfigurations,heuristic-guidedsearchhasbeenused[18],[88]. Graph-based\nformulations can capture dependencies among objects that block one another [80], [81]. Beyond classic motionplanning frameworks, data-driven or learning-based rearrangement\nmethods have emerged, harnessing deep neural networks or reinforcement learning to guide\nactionselection[79].",
      "size": 835,
      "sentences": 7
    },
    {
      "id": 64,
      "content": "1]. Beyond classic motionplanning frameworks, data-driven or learning-based rearrangement\nmethods have emerged, harnessing deep neural networks or reinforcement learning to guide\nactionselection[79]. 2.8 TaskandMotionPlanning(TAMP)\nTask and motion planning (TAMP) deals with orchestrating high-level actions (tasks)\nwhile simultaneously ensuring geometricand kinematic feasibility (motion planning)[90]–\n[93]. Compared to discrete, rule-based domains (e.g., board games), TAMP operates\nover continuous state and action spaces. Traditional TAMP approaches often combine\nsymbolic reasoning with sampling-based motion planning [94], [95]. More recent work\nleverageslearning—suchaslearningtoguidesearch,predictoutcomesofactions,orestimate\nfeasibility—to navigate the large search space [12], [96]–[99].",
      "size": 798,
      "sentences": 6
    },
    {
      "id": 65,
      "content": "ng-based motion planning [94], [95]. More recent work\nleverageslearning—suchaslearningtoguidesearch,predictoutcomesofactions,orestimate\nfeasibility—to navigate the large search space [12], [96]–[99]. In the context of object\nretrieval orrearrangement, TAMP can formalizethe problem: the tasklayer decides which\nobjecttomoveandhow,whilethemotionplannerensuresacollision-freetrajectoryforeach\nmanipulation primitive. When clutter is dense, the space of feasible moves can be large,\nmakingefficientsearchstrategiesorlearnedheuristicscrucial. 2.9 MonteCarloTreeSearch(MCTS)forManipulation\nMonte Carlo Tree Search (MCTS) has shown promise in multi-step decision-making for\nmanipulation,particularlyinclutteredscenes[79]. MCTSincrementallyexpandsalookahead\nsearchtree,simulatingpotentialactionsequencestoestimatetheiroutcomes(e.g.,clearing\nclutter to expose the target).",
      "size": 864,
      "sentences": 6
    },
    {
      "id": 66,
      "content": "or\nmanipulation,particularlyinclutteredscenes[79]. MCTSincrementallyexpandsalookahead\nsearchtree,simulatingpotentialactionsequencestoestimatetheiroutcomes(e.g.,clearing\nclutter to expose the target). Often, these simulations rely on predictive models—either\nanalytical or learned—to approximate object dynamics, collisions, and future states [25],\n[62]–[64], [79]. When integrated with data-driven push or grasp predictors, MCTS can\n=== 페이지 31 ===\n13\nefficientlyexploreextendedactionsequences,balancingexplorationofdifferentmoveswith\nexploitationofpromisingtrajectories[78],[79]. Recentworkalsoexploresnetwork-based\npredictionsofmulti-objectcollisionsunderpushingtoacceleratethesimulationphase[65]–\n[68]. Nevertheless,designingaccuratepredictivenetworksremainschallenginginhighly\ncluttered or diverse object sets [100].",
      "size": 819,
      "sentences": 6
    },
    {
      "id": 67,
      "content": "ictionsofmulti-objectcollisionsunderpushingtoacceleratethesimulationphase[65]–\n[68]. Nevertheless,designingaccuratepredictivenetworksremainschallenginginhighly\ncluttered or diverse object sets [100]. Overall, MCTS-based approaches hold substantial\npotential for manipulation tasksthat require long-horizon reasoning, such as objectretrieval\norcomplex rearrangement, bycombining learnedpredictivemodels withsystematicsearch\ntoreducetrial-and-errorinthephysicalenvironment. By unifying insights from these diverse research areas—prehensile and non-prehensile\nactions, pushing and grasping, singulation, object retrieval, rearrangement, TAMP, and\nMCTS-based planning—we see that integrated strategies are critical for addressing the\nchallengesposedbydenseclutter. Theremainderofthisdissertationbuildsonthesefindings,\nfocusing on how to combine model-based predictions, data-driven methods, and efficient\nplanningtechniquestoenablerobust,long-horizonmanipulationinreal-worldsettings.",
      "size": 979,
      "sentences": 5
    },
    {
      "id": 68,
      "content": "issertationbuildsonthesefindings,\nfocusing on how to combine model-based predictions, data-driven methods, and efficient\nplanningtechniquestoenablerobust,long-horizonmanipulationinreal-worldsettings. === 페이지 32 ===\n14\nCHAPTER3\nDIPN:DEEPINTERACTIONPREDICTIONNETWORKWITHAPPLICATION\nTOCLUTTERREMOVAL\n3.1 Introduction\nWeproposeaDeepInteractionPredictionNetwork(DIPN)forlearningobjectinteractions\ndirectly from examples and using the trained network for accurately predicting the poses\nofthe objectsafter anarbitrarypush action(Figure 3.1). To demonstrateits effectiveness,\nWe integrate DIPN with a deep Grasp Network (GN) for completing challenging clutter\nremovalmanipulationtasks. Givengraspandpushactionstochoosefrom,theobjectiveis\ntoremoveallobjectsfromthescene/workspacewithaminimumnumberofactions. Inan\niterationofpush/pickselection(Figure3.1c),thesystemexaminesthesceneandsamplesa\nlargenumberofcandidategraspandpushactions.",
      "size": 926,
      "sentences": 5
    },
    {
      "id": 69,
      "content": "eis\ntoremoveallobjectsfromthescene/workspacewithaminimumnumberofactions. Inan\niterationofpush/pickselection(Figure3.1c),thesystemexaminesthesceneandsamplesa\nlargenumberofcandidategraspandpushactions. GraspsareimmediatelyscoredbyGN,\nwhereas for each candidate push action, DIPN generates an image corresponding to the\npredicted outcome. In a sense, DIPN “imagines” what happens to the current scene if the\nrobotexecutesacertainpush. ThepredictedfutureimagesarealsoscoredbyGN;theaction\nwiththehighestexpectedscore,eitherapushoragrasp,isthenexecuted. OurextensiveevaluationdemonstratesthatDIPNcanaccuratelypredictobjects’poses\nafterapushactionwithcollisions,resultinginlessthan10%averagesingleobjectposeerror\nin terms of IoU (Intersection-over-Union), a significant improvement over the compared\nbaselines. PushpredictionbyDIPNgeneratesclearsyntheticimagesthatcanbeusedbyGN\ntoevaluategraspactionsinfuturestates.",
      "size": 908,
      "sentences": 7
    },
    {
      "id": 70,
      "content": "terms of IoU (Intersection-over-Union), a significant improvement over the compared\nbaselines. PushpredictionbyDIPNgeneratesclearsyntheticimagesthatcanbeusedbyGN\ntoevaluategraspactionsinfuturestates. TogetherwithGN,ourentirepipelineachieves34%\nhighercompletionrate,20.9%highergraspsuccessrate,and30.4%higheractionefficiency\nin comparison to [27] on challenging clutter removal scenarios. Moreover, experiments\nsuggest that DIPN can learn from randomly generated scenarios with the learned policy\n=== 페이지 33 ===\n15\n(a) (b)\nStateobservation\nDeep\nGraspNetwork\n(GN)\nDeepInteraction\nPredictionNetwork Grasporpush\naction\n(DIPN) Predictedstatesafterpush\n(c)\nFigure3.1: (a)The systemsetup includesa workspace withobjects toremove, aUniversal\nRobotsUR-5emanipulatorwithaRobotiq2F-85two-fingergripper,andanIntelRealSense\nD435 RGB-D camera. (b) An example push action and superimposed images of scenes\nbeforeandafterthepush.",
      "size": 913,
      "sentences": 5
    },
    {
      "id": 71,
      "content": "oremove, aUniversal\nRobotsUR-5emanipulatorwithaRobotiq2F-85two-fingergripper,andanIntelRealSense\nD435 RGB-D camera. (b) An example push action and superimposed images of scenes\nbeforeandafterthepush. (c)Systemarchitectureofourpipeline,andonepredictedimage\nthatDIPNcangenerateforthepushshownin(b). Noticethesimilaritybetweenthepredicted\nsyntheticimageandtherealimageresultingfromthepushaction. maintaininghighlevelsofperformanceonchallengingtasksinvolvingpreviouslyunseen\nobjects. Remarkably,DIPN+GNachievesevenbetterperformanceonrealrobotichardware\nthaninthesimulationenvironmentwhereitwasdeveloped. 3.2 ProblemFormulation\nWeformulatetheclutterremovalproblem(Figure3.1a)asPushingAssistedGrasping(PaG). Ina PaG,theworkspaceofthemanipulator isasquareregion containingmultipleobjects\nand the volume directly above it. A camera is placed on top of the workspace for state\n[표 데이터 감지됨]\n\n=== 페이지 34 ===\n16\nobservation.",
      "size": 911,
      "sentences": 9
    },
    {
      "id": 72,
      "content": "eworkspaceofthemanipulator isasquareregion containingmultipleobjects\nand the volume directly above it. A camera is placed on top of the workspace for state\n[표 데이터 감지됨]\n\n=== 페이지 34 ===\n16\nobservation. Given camera images, all objects must be removed using two basic motion\nprimitives,grasp,andpush,withaminimumnumberofactions. Inourexperimentalsetup,theworkspacehasauniformbackgroundcolorandtheobjects\nhave different shapes, sizes, and colors. The end-effector is a two-finger gripper with a\nnarrowstrokethatisslightlylargerthanthesmallestdimensionofindividualobjects. Objects\nare removed one by one, which requires a sequence of push and grasp actions. When\ndecidingonthenextaction,astateobservationisgivenasanRGB-Dimage,re-projected\northographically,croppedtotheworkspace’sboundary,anddown-sampledto224×224. Fromthedown-sampledimage,alargesetofcandidateactionsisgeneratedbyconsidering\neachpixelintheimageasapotentialcenterofagraspactionorinitialcontactpointofapush\naction.",
      "size": 973,
      "sentences": 8
    },
    {
      "id": 73,
      "content": "boundary,anddown-sampledto224×224. Fromthedown-sampledimage,alargesetofcandidateactionsisgeneratedbyconsidering\neachpixelintheimageasapotentialcenterofagraspactionorinitialcontactpointofapush\naction. Agraspactionagrasp = (x,y,θ)isaverticaltop-downgraspcenteredatpixelposition\n(x,y) with the end-effector rotation set to θ around the vertical axis of the workspace; a\ngraspedobject issubsequentlytransferred outsideoftheworkspaceandremovedfromthe\nscene. Similarly,apushactionapush = (x,y,θ)isahorizontalsweepmotionthatstartsat\n(x,y)andproceedsalongθ directionforafixeddistance. Theorientationθ canbeoneof\n16 values evenly distributed between 0 and 360 degrees. That is, the entire action space\nincludes2×224×224×16differentgrasp/pushactions.",
      "size": 740,
      "sentences": 6
    },
    {
      "id": 74,
      "content": "salongθ directionforafixeddistance. Theorientationθ canbeoneof\n16 values evenly distributed between 0 and 360 degrees. That is, the entire action space\nincludes2×224×224×16differentgrasp/pushactions. Theproblemstudiedinthispaperisdefinedas:\nProblem1 PushingAssistedGrasping(PaG).Givenobjectsinclutterwithinthedescribed\nsystemsetup,determineasequenceofpushandgraspactions,usingonlyvisualinputfrom\ntheworkspace,toremoveallobjectswhileminimizingthetotalnumberofactionsexecuted. 3.3 Methodology\nWedescribetheDeepInteractionPredictionNetwork(DIPN),theGraspNetwork(GN),and\ntheintegratedpipelineforsolvingPaGchallenges. === 페이지 35 ===\n17\n3.3.1 DeepInteractionPredictionNetwork(DIPN)\nThe architecture of our proposed DIPN is outlined in Figure 3.2.",
      "size": 740,
      "sentences": 6
    },
    {
      "id": 75,
      "content": "),theGraspNetwork(GN),and\ntheintegratedpipelineforsolvingPaGchallenges. === 페이지 35 ===\n17\n3.3.1 DeepInteractionPredictionNetwork(DIPN)\nThe architecture of our proposed DIPN is outlined in Figure 3.2. At a high level, given\nan image and a candidate push action as inputs, DIPN segments the image and then\npredicts2Dtransformations(translationsandrotations)forallobjects,andparticularlyfor\nthoseaffectedbythepushaction,directlyorindirectlythroughacascadeofobject-object\ninteractions. A predicted image of the post-push scene is synthesized by applying the\npredictedtransformationsonthesegments. Weoptedagainstanend-to-end,pixel-to-pixel\nmethodassuchmethods(e.g.,[62])oftenleadtoblurryorfragmentedimages,whichare\nnotconducivetopredictingthequalityofapotentialfuturegraspaction.",
      "size": 774,
      "sentences": 5
    },
    {
      "id": 76,
      "content": "onsonthesegments. Weoptedagainstanend-to-end,pixel-to-pixel\nmethodassuchmethods(e.g.,[62])oftenleadtoblurryorfragmentedimages,whichare\nnotconducivetopredictingthequalityofapotentialfuturegraspaction. I\n2\nn\n2\np\n4\nut\n×\nim\n2\nag\n2\ne\n4\nI\n×3 Pushactionp BinarypushactionimageMp\nMp MI\nBinaryimageMI\n(83,140,22.5◦) MLP1 224×224×{0,1} Push action image Encoded image 224×224×{0,1}\nMLP3\nMaskimage{Mi}, Encoded Direct\nInput image 60×60×{0,1}, transformation\nS 2 e 2 g 4 m × ent 2 at 2 io 4 n ×1 center ( p 5 o M 4 s a , i s t 6 k io 9 n M ) R { 1 e c s i N M } et L 2 P2 Encoded MLP4 tra I n n s te fo ra rm cti a v ti e on Add M R LP e 5 sNe T t r 1 ( a 0 n , sf 0 or , m 0 a ◦ ti ) on O 22 ut 4 pu × tim 22 ag 4 e × Iˆ 4\nMLP4\nMaskR-CNN Segmented image (6 M 7 a , s 9 k 0 M ) 2 Encoded tra I n n s te fo ra rm cti a v ti e on ( T 1 ra 8 n , sf − or 3 m , a 6 ti ◦ on ) Output Image\nMaskM3 Encoded Transformation\n(73,112) (15,1,−3◦)\nFigure 3.2: DIPNflow with anexample.",
      "size": 958,
      "sentences": 3
    },
    {
      "id": 77,
      "content": "0 M ) 2 Encoded tra I n n s te fo ra rm cti a v ti e on ( T 1 ra 8 n , sf − or 3 m , a 6 ti ◦ on ) Output Image\nMaskM3 Encoded Transformation\n(73,112) (15,1,−3◦)\nFigure 3.2: DIPNflow with anexample. The network componentsdedicated to anobject\nare color-coded to match the object. We only show the full network for the blue triangle\nobject;theinstance-specificstructuresfortheotherobjectssharethesameweightsandare\nsimplified as dashed lines. Components inside the orange dotted line are the core of the\nDIPN. The output image is synthesized by applying the predicted transformations to the\nobjectsegments. Segmentation. DIPNemploysMaskR-CNN[101]forobjectsegmentation(instance\nlevelonly,withoutsemanticsegmentation). Theresultingbinarymasks(m )andtheircenters\ni\n(c ), one per object, serve as the input to the push prediction module of DIPN. Our Mask\ni\nR-CNNsetuphastwoclasses,oneforthebackgroundandonefortheobjects.",
      "size": 914,
      "sentences": 9
    },
    {
      "id": 78,
      "content": "Theresultingbinarymasks(m )andtheircenters\ni\n(c ), one per object, serve as the input to the push prediction module of DIPN. Our Mask\ni\nR-CNNsetuphastwoclasses,oneforthebackgroundandonefortheobjects. Thenetwork\nistrainedfromscratchinaself-supervised mannerwithoutanyhumanintervention: objects\narerandomlydroppedintotheworkspace,anddataisautomaticallycollected. Imagesthat\ncanbeeasilysegmentedintoseparateinstancesbasedoncolor/depthinformation(distinct\n=== 페이지 36 ===\n18\ncolorblobs)areautomaticallylabeledbythesystemassingleinstancesfortrainingtheMask\nR-CNN.Theself-trainedMaskR-CNNcanthenaccuratelyfindedgesinimagesoftightly\npacked scenes and even in scenes with novel objects. Note that the data used for training\nthe segmentation module are also counted in our evaluation of the data efficiency of our\ntechniqueandthecomparisonstoalternativetechniques. Pushsampling. Basedonforegroundsegmentation,candidatepushactionsaregenerated\nFigure3.3: Sampledactioninpurplearrowsaroundeachobject.",
      "size": 987,
      "sentences": 7
    },
    {
      "id": 79,
      "content": "ficiency of our\ntechniqueandthecomparisonstoalternativetechniques. Pushsampling. Basedonforegroundsegmentation,candidatepushactionsaregenerated\nFigure3.3: Sampledactioninpurplearrowsaroundeachobject. byuniformlysamplingpushcontactlocationsonthecontourofobjectclusters(seeFigure3.3). Pushdirectionspointtothecentersoftheobjects. Pushesthatcannotbeperformedphysically,\ne.g.,fromtheinsideofanobjectbundleorinnarrowspacesbetweenobjects,arefilteredout\nbasedonthemasksreturnedbyR-CNN.Inthefigure,forexample,samplesbetweenthetwo\nobjectclustersareremoved. Asampledpushisdefinedasp = (x,y,θ) ∈ SE(2)wherex,\ny arethe startlocationof thepushandθ indicatesthe horizontalpushdirection. The push\ndistanceisfixed. Input to push prediction. The initial scene image I, scene object masks and centers\n(m ,c ),andasampledpushaction p = (x,y,θ)arethemaininputstothepushprediction\ni i\nmodule. Toreduceredundancy,wetransformallinputsusinga2Dhomogeneoustransfor-\n=== 페이지 37 ===\n19\nmation matrix T such that pT = (40,112,0).",
      "size": 1000,
      "sentences": 11
    },
    {
      "id": 80,
      "content": "dpushaction p = (x,y,θ)arethemaininputstothepushprediction\ni i\nmodule. Toreduceredundancy,wetransformallinputsusinga2Dhomogeneoustransfor-\n=== 페이지 37 ===\n19\nmation matrix T such that pT = (40,112,0). The position of the push is normalized for\neasier learning such that a push will always go from left to right in the middle of the left\nside of the workspace. From here, it is understood that all inputs are with respect to this\nupdatedcoordinateframedefined byT,i.e.,p ← pT,c ← c T,andsoon. Apartfrom the\ni i\ninputs mentioned so far, we also generate: (1) one binary push action image M with all\np\npixelsblackexceptinasmallsquarewithtop-left corner(40,100)andbottom-rightcorner\n(65,124)whichisthefingermovementspace,(2)one224×224binaryimageM withthe\nI\nforeground of I set to white, and (3) one 60×60 binary mask image M for each object\ni\nmask m , centered at c .",
      "size": 862,
      "sentences": 5
    },
    {
      "id": 81,
      "content": "om-rightcorner\n(65,124)whichisthefingermovementspace,(2)one224×224binaryimageM withthe\nI\nforeground of I set to white, and (3) one 60×60 binary mask image M for each object\ni\nmask m , centered at c . Despite being constant relative to the image transformed by T,\ni i\npushimageM isusedasaninputbecausewenoticedfromourexperimentsthatithelpsthe\np\nnetworkfocusmoreonthepushingarea. Pushprediction. With global(binaryimagesM ,M )and local(mask imageM and\np I i\nthe center c of each object)information, DIPN proceeds topredict objects’ transformations. i\nTostart,aMulti-LayerPerceptron(MLP)andaResNet[102](withnopre-training)areused\ntoencodethepushactionandtheglobalinformation,respectively:\ne = MLP (p), e = ResNet (M ,M ). p 1 AB 1 p I\nA similar procedure is applied to individual objects. For each object o , its center c and\ni i\nmaskimageM areencodedusingResNet(again,withnopre-training)andMLPas:\ni\ne = (ResNet (M ),MLP (c )).",
      "size": 924,
      "sentences": 7
    },
    {
      "id": 82,
      "content": "1 p I\nA similar procedure is applied to individual objects. For each object o , its center c and\ni i\nmaskimageM areencodedusingResNet(again,withnopre-training)andMLPas:\ni\ne = (ResNet (M ),MLP (c )). i 2 i 2 i\nAdopting the design philosophy from [64], the encoded information is then passed to a\ndirect transformation (DT) MLP module (blocks in Figure 3.2 with orange background)\nand multiple interactive transformation (IT) MLP modules (blocksin Figure 3.2 with cyan\nbackground):\n∀1 ≤ i ≤ n : DT = MLP (e ,e ),\ni 3 p i\n=== 페이지 38 ===\n20\n∀1 ≤ i,j ≤ n,j ̸= i : IT = MLP (e ,e ,e ). ij 4 p i j\nHere,thedirecttransformationmodulescapturetheeffectoftherobotdirectlytouchingthe\nobjects (if any), while the interactive transformation modules consider collision between an\nobjectandeveryotherobject(ifany).",
      "size": 798,
      "sentences": 4
    },
    {
      "id": 83,
      "content": "directtransformationmodulescapturetheeffectoftherobotdirectlytouchingthe\nobjects (if any), while the interactive transformation modules consider collision between an\nobjectandeveryotherobject(ifany). Then,allaforementionedencodingisputtogethertoa\ndecodingMLPtoderivetheoutput2Dtransformationforeachobjecto inthepushaction’s\ni\nframe: ∀1≤i≤n,\n(cid:88)\n(xˆ ,yˆ,θ ˆ ) = MLP (e ,DT + IT ),\ni i i 5 AB i ij\n1≤j≤n,j̸=i\nwhichcanbemappedbacktotheoriginalcoordinateframeviaT−1. Thisyieldspredicted\nposesofobjects. Fromthese,an“imagined”pushpredictionimageisreadilygenerated. In our implementation, both ResNet and ResNet are ResNet-50. MLP and MLP ,\n1 2 1 2\nencodingthepushactionandsingleobjectposition,bothhavetwo(hidden)layerswithsizes\n8and16. MLP ,connectingencodedanddirecttransformations,hastwolayersofauniform\n3\nsize128. MLP ,connectingencodedandinteractivetransformations,hasthreelayerswitha\n4\nsizeof128each. ThefinaldecoderMLP hasfivelayerswithsizes[256,64,32,16,3].",
      "size": 964,
      "sentences": 9
    },
    {
      "id": 84,
      "content": "cttransformations,hastwolayersofauniform\n3\nsize128. MLP ,connectingencodedandinteractivetransformations,hasthreelayerswitha\n4\nsizeof128each. ThefinaldecoderMLP hasfivelayerswithsizes[256,64,32,16,3]. The\n5\nnumberofobjectsnvariesacrossscenes. Thenetworkhandlesavariablenumberofobjects\nbecausethesameweight-sharednetworks(MLP andResNet )processeachobjectand\n2-5 2\nobjectpair. Training. For training in simulation and for real experiments, objects are randomly\ndroppedontotheworkspace. Therobotthenexecutesrandompushestocollecttrainingdata. SmoothL1Loss(HuberLoss)isusedasthelossfunction. Giveneachobject’struepost-push\ntransformation(x ,y ,θ )andthepredicted(xˆ ,yˆ,θ ˆ ),thelossiscomputedasthesumof\ni i i i i i\ncoordinate-wise SmoothL1Loss between the two. DIPN performs well on unseen objects\nandcanbecompletelytrainedinsimulationandtransferredtothereal-world(Sim-to-Real). It is also robust with respect to changes in objects’ physical properties, e.g., variations in\nmassandfrictioncoefficients.",
      "size": 997,
      "sentences": 12
    },
    {
      "id": 85,
      "content": "nbecompletelytrainedinsimulationandtransferredtothereal-world(Sim-to-Real). It is also robust with respect to changes in objects’ physical properties, e.g., variations in\nmassandfrictioncoefficients. === 페이지 39 ===\n21\n3.3.2 TheGraspNetwork(GN)\nWebrieflydescribeGN,whichsharesasimilararchitecturetotheDQNusedin[27]. Given\nanobservedimageandcandidategraspactions,GNfindstheoptimalpolicyformaximizing\nasingle-stepgraspreward,definedas1forasuccessfulgraspand0otherwise. GNfocuses\nits attention on local regions that are relevant to each single grasp and uses image-based\nself-supervisedpre-trainingtoachieveagoodinitializationofnetworkparameters. Theproposedmodifiednetwork’sarchitectureisillustratedinFig.Figure3.4. Ittakesan\ninputimageandoutputsascoreforeachcandidategraspcenteredateachpixel. Theinput\nimageisrotatedtoalignitwiththeend-effectorframe(seetheleftimageinFig.Figure3.4).",
      "size": 880,
      "sentences": 8
    },
    {
      "id": 86,
      "content": "reisillustratedinFig.Figure3.4. Ittakesan\ninputimageandoutputsascoreforeachcandidategraspcenteredateachpixel. Theinput\nimageisrotatedtoalignitwiththeend-effectorframe(seetheleftimageinFig.Figure3.4). ResNet-50 FPN [102] is used as the backbone; we replace the last layer with our own\ncustomizedheadstructureshowninFig.Figure3.4. Weobservethatourstructureleadsto\nfastertrainingandinferencetimewithoutlossofaccuracy. Giventhatthenetworkcomputes\npixel-wisevaluesinfavorofalocalgraspregion,attheendofthenetwork,weplacetwo\nconvolutionallayerswithkernelsize11×57,whichwasdeterminedbasedontheclearance\nofthegripper. 320×320×4 256-80×80 64-160×160 1-320×320 320×320×1\nResNet-50 Conv128,3×3 Conv32,3×3 Conv1,11×57\nFPNP2 Batchnorm2d(128) Batchnorm2d(32) ReLU\nReLU ReLU Conv1,11×57\nConv128,3×3 Dropout(0.1)\nBatchnorm2d(128) Conv1,3×3\nReLU Interpolate2×\nInterpolate2×\nFigure3.4: Architectureof GN.Pink,blue,andgreentextareusedforchannelcount,image\nsize,andkernelsize,respectively.",
      "size": 968,
      "sentences": 7
    },
    {
      "id": 87,
      "content": "7\nConv128,3×3 Dropout(0.1)\nBatchnorm2d(128) Conv1,3×3\nReLU Interpolate2×\nInterpolate2×\nFigure3.4: Architectureof GN.Pink,blue,andgreentextareusedforchannelcount,image\nsize,andkernelsize,respectively. IntrainingGN,image-basedpre-training[103]wasemployed. Thepre-trainingprocess\ntreats pixel-wise grasping as a vision task to obtain a good network initialization. The\nprocess automatically labels with 0 or 1 all the pixels in a small set of arbitrary images,\n[표 데이터 감지됨]\n\n=== 페이지 40 ===\n22\ndependingonwhethergraspscenteredateachpixelwouldleadtoafingercollisionwithan\nobject,basedonlyoncolor/depthandwithoutactuallysimulatingorexecutingthegrasps\nphysically. Thepre-trainingdatasetdoesnotincludeobjectsusedfortesting. 3.3.3 TheCompleteAlgorithmicPipeline\nThetrainingprocessof DIPN+GNisoutlinedinAlgorithm1.",
      "size": 803,
      "sentences": 6
    },
    {
      "id": 88,
      "content": "tactuallysimulatingorexecutingthegrasps\nphysically. Thepre-trainingdatasetdoesnotincludeobjectsusedfortesting. 3.3.3 TheCompleteAlgorithmicPipeline\nThetrainingprocessof DIPN+GNisoutlinedinAlgorithm1. Inline1,animagedatasetis\ncollectedfortrainingMaskR-CNN(i.e.,forpushpredictionsegmentation)andinitializing\n(i.e.,pre-training)GN.NotethattrainingdataforMaskR-CNNandpre-trainingdataforGN\nare essentially free, with no physics involved. After pre-training, the training process for\npush(line2)andgrasp(line3)predictionscanbeexecutedonPaGscenesinanyorder. Algorithm1: TrainingDIPN+GN\nOutput: trainedDIPN+GN. 1 GN,MaskR-CNN←GetImageDataSetAndPre-Train()\n2 DIPN←TrainOnPaGPushOnly(MaskR-CNN)\n3 GN←TrainOnPaGGraspOnly(GN)\nThehigh-levelworkflowofourframeworkonPaGisdescribedin Algorithm2. When\nworkingonaninstance,ateverydecision-makingstept,animageM isfirstobtained(line2).",
      "size": 865,
      "sentences": 8
    },
    {
      "id": 89,
      "content": "ushOnly(MaskR-CNN)\n3 GN←TrainOnPaGGraspOnly(GN)\nThehigh-levelworkflowofourframeworkonPaGisdescribedin Algorithm2. When\nworkingonaninstance,ateverydecision-makingstept,animageM isfirstobtained(line2). t\nThen,theimageM ,alongwithsampledpushactionsApush,aresenttothetrainedDIPNto\nt\ngeneratepredictedsyntheticimagesM ˆ aftereachimaginedpusha(line3-line4). With\nt+1\nAgrasp denotingthe setof allgrasp actions,their discountedaveragerewardon thepredicted\nnextimageM ˆ isthencomparedwiththeaverageofgraspingrewardsinthecurrentimage\nt+1\n(line6): recallthatGNtakesanimageandagraspactionasinput,andoutputsascalargrasp\nrewardvalue. Ifthereexistsapushactionwithahigherexpectedaveragegraspingreward\nin the predicted next image, the best push action is then selected and executed (line 7);\notherwise,thebestgraspactionisselectedandexecuted(line8). Becauseitisdesirableto\nhaveasinglepushactionthatsimultaneouslyrendersmultipleobjectsgraspable,theaverage\ngrasprewardisusedinsteadofonlythemaximum.",
      "size": 979,
      "sentences": 6
    },
    {
      "id": 90,
      "content": "wise,thebestgraspactionisselectedandexecuted(line8). Becauseitisdesirableto\nhaveasinglepushactionthatsimultaneouslyrendersmultipleobjectsgraspable,theaverage\ngrasprewardisusedinsteadofonlythemaximum. Theframeworkcontainstwohyperparameters. The firstone, γ, isthediscount factor of\n=== 페이지 41 ===\n23\nAlgorithm2: ExecutingDIPN+GN\nInput: trainedGNandDIPN,discountfactorγ\n1 whilethereareobjectsinworkspacedo\n2 A push ← ∅,M ←GetImage();\nt\n3 forainSamplePushActions(M )do\nt\n4 A push ← A push ∪{a};Mˆ ←DIPN(M ,a);\nt+1 t\n5 Q(M t ,a) = |Ag γ rasp| (cid:80) a′∈Agrasp GN(Mˆ t+1 ,a′);\n6 if max a∈Apush Q(M t ,a) > |Ag 1 rasp| (cid:80) GN(M t ,a′)then\na′∈Agrasp\n7 Executeargmax a∈Apush Q(M t ,a);\n8 else Executeargmax a∈Agrasp GN(M t ,a);\nthe Markov Decision Process. For a push action to be selected, the estimated discounted\ngrasprewardafterapushmustbelargerthangraspingwithoutapush,sincethepushand\nthengrasptakestwoactions. Inourimplementation,wesetγ tobe0.9.",
      "size": 950,
      "sentences": 6
    },
    {
      "id": 91,
      "content": "ess. For a push action to be selected, the estimated discounted\ngrasprewardafterapushmustbelargerthangraspingwithoutapush,sincethepushand\nthengrasptakestwoactions. Inourimplementation,wesetγ tobe0.9. Theotheroptional\nhyperparameter isused for accelerating inference: ifthe maximum grasp rewardis higher\nthan a threshold, we directly execute the grasp action without calling the push prediction. Thishyperparameterrequirestuning. Inourimplementation,thethresholdvalueissettobe\n0.7. Notethatthemaximumrewardforasinglegraspis1. 3.4 ExperimentalEvaluation\nWefirstevaluateGNandDIPNseparatelyandthencomparethefullsystem’sperformance\nwiththestate-of-the-artmodel-freeRLtechniquepresentedin[27],whichistheclosestwork\nto ours. Apart from evaluating our approach on a real robotic system, we also conducted\nextensive evaluationsin theCoppeliaSim [104]simulator. Weuse anNvidia GeForceRTX\n2080 Ti graphics card to train and test the algorithms.",
      "size": 933,
      "sentences": 10
    },
    {
      "id": 92,
      "content": "ating our approach on a real robotic system, we also conducted\nextensive evaluationsin theCoppeliaSim [104]simulator. Weuse anNvidia GeForceRTX\n2080 Ti graphics card to train and test the algorithms. All simulation experiments are\nrepeated30times;allrealexperimentsarerepeatedfor5timestogetthemeanmetrics. 3.4.1 DeepInteractionPredictionNetwork(DIPN)\nToevaluatehowaccurately DIPN canpredictthenextimageafterapushaction, DIPN is\nfirsttrainedonrandomlygeneratedPaGinstancesinsimulation. Atthestartofeachepisode,\n=== 페이지 42 ===\n24\nrandomly generated objects with random colors and shapes are randomly dropped from\nmid-air to construct the scene. Up to 7 objects are generated per scene. We calculate the\npredictionaccuracybymeasuringtheIntersection-over-Union(IoU)betweenapredicted\nimage andthe corresponding ground-truth after pushing. TheIoU calculation is performed\nattheobjectlevelandthenaveraged. Figure 3.5: DIPN learning curve with standard deviation shown as shaded regions.",
      "size": 979,
      "sentences": 9
    },
    {
      "id": 93,
      "content": "mage andthe corresponding ground-truth after pushing. TheIoU calculation is performed\nattheobjectlevelandthenaveraged. Figure 3.5: DIPN learning curve with standard deviation shown as shaded regions. The\nx-axisisthenumberofpushesfortrainingDIPN.They-axisisthepredictionerror: 1−IoU. Thedottedanddashedlinesarebaselines. DIPNiscomparedwithtwobaselines: thefirstone,calledstatic,assumesthatallobjects\nstaystill. Thesecond one,called trans, alwaysassumesthat onlythepushed objectmoves,\nand that it moves exactly by the push distance along the push direction. Both baselines\nareengineeredmethodsthatdonotrequiretraining. Thepushpredictionerrors(1−IoU)\nare illustrated in Figure 3.5 as learning curves in simulation. Mask R-CNN is trained\n(i.e., Algorithm 1, line 1) using an additional 100 images, which is why Figure 3.5 starts\nfrom 100. We observe, for different push distances, that DIPN outperforms the baselines\nwithalargemarginaftersufficienttraining.",
      "size": 953,
      "sentences": 11
    },
    {
      "id": 94,
      "content": "line 1) using an additional 100 images, which is why Figure 3.5 starts\nfrom 100. We observe, for different push distances, that DIPN outperforms the baselines\nwithalargemarginaftersufficienttraining. Afterconvergence,thepredictionerrorforDIPN\nislessthan0.1for a5cmpush,whichindicates thatthepredictedposeofan objectoverlaps\n90%+withthegroundtruth. Asexpected,DIPNismoreaccurateandmoresampleefficient\nwith a shorter push distance. On the other hand, longer push distances generally result in\nbetter overall performance for PaG challenges even though push predictions become less\naccurate,sincelargeractionsaremoreeffectiveintermsofchangingthescene. [표 데이터 감지됨]\n\n=== 페이지 43 ===\n25\nFigure 3.6 shows typical predictions by DIPN. The network is learned in simulation\nwithrandomlyshapedandcoloredobjects,anddirectlytransferredtotherealsystem. We\nobservethatDIPNcanaccuratelypredictthestateafterapush,withgoodaccuracyonobject\norientationandtranslation. Figure3.6: TypicalDIPNresults.",
      "size": 976,
      "sentences": 9
    },
    {
      "id": 95,
      "content": "pedandcoloredobjects,anddirectlytransferredtotherealsystem. We\nobservethatDIPNcanaccuratelypredictthestateafterapush,withgoodaccuracyonobject\norientationandtranslation. Figure3.6: TypicalDIPNresults. Thefiguresfrom lefttorightare: originaland predicted\nimagesinsimulation,andoriginalandpredictedimagesinarealexperiment. Theground\ntruthimagesafterapushareoverlaidonthepredictedimageswithtransparency. Thearrows\nvisualizethepushactions. Figure3.7: Manuallygeneratedhardinstanceslargelysimilartotheonesin[27]. Thecases\nareusedinbothsimulationandrealexperiment. 3.4.2 GraspNetwork(GN)\nWetrainand evaluateGNasastandalonemoduleandcompare it withthestate-of-the-art\nDQN-basedmethodknownasVisualPushingandGrasping(VPG)[27],whichlearnsboth\ngraspand pushat thesame time.",
      "size": 760,
      "sentences": 9
    },
    {
      "id": 96,
      "content": ".2 GraspNetwork(GN)\nWetrainand evaluateGNasastandalonemoduleandcompare it withthestate-of-the-art\nDQN-basedmethodknownasVisualPushingandGrasping(VPG)[27],whichlearnsboth\ngraspand pushat thesame time. SinceGNisonlytrained ongrasp actions,and for afair\ncomparison, we also tested a third method that learns both grasp and push actions: this\nmethod, denoted by DQN+GN, usesGNfor learning grasp actions and theDQN structure\nof [27] to learn push actions. The algorithms are compared using the grasp success rate\n=== 페이지 44 ===\n26\nmetric,i.e.,thenumberofobjectsremoveddividedbythetotalnumberofgrasps. Wetrain\nallalgorithmsdirectlyonrandomlygeneratedPaGinstanceswith10objects. The learning curve in simulation is provided in Figure 3.8. The pre-training process\n(Algorithm 1,line 1,which isalso self-supervised)for GNtakes100offlineimages thatare\nnotreportedintheplot.",
      "size": 862,
      "sentences": 6
    },
    {
      "id": 97,
      "content": "bjects. The learning curve in simulation is provided in Figure 3.8. The pre-training process\n(Algorithm 1,line 1,which isalso self-supervised)for GNtakes100offlineimages thatare\nnotreportedintheplot. ComparingDQN+GNwhichreaches> 90%successratewithless\nthan 300 (grasp and push) samples, and baseline VPG, which converges at 82% success\nrate with more than 2000 (grasp and push) samples, it is clear that GN has significantly\nhigher grasp success rate and sample efficiency than the baseline VPG. As shown by the\ncomparisonbetweenGNandDQN+GN,whentrainingusingonlygraspactions,GNcan\nbemoresampleefficientwithoutsacrificingsuccessrate. Theresultalsoindicatesthatfor\nrandomlygeneratedPaG,pushingisoftenunnecessary. Figure3.8: Grasplearningcurvesofalgorithmsfor PaGinsimulation. Thex-axisisthe\ntotalnumberoftrainingsteps,i.e.,numberofactionstaken,includingpushandgrasp. The\ny-axisisthegraspsuccessrate. Thedashedlinesdenotethesuccessrateforagraspright\nafterapushaction.",
      "size": 964,
      "sentences": 10
    },
    {
      "id": 98,
      "content": "ulation. Thex-axisisthe\ntotalnumberoftrainingsteps,i.e.,numberofactionstaken,includingpushandgrasp. The\ny-axisisthegraspsuccessrate. Thedashedlinesdenotethesuccessrateforagraspright\nafterapushaction. 3.4.3 EvaluationoftheCompletePipeline\nWe evaluate the learned policies on PaG with up to 30 objects, first in a simulation, then\non a real system. Four algorithms are tested: VPG [27], DQN+GN, REA+DIPN, and\nDIPN+GN(ourfullpipeline). Here,REA+DIPNfollowsAlgorithm2butusesthereactive\ngraspnetworkfrom[27]insteadof GNforgrasprewardestimation. Weusethreemetrics\n[표 데이터 감지됨]\n\n=== 페이지 45 ===\n27\nforcomparison: (i)Completion,calculatedasthenumberofPaGinstanceswhereallobjects\ngot removed divided by the total number of instances; incomplete tasks typically occur if\nobjectsarepushedoutoftheworkspace,orif thetaskisnotcompletedwithinapredefined\naction limit (e.g., three times the number of objects).",
      "size": 892,
      "sentences": 8
    },
    {
      "id": 99,
      "content": "he total number of instances; incomplete tasks typically occur if\nobjectsarepushedoutoftheworkspace,orif thetaskisnotcompletedwithinapredefined\naction limit (e.g., three times the number of objects). (ii) Grasp success, calculated as\nthe total number of objects grasped divided by the total number of grasp actions. (iii)\nAction efficiency, calculated as the total number of objects removed divided by the total\nnumber of actions (grasp and push). For grasp success rate and action efficiency, we use\ntwoformulations: one doesnot countincomplete tasks (reportedin graytext), whichis the\nsameastheoneusedin[27],andtheotheronecountsincompletetasks,whichwebelieveis\nmorereflective. Therobotcouldgraspmorethanoneobjectatatime. Weconsideritasa\nsuccessfulgrasp,asthegoalistoclearallobjectsfromthetable. Table 3.1 reports simulation results on 30 randomly generated PaG instances and 10\nmanuallyplacedhardinstances(illustratedinFigure3.7)wherepushactionsarenecessary.",
      "size": 960,
      "sentences": 7
    },
    {
      "id": 100,
      "content": "oalistoclearallobjectsfromthetable. Table 3.1 reports simulation results on 30 randomly generated PaG instances and 10\nmanuallyplacedhardinstances(illustratedinFigure3.7)wherepushactionsarenecessary. Thealgorithmswerenottrainedonthesehardinstances. Thenumberoftrainingsamplesfor\neach algorithm are: 2500 actions (grasp and push) for VPG [27], 1500 grasp actions and\n2000pushactionsforREA+DIPN,1500actions(graspandpush)forDQN+GN,and500\ngrasp actions and 1500 push actions for DIPN+GN (our full pipeline). The results show\nthat DIPN and GN both aresample efficientin comparisonwith thebaseline andprovide\nsignificantimprovementinPaG metrics;whencombined,DIPN+GNreachesthehighest\nperformanceonallmetrics. Werepeatedtheevaluationonarealsystem(see.Figure3.1a). Eachrandominstance\ncontains10randomlyselectedobjects;thehardinstancesareshowninFigure3.7. Figure3.9\nshows grasp learning curve.",
      "size": 883,
      "sentences": 8
    },
    {
      "id": 101,
      "content": "anceonallmetrics. Werepeatedtheevaluationonarealsystem(see.Figure3.1a). Eachrandominstance\ncontains10randomlyselectedobjects;thehardinstancesareshowninFigure3.7. Figure3.9\nshows grasp learning curve. We compare VPG [27] (trained with 2000 grasp and push\nactions) and the proposed DIPN+GN pipeline (pre-trained with 100 unlabeled RGB-D\nimagesforsegmentation,trainedGNwith500graspactionsandDIPNwith1500simulated\npush actions). The evaluation result is reported in Table 3.2. Remarkably, our networks,\nwhile being developed using only simulation based training, perform even better when\n=== 페이지 46 ===\n28\ntrained/evaluatedonlyonrealhardware. Figure 3.9: Grasp learning curves for PaG in real experiment. Solid lines indicate grasp\nsuccessrate anddottedlines indicatepush-then-grasp successratesover trainingsteps. The\nGNistrainedinagrasponlymanner.",
      "size": 845,
      "sentences": 10
    },
    {
      "id": 102,
      "content": "3.9: Grasp learning curves for PaG in real experiment. Solid lines indicate grasp\nsuccessrate anddottedlines indicatepush-then-grasp successratesover trainingsteps. The\nGNistrainedinagrasponlymanner. Table3.1: Simulation,randomandhardinstances(mean%)\nMethod Completion Graspsuccess Actionefficiency\nVPG[27] 20.01 69.0 52.6 66.3 52.6\nREA[27]+DIPN 83.3 79.5 77.9 77.4 76.3\nRand\nDQN[27]+GN 46.7 85.2 83.9 83.4 81.7\nDIPN+GN 83.3 86.7 85.2 84.4 83.3\nVPG[27] 77.7 67.4 60.0 60.8 57.6\nREA[27]+DIPN 90.3 81.5 76.6 64.7 62.6\nHard\nDQN[27]+GN 86.0 91.1 87.1 70.2 67.9\nDIPN+GN 100.0 93.3 93.3 74.4 74.4\nTable3.2: Realsystem,randomandhardinstances(mean%)\nMethod Completion Graspsuccess Actionefficiency\nVPG[27] 80.0 85.5 79.0 75.3 67.9\nRand\nDIPN+GN 100.0 94.0 94.0 98.2 98.2\nVPG[27] 64.0 75.1 69.0 51.9 47.8\nHard\nDIPN+GN 98.02 89.9 89.9 77.6 78.2\nWithDIPNandGNoutperformingthecorrespondingcomponentsfromVPG[27],itis\nunsurprisingthat DIPN+GNdoesmuchbetter.",
      "size": 942,
      "sentences": 4
    },
    {
      "id": 103,
      "content": "4.0 94.0 98.2 98.2\nVPG[27] 64.0 75.1 69.0 51.9 47.8\nHard\nDIPN+GN 98.02 89.9 89.9 77.6 78.2\nWithDIPNandGNoutperformingthecorrespondingcomponentsfromVPG[27],itis\nunsurprisingthat DIPN+GNdoesmuchbetter. Inparticular,DIPNarchitectureallowsitto\n1Thelowcompletionrateisprimarilyduetopushingobjectsoutsideoftheworkspace. 2Thesinglefailurewasduetoanobjectthatwassuccessfullygraspedbutslippedoutofthegripperbefore\nthetransferwascomplete. [표 데이터 감지됨]\n\n=== 페이지 47 ===\n29\nlearnintelligent,gradedpushbehaviorefficiently. Incontrast,VPG[27]hasafixed0.5push\nreward,whichsometimesnegativelyimpactsperformance: VPGcouldpushunnecessarily\nformanytimeswithoutagraspwhenitisnotconfidentenoughtograsp. Italsoriskspushing\nobjects outside of the workspace. Following [27], we tested DIPN+GN with previously\nunseenobjects,suchassoapboxesandplasticbottles. Ourmethodmaintainedasimilarlevel\nofperformancetothatreportedinTable3.2.",
      "size": 902,
      "sentences": 8
    },
    {
      "id": 104,
      "content": "cts outside of the workspace. Following [27], we tested DIPN+GN with previously\nunseenobjects,suchassoapboxesandplasticbottles. Ourmethodmaintainedasimilarlevel\nofperformancetothatreportedinTable3.2. 3.5 Summary\nInthiswork,wehavedevelopedaDeepInteractionPredictionNetwork(DIPN)forlearning\nto predict the complex interactions that occur as a robot manipulator pushes objects in\nclutter. Unlikemostexistingend-to-endtechniques,DIPNiscapableofgeneratingaccurate\npredictionsintheformofclearlylegiblesyntheticimagesthatcanbefedasinputstoadeep\nGraspNetwork(GN),whichcanthenpredictsuccessesoffuturegrasps. Wedemonstrated\nthat DIPN, GN, and DIPN+GN all have excellent sample efficiency and significantly\noutperformthepreviousstate-of-the-artlearning-basedmethodfor PaGchallenges,while\nusingonlyafractionoftheinteractiondatausedbythealternative. Ournetworksaretrained\ninafullyself-supervisedmanner,withoutanymanuallabelingorhumaninputs,andexhibit\nhigh levels ofgeneralizability.",
      "size": 969,
      "sentences": 7
    },
    {
      "id": 105,
      "content": ",while\nusingonlyafractionoftheinteractiondatausedbythealternative. Ournetworksaretrained\ninafullyself-supervisedmanner,withoutanymanuallabelingorhumaninputs,andexhibit\nhigh levels ofgeneralizability. Theproposed system, initially developed insimulation, also\nperforms effectively when trained and deployed on real hardware with physical objects. DIPN+GN demonstrates high robustness to variations in object properties such as shape,\nsize,color,andfriction. === 페이지 48 ===\n30\nCHAPTER4\nVISUALFORESIGHTTREESFOROBJECTRETRIEVALFROMCLUTTER\nWITHNONPREHENSILEREARRANGEMENT\n4.1 Introduction\nInmanyapplicationdomains,robotsaretaskedwithretrievingobjectsthataresurrounded\nby multiple tightly packed objects. To enable the grasping of target object(s), a robot\nneedsto rearrangethescene tocreatesufficientclearance beforeattemptingagrasp. Scene\nrearrangementcanbeachievedthroughnestedsequentialpushactions,eachmovingmultiple\nobjects simultaneously.",
      "size": 936,
      "sentences": 7
    },
    {
      "id": 106,
      "content": "s), a robot\nneedsto rearrangethescene tocreatesufficientclearance beforeattemptingagrasp. Scene\nrearrangementcanbeachievedthroughnestedsequentialpushactions,eachmovingmultiple\nobjects simultaneously. In this paper, we address the problem of finding the minimum\nnumberofpushactionstocreateascenewherethetargetobjectcanbegraspedandretrieved. Tosolvetheobjectretrievalproblem,therobotmustimaginehowthescenewouldlook\nafter any given sequence of pushing actions, and select the shortest sequence that leads\nto a state where the target object can be grasped. The huge combinatorial search space\nmakes this problem computationally challenging, hence the need for efficient planning\nalgorithms,aswellasfast predictivemodelsthatcanreturnthepredictedfuturestatesina\nfew milliseconds. Moreover, objectsincluttertypicallyhave unknownphysicalproperties\nsuch as mass and friction coefficients.",
      "size": 879,
      "sentences": 6
    },
    {
      "id": 107,
      "content": "ithms,aswellasfast predictivemodelsthatcanreturnthepredictedfuturestatesina\nfew milliseconds. Moreover, objectsincluttertypicallyhave unknownphysicalproperties\nsuch as mass and friction coefficients. While it is possible to utilize off-the-shelf physics\nenginestosimulatecontactsandcollisionsofrigidobjectsinclutter,simulationishighly\nsensitivetotheaccuracyoftheprovidedmechanicalparameters. Toovercometheproblemof\nmanuallyspecifyingtheseparameters,andtoenablefullautonomyoftherobot,mostrecent\nworks on object manipulation utilize machine learning techniques to train predictive models\nfrom data [105]–[107]. The predictive models take the state of the robot’s environment a\ncontrolactionasinputsandpredictthestateafterapplyingthecontrolaction.",
      "size": 744,
      "sentences": 5
    },
    {
      "id": 108,
      "content": "ng techniques to train predictive models\nfrom data [105]–[107]. The predictive models take the state of the robot’s environment a\ncontrolactionasinputsandpredictthestateafterapplyingthecontrolaction. Inthiswork,weproposetoemployvisualforesighttrees(VFT)toaddressthecompu-\n=== 페이지 49 ===\n31\n(b)Firstpush (d)Thirdpush\n(a)Hardwaresetup (c)Secondpush (e)Grasp\nFigure 4.1: (a) The hardware setup for object retrieval in a clutter includes a Universal\nRobotsUR-5emanipulatorwithaRobotiq2F-85two-fingergripper,andanIntelRealSense\nD435 RGB-D camera. The objects are placed in a square workspace. (b), (c), (d) Three\nsequentialpushactions(greenarrows)createspacetoaccessthetarget(purple)object. The\npushdirectionsaretowardtop-left,top-right,andbottom-right,respectively. (e)Thetarget\nobjectissuccessfullygraspedandretrieved. tational and modeling challenges related to the object retrieval problem.",
      "size": 889,
      "sentences": 8
    },
    {
      "id": 109,
      "content": "pushdirectionsaretowardtop-left,top-right,andbottom-right,respectively. (e)Thetarget\nobjectissuccessfullygraspedandretrieved. tational and modeling challenges related to the object retrieval problem. A key building\nblockofVFTisaConvolutionalNeuralNetwork(CNN)extendingDIPN[12],capableof\npredictingmulti-steppush outcomesinvolving multipleobjects. Asecond CNNevaluates\nthegraspabilityofthetargetobjectinpredictedfutureimages. AMonteCarloTreeSearch\nutilizes the two CNNs to obtain the shortest sequence of pushing actions that lead to an\narrangementwherethetargetcanbegrasped. Toourknowledge,theproposedtechniqueisthefirstmodel-basedlearningsolutionto\nthe object retrieval problem. Extensive experiments ona real robot withphysical objects, as\nexemplified in Figure 4.1, demonstrate that the proposed approach succeeds in retrieving\ntargetobjectswithmanipulationsequencesthatareshorterthanmodel-freereinforcement\nlearningtechniquesandalimited-horizonplanningtechnique.",
      "size": 966,
      "sentences": 8
    },
    {
      "id": 110,
      "content": ", demonstrate that the proposed approach succeeds in retrieving\ntargetobjectswithmanipulationsequencesthatareshorterthanmodel-freereinforcement\nlearningtechniquesandalimited-horizonplanningtechnique. === 페이지 50 ===\n32\n4.2 ProblemFormulation\n4.2.1 ProblemStatement\nTheObjectRetrievalfromClutter(ORC)challengeasksarobotmanipulatortoretrievea\ntargetobjectfromasetofobjectsdenselypackedtogether. Theobjectsmayhavedifferent\nshapes,sizes,andcolors. Objectsotherthanthetargetobjectareunknownaprior. Focusingonamostlyplanar\nsetup,thefollowingassumptionsaremade:\n1. Thehardwaresetup(Figure4.1a)containsamanipulator,aplanarworkspacewitha\nuniformbackgroundcolor,andacameraontopoftheworkspace. 2. Theobjects arerigid andare amenabletothe gripper’s prehensileand non-prehensile\ncapabilities,limitedtostraight-lineplanarpushactionsandtop-downgraspactions. 3. Theobjectsareconfinedtotheworkspacewithoutoverlapping. Asaresult,theobjects\narevisibletothecamera. 4.",
      "size": 946,
      "sentences": 12
    },
    {
      "id": 111,
      "content": "nd non-prehensile\ncapabilities,limitedtostraight-lineplanarpushactionsandtop-downgraspactions. 3. Theobjectsareconfinedtotheworkspacewithoutoverlapping. Asaresult,theobjects\narevisibletothecamera. 4. Thetargetobject,toberetrieved,isvisuallydistinguishablefromtheothers. Undertheseassumptions,theobjectiveistoretrieveonlythetargetobject,whileminimizing\nthe number of pushing/grasping actions that are used. Each grasp or push is considered\nas one atomic action. While a mostly planar setup is assumed in our experiments, the\nproposeddata-drivensolutionisgeneralandcanbeappliedtoarbitraryobjectshapesand\narrangements. Intheexperiments,wemainlyworkwithwoodblocks;wealsoevaluatethe\nproposedapproachonnovelobjectssuchassoapboxes,whicharechallengingastheirwidths\nareclosetothemaximumdistancesbetweenthegripper’sfingers.",
      "size": 813,
      "sentences": 10
    },
    {
      "id": 112,
      "content": ". Intheexperiments,wemainlyworkwithwoodblocks;wealsoevaluatethe\nproposedapproachonnovelobjectssuchassoapboxes,whicharechallengingastheirwidths\nareclosetothemaximumdistancesbetweenthegripper’sfingers. 4.2.2 ManipulationMotionPrimitives\nSimilar to studies closely related to the ORC challenge, e.g., [12], [27], [48], we employ\na set of pre-defined and parameterized pushing/grasping manipulation primitives. The\n=== 페이지 51 ===\n33\nYes Perform the\nDirectly No best grasp\ngraspable? Monte Carlo Tree Search (MCTS)\nReward calculation\nExpansion\nGrasp Network (GN)\nDIPN Push Prediction Perform\nthe best push\nMask\nR-CNN\nObservation\n... ... ...\nGraspable\nBefore push After push\nFigure 4.2: Overview of the proposed technique for object retrieval from clutter with\nnonprehensilerearrangement. Theproblemisiterativelysolvedbyobservingtheenvironment\nateachtimestep,takingthecurrentstateasinput,andreturningthebestaction. Itisrepeated\nuntiltheobjectisretrieved.",
      "size": 948,
      "sentences": 7
    },
    {
      "id": 113,
      "content": "with\nnonprehensilerearrangement. Theproblemisiterativelysolvedbyobservingtheenvironment\nateachtimestep,takingthecurrentstateasinput,andreturningthebestaction. Itisrepeated\nuntiltheobjectisretrieved. decision-making problem then entails the search for the optimal order and parameters of\ntheseprimitives. Agraspactionagrasp = (x,y,θ)isdefinedasatop-downoverheadgrasp\nmotion at image pixel location (x,y), with the end-effector rotated along with the world\nz-axis by θ degrees. In our implementation, a grasp center (x,y) can be any pixel in a\ndown-sampled 224×224 image of the planar scene, while rotation angle θ can be one of\n16 values evenly distributed between 0 and 2π. To perform a complete grasp action, the\nmanipulator moves the open gripper above the specified location, then moves the gripper\ndownwardsuntilacontactwiththetargetobjectisdetected,closesthefingers,andtransfers\nthegraspedobjectoutsideoftheworkspace.",
      "size": 922,
      "sentences": 7
    },
    {
      "id": 114,
      "content": "or moves the open gripper above the specified location, then moves the gripper\ndownwardsuntilacontactwiththetargetobjectisdetected,closesthefingers,andtransfers\nthegraspedobjectoutsideoftheworkspace. When objects are densely packed, the target object is generally not directly graspable\ndue to collisions between the gripper and surrounding objects. When this happens, non-\nprehensilepushactionscanbeusedtocreateopportunitiesforgrasping. Forapushaction\napush = (x ,y ,x ,y ), thegripperperforms aquasi-static horizontalmotion. Here, (x ,y )\n0 0 1 1 0 0\nand(x ,y )arethestartandendlocationofthegrippercenter,respectively. Thegripper’s\n1 1\norientationisfixedalongthemotiondirectionduringapushmaneuver. [표 데이터 감지됨]\n\n=== 페이지 52 ===\n34\n4.3 Methodology\n4.3.1 OverviewoftheProposedApproach\nWhenobjectsaretightlypacked,therobotneedstocarefullyselectanappropriatesequence\nof pushes that create a sufficient volume of empty space around the target object before\nattempting to grasp it.",
      "size": 975,
      "sentences": 7
    },
    {
      "id": 115,
      "content": "proach\nWhenobjectsaretightlypacked,therobotneedstocarefullyselectanappropriatesequence\nof pushes that create a sufficient volume of empty space around the target object before\nattempting to grasp it. In this work, we are interested in challenging scenarios where\nmultiple push actions may be necessary to de-clutter the surroundings of the target, and\nwherethelocation,direction,anddurationofeachpushactionshouldbecarefullyoptimized\nto minimize the total number of actions. Collisions among multiple objects often occur\nwhile pushing a single object, further complicating the matter. To address the challenge,\nweproposeasolutionthatusesaneuralnetworktoforecasttheoutcomeofasequenceof\npushactionsin thefuture,and estimates theprobabilityofsucceeding ingraspingthe target\nobjectintheresultingscene. Theoptimalpushsequenceisselectedbasedontheforecasts. Ahigh-leveldescriptionoftheproposedsolutionpipelineisdepictedinFigure4.2.",
      "size": 923,
      "sentences": 6
    },
    {
      "id": 116,
      "content": "eprobabilityofsucceeding ingraspingthe target\nobjectintheresultingscene. Theoptimalpushsequenceisselectedbasedontheforecasts. Ahigh-leveldescriptionoftheproposedsolutionpipelineisdepictedinFigure4.2. At\nthestartofaplanningiteration,anRGB-Dimageofthesceneistaken,andtheobjectsare\ndetectedandclassifiedasunknownclutterortargetobject. Withthetargetobjectlocated,a\nsecondnetworkcalledGraspNetwork(GN)predictstheprobabilityofgraspingthetarget. GN is a Deep Q-Network (DQN) [108] adopted from prior works [12], [27] for ORC. It\ntakes the image input, and outputs the estimated grasp success probability for each grasp\naction. Thetargetobject isconsidered directlygraspableif themaximum estimatedgrasp\nsuccessprobabilityislargerthanathreshold. Therobotexecutesthecorrespondingoptimal\ngraspaction;otherwise,pushactionsmustbeperformedtocreatespaceforgrasping.",
      "size": 850,
      "sentences": 9
    },
    {
      "id": 117,
      "content": "rectlygraspableif themaximum estimatedgrasp\nsuccessprobabilityislargerthanathreshold. Therobotexecutesthecorrespondingoptimal\ngraspaction;otherwise,pushactionsmustbeperformedtocreatespaceforgrasping. Whenpushactionsareneeded,thenextactionisselectedusingMonte-CarloTreeSearch\n(MCTS).Inourimplementation,whichwecalltheVisualForesightTree(VFT),eachsearch\nstate corresponds to an image observation of the workspace. Given a push action and a\nstate,VFTusestheDeepInteractionPredictionNetwork(DIPN)[12]asthestatetransition\nfunction. Here,DIPNisanetworkthatpredictsthemotionsofmultipleobjectsandgenerates\n=== 페이지 53 ===\n35\na synthetic image corresponding to the scene after the imagined push. VFT uses GN to\nobtainarewardvalueforeachsearchnodeanddetectwhetherthesearchterminates. Both\nDIPNandGNaretrainedofflineondifferentobjects. 4.3.2 VisualForesightTrees\nThis section discusses the three main components of VFT: GN, DIPN, and Monte-Carlo\nTreeSearch(MCTS).",
      "size": 951,
      "sentences": 8
    },
    {
      "id": 118,
      "content": "thesearchterminates. Both\nDIPNandGNaretrainedofflineondifferentobjects. 4.3.2 VisualForesightTrees\nThis section discusses the three main components of VFT: GN, DIPN, and Monte-Carlo\nTreeSearch(MCTS). 4.3.3 GraspNetwork\nThe Grasp Network (GN), adapted from [12], takes the image s as input, and outputs a\nt\npixel-wiserewardpredictionR(s ) = [R(s ,a1),...,R(s ,an)]forgraspsa1,...,an. The\nt t t\noutputisa2Dmapwiththesamesizeastheinputimage,andwhereeachpointcontainsthe\npredictedrewardofperformingagraspatthecorrespondinginputpixel. TableR(s )isa\nt\nsingle-channelimagewiththesamesizeasinputimages (224×224inourexperiments),\nt\nand a value R(s ,ai) represents the expected reward of the grasp at the corresponding\nt\naction. TotrainGN,wesettherewardtobe1forgraspswheretherobotsuccessfullypicks\nup only the target object, and 0 otherwise. GN is the reward estimator for states in VFT\n(subsection4.3.5). Agraspactionagrasp = (x,y,θ)specifiesthegrasplocationandtheend-effectorangle.",
      "size": 973,
      "sentences": 9
    },
    {
      "id": 119,
      "content": "essfullypicks\nup only the target object, and 0 otherwise. GN is the reward estimator for states in VFT\n(subsection4.3.5). Agraspactionagrasp = (x,y,θ)specifiesthegrasplocationandtheend-effectorangle. GNistrainedwhilekeepingtheorientationoftheend-effectorfixedrelativetothesupport\nsurface,whilerandomlyvaryingtheposesoftheobjects. Therefore,GNassumesthatthe\ngraspsarealignedtotheprincipalaxisoftheinputimage. TocomputerewardR forgrasps\nwith θ ̸= 0, the input image is rotated by θ before passing it to GN. As a result, for each\ninputimage,GNgenerates16differentgraspR rewardtables. Thetrainingprocessof theGNusedinthis workisbasedonpreviousworks[12],[27]\nbut differs in terms of objectives, which requires a significant modification, explained in\nthefollowing. Theobjectiveinpreviousworksistograspalltheobjects;thegoalof ORC\n=== 페이지 54 ===\n36\nis to retrieve a specific target among a large number of obstacles.",
      "size": 909,
      "sentences": 9
    },
    {
      "id": 120,
      "content": "nificant modification, explained in\nthefollowing. Theobjectiveinpreviousworksistograspalltheobjects;thegoalof ORC\n=== 페이지 54 ===\n36\nis to retrieve a specific target among a large number of obstacles. We noticed from our\nexperimentsthatif GNistrainedtograspalltheobjects,thenagreedypolicywillbelearned,\nand it will always select the most accessible object to grasp. In contrast, all other objects\nthat canalso be directlygrasped are ignored because they have low predicted rewards. This\ncauses the problem that GN cannot correctly predict the grasp success rate of a specific\ntarget object. One straightforward adaptation to this new objective is only to give reward\nwhen the grasp center is inside the target object, which is the approach that was followed\nin[48]. However,wefoundthatwecanachieveahighersampleefficiencybyprovidinga\nrewardforsuccessfullygraspinganyobject. Theproposedtrainingapproachissimilarin\nspirit to Hindsight Experience Replay (HER) [109].",
      "size": 961,
      "sentences": 8
    },
    {
      "id": 121,
      "content": "]. However,wefoundthatwecanachieveahighersampleefficiencybyprovidinga\nrewardforsuccessfullygraspinganyobject. Theproposedtrainingapproachissimilarin\nspirit to Hindsight Experience Replay (HER) [109]. To balance between exploration and\nexploitation,graspactionsarerandomlysampledfromP(s,agrasp) ∝ bR(s,agrasp)b−1 where\nbissetto3/2intheexperiments. Aftertraining, GNcanbe usedforselecting graspingactions innew scenes. Sincethe\nnetwork returnsrewardR for allpossible grasps, and not onlyfor the target object, the first\npost-processing step consists in selectinga small set ofgrasps that overlap withthe target\nobject. Thisisachievedbycomputingtheoverlapbetweenthesurfaceofthetargetobject\nandtheprojected footprintofthe robotic hand,andkeepingonlygraspsthat maximizethe\noverlap. Then, graspswiththehighestpredicted valuesobtainedfromthetrained network\nareranked,andthebestchoicewithoutincurringcollisionsisselectedforexecution.",
      "size": 925,
      "sentences": 8
    },
    {
      "id": 122,
      "content": "hand,andkeepingonlygraspsthat maximizethe\noverlap. Then, graspswiththehighestpredicted valuesobtainedfromthetrained network\nareranked,andthebestchoicewithoutincurringcollisionsisselectedforexecution. 4.3.4 PushPredictionNetwork\nDIPN[12]isanetworkthattakesanRGB-Dimage,2Dmasksofobjects,centerpositions\nofobjects,andavectorofthestartingandendpointsofapushaction. Itoutputspredicted\ntranslations and rotations for each passed object. The predicted poses of objects are then\nusedtocreateasyntheticimage. Effectively,DIPNimagineswhathappenstotheclutterif\ntherobotexecutesacertainpush. The de-cluttering tasks considered in [12] required only single-step predictions. The\n=== 페이지 55 ===\n37\nORCchallengerequireshighlyaccuratepredictionsformultipleconsecutivepushesinthe\nfuture. ToadaptDIPNfor ORC,wefine-tuneditsarchitecture,replacingResNet-18with\nResNet-10 [110] while increasing the output feature dimension from256to512to predict\nmotionsofmoreobjects simultaneouslyandefficiently.",
      "size": 976,
      "sentences": 9
    },
    {
      "id": 123,
      "content": "tDIPNfor ORC,wefine-tuneditsarchitecture,replacingResNet-18with\nResNet-10 [110] while increasing the output feature dimension from256to512to predict\nmotionsofmoreobjects simultaneouslyandefficiently. ThenumberofdecoderMLP layers\nisalsoincreasedtosix,withsizes[768,256,64,16,3,3]. Otheraugmentationsarereported\ninsectionofexperiments. Finally,wetrainedthenetworkwith200,000randompushactions\nappliedonvariousobjects. Thisnumberishigherthanthe1,500actionsusedin[12]aswe\naimfortheaccuracyneededforlong-horizonvisualforesight. Givenasequenceofcandidate\npushactions,thefine-tunedDIPNpredictscomplexinteractions,e.g.,Figure4.3. 4.3.5 VisualForesightTreeSearch(VFT)\nWeintroduceDIPNforpredictingsingle-steppushoutcomeandGNforgenerating/rating\ngraspsasbuildingblocksforamulti-stepprocedurecapableoflong-horizonplanning. A\nnaturalchoiceisMonte-CarloTreeSearch(MCTS)[111],whichbalancesscalabilityand\noptimality.",
      "size": 899,
      "sentences": 8
    },
    {
      "id": 124,
      "content": "houtcomeandGNforgenerating/rating\ngraspsasbuildingblocksforamulti-stepprocedurecapableoflong-horizonplanning. A\nnaturalchoiceisMonte-CarloTreeSearch(MCTS)[111],whichbalancesscalabilityand\noptimality. Inessence,VFTfusesMCTSandDIPNtogenerateanoptimalmulti-steppush\nprediction,asgradedbyGN.Asearch nodeinVFTcorrespondstoaninputsceneorone\nimaginedby DIPN.MCTSprioritizesthemostpromisingstateswhenexpandingthesearch\ntree; in VFT, such states are the ones leading to a successful target retrieval in the least\nnumber of pushes. In a basic search iteration, MCTS has four essential steps: selection,\nexpansion, simulation, and back-propagation. First, the selection stage samples a search\nnode and a push action based on a selection function. Then, the expansion stage creates\na child node of the selected node. After that, the reward value of the new child node is\ndetermined by a simulation from the node to an end state. Finally, the back-propagation\nstageupdatestheestimatedQ-valuesoftheparentnodes.",
      "size": 996,
      "sentences": 8
    },
    {
      "id": 125,
      "content": "d node. After that, the reward value of the new child node is\ndetermined by a simulation from the node to an end state. Finally, the back-propagation\nstageupdatestheestimatedQ-valuesoftheparentnodes. FordescribingMCTSwithvisualforesight,letN(n)bethenumberofvisitstoanoden\nandQ(n) = {r ,...,r }astheestimatedQ-valuesofeachvisit. WeuseN todenote\n1 N(n) max\nthenumberofiterationstheMCTSperformed;wemayalsouseanalternativecomputational\n=== 페이지 56 ===\n38\nSimulation Simulation Real-world Real-world Real-world\nPredictions Ground Truth Predictions Ground Truth Side View\nInput\nT\n1st\nPush\nT+1\n2nd\nPush\nT+2\n3rd\nPush\nT+3\n4th\nPush\nT+4\nFigure 4.3: Example of 4 consecutive pushes showing that DIPN can accurately predict\npushoutcomesoveralonghorizon. Weusepurplearrowstoillustratepushactions. Thefirst\nand secondcolumns are the predictionsand ground truth (objects’ positions after executing\nthe pushes) in simulation. The third and fourth columns show results ona real system.",
      "size": 966,
      "sentences": 8
    },
    {
      "id": 126,
      "content": "tepushactions. Thefirst\nand secondcolumns are the predictionsand ground truth (objects’ positions after executing\nthe pushes) in simulation. The third and fourth columns show results ona real system. The\nlastcolumnisthesideviewofthepushresult. Eachrowrepresentsthepushoutcomewith\nthepreviousrowastheinputobservation. budget to stop the search [111]. The high-level workflow of our algorithm is depicted\ninAlgorithm3,andillustratedinFigure4.2. Wewilldescribeoneiteration(line11-line29)\nof MCTSinVFTalongwiththepseudo-codeintheremainingofthissection. Selection.",
      "size": 559,
      "sentences": 9
    },
    {
      "id": 127,
      "content": "evel workflow of our algorithm is depicted\ninAlgorithm3,andillustratedinFigure4.2. Wewilldescribeoneiteration(line11-line29)\nof MCTSinVFTalongwiththepseudo-codeintheremainingofthissection. Selection. Thefirststepof MCTSistoselectanexpandablesearchnode(line12-line13)\n=== 페이지 57 ===\n39\nAlgorithm3: VisualForesightTreeSearch\n1 FunctionVFT(s )\nt\n2 whilethereisatargetobjectinworkspacedo\n3 R(s ) ← GN(s )\nt t\n4 if max agraspR(s\nt\n,a grasp ) > R\ng\n∗ then\n5 Executeargmax agrasp R(s t ,a grasp ) //Grasp\n6 elseExecuteMCTS(s ) //Push\nt\n7 FunctionMCTS(s ):\nt\n8 Createrootnoden withstates\n0 t\n9 N(·) ← 0,Q(·) ← ∅ //DefaultN,Qforasearchnode\n10 fori ← 1,2,...,N do\nmax\n11 n ← n\nc 0\n▷SelectionandExpansion\n12 whilen isnotexpandabledo\nc\n13 n\nc\n← πtree(n\nc\n) //Use(Equation4.1)tofindachildnode\n14 a push ←samplefromuntriedpushactionsinn\nc\n15 n ← DIPN(n ,a push ) //Generatenodebypushprediction\nc c\n▷Simulation\n16 r ← 0,d ← 1,s ← n .state //sisthestateofn\nc c\n17 whilesisnotaterminalstatedo\n18 a push ←randomlyselectapushactionins\n19 s ← DIPN(s,a push ) //Simulatetonextstate\n20 R(s) ← GN(s)\n21 r ← max{r,γdmax agraspR(s,a grasp )}\n22 d ← d+1\n▷Back-propagation\n23 whilen isnotrootdo\nc\n24 N(n ) ← N(n )+1\nc c\n25 R(n .state) ← GN(n .state)\nc c\n26 r ← max{r,max agraspR(n\nc\n.state,a grasp )}\n27 Q(n ) ← Q(n )∪{r} //Recordthereward\nc c\n28 r ← r·γ\n29 n ← parentofn\nc c\n3 3 1 0 r n e b t e u st r ← np a u r s g h m ac a t x io n n i∈ a c p h u il s d h re t n h o a f t n l 0 e ( a U d C st T o (n n i b , es n t 0 f ) ro ) mtheroot\n=== 페이지 58 ===\n40\nusingatreepolicyπ .",
      "size": 1550,
      "sentences": 4
    },
    {
      "id": 128,
      "content": "3 1 0 r n e b t e u st r ← np a u r s g h m ac a t x io n n i∈ a c p h u il s d h re t n h o a f t n l 0 e ( a U d C st T o (n n i b , es n t 0 f ) ro ) mtheroot\n=== 페이지 58 ===\n40\nusingatreepolicyπ . Here,expandablemeansthenodehassomepushactionsthatarenot\ntree\ntriedviaselection-expansion;moredetailsofthepushactionspacewillbediscussedlaterin\ntheexpansionpart. Tobalancebetweenexplorationandexploitation,whenthecurrentnode\nn isalreadyfullyexpanded,π usesUpperConfidenceBoundsforTrees(UCT)[111]to\nc tree\nrankitschildnoden . WecustomizeUCTas\ni\n(cid:115)\nQm(n ) lnN(n )\nUCT(n ,n ) = i +C c . (4.1)\ni c\nmin{N(n ),m} N(n )\ni i\nHere, C is an exploration weight. In the first term of (Equation 4.1), unlike typical\nUCT that favours thechild node that maximizes Q(n ), we keep only the most promising\ni\nrolloutsofn anddenotebyQm(n )theaveragereturnsofthetopmrolloutsofn . Inour\ni i i\nimplementation,m = 3andC = 2.",
      "size": 905,
      "sentences": 7
    },
    {
      "id": 129,
      "content": "l\nUCT that favours thechild node that maximizes Q(n ), we keep only the most promising\ni\nrolloutsofn anddenotebyQm(n )theaveragereturnsofthetopmrolloutsofn . Inour\ni i i\nimplementation,m = 3andC = 2. Wealsouse(Equation4.1)withparametersm = 1and\nC = 0 to find the best node, and thus the best push action to execute, after the search is\ncompleted,asshowninline30. Expansion. Givenaselectednoden,weuseDIPNtogenerateachildnodebyrandomly\nchoosinganuntriedpushactionapush (line14-line15). Theactionapush isuniformlysampled\natrandomfromtheselectednode’sactionspace,whichcontainstwotypesofpushactions:\n1. For each object, we apply principal component analysis to compute its feature axis. For example, for a rectangle object, the feature axis will be parallel to its long side. Four push actions are then sampled with directions perpendicular or parallel to the\nfeatureaxis,pushingtheobjectfromtheoutsidetoitscenter. 2.",
      "size": 912,
      "sentences": 10
    },
    {
      "id": 130,
      "content": "ject, the feature axis will be parallel to its long side. Four push actions are then sampled with directions perpendicular or parallel to the\nfeatureaxis,pushingtheobjectfromtheoutsidetoitscenter. 2. Tobuildamorecompleteactionspace,eightadditionalactionsareevenlydistributed\noneachobject’scontour,withpushdirectionalsotowardstheobject’scenter. Simulation. Afterwegeneratedanewnodeviaexpansion,inline16-line22,weestimate\nthe node’sQ-valueby uniformlyrandomlyselect pushactions atrandom (line18) anduse\nDIPNtopredictfuturestates(line19)untiloneofthefollowingtwoterminationcriteriais\nmet:\n=== 페이지 59 ===\n41\n1. The total number of push actions used to reach a simulated state is larger than a\nconstantD∗. 2. ThemaximumpredictedrewardvalueofasimulatedstateexceedsathresholdR∗ . gp\nIn line 21, when calculating r, a discount factor γ is used to penalize a long sequence of\naction. Here, weusemaxGNtoreferencethemaximum valueina graspreward table.",
      "size": 940,
      "sentences": 11
    },
    {
      "id": 131,
      "content": "latedstateexceedsathresholdR∗ . gp\nIn line 21, when calculating r, a discount factor γ is used to penalize a long sequence of\naction. Here, weusemaxGNtoreferencethemaximum valueina graspreward table. In\nour implementation, GN is only called once for each unique state and the output is saved by\nahashmap. Back-propagation. After simulation, the terminal grasp reward is back-propagated\n(line23-line29)through itsparentnodestoupdatetheirN(n)andQ(n). Denotebyr the\n0\nmaxgrasprewardofanewlyexpandednoden ,andn ,n ,...,n asthesequenceofn ’s\n0 1 2 k 0\nparentsintheascendingorderuptonoden . WithQ(n ) = {r },theQ-valueofn inthis\nk 0 0 k\niterationisthenmax γk−jmaxQ(n ),whichcorrespondsto themaxrewardofstates\n0≤j<k j\nalongthepath[79]. Here,γ isadiscountfactortopenalizealongsequenceofactions. Asa\nresult,foreachparentn ,N(n )increasesby1,andmax γk−jmaxQ(n )isaddedto\nk k 0≤j<k j\nQ(n ).",
      "size": 879,
      "sentences": 10
    },
    {
      "id": 132,
      "content": "o themaxrewardofstates\n0≤j<k j\nalongthepath[79]. Here,γ isadiscountfactortopenalizealongsequenceofactions. Asa\nresult,foreachparentn ,N(n )increasesby1,andmax γk−jmaxQ(n )isaddedto\nk k 0≤j<k j\nQ(n ). k\n4.4 ExperimentalEvaluation\nWe performed an extensive evaluation of the proposed method, VFT, in simulation and\non the real hardware system illustrated in Figure 4.1. VFT is compared with multiple\nstate-of-the-artapproaches[27],[48]andDIPNinChapter3,withnecessarymodifications\nforsolving ORC, i.e.,minimizing thenumber of actionsin retrievinga target. The results\nconvincinglydemonstrateVFTtoberobustandmoreefficientthanthecomparedapproaches. Bothtrainingand inferenceareperformed onamachine withanNvidiaGeForce RTX\n2080Tigraphicscard,anInteli7-9700KCPU,and32GBofmemory.",
      "size": 771,
      "sentences": 7
    },
    {
      "id": 133,
      "content": "inglydemonstrateVFTtoberobustandmoreefficientthanthecomparedapproaches. Bothtrainingand inferenceareperformed onamachine withanNvidiaGeForce RTX\n2080Tigraphicscard,anInteli7-9700KCPU,and32GBofmemory. === 페이지 60 ===\n42\ncase 1 case 2 case 3 case 4 case 5 case 6 case 7 case 8\ncase 9 case 10 case 11 case 12 case 13 case 14 case 15 case 16\ncase 17 case 18 case 19 case 20 case 21 case 22\nFigure4.4: 22Testcasesusedinbothsimulationandrealworldexperiments. Thetarget\nobjectsareblue. Imagesarezoomedinforbettervisualization. 4.4.1 ExperimentSetup\nThecompletetestcasesetincludes\n1. thefullsetof14testcasesfrom[48]\n2. 18hand-designedandmorechallengingtestcaseswheretheobjectsaretightlypacked. Alltestcasesareconstructedusingwoodblockswithdifferentshapes,colors,andsizes. We\nsettheworkspace’sdimensionsto44.8cm×44.8cm. Thesizeoftheimagesis224×224. Push actions have a minimum 5cm effective push distance, defined as the end-effector’s\nmovingdistanceafterobjectcontact.",
      "size": 959,
      "sentences": 11
    },
    {
      "id": 134,
      "content": "s. We\nsettheworkspace’sdimensionsto44.8cm×44.8cm. Thesizeoftheimagesis224×224. Push actions have a minimum 5cm effective push distance, defined as the end-effector’s\nmovingdistanceafterobjectcontact. Multipleplannedpushactionsmaybeconcatenatedif\ntheyareinthesamedirectionandeachaction’sendlocationisthesameasthenextaction’s\nstartlocation. Inallscenes,thetargetobjectisroughlyatthecenterofthescene. Thehyperparametersfor VFTaresetasfollows. ThenumberofiterationsN = 150.\nmax\nThediscountfactorγ = 0.8. ThemaximumdepthD∗ofthetreeiscappedat4. Theterminal\nthresholdofgrasprewardR∗ = 1.0. ThresholdR∗ thatdecidestograsportopushis0.8in\ngp g\nthesimulationexperimentsand0.7intherealhardwareexperiments. Suchthresholdscan\npotentiallybefullyoptimizedforaproductionsystem;itisnotcarriedoutinthisworkas\n=== 페이지 61 ===\n43\nreasonablygoodvaluesareeasilyobtainedwhileitisprohibitivelytime-consumingtocarry\noutafull-scaleoptimization. 4.4.2 NetworkTrainingProcess\nVFT contains two deep neural networks: GN and DIPN.",
      "size": 997,
      "sentences": 12
    },
    {
      "id": 135,
      "content": "61 ===\n43\nreasonablygoodvaluesareeasilyobtainedwhileitisprohibitivelytime-consumingtocarry\noutafull-scaleoptimization. 4.4.2 NetworkTrainingProcess\nVFT contains two deep neural networks: GN and DIPN. Both are trained in simulationwith\nthesameobjectsasusedinrealexperimentstocapturethephysicalpropertiesanddynamics\noftheenvironment. Nopriorknowledgeisgiventothenetworksexceptthedimensionsof\nthegripperfingers. GNistrainedon-policywith20000graspactions. Similarto[12],[27],[48],randomly-\nshapedobjectsareuniformlydroppedontotheworkspacetoconstructthetrainingscenarios. Asuccessful graspisdecided bychecking thedistancebetweengrippers,which shouldbe\ngreater than 0. A Huber loss on the pixel where the robot performed the grasp action is\nused. Allotherpixelsdonotcontributetothelossduringback-propagation. Image-based\npre-training[12],[103]wasemployedtoinitializethetrainingparameters. Wethentrainthe\nGN by stochastic gradient descent with the momentum of 0.9, weight decay of 10−4, and\nbatchsizeof12.",
      "size": 998,
      "sentences": 11
    },
    {
      "id": 136,
      "content": "on. Image-based\npre-training[12],[103]wasemployedtoinitializethetrainingparameters. Wethentrainthe\nGN by stochastic gradient descent with the momentum of 0.9, weight decay of 10−4, and\nbatchsizeof12. Thelearningrateissetto5×10−5 andbyhalfevery2000iteration. DIPN [12] is trained in a supervised manner with 200000 random push actions from\nsimulation. In the push data set, 20% of the scenes contain randomly placed objects, and\n80%containdenselypackedobjects. ThepushdistanceforDIPNisfixedto7.4cm(effective\ntouchdistanceis5cm). IntheDIPN(Chapter3), thedistance was5cm and10cmwithout\nconsideringtheeffectiverange. Wenotethatatotalof2000actions(500graspsand1500pushes)aresufficientforthe\nnetworkstoachievefairlyaccurateresults(see,Chapter3). Becausetrainingsamplesare\nreadily available from simulation, it is not necessary to skimp on training data. We thus\noptedtotrainwithmoredatatoevaluatethefullpotentialof VFT. ASmooth L1Losswith betaequalsto2isused insteadof1inChapter 3.",
      "size": 975,
      "sentences": 12
    },
    {
      "id": 137,
      "content": "ailable from simulation, it is not necessary to skimp on training data. We thus\noptedtotrainwithmoredatatoevaluatethefullpotentialof VFT. ASmooth L1Losswith betaequalsto2isused insteadof1inChapter 3. Wetrainthe\nDIPNbystochasticgradientdescentwiththemomentumof0.9,weightdecayof10−4,and\n=== 페이지 62 ===\n44\nusing cosine annealing schedule [112] with learning rates of learning rate of 10−3 for 76\nepochs,andthebatchsizeis128. 4.4.3 ComparedMethodsandEvaluationMetrics\nGoal-ConditionedVPG(gc-VPG).Goal-conditionedVPG(gc-VPG)isamodifiedversion\nof Visual Pushing Grasping (VPG) [27], which uses two DQNs [108] for pushing and\ngraspingpredictions. VPGbyitselfdoesnotfocusonspecificobjects;itwasconditioned\n[48]tofocusonthetargetobjecttoserveasacomparisonpoint,yieldinggc-VPG. Goal-Oriented Push-Grasping. In [48], many modifications are applied to VPG to\nrendertheresultingnetworkmoresuitableforsolvingORC,includingadoptingathree-stage\ntrainingstrategyandanefficientlabelingmethod[113].",
      "size": 978,
      "sentences": 8
    },
    {
      "id": 138,
      "content": "ed Push-Grasping. In [48], many modifications are applied to VPG to\nrendertheresultingnetworkmoresuitableforsolvingORC,includingadoptingathree-stage\ntrainingstrategyandanefficientlabelingmethod[113]. Forconvenience,werefertothis\nmethodasgo-PGN(theauthorsof[48]didnotprovideashortnameforthemethod). DIPN.Asanablationbaselineforevaluatingtheutilityofemployingdeeptreesearch,\nwereplaceMCTSfromVFTwithasearchtreeofdepthone. Inthisbaseline,DIPNisused\ntoevaluateallcandidatepushactions. Thepushactionwhosepredictednextstatehasthe\nhighest grasp reward for the target object is then chosen. This is similar to how DIPN is\nusedinChapter3;wethusrefertoitsimplyasDIPN. Inour evaluation, themain metricis thetotal numberofpush andgrasp actionsusedto\nretrievethetargetobject. Foracompletecomparisonto[27],[48],wealsolist VFT’sgrasp\nsuccess rate, which is the ratio of successful grasps in the total number of grasps during\ntesting.",
      "size": 918,
      "sentences": 9
    },
    {
      "id": 139,
      "content": "rasp actionsusedto\nretrievethetargetobject. Foracompletecomparisonto[27],[48],wealsolist VFT’sgrasp\nsuccess rate, which is the ratio of successful grasps in the total number of grasps during\ntesting. Thecompletionrate,i.e.,thechanceofeventuallygraspingthetargetobject,isalso\nreported. SimilartoChapter3,whenDIPNisused,a100%completionrateoftenreached. We only collected evaluation data on DIPN and VFT. For the other two baselines,\ngc-VPGandgo-PGN,results aredirectlyquoted from[48](at thetimeof oursubmission,\nwe could not obtain the trained model or the information necessary for the reproduction\nof gc-VPG and go-PGN). While our hardware setup is identical to that of [48], and the\nposesofobjectsarealsoidentical,wenotethattherearesomesmalldifferencesbetweenthe\n=== 페이지 63 ===\n45\nevaluationsetups:\n1. WeusePyBullet[114]forsimulation,while[48]usesCoppeliaSim;thephysicsengine\nisthesame(Bullet). 2.",
      "size": 898,
      "sentences": 9
    },
    {
      "id": 140,
      "content": "realsoidentical,wenotethattherearesomesmalldifferencesbetweenthe\n=== 페이지 63 ===\n45\nevaluationsetups:\n1. WeusePyBullet[114]forsimulation,while[48]usesCoppeliaSim;thephysicsengine\nisthesame(Bullet). 2. [48]usesanRD2gripperinsimulationandaRobotiq2F-85gripperforrealexperiment;\nallofourexperimentsuse2F-85. 3. [48]hasa13cmpushdistance,whileweonlyusea5cmeffectivedistance(thedistance\nwherefingerstouchtheobjects)\n4. [48]usesextratop-slidingpusheswhichexpandthepushactionset. We believe these relatively minor differences in experimental setup do not provide our\nalgorithmanunfairadvantage. 4.4.4 SimulationStudies\nFigure4.5andTable4.1showtheevaluationresultsofallalgorithmsonthe10simulation\ntest cases from [48]. Each experiment is repeated 30 times, and the average number of\nactions until task completion in each experiment is reported. Our proposed method,VFT,\nwhich uses an average of 2.00 actions, significantly outperforms the compared methods.",
      "size": 945,
      "sentences": 11
    },
    {
      "id": 141,
      "content": "and the average number of\nactions until task completion in each experiment is reported. Our proposed method,VFT,\nwhich uses an average of 2.00 actions, significantly outperforms the compared methods. Specifically, VFTusesonepushactionandonegraspactiontosolvethemajorityofcases,\nexceptforoneinstancewithahalf-cylindershapedobject,whichisnotincludedduringthe\ntrainingofthenetworks. Interestingly,whenonlyonepushisnecessary,VFT,withitsmain\nadvantageasmulti-stepprediction,stilloutperformsDIPNduetoitsextrasimulationsteps. Thealgorithmswithpushpredictionperformbetterthangc-VPGandgo-PGNinallmetrics. To probe the limit of VFT’s capability, we evaluated the methods on harder cases\ndemanding multiple pushes. The test set includes 18 manually designed instances and 4\ncases from [48] (see Figure 4.4). As shown in Figure 4.6 and Table 4.2, VFT uses fewer\nactionsthanDIPNasVFTlooksfurtherintothefuture.",
      "size": 896,
      "sentences": 8
    },
    {
      "id": 142,
      "content": "ushes. The test set includes 18 manually designed instances and 4\ncases from [48] (see Figure 4.4). As shown in Figure 4.6 and Table 4.2, VFT uses fewer\nactionsthanDIPNasVFTlooksfurtherintothefuture. Thoughwecouldnotevaluatethe\n=== 페이지 64 ===\n46\nFigure4.5: Simulationresultsper testcaseforthe10problemsfrom[48]. Thehorizontal\naxisshowstheaveragenumberofactionsusedtosolveaprobleminstance: thelower,the\nbetter. Completion GraspSuccess NumberofActions\ngc-VPG[48] 89.3% 41.7% 5.78\ngo-PGN[48] 99.0% 90.2% 2.77\nDIPN[12](Chapter3) 100% 100% 2.30\nVFT(Chapter4) 100% 100% 2.00\nTable4.1: Simulationresultsforthe10testcasesfrom[48]. performance of gc-VPG and go-PGN on these settings for direct comparison because we\ncouldnotobtaintheinformationnecessaryforthereproductionofthesesystems,notably,\ntheaveragenumberofactions(2.45)usedbyVFTonharderinstancesisevensmallerthan\nthenumberofactions(2.77)go-PGNusedonthe10simplercases. Completion GraspSuccess Num.",
      "size": 944,
      "sentences": 8
    },
    {
      "id": 143,
      "content": "orthereproductionofthesesystems,notably,\ntheaveragenumberofactions(2.45)usedbyVFTonharderinstancesisevensmallerthan\nthenumberofactions(2.77)go-PGNusedonthe10simplercases. Completion GraspSuccess Num. ofActions\nDIPN[12](Chapter3) 100% 98.3% 4.31\nVFT(Chapter4) 100% 98.8% 2.45\nTable4.2: Simulationresultforthe22testcasesin Figure4.4. 4.4.5 EvaluationonaRealSystem\nWe repeated the 22 hard test cases on a real robot system (Figure 4.1a). Both VFT and\nDIPNareevaluated. Wealsobringtheexperimentresultfrom[48]onits4realtestcases\nforcomparison. Allcasesare repeatedatleast5 timestogetthe meanmetrics. Theresult,\n[표 데이터 감지됨]\n\n=== 페이지 65 ===\n47\nFigure 4.6: Simulation result per test case for the 22 harder problems (Figure 4.4). The\nhorizontalaxisshows theaveragenumberofactions usedtosolveaprobleminstance: the\nlower,thebetter. showninFigure4.7,Table4.3,andTable4.4closelymatchestheresultsfromsimulation. Weobserveaslightlylowergraspsuccessrateduetothemorenoisydepthimageonthereal\nsystem.",
      "size": 982,
      "sentences": 11
    },
    {
      "id": 144,
      "content": "aprobleminstance: the\nlower,thebetter. showninFigure4.7,Table4.3,andTable4.4closelymatchestheresultsfromsimulation. Weobserveaslightlylowergraspsuccessrateduetothemorenoisydepthimageonthereal\nsystem. The real workspace’s surface friction is also different from simulation. However,\nVFTandDIPNcanstillgenerateaccurateforesight. Figure 4.7: Real experiment results per test case for the 22 harder problems (Figure 4.4). Thehorizontalaxisshowstheaveragenumberofactionsusedtosolveaprobleminstance:\nthelower,thebetter. Wealsoexploredoursystemoneverydayobjects(Figure4.8),wherewewanttoretrieve\na small robotic vehicle surrounded by soapboxes. Although the soapboxes and the small\nvehiclesareunseentypesofobjectsduringtraining,therobotisabletostrategicallypush\n=== 페이지 66 ===\n48\nCompletion GraspSuccess Num. ofActions\nDIPN[12](Chapter3) 100% 97.0% 4.78\nVFT(Chapter4) 100% 98.5% 2.65\nTable4.3: Realexperimentresultsforthe22TestcasesinFigure4.4. Completion GraspSuccess Num.",
      "size": 965,
      "sentences": 11
    },
    {
      "id": 145,
      "content": "==\n48\nCompletion GraspSuccess Num. ofActions\nDIPN[12](Chapter3) 100% 97.0% 4.78\nVFT(Chapter4) 100% 98.5% 2.65\nTable4.3: Realexperimentresultsforthe22TestcasesinFigure4.4. Completion GraspSuccess Num. ofActions\ngo-PGN[48] 95.0% 86.6% 4.62\nDIPN[12](Chapter3) 100% 100% 4.00\nVFT(Chapter4) 100% 100% 2.60\nTable4.4: Realexperimentresultsforcases19to22inFigure4.4. thesoapboxesawayintwomovesonlyandretrievethevehicle. Figure4.8: Testscenariowithsoapboxesandmasked(inpurple)3Dprintedvehicle. Two\npushactionsandonegraspaction. Wereportthattherunningtimetodecideonepushactionisaround3minutesonaverage\nwhen the number of MCTS iterations is set to be 150. A single push prediction of DIPN\ntook30milliseconds. WhileusingthesimulatorasthetransitionfunctioninMCTSundera\nsimilarcriterionwouldtake8minutesonaveragetodecideonepushaction. Inthischapter,\nourprimaryfocusisactionoptimization.",
      "size": 872,
      "sentences": 11
    },
    {
      "id": 146,
      "content": "DIPN\ntook30milliseconds. WhileusingthesimulatorasthetransitionfunctioninMCTSundera\nsimilarcriterionwouldtake8minutesonaveragetodecideonepushaction. Inthischapter,\nourprimaryfocusisactionoptimization. 4.5 Summary\nInconclusion,throughanorganicfusionofDeepInteractionPredictionNetwork(DIPN)and\nMCTS,theproposedVisualForesightTrees(VFT)canmakeahigh-qualitymulti-horizon\nprediction for optimized object retrieval from dense clutter. The effectiveness of VFT is\nconvincinglydemonstratedwithextensiveevaluation. Astothelimitationsof VFT,thetime\nrequired isrelatively longbecause of thelarge MCTStree thatneeds to becomputed. This\n[표 데이터 감지됨]\n\n=== 페이지 67 ===\n49\ncan be improved with multi-threading because the rollouts have sufficient independence. Currently,onlyasinglethreadisusedtocompletetheMCTS.Itwouldalsobeinteresting\nto develop a learned heuristic or value function to guide or truncate the rollout phase,\npotentiallyreducinginferencetime.",
      "size": 940,
      "sentences": 8
    },
    {
      "id": 147,
      "content": "Currently,onlyasinglethreadisusedtocompletetheMCTS.Itwouldalsobeinteresting\nto develop a learned heuristic or value function to guide or truncate the rollout phase,\npotentiallyreducinginferencetime. ThistechniquewouldbesimilarinspirittotheMuZero\nalgorithm[115],whichhasbeenshowntobeefficientbycombiningMonteCarlotreesearch\nandlearningbyself-playinginanend-to-endmanner. Thelearnedrolloutpolicycouldlead\nto better performance, whichwill becovered inChapter 5. One issuerelated toend-to-end\ntraining is data efficiency, which is why this type of technique has been limited to games. Improvingthedataefficiencyofend-to-endtechniquesiscrucialtothedeploymentofthese\ntechniquesonrobotictasks.",
      "size": 686,
      "sentences": 5
    },
    {
      "id": 148,
      "content": "d\ntraining is data efficiency, which is why this type of technique has been limited to games. Improvingthedataefficiencyofend-to-endtechniquesiscrucialtothedeploymentofthese\ntechniquesonrobotictasks. === 페이지 68 ===\n50\nCHAPTER5\nINTERLEAVINGMONTECARLOTREESEARCHANDSELF-SUPERVISED\nLEARNINGFOROBJECTRETRIEVALINCLUTTER\n5.1 Introduction\nKahneman[116]proposedathought-provokinghypothesisofhumanintelligence: insolving\nreal-worldproblems,humansengagefastor“System1”(S1)typeofthinkingformaking\nsplit-seconddecisions,e.g.,speech,driving,andsoon. Forotherdecision-makingprocesses,\ne.g., playing chess, a slow or “System 2” (S2) approach is taken, where the brain would\nperformasearchoversomestructureddomainforthebestactionstotake. Afterrepeatedly\nusingS2thinkingtosolveagivenproblem,patternscanbedistilledovertimeandburnedinto\nS1toacceleratetheoverallprocess. Inplayingchess,forexample,goodchessplayerscan\ninstinctivelyidentifygoodcandidatemoves.",
      "size": 936,
      "sentences": 6
    },
    {
      "id": 149,
      "content": "usingS2thinkingtosolveagivenproblem,patternscanbedistilledovertimeandburnedinto\nS1toacceleratetheoverallprocess. Inplayingchess,forexample,goodchessplayerscan\ninstinctivelyidentifygoodcandidatemoves. First-timeorbeginnerdriversrelyheavilyonS2\nandgraduallyconvergetoS1astheygainmoreexperience. ThisS2→S1thinkinghasgained\nsignificantattentionandhasbeenexploredinmanydirectionsinmachinelearning,including\nattemptsatbuildingmachineswithconsciousness[117]. Perhapsthemostprominentlineof\nworkinreinforcementlearning[118]thatcloselyalignswiththisparadigmistheapplication\nof Monte Carlo Tree Search (MCTS) for carrying out self-supervised learning in games\n[115],[119],wherean“understanding”ofagameemergesfromalifelongself-playandis\ngradually distilled so that it significantly reduces the search effort. Gradually, the overall\nsystemlearnsenoughusefulinformationthatallowsittoplay perfectgameswithmuchless\ntimeandcomputingresources.",
      "size": 925,
      "sentences": 6
    },
    {
      "id": 150,
      "content": "radually distilled so that it significantly reduces the search effort. Gradually, the overall\nsystemlearnsenoughusefulinformationthatallowsittoplay perfectgameswithmuchless\ntimeandcomputingresources. Inspired by [115], [119] that show a search-and-learn approach for realizing S2→S1\napplieswelltogame-likesettingswithrelativelywell-definedrules,wesetouttofindout\nwhetherwecouldbuildasimilarframeworkthatenablesrealrobotstointeractwithreal-world\n=== 페이지 69 ===\n51\n(b)Firstpush\n(c)Secondpush\n(a)Hardwaresetup (d)Grasp\nGuided MCTS\n…………\n…………\nGrasp Prediction Push Prediction Physics\nNetwork Network Simulator\nGrasp Push (e)\nFigure 5.1: (a) The hardware setup for object-retrieval-from-clutter includes a Universal\nRobotsUR5e manipulatorwith a Robotiq 2F-85two-fingergripper, and anIntel RealSense\nD455RGB-Dcamera. The objectsareplaced inasquareworkspaceand thetarget objectis\nmasked in purple. (b)(c) Two push actions (shown with green arrows) are used to enable\nthegraspingofthetarget(purple)object.",
      "size": 996,
      "sentences": 5
    },
    {
      "id": 151,
      "content": "RGB-Dcamera. The objectsareplaced inasquareworkspaceand thetarget objectis\nmasked in purple. (b)(c) Two push actions (shown with green arrows) are used to enable\nthegraspingofthetarget(purple)object. (d)Thetargetobjectissuccessfullygraspedand\nretrieved. (e)Theoverviewofouroverallsystem. physicsanduncertaintiestoperformphysicaltasks,somewhatakinto[120]. Specifically,\nwefocusonthetaskofretrievinganobjectenclosedinclutterusingnon-prehensileactions,\nsuch as pushing and poking, followed by prehensile two-finger grasping. The goal is to\n=== 페이지 70 ===\n52\nobtainacomputationallyefficientsystemandproducehigh-qualitysolutions(i.e.,usingthe\nminimumnumberofactions). As pointed out by Valpola [121], due to the difficulty in exploring the landscape of\nthe state space of real-world problems, in addition to uncertainty, naive applications of\nthe S2→S1 paradigm often lead to undesirable behavior. Non-trivial design as well as\nengineeringeffortsareneededtobuildsuchS2→S1systems.",
      "size": 974,
      "sentences": 10
    },
    {
      "id": 152,
      "content": "ld problems, in addition to uncertainty, naive applications of\nthe S2→S1 paradigm often lead to undesirable behavior. Non-trivial design as well as\nengineeringeffortsareneededtobuildsuchS2→S1systems. Intheobject-retrieval-from-\ncluttersetting,thechallengeliesinthedifficultyofpredictingtheoutcomeofpushactions,\nwiththetipofthegripper,whenmanyobjectsareinvolved. Thisisduetodiscontinuities\ninherent in object interactions; for example, while a certain pushing action might move a\ngivenobject,aslightlydifferentpushdirectioncouldmissthatsameobjectentirely. The main contribution of this work is proving the feasibility of applying the S2→S1\nphilosophytobuildaself-supervisedroboticobjectretrievalsystemcapableofcontinuously\nimprovingitscomputationalefficiency,throughcloningthebehaviorofthetime-consuming\ninitial MCTS phase.",
      "size": 822,
      "sentences": 5
    },
    {
      "id": 153,
      "content": "lying the S2→S1\nphilosophytobuildaself-supervisedroboticobjectretrievalsystemcapableofcontinuously\nimprovingitscomputationalefficiency,throughcloningthebehaviorofthetime-consuming\ninitial MCTS phase. Through the careful design and integration of two Deep Neural\nNetworks(DNNs)withMCTS,ourproposedself-supervisedmethod,namedMonteCarlo\ntree search and learning for Object REtrieval (MORE), enables a DNN to learn from the\nmanipulation strategies discovered by MCTS. Then, learned DNNs are fed back to the\nMCTSprocesstoguidethesearch. MOREsignificantlyreducesMCTScomputationload\nandachievesidentical orbetter outcomes,i.e., retrievingthe objectusing veryfewstrategic\npushactions. Inotherwords,ourmethod“closestheloop”. Thiscontrastswith[120],which\nonlylearnstoreplacetherolloutfunctionofMCTS. 5.2 ProblemFormulation\nThe Object Retrieval from Clutter task consists in using a robot manipulator to retrieve a\nhard-to-reachtargetobject(Figure5.1).",
      "size": 941,
      "sentences": 7
    },
    {
      "id": 154,
      "content": "ch\nonlylearnstoreplacetherolloutfunctionofMCTS. 5.2 ProblemFormulation\nThe Object Retrieval from Clutter task consists in using a robot manipulator to retrieve a\nhard-to-reachtargetobject(Figure5.1). Objects arerigidbodieswith variousshapes,sizes,\nand colors; the target object is assigned a unique color. Similar to Chapter 3, a top-down\nfixed cameraisinstalled toobserve theworkspace. ThecameratakesanRGB-Dimage of\n=== 페이지 71 ===\n53\ntheworkspace(e.g.,thetop-leftimageofFigure5.1),whichservesastheonlyinputtoour\nsystem. Pushing and grasping actions are allowed, the execution of each is considered as one\natomicaction. Agraspactionisdefinedasatop-downoverheadgraspmotionag = (x,y,θ),\ncorrespondingtothegripper’stargetlocationandorientation,basedonacoordinatesystem\ndefined over the input image. A push action is defined as a quasi-static planar motion\nap = (x ,y ,x ,y ) where (x ,y ) and (x ,y ) are the start and the end locations of the\n0 0 1 1 0 0 1 1\ngrippertip.",
      "size": 968,
      "sentences": 8
    },
    {
      "id": 155,
      "content": "ined over the input image. A push action is defined as a quasi-static planar motion\nap = (x ,y ,x ,y ) where (x ,y ) and (x ,y ) are the start and the end locations of the\n0 0 1 1 0 0 1 1\ngrippertip. Thehorizontalpushdistanceisfixedanditis10cminourexperiments. Each\nprimitive action is transformed to the real-world coordinates for execution, but all the\nplanning and reasoning are in image coordinates. The robotic arm keeps pushing objects\nuntil the target object can be grasped or until the target object is pushed outside of the\nworkspace,inwhichcasethetaskisconsideredafailure. Theproblemistofindapolicythat\nmaximizesthefrequencyofsuccessfullygraspingthetargetobject,whilealsominimizing\nthenumberofpre-grasppushingactions. 5.3 Methodology\nThe MORE framework consists of three components: a Grasp Network (GN), a Monte\nCarloTreeSearch(MCTS)routine,andaPushPredictionNetwork(PPN).GNisaneural\nnetworkthatpredictsthesuccessprobabilitiesofgraspactions. Itistrainedonlinesimilarly\nto Chapter 4.",
      "size": 993,
      "sentences": 8
    },
    {
      "id": 156,
      "content": "Grasp Network (GN), a Monte\nCarloTreeSearch(MCTS)routine,andaPushPredictionNetwork(PPN).GNisaneural\nnetworkthatpredictsthesuccessprobabilitiesofgraspactions. Itistrainedonlinesimilarly\nto Chapter 4. The success probabilities can be interpreted as immediate rewards. MCTS\nusesaphysicsengineasatransitionfunctiontosimulatelongsequencesofconsecutivepush\nactionsthatendwithaterminalgraspaction. EachbranchinMCTSiscomposedofpush\nactionsasinternalnodes,andagraspactionasaleaf. GraspactionsareevaluatedwithGN,\nandthereturnedrewardsareback-propagatedtoevaluatetheircorrespondingbranches. The\nbranchwiththehighestdiscountedreward,orQ-value,isselectedforexecutionbytherobot. Whilehighlyeffectiveinfindingnear-optimalpaths,MCTSsuffersfromahighcomputa-\ntiontimethatmakesitimpractical. Tosolvethis,MOREemploysasecondneuralnetwork,\n=== 페이지 72 ===\n54\nPPN, to prioritize the action selection in the rollout policy.",
      "size": 898,
      "sentences": 9
    },
    {
      "id": 157,
      "content": "optimalpaths,MCTSsuffersfromahighcomputa-\ntiontimethatmakesitimpractical. Tosolvethis,MOREemploysasecondneuralnetwork,\n=== 페이지 72 ===\n54\nPPN, to prioritize the action selection in the rollout policy. The robot starts by relying\nentirely on MCTS (S2 type of thinking) to solve various instances of the object-retrieval\nproblem. Instead of throwing away the computation performed by MCTS for solving the\nvariousinstances,weusethecomputedQ-valuesasground-truthtotrainPPN.Notethatthis\ncomputation data is free, since it is generated by the simulations performed by MCTS as\na byproduct of solving the actual problem. PPN is a neural network that learns to imitate\nMCTSandcloneitsbehavior,whileavoidingheavycomputationandphysicssimulationsby\nMCTS.AsPPNbecomesmoreaccurateinpredictingtheoutcomeofMCTS,therobotstarts\nrelyingonbothMCTSandPPNforactionselection.",
      "size": 851,
      "sentences": 5
    },
    {
      "id": 158,
      "content": "mitate\nMCTSandcloneitsbehavior,whileavoidingheavycomputationandphysicssimulationsby\nMCTS.AsPPNbecomesmoreaccurateinpredictingtheoutcomeofMCTS,therobotstarts\nrelyingonbothMCTSandPPNforactionselection. Inanutshell,PPNisusedfororienting\nthe search in MCTS toward more promising push actions that rearranges the scene and\nrendersthetargetobjectgraspable. Afteralongexperience,PPN’saccuracyinpredicting\ntheQ-valuesofpushactionsmatchesthatofMCTS, andtherobotswitchesentirelytoPPN\ntomakedecisionsinafewmilliseconds(S1typeofthinking). 5.3.1 Monte-CarloTreeSearch\nMonte-Carlo Tree Search (MCTS) [122] is used in MORE for both decision-making and\ntraining PPN. A typicalMCTS routine has four steps: selection, expansion, simulation, and\nback-propagation. Inour case,thegoal ofthesearch istofind theshortestaction sequence;\nwecanstopthesearchassoonasthebestsolutionisfoundwithoutexploringtherest.",
      "size": 885,
      "sentences": 6
    },
    {
      "id": 159,
      "content": "s: selection, expansion, simulation, and\nback-propagation. Inour case,thegoal ofthesearch istofind theshortestaction sequence;\nwecanstopthesearchassoonasthebestsolutionisfoundwithoutexploringtherest. The\nsearchstopsintwocases:\n1. thenumberofiterationsnexceedsapre-setbudgetN ,or\nmax\n2. the expanded node with state that the target object can be grasped, and all nodes in\nparentlevelareexpanded. Anodeisconsideredasaleafifmax R (s )[i,j,θ] > R whereR (s )isobtained\ni,j,θ gm t g∗ gm t\nfromGNandR isapre-definedhighprobability. Themaximumdepthofthetreeislimited\ng∗\ntod,wheredissetto4inourexperiments.",
      "size": 598,
      "sentences": 5
    },
    {
      "id": 160,
      "content": "eisconsideredasaleafifmax R (s )[i,j,θ] > R whereR (s )isobtained\ni,j,θ gm t g∗ gm t\nfromGNandR isapre-definedhighprobability. Themaximumdepthofthetreeislimited\ng∗\ntod,wheredissetto4inourexperiments. === 페이지 73 ===\n55\nIntheselectionphase,wefindanexpandablenodestartingfromtherootaccordingto\nthesearchpolicy\n(cid:115)\nlnN(s)\nπ (s) = argmax(Q(s,ap)+C ), (5.1)\nn N(s,ap)\nap\nwhere N(s) is the number of visits to node (state) s and N(s,ap) is the number of times\npushactionap hasbeenselectedatnode(state)s. TheQ-valueiscalculatedas\n(cid:80)m r (s,ap)\nQ(s,ap) = i=1 i , (5.2)\nmin{N(n ),m}\ni\nwherer(s,ap)is the returned long-term reward andmis a pre-setmaximum. Only thebest\nm terms r (s,ap) are used to compute the Q-value in the equation above. m is set to 10\ni\nwhen expanding nodes and 1 when selecting the best solution. C is the coefficient of the\nexploration term, and it is set to 2 when expanding nodes and 0 when selecting the best\nsolution.",
      "size": 944,
      "sentences": 6
    },
    {
      "id": 161,
      "content": "set to 10\ni\nwhen expanding nodes and 1 when selecting the best solution. C is the coefficient of the\nexploration term, and it is set to 2 when expanding nodes and 0 when selecting the best\nsolution. In the expansion phase, we use a physics simulator to execute the chosen push\naction ap at state s and predict new state s . Then, a random policy is used to sample\ni i+1\nactions to simulate until a grasp is possible or a failure is encountered. The reward r is\npredictedbyGNataterminalstates . Rewardr issetto1ifmax R (s )[i,j,θ] > R ,\nt i,j,θ gm t g∗\nand0otherwise. Oneadditionaltermδmax(R (s ))isaddedtor,todistinguishbetween\ngm t\ngoodandbadpushactions. Wesetδ tobe0.2. Inthelaststep,rewardr ispropagatedback\ntoitsparentnodestoupdatetheirQ-valueswithadiscountfactorγ = 0.5. As the push action space is enormous even after discretization, we further sample a\nsubset ofactions such thatall pushactions start aroundthe contourof anobject andpoint to\nthe center of the object (Figure 5.2).",
      "size": 987,
      "sentences": 10
    },
    {
      "id": 162,
      "content": "action space is enormous even after discretization, we further sample a\nsubset ofactions such thatall pushactions start aroundthe contourof anobject andpoint to\nthe center of the object (Figure 5.2). This action sampling method has been discussed in\nVFT(Chapter4)andwasempiricallyprovenefficientforasimilarsetupofobjectretrieval. In our implementation, N is set to 300 when MCTS is used to collect data to train\nmax\nPPN. The second and the third conditions for stopping the search are only activated after\natleast50roll-outs,sothatthenumberofvisitstoastateisstatisticallysignificantandto\nreducethevarianceof PPN. === 페이지 74 ===\n56\nFigure5.2: Sampledpushactions. 5.3.2 PushPredictionNetwork(PPN)\nAspreviouslymentioned,PPNlearnstoimitateMCTS.PPNisadeepneuralnetworkwith\nResNet-34FPN[102],[110]asthebackbone,wheretheP2leveloftheFPNconnectstothe\nhead.",
      "size": 847,
      "sentences": 6
    },
    {
      "id": 163,
      "content": "dpushactions. 5.3.2 PushPredictionNetwork(PPN)\nAspreviouslymentioned,PPNlearnstoimitateMCTS.PPNisadeepneuralnetworkwith\nResNet-34FPN[102],[110]asthebackbone,wheretheP2leveloftheFPNconnectstothe\nhead. It takes a two-channel input and outputs a single channel pixel-wise push Q-value\nmap,similartotherewardmapproducedbyGN.AnexampleinputisshowninFigure5.3,\nwherethefirstchannelisasegmentedimageofallobjectsandthesecondchannelisabinary\nimage of the target object. The output is the image on the right of Figure 5.3, where the\narrowshowsapushactionwiththehighestQ-value. PPNestimatestheQ-value(discounted\nrewards) Q (s ) of executing push actions at the corresponding pixel, where the action is\np t\nassumedtopush10cmtotheright. max(Q (s))islimitedtotherange[0,η],whereη isthe\np\nmaximumrewardofaterminalstate. WhenMCTSisusedtogeneratetrainingcases,itbuildsatreeandsavesthetransitions\nforeachcase: thestate(image)s,thepushactionap,theQ-valueQ(s,ap),andthevisited\nnumber N(s,ap).",
      "size": 971,
      "sentences": 7
    },
    {
      "id": 164,
      "content": "maximumrewardofaterminalstate. WhenMCTSisusedtogeneratetrainingcases,itbuildsatreeandsavesthetransitions\nforeachcase: thestate(image)s,thepushactionap,theQ-valueQ(s,ap),andthevisited\nnumber N(s,ap). As such, PPN is trained in a self-supervised manner. The input image\nis rotatedbased on apush action sothat the corresponding push actionpoints to theright. === 페이지 75 ===\n57\nPush\nPrediction\nNetwork\nFigure5.3: ThelefttwofiguresaretheinputtoPPN.Thefirstisasegmentationofobjects;\nthesecondisthemaskofthetargetobject. TheimageontherightistheoutputfromthePPN. WeuseJetcolormaptorepresenttherewardvalue,wherethevaluerangesfromred(high)\ntoblue(low). ThepixelwiththehighestQ-valueisplottedwithacircleandattachedwith\nanarrowontherightimage,representingpushingactionstartingatthecircleandmovingto\ntherightwithadistanceof10cm.",
      "size": 815,
      "sentences": 8
    },
    {
      "id": 165,
      "content": "fromred(high)\ntoblue(low). ThepixelwiththehighestQ-valueisplottedwithacircleandattachedwith\nanarrowontherightimage,representingpushingactionstartingatthecircleandmovingto\ntherightwithadistanceof10cm. BecauseasingleactionisgeneratedbyMCTS(i.e.,aδ signalovertheentireinput),which\nis not conducive to training PPN, we “expand” the Q-value over a 3 × 3 patch centered\naroundMCTSactionbutsetinvalidpushes(e.g.,ifpartofthepatchisinsideanobject)tobe\nzero. Now,thelabelis relativelydensecompared toaone-hotpixel,sowecanuseSmooth\nL1lossfromPytorch[123]withβ equalsto0.8toregress. Onlygradientsonthelabeled\npixelsare used. Lossweightingis alsoapplied: labelvaluesfromtheMCTSareweighted\nbasedon N(s,ap),label values(zeroQ-value)fromvoid pushactionsareweightedwith a\nsmallnumber,0.001forcollisionand0.0001forpushingthinair. Weobservethat PPNhas\ndifficultylearningtocreateclearancearoundthetargetobject.",
      "size": 890,
      "sentences": 7
    },
    {
      "id": 166,
      "content": ",label values(zeroQ-value)fromvoid pushactionsareweightedwith a\nsmallnumber,0.001forcollisionand0.0001forpushingthinair. Weobservethat PPNhas\ndifficultylearningtocreateclearancearoundthetargetobject. Dataaugmentationisapplied\nheresothatforeachtrainingcase,wealsorandomlychoosethetargetobjectfortheMCTS\ntosolve; so eacharrangement becomesmany trainingcases. Itmitigates over-fitting; given\nsimilar visual information, it could learn different strategies, as the target object could be\nanywhere. The head model is an FCN with four layers, where the first two layers have a kernel\nsize of 3, the last two 1, and the strides of four layers are all 1. Batch normalization is\nusedateachlayeroftheheadmodelexceptthelast. Bilinearinterpolation(×2)isapplied\ninterleavedbetweenthelastthreelayersoftheheadmodeltoscaleupthehiddenstatetothe\nsamesizeastheinputimage. Thetrainingprocesshastwostages,one totrainthenetwork\n=== 페이지 76 ===\n58\nwithabatchsizeof8,learningratestartsat1e−4,for50epochs.",
      "size": 979,
      "sentences": 8
    },
    {
      "id": 167,
      "content": "ayersoftheheadmodeltoscaleupthehiddenstatetothe\nsamesizeastheinputimage. Thetrainingprocesshastwostages,one totrainthenetwork\n=== 페이지 76 ===\n58\nwithabatchsizeof8,learningratestartsat1e−4,for50epochs. Thelearningratedecays\nwithcosineannealing[112],wherethemaximumnumberofiterationsissettobethesame\nastheepochnumber50andtheminimumlearningrateis1e−8. Thesecondisafine-tuning\nstage;weincreasethebatchsizeto28andthelearningrateto1e−5withanepochof20. 5.3.3 GuidedMonte-CarloTreeSearch\nWith the trained GN and PPN, a guided MCTS is implemented to accelerate the search\nprocess,cuttingcostfromtime-consumingexpansionandsimulationphases. GNisagain\nusedto determinetheterminal stateand ifso, calculateits estimatedreward, asdiscussed\ninsubsection5.3.1. PPN,trainedwithdatafromMCTS,canestimatehowmuchrewardcan\nbegainedfromtakingapushactionatacertainstate.",
      "size": 844,
      "sentences": 7
    },
    {
      "id": 168,
      "content": "determinetheterminal stateand ifso, calculateits estimatedreward, asdiscussed\ninsubsection5.3.1. PPN,trainedwithdatafromMCTS,canestimatehowmuchrewardcan\nbegainedfromtakingapushactionatacertainstate. Forthiscombinationof MCTSwithPPN,someadditionalupdatesaremade(compared\ntosubsection5.3.1)toincorporatetheguidancefromPPN.Theexplorationtermisremoved\nfrom thesearch policy,soC in equationEquation 5.1is setto 0. Similarto [124],weuse\ntheestimatedrewardfromPPNasaprior,sotheQ-valueiscalculatedasfollows\nmax(Q (s))+ (cid:80)m r (s,ap)\nQ (s,ap) = p i=1 i , (5.3)\nguide N(s,ap)\nwheremissetto3whenexpandingnodesandN(s,ap)isinitializedto1forallstate-action\npairs. InsteadofcomputinganaverageasstandardMCTS,onlybestmofQ areconsidered,\np\nthisisduetothenumberofrolloutissmall,agoodactioncouldbeaveragedout. Toselect\nthebestactionasthenextstepsolution,theQ-valueiscalculatedwithoutthedenominator\nQ (s,ap) = max(Q (s))+max(r (s,ap)), (5.4)\nbest p i\nwhereonlythebestexploredsolutionisconsidered.",
      "size": 980,
      "sentences": 6
    },
    {
      "id": 169,
      "content": "eaveragedout. Toselect\nthebestactionasthenextstepsolution,theQ-valueiscalculatedwithoutthedenominator\nQ (s,ap) = max(Q (s))+max(r (s,ap)), (5.4)\nbest p i\nwhereonlythebestexploredsolutionisconsidered. ThepushactionspaceoftheguidedMCTSislimitedtoasubset(likeFigure5.2)sothat\ntheestimatedrewardfromPPNismoreaccurateandthebranchingfactorofthetreeisofa\n=== 페이지 77 ===\n59\nreasonablesize. Tomaketheselectionmimicthetrainingdata,werotatetheimageforeach\nsampledpushactionsuchthatthepushactionintherotatedimageisalwayspointingtothe\nright. Then,weonlyusetheestimatedQ-valueatthecorrespondingpixel(pushaction)of\ntheoutputQ-valuemap. AnexampleofguidedMCTSisgiveninFigure5.4. Theexpansion\nof the tree is prioritized by PPN, where the push action with higher Q-value is sampled\nearlier,andthe rolloutpolicyisalsoprioritized. Themaximumdepthofthe treeislimited\nto3insteadof4asusedintheearlierversionofMCTSforcollectingdatatotrainPPN.",
      "size": 917,
      "sentences": 8
    },
    {
      "id": 170,
      "content": "e push action with higher Q-value is sampled\nearlier,andthe rolloutpolicyisalsoprioritized. Themaximumdepthofthe treeislimited\nto3insteadof4asusedintheearlierversionofMCTSforcollectingdatatotrainPPN. Search sequence\n0.57 0.54 0.46 0.45 <0.45\nTarget Graspable\nFigure 5.4: An example of the guided MCTS with a budget of 10 iterations. State with\nlarger image have higher estimated Q-values. All expanded nodes are plotted. The numbers\ninthefirstlevelsrepresenttheestimatedQ-valuereturnedbyPPNforcorrespondingpush\naction. These values, together with the reward returned from simulation, guide the tree\nsearch. 5.4 ExperimentalEvaluation\nWeevaluatedthe proposed technique both in simulation(PyBullet [125]) and onadversarial\ntest cases on a UR5e robot with a Robotiq 2F-85 gripper using real objects. The robot,\n=== 페이지 78 ===\n60\nworkspace,objects,andcameraarethesameinsimulationandreal-worldexperiments,so\nthatwecanseamlesslytransferfromsimulationtotherealsetup.",
      "size": 959,
      "sentences": 9
    },
    {
      "id": 171,
      "content": "iq 2F-85 gripper using real objects. The robot,\n=== 페이지 78 ===\n60\nworkspace,objects,andcameraarethesameinsimulationandreal-worldexperiments,so\nthatwecanseamlesslytransferfromsimulationtotherealsetup. The workspaceislimited\ntoasquarewithasidelengthof0.448m;itisdiscretizedasagridof224×224cellsduring\nthe image processing step. The friction of objects and table surface cannot be accurately\nmeasured; nevertheless, high-fidelity physical properties do not seem to be needed for\nthis particularapplication. The results demonstrate that theproposed method significantly\noutperformsMCTSin Chapter4[16]intermsoftimeefficiency whilereturning plansof\nequalquality. Theplansreturnedbytheproposedtechniquecontainfeweractionsandyield\nhigher success rates than those returned by the purely learning-based solution presented\nin[48]. TrainingandevaluationarecompletedonamachinewithanInteli7-9700KCPU\nandanNvidiaGeForceRTX2080Ti. 5.4.1 Simulationexperiments\nTasks.",
      "size": 949,
      "sentences": 8
    },
    {
      "id": 172,
      "content": "hose returned by the purely learning-based solution presented\nin[48]. TrainingandevaluationarecompletedonamachinewithanInteli7-9700KCPU\nandanNvidiaGeForceRTX2080Ti. 5.4.1 Simulationexperiments\nTasks. Givenanarrangementofheterogeneousandtightlypackedobjects,atargetobjectis\nto be retrieved using push and grasp actions from a two-finger gripper. In simulation, we\nbenchmarkon22adversarialtestcasesfromChapter4(Figure4.4)and10from[27],[48]. Here“adversarial”meansthatatleastonepushactionhastobeexecutedforagraspaction\nto be feasible (insert gripper without collision). Random cases, which are too easy from\nChapter3andChapter4[12],[16],arenotdiscussedhere. Metrics.",
      "size": 663,
      "sentences": 8
    },
    {
      "id": 173,
      "content": "tatleastonepushactionhastobeexecutedforagraspaction\nto be feasible (insert gripper without collision). Random cases, which are too easy from\nChapter3andChapter4[12],[16],arenotdiscussedhere. Metrics. Weusefourmetrics:\n1. thenumberofactionsusedtoretrievethetargetobject,\n2. thetotaltimeusedforretrievingthetargetobject,whichincludesbothplanningtime\nandexecutiontimeforsimulationresults,\n3. thecompletionrate,failuresoccurwhenthetargetobjectispushedoutoftheworkspace,\nand\n=== 페이지 79 ===\n61\n4. thegraspsuccess rate,whichisthe numberofsuccessfulgraspsdivided bythetotal\nnumberofgraspingattempts. Thenumberofre-arrangementactionsthatareneededtomakethetargetobjectgraspable\nandtimearethetwomainmetrics. Thecompletionandgraspsuccessratesarealsoreported\nbutarenotthemainfocusastheyareoftencloseto100%. BaselineMethods. Wecomparewiththreemethods:\n1.",
      "size": 840,
      "sentences": 8
    },
    {
      "id": 174,
      "content": "kethetargetobjectgraspable\nandtimearethetwomainmetrics. Thecompletionandgraspsuccessratesarealsoreported\nbutarenotthemainfocusastheyareoftencloseto100%. BaselineMethods. Wecomparewiththreemethods:\n1. A self-supervised reinforcement learning method denoted as go-PGN [48], which\ntrainsagraspDQNandapushDQNthenselectsanactionwiththehighestQ-value\noutofthetwonetworkstoexecute. 2. MCTSasdescribedinSectionsubsection5.3.1. Thisisadaptedfrom[16],butweuse\nhereasimulatortopredictthenextstateinsteadoftheoriginallyusedlearnedmodel,\nforfaircomparisons. 3. PPNasdescribedinSectionsubsection5.3.2. PPNproposespushactionsbasedon\ntheir predicted Q-values and the robot executes those actions until the target object\ncanbegraspedaccordingtoGN. Simulation Studies. We ran our method and the three alternative methods on 22\ncases Chapter 4 and 10 cases [27], [48], in simulation first.",
      "size": 870,
      "sentences": 13
    },
    {
      "id": 175,
      "content": "actions until the target object\ncanbegraspedaccordingtoGN. Simulation Studies. We ran our method and the three alternative methods on 22\ncases Chapter 4 and 10 cases [27], [48], in simulation first. Table 5.1 and Table 5.2 show\ntheoverallperformanceofthefourmethods,whereMCTSbasedmethodsarelimitedtoa\nbudgetof50iterationspertestcase. Inthispaper,wedenotethetreesearchmethodswith\ndifferentbudgetsofsearchiterationsasMCTS-10/20/50andMORE-10/20/50,wherethe\nsuffix denotes the iterations limit. The 22 cases are generally harder to solve than the 10\ncases,wherethetargetobjectcanberetrievedafteronepushaction. Thetimemetricrecords\ntheaveragetime(outof 5trials)forretrievingthe object,includingplanning andexecution\ntimes. Forthebaselinego-PGN,resultson10casesaredirectlyquotedfromthepaper(atthe\ntimeofoursubmission,wecouldnotobtainthetrainedmodelortheinformationnecessary\n=== 페이지 80 ===\n62\nforfullyreproducing go-PGN).MOREusesthefewestnumberofactionstosolvethe task.",
      "size": 962,
      "sentences": 8
    },
    {
      "id": 176,
      "content": "tlyquotedfromthepaper(atthe\ntimeofoursubmission,wecouldnotobtainthetrainedmodelortheinformationnecessary\n=== 페이지 80 ===\n62\nforfullyreproducing go-PGN).MOREusesthefewestnumberofactionstosolvethe task. Performancedetailson22casescanbefoundinFigure5.5forthenumberofactionsand\nFigure 5.6for therunningtime. PPNis fast asit is aone-stageDNNs solution. Itlearned\na policy that creates free spaces around the target object, but it is less consistent and less\nstablethanthetreesearchsolutions. Fromourobservation,PPNcanproposenon-prevailing\npushingactions. MCTSprovidesaconsistentandgoodqualitysolution,butrequiresamuch\nlonger planning time. MORE, combining the benefits of both, reduces the planning time\nanddelivershigh-qualitysolutions. Num. ofActions Time Completion GraspSuccess\nMORE-50 2.61 82s 100% 99.2%\nMCTS-50[16] 2.69 208s 100% 99.1%\nPPN 3.68 8s 100% 97.7%\nTable 5.1: Simulate experiment results for 22 cases Chapter 4. Budgets of MCTS and\nMOREarelimitedupto50iterations. Num.",
      "size": 979,
      "sentences": 11
    },
    {
      "id": 177,
      "content": "-50 2.61 82s 100% 99.2%\nMCTS-50[16] 2.69 208s 100% 99.1%\nPPN 3.68 8s 100% 97.7%\nTable 5.1: Simulate experiment results for 22 cases Chapter 4. Budgets of MCTS and\nMOREarelimitedupto50iterations. Num. ofActions Time Completion GraspSuccess\nMORE-50 2.10 16s 100% 100%\nMCTS-50[16] 2.20 32s 100% 93.4%\nPPN 2.70 4s 100% 95.0%\ngo-PGN[48] 2.77 − 99.0% 90.0%\nTable5.2: Simulateexperimentresultsfor10cases[48]. Budgetsof MCTSandMOREare\nlimitedupto50iterations. AblationStudiesAlthoughthedatageneratedbyMCTSfortrainingPPNisfreebecause\nitis collectedfullyautomatically insimulation, wesetto exploredataefficiency intraining,\nwhichcanbeimportantforbuildinglargermodelsinpractice. Forthispurpose,wecollected\n243trainingcases(65384transitionsin30hourswithPyBullet)withMCTSasdescribed\ninsubsection5.3.1. TrainingonPPNonalldatatookapproximately22hours. Asshown\ninFigure5.7,wetestedMCTSandMOREwithdifferentbudgets. Also,MOREistrainedon\ndifferent numbersof trainingdata.",
      "size": 952,
      "sentences": 10
    },
    {
      "id": 178,
      "content": "hMCTSasdescribed\ninsubsection5.3.1. TrainingonPPNonalldatatookapproximately22hours. Asshown\ninFigure5.7,wetestedMCTSandMOREwithdifferentbudgets. Also,MOREistrainedon\ndifferent numbersof trainingdata. Clearly,the problemcan be solved by alltested methods\nwithfeweractions whenthesearchiterationlimitsareincreased. Butthetimeforsolving\n[표 데이터 감지됨]\n\n=== 페이지 81 ===\n63\n6\n5\n4\n3\n2\n1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22\nIndex of cases\nsnoitca\nfo\nrebmuN\nMORE-50\n9\nMCTS-50\nPPN\nFigure 5.5: The average number (out of 5 trials) of action used to solve one case for 22\ncases. 800\n600\n400\n200\n0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22\nIndex of cases\n)s(\nemiT\nMORE-50\nMCTS-50\nPPN\nFigure5.6: Theaveragetime(of5trials)usedtosolveonecasefor22cases. theproblemalsoincreasesasaconsequence. TheproposedMOREtechniquecanretrieve\ntargetobjectswithonly2.8executedactionsandusingonly10iterationsofMCTSthatlast\n36secondsonaverage.",
      "size": 942,
      "sentences": 9
    },
    {
      "id": 179,
      "content": "dtosolveonecasefor22cases. theproblemalsoincreasesasaconsequence. TheproposedMOREtechniquecanretrieve\ntargetobjectswithonly2.8executedactionsandusingonly10iterationsofMCTSthatlast\n36secondsonaverage. ThisisclosetothebestthatMCTSwithoutPPNcanachieve,2.69\nactions,after50iterationsthatlast208seconds. Whenwelimitthenumberofiterationsof\nMCTS (without MORE) to 10, the number of executed actions increases to 3.19, and the\nsearchtimeremainsrelativelyhigh(127seconds). Thisclearlydemonstratesthesuperior\nperformanceoftheproposedapproachintermsofbothtimeandactionefficiency. 5.4.2 RobotExperiments\nWeevaluatedthefourmethodsonsixrealtestcases(fourfrom[48]andtwofromChapter4).",
      "size": 668,
      "sentences": 7
    },
    {
      "id": 180,
      "content": "learlydemonstratesthesuperior\nperformanceoftheproposedapproachintermsofbothtimeandactionefficiency. 5.4.2 RobotExperiments\nWeevaluatedthefourmethodsonsixrealtestcases(fourfrom[48]andtwofromChapter4). Thesesixtestcasesarerepresentativeinthattheycontainmoreobjectsandoftenrequireat\n[표 데이터 감지됨]\n\n=== 페이지 82 ===\n64\n3.2\n3.0\n2.8\n2.6\n10 20 50\nIteration\nsnoitca\nfo\nrebmuN\n200\n150\n100\n50\n10 20 50\nIteration\nemiT\nMORE with 100.0% of data MORE with 25.0% of data MCTS\nMORE with 50.0% of data MORE with 12.5% of data\nFigure5.7: DifferentamountsoftrainingdataareusedtotrainPPN,whichareevaluatedon\nMOREwithdifferentbudgets(iteration). Thisistheevaluationofthe22cases. leasttwopushactionstosolve. Fortheserealexperiments,theresultsareshowninTable5.3\nandFigure5.9. Thebudgetof MCTSandMOREislimitedto10iterations. Wenotethatthe\nresultsforgo-PGNaretakenfrom[48]. Theexecutiontimeof PPNisnotlistedinTable5.3\nas it is a near-constant small value as we had in the simulationexperiments.",
      "size": 965,
      "sentences": 9
    },
    {
      "id": 181,
      "content": "ndMOREislimitedto10iterations. Wenotethatthe\nresultsforgo-PGNaretakenfrom[48]. Theexecutiontimeof PPNisnotlistedinTable5.3\nas it is a near-constant small value as we had in the simulationexperiments. From the result,\nwe observe only negligible performance degradation in comparison to simulation, which\nmay be due to differences in friction, slight differences in the dimensions of the objects\nbetweensimulationandrealworld,statisticalerror,or acombinationofthese. Overall,the\nsim-to-realtransferwasverysuccessfulandshowedthat MOREcanlearninsimulationand\ndirectlyapplythelearnedskilltoreal-worldtasks. Weassumemodelsofobjectsareknown,\nsuchthatsimpleposeestimationcanbeusedtolocateobjectsintherealworldandplacedin\nsimulationforplanning. Wecouldalsousesophisticatedtrackingsystems[126]–[128]for\ngeneralpurpose. Figure 5.8: Manually generated cases similar to [48] and Chapter 4. The target object is\nmaskedinpurple. ThesecasesareusedalsoinsimulationexperimentsasshowninFigure4.4.",
      "size": 977,
      "sentences": 10
    },
    {
      "id": 182,
      "content": "[126]–[128]for\ngeneralpurpose. Figure 5.8: Manually generated cases similar to [48] and Chapter 4. The target object is\nmaskedinpurple. ThesecasesareusedalsoinsimulationexperimentsasshowninFigure4.4. [표 데이터 감지됨]\n\n=== 페이지 83 ===\n65\nNum. ofActions Time Completion GraspSuccess\nMORE-10 2.83 36s 100% 100%\nMCTS-10[16] 3.67 190s 100% 95.8%\nPPN 3.72 3s 94.5% 95.8%\ngo-PGN[48] 4.62 − 95.0% 86.6%\nTable 5.3: Real experiment results for six cases as shown in Figure 5.8. The budget of\nMCTSandMOREislimitedto10iterations. Forgo-PGN,onlythefirstfourcasesapply,\nandresultsarefrom[48]. Onlyplanningtimeisrecorded(robotexecutionwasintentionally\nslowed down for safety). The computation time for PPN to solve a task is 3 seconds on\naverage(estimated). 6\n4\n2\n10 13 19 20 21 22\nIndex of cases\nsnoitca\nfo\nrebmuN\n300\n200\n100\n0\n10 13 19 20 21 22\nIndex of cases\nemiT\nMORE-10 MCTS-10 PPN go-PGN\n2\nFigure5.9: Thenumberofactionandtimeusedonsolvingsixcases. Thebudgetisupto\n10iterationsfor MCTSandMORE.",
      "size": 977,
      "sentences": 12
    },
    {
      "id": 183,
      "content": "oitca\nfo\nrebmuN\n300\n200\n100\n0\n10 13 19 20 21 22\nIndex of cases\nemiT\nMORE-10 MCTS-10 PPN go-PGN\n2\nFigure5.9: Thenumberofactionandtimeusedonsolvingsixcases. Thebudgetisupto\n10iterationsfor MCTSandMORE. 5.5 Summary\nThe main limitation of this work is that we need to know the models of the objects to do\nthe planning. One possible solution is instead of using an explicit simulator, we can use\na learned model Chapter 3 to simulate the push results. Generalization to novel objects\ncouldthenbepossible. WecanfurtherutilizethePushPredictionNetwork toestimatethe\nsimulation (rollout) result instead of using a physics engine. However, this can introduce\nadditionaluncertaintiesthattypicallyresultfromusingDNNs,whichcancauseunexpected\nbehaviorssuchaspushingobjectsoutoftheworkspace. Buildingontheknow-howgained\nfromdevelopingMORE,weareexploringotherreal-worldroboticmanipulationtasksthat\nwouldbenefitfromtheS2→S1search-and-learnphilosophy.",
      "size": 933,
      "sentences": 8
    },
    {
      "id": 184,
      "content": "iorssuchaspushingobjectsoutoftheworkspace. Buildingontheknow-howgained\nfromdevelopingMORE,weareexploringotherreal-worldroboticmanipulationtasksthat\nwouldbenefitfromtheS2→S1search-and-learnphilosophy. Wepointoutthat MOREcan\nbefurtherspedupbyimplementingaparallelversionof MCTS,asweonlyutilizedasingle\n[표 데이터 감지됨]\n\n=== 페이지 84 ===\n66\nCPUthreadinourimplementationandPPN(onGPU)isnotbeingusedmostofthetime. === 페이지 85 ===\n67\nCHAPTER6\nPARALLELMONTECARLOTREESEARCHWITHBATCHEDRIGID-BODY\nSIMULATIONSFORSPEEDINGUPLONG-HORIZONEPISODICROBOT\nPLANNING\n6.1 Introduction\nThepastdecadehaswitnesseddramaticleapsinrobotmotionplanningforsolvingproblems\nthatinvolvesophisticatedinteractionbetweentherobotanditsenvironment,withmilestones\nincluding teaching quadrupeds to perform impressive tricks [129], [130] and navigate\nchallengingterrains[131],enablinghigh-DOFrobothandstosolvetheRubik’scube[10],\nand so on.",
      "size": 888,
      "sentences": 4
    },
    {
      "id": 185,
      "content": "nvironment,withmilestones\nincluding teaching quadrupeds to perform impressive tricks [129], [130] and navigate\nchallengingterrains[131],enablinghigh-DOFrobothandstosolvetheRubik’scube[10],\nand so on. While some of the success can be attributed to the rapid advancement in deep\nlearning [132] and deep reinforcement learning [133], another undeniable factor is the\navailability of fast, high-fidelity physics engines, including PyBullet [125] and MoJuCo\n[134]. These physics engines allow the simulation of the physics of complex rigid-body\nsystems,sometimesfasterthanreal-time, whichenablesthecollectionoflargeamountsof\nrealisticsystembehaviordatawithouteventouchingtheactualrobothardware. Nevertheless,\nmostphysicssimulatorsareCPU-based,whichcanonlysimulatealimitednumberofrobots\nsimultaneously; thishas led tosome studies seekingparallelism byusing a massive amount\nof computing resources.",
      "size": 891,
      "sentences": 4
    },
    {
      "id": 186,
      "content": "vertheless,\nmostphysicssimulatorsareCPU-based,whichcanonlysimulatealimitednumberofrobots\nsimultaneously; thishas led tosome studies seekingparallelism byusing a massive amount\nof computing resources. For example, the OpenAI hand study [10] used a total of 6,144\nCPUcorestotraintheirmodelforover40hours,whichiscostlyandtime-consuming. As physics simulation starts to become a bottleneck in solving robotic tasks, GPU-based\nphysicsengines have recentlybegun to emerge, including IsaacGym [135] andBrax [136],\nto address the issue by enabling large-scale rigid body simulation. Earlyresults are fairly\npromising;forexample,the trainingoftheOpenAIhandusingIsaacGymcanbedoneona\nsingleGPUinonehour,translatingtoacombinedresource-timesavingofseveralordersof\n=== 페이지 86 ===\n68\nmagnitude. Similar success has also been realized in applying reinforcement learning on\nquadrupeds,roboticarms,andsoon[135]. (a) (b)\nCan the target be\nGrasp Network\nNo grasped?",
      "size": 945,
      "sentences": 6
    },
    {
      "id": 187,
      "content": "rsof\n=== 페이지 86 ===\n68\nmagnitude. Similar success has also been realized in applying reinforcement learning on\nquadrupeds,roboticarms,andsoon[135]. (a) (b)\nCan the target be\nGrasp Network\nNo grasped? Yes\nGrasp\nMCTS\nPhysics Simulation\nRobot\nAction Sampler Push Execution\nGrasp Classifier\nUntil the target object is retrieved\n(c)\nFigure 6.1: (a) The hardware setup includes a Universal Robots UR-5e with a Robotiq\n2F-85two-fingergripperandanIntelRealSenseD455RGB-Dcamera. (b)Planningand\nsimulation carried out in physics simulator where thousands of virtual robots operate in\nparallel. (c)Overviewofoursystem;thesmallbluecylinderatthecenteristhetargetobject\ntoberetrieved. In this work, we exploit the power of large-scale rigid body simulation for optimally\n=== 페이지 87 ===\n69\nsolving long-horizon episodic robotic planning tasks, such as multi-step object retrieval\nfromclutter,leveragingthestrengthofanotherpowerfultoolthathasattractedagreatdeal\nofattention–MonteCarlotreesearch(MCTS)[122].",
      "size": 990,
      "sentences": 7
    },
    {
      "id": 188,
      "content": "izon episodic robotic planning tasks, such as multi-step object retrieval\nfromclutter,leveragingthestrengthofanotherpowerfultoolthathasattractedagreatdeal\nofattention–MonteCarlotreesearch(MCTS)[122]. MCTSdemonstratesclearadvantages\nin solving long-horizon optimization problems without the need for significant domain\nknowledge [137], and was already employed for solving challenging manipulation tasks\nChapter4[16]andChapter5[19]. However,evenwithsignificantguidanceusingdomain\nknowledgeChapter5,MCTSincursfairlylongplanningtimesduetoitsneedofcarrying\noutnumerousroundsofsequentialselection-expansion-simulation-backpropagationcycles. Thelongplanningtime,sometimesseveralminutesperdecisionstep,limitstheapplicability\nofthemethodsfromChapter4andChapter5towardreal-timedecisionmaking.",
      "size": 783,
      "sentences": 4
    },
    {
      "id": 189,
      "content": "lection-expansion-simulation-backpropagationcycles. Thelongplanningtime,sometimesseveralminutesperdecisionstep,limitstheapplicability\nofthemethodsfromChapter4andChapter5towardreal-timedecisionmaking. Through combining MCTS and large-scale rigid-body simulation with Isaac Gym\n[135],andcarefullyintroducingparallelismintothemix,wehavedevelopedanewlineof\nparallel MCTS algorithms for efficiently solving long-horizon episodic robotic planning\ntasks. Thedevelopmentofthelarge-scalerigid-bodysimulationenabledparallelMCTSis\nthe key contribution of this research, which is highly non-trivial. This is because MCTS\nhas an inherently serial characteristic; as will be explained in more detail, the selection\nphaseofanMCTSiterationdependsonthecompletionofthepreviousselection-expansion-\nsimulation-backpropagationiteration.",
      "size": 815,
      "sentences": 5
    },
    {
      "id": 190,
      "content": "inherently serial characteristic; as will be explained in more detail, the selection\nphaseofanMCTSiterationdependsonthecompletionofthepreviousselection-expansion-\nsimulation-backpropagationiteration. FusingMCTSandIsaacGymforsolvinglong-horizon\nmanipulationplanning tasksalsobrings significanttechnicalintegrationchallengesbecause\nmanycomputationalbottlenecksmustbeaddressedfortheparallelMCTSimplementation\ntobeefficient. WecallouralgorithmParallelMonteCarlotreesearchwithBatchedSimulation(PMBS). Asitsnamesuggests,PMBSrealizesparallelMCTScomputationthroughbatchedrigid-body\nsimulation enabled by Isaac Gym. Efficiently combining MCTS and Isaac Gym, PMBS\nachieves over 30× speedups in planning efficiency for solving the task of object retrieval\ninclutter,whilestillachievingbettersolutionquality,ascomparedtoanoptimizedserial\nMCTSimplementation,usingidenticalcomputinghardware.",
      "size": 877,
      "sentences": 5
    },
    {
      "id": 191,
      "content": "ps in planning efficiency for solving the task of object retrieval\ninclutter,whilestillachievingbettersolutionquality,ascomparedtoanoptimizedserial\nMCTSimplementation,usingidenticalcomputinghardware. PMBSdropsthesingle-step\n=== 페이지 88 ===\n70\ndecisionmaking timetoa fewseconds onaverage,whichis closetobeingable tosolvethe\ntaskinreal-time. WefurtherdemonstratethatPMBScanbedirectlyappliedtorealrobot\nhardwarewithnegligiblesim-to-realdifferences. 6.2 ProblemFormulation\nIn this paper, we task a robot equipped with a camera and a two-finger gripper to grasp a\ndesiredobjectfromadenselypackedclutter,asaconcreteinstanceoflong-horizonepisodic\nrobotplanningproblems. Theworkspaceisaconfinedplanarsurface. Twotypesofprimitive\nactions are allowed: pushing and grasping. All objects are rigid; the target object has a\ndifferent color to facilitate its detection. The only observation available to the robot is an\nRGB-Dimagethatistakenbyatop-downfixedcamera,asshowninFigure6.1.",
      "size": 968,
      "sentences": 8
    },
    {
      "id": 192,
      "content": "jects are rigid; the target object has a\ndifferent color to facilitate its detection. The only observation available to the robot is an\nRGB-Dimagethatistakenbyatop-downfixedcamera,asshowninFigure6.1. Everytime\nthe robot executes a push or a grasp action, a new image is taken. A similar problem has\nbeenpreviouslydefinedinChapter3[12],Chapter4[16],andChapter5[19]. Compared\nto[16], [19], theproblemaddressed inthe presentworkissignificantlymore challengingto\nsolvebecausetheworkspaceisconfinedtoasubstantiallysmallerarea,whilekeepingthe\nnumberandsizesofobjectsthesame. Consequently,thefreespacebetweentheobjectsis\nreduced,andtherobotneedstofindalargernumberofshortersurgicalpushactionsinorder\ntofreethetargetobjectand grasp it. Infact,wefoundfromourexperiments (section6.4)\nthat the original setup considered in [16], [19] can be solved using a brute-force parallel\nsearchinaGPU-basedphysicssimulator,withoutaMonteCarlotreesearch.",
      "size": 930,
      "sentences": 7
    },
    {
      "id": 193,
      "content": "act,wefoundfromourexperiments (section6.4)\nthat the original setup considered in [16], [19] can be solved using a brute-force parallel\nsearchinaGPU-basedphysicssimulator,withoutaMonteCarlotreesearch. 6.3 Methodology\nMCTSbuildsasearchtree,balancingexplorationandexploitation,byiterativelyperforming\nselection-expansion-simulation-backpropagationoperations. Intheselectionphase,MCTS\nselectsabestnodetogrowthetree. Apopularnodeselectioncriterionisbasedontheupper\n=== 페이지 89 ===\n71\nconfidencebound (UCB)[138],[139],\n(cid:115)\nQ(n′) 2lnN(n)\nargmax +c , (6.1)\nN(n′) N(n′)\nn′∈childrenofn\nwhereQ(n)isthesumofrewardscollectedstartingfromthestatecorrespondingtonoden,\nN(n)isthenumber oftimesnwasselectedso far. The selectionprocesscontinuesuntilit\nfindsanodethatcorrespondstoaterminalstateoranodethathasnever-exploredchildren. We notethata nodenisalways associatedwith astatesandan observationo; sometimesa\nnodenand thecorrespondingstatesare usedinterchangeably.",
      "size": 952,
      "sentences": 6
    },
    {
      "id": 194,
      "content": "orrespondstoaterminalstateoranodethathasnever-exploredchildren. We notethata nodenisalways associatedwith astatesandan observationo; sometimesa\nnodenand thecorrespondingstatesare usedinterchangeably. After a nodenis selected,if\nitisnotaterminalnode,itisexpanded,anditsnewchild,sayn′,isaddedtothesearchtree. Subsequently,asimulationwillbecarriedoutatn′. This selection-expansion-simulation\nprocessisrepeateduntilaterminalstate(orastoppingcondition)isreached,whichyields\na reward. The obtained terminal reward is propagated back from n′ all the way to the\nrootnode,whileupdatingthesumofreward(Q(n))andincrementingthenumberofvisits\n(N(n))forallthenodesalongthepath. Effectively employingMCTS totacklelong-horizonepisodic robot planningrequires\na highly non-trivial adaptation of MCTS.",
      "size": 781,
      "sentences": 7
    },
    {
      "id": 195,
      "content": "ofreward(Q(n))andincrementingthenumberofvisits\n(N(n))forallthenodesalongthepath. Effectively employingMCTS totacklelong-horizonepisodic robot planningrequires\na highly non-trivial adaptation of MCTS. In this section, we first describe the necessary\npreparationforintegratingMCTSandphysicssimulationforobjectretrieval,thendescribe\naugmentations to the architecture for GPU-based processing, and finally outline our key\nideasanddesignchoicesinourparallelizationeffort. 6.3.1 SerialMCTSforObjectRetrievalfromClutter\nTouseMCTSfortheobjectretrievaltaskandsolverealinstances,weintegrateitinaprocess\nthatalternatesbetweensearchinsimulationandexecutionontherealsystem. OurMCTS\nprocesstakesinascenethatissegmentedintoobjects,andusesphysicssimulationtoreason\nabouttheproperpushactionstofacilitatethefinalretrievalofthetargetobject. Anoverview\nof the MCTS process is provided in Figure 6.1.",
      "size": 879,
      "sentences": 6
    },
    {
      "id": 196,
      "content": "sinascenethatissegmentedintoobjects,andusesphysicssimulationtoreason\nabouttheproperpushactionstofacilitatethefinalretrievalofthetargetobject. Anoverview\nof the MCTS process is provided in Figure 6.1. In other words, we first replicate in the\n=== 페이지 90 ===\n72\nsimulatortherealperceivedsceneatthebeginningofeachepisode,performcomputationand\nsimulation,andthenexecutewiththerealrobottheactionthatresultsfromthesimulationto\nguidetheresolutionoftheretrievaltaskontherealobjects. WenowdescribethedetailsofourbasicserialMCTSadaptation. Fortheselectionstep,\nthestandardUCBformulaisused. Fortheexpansionstep,foraselectednodenthathasnot\nbeenexpanded,wesamplemanypotentialpushdirectionsbyexaminingthecontourofthe\nobjects. Thesesampledpushesbecomethecandidateactionsundernforexpansion. Aftera\nsampledpushactionischosen,theactionisexecutedinthephysicssimulationandanew\nnode is added to the tree.",
      "size": 883,
      "sentences": 8
    },
    {
      "id": 197,
      "content": "ingthecontourofthe\nobjects. Thesesampledpushesbecomethecandidateactionsundernforexpansion. Aftera\nsampledpushactionischosen,theactionisexecutedinthephysicssimulationandanew\nnode is added to the tree. The MCTS simulation step is then carried out with additional\nconsecutiverandompushestoobtainarewardforthenewlyaddednode. Notethatforeach\nsimulationstep, we mustdecidewhether theresultingstate isa terminal state;this isdone\nusingagraspclassifier,tobeexplainedlater. Animportantdesigndecisionwemakehere,torenderMCTScomputationmoretractable,\nistolimitthedepthofthetree. Welimitthedepthoftheoveralltreetobenomorethansome\nd . Thesimulationcanbecarriedoutforatleastd steps. Thismeansthatthemaximum\nT s\ndepth reached by MCTS does not exceed d +d . If expansion happens at depth d , we\nT s T\nallow the state to be simulated further until d +d .",
      "size": 836,
      "sentences": 10
    },
    {
      "id": 198,
      "content": "becarriedoutforatleastd steps. Thismeansthatthemaximum\nT s\ndepth reached by MCTS does not exceed d +d . If expansion happens at depth d , we\nT s T\nallow the state to be simulated further until d +d . Given our goal of finding the least\nT s\nnumber of pushesfor retrieving the targetobject, d andd can potentiallybe dynamically\nT s\nupdatedwhenanidentifiedterminalnodehasadepthdsmallerthand . Inthiscase,weset\nT\nd = dandd = 0. WeterminateanMCTSprocessif: (1)theelapsedtimeexceedsapreset\nT s\nbudget T , (2) the tree is fully explored, or (3) the target can be grasped in an explored\nmax\nnodeandallnodesatitsparent’slevelhavebeenexplored. After each full MCTS run, we execute the best action it returns on the actual scene\n(simulated or real), and then use a grasp network (GN) from Chapter 5 to tell us whether\nthetargetobjectisretrievable. Ifitis,GNfurthertellsushowtograspit;thetaskisthen\ncompleted. DetailsofGN,forreplicationpurposes,canbefoundintheonlinesupplementary\nmaterial.",
      "size": 977,
      "sentences": 9
    },
    {
      "id": 199,
      "content": "Chapter 5 to tell us whether\nthetargetobjectisretrievable. Ifitis,GNfurthertellsushowtograspit;thetaskisthen\ncompleted. DetailsofGN,forreplicationpurposes,canbefoundintheonlinesupplementary\nmaterial. === 페이지 91 ===\n73\n6.3.2 AdaptionsforGPU\nBesidessimulation,whichcanbespedupusingGPU-basedphysicsengines,therearethree\nadditionalbottlenecksintheprocesstoparallelizeMCTSforobjectretrieval. Oneofthese\nis the parallelization of MCTS itself and the other two are specific to the object retrieval\nproblem: actionsamplingandgraspfeasibilityprediction. Weleavethefirstbottleneckfor\nsubsection6.3.3andaddressthelattertwohere. Figure6.2: Sampledpushactions. Speeding up Action Sampling. Because the number of push action choices is un-\ncountably infinite, action sampling is necessary. We modified the action sampler from\nChapter4andChapter5withslightchangesandamoreefficientimplementation. Asshown\ninFigure6.2,foragiveno ,actionsap aresampledaroundtheclutter.",
      "size": 950,
      "sentences": 11
    },
    {
      "id": 200,
      "content": "n sampling is necessary. We modified the action sampler from\nChapter4andChapter5withslightchangesandamoreefficientimplementation. Asshown\ninFigure6.2,foragiveno ,actionsap aresampledaroundtheclutter. N actionsareevenly\nt t a\nsampled around the contour of each object, from edge to center. Actions that cannot be\nexecuted due to collisions are discarded. Further speedups are obtained by pre-computing\nthe sampled actions for each object and only performing collision checking between the\nrobot’sstartposeofpushandobjectsatruntime. Grasp Evaluation. Previous learning-based methods for object retrieval use a grasp\n=== 페이지 92 ===\n74\nGrasp Grasp\nprobability: 0.57 probability: 0.99\nFigure 6.3: Examples of using the grasp classifier to produce probabilities to grasp the\nobject at center (blue in this case). Here we used an RGB image for illustration purpose\n(inputshouldbeadepthimage).",
      "size": 885,
      "sentences": 9
    },
    {
      "id": 201,
      "content": "6.3: Examples of using the grasp classifier to produce probabilities to grasp the\nobject at center (blue in this case). Here we used an RGB image for illustration purpose\n(inputshouldbeadepthimage). network (GN) to evaluate the feasibility of grasping the target object (Chapter 4 and\nChapter 5), which becomes a time-consuming bottleneck when parallelized directly. GN\nis relatively slow because it evaluates a large number of possible grasp poses. However,\nknowing thebestgrasp poseis unnecessaryif we only want toknow how ’graspable’ astate\nis. Giventhisobservation, we developasimplifiedgraspclassifier (GC)thatonlyreturns a\ngraspprobability(Figure6.3). GCisanEfficientNet-b0[140]thattakesadepthimageas\ninputandoutputsaprobabilitybetween0and1. Givenadepthimageandatargetobject,\nwecanqueryGCwhetherthetargetobjectisgraspablebycomparingitsoutputtoapreset\nthresholdR∗. DetailsaboutGC’simplementationandtraininginsimulationcanbefoundin\nc\ntheonlinesupplementarymaterial.",
      "size": 969,
      "sentences": 9
    },
    {
      "id": 202,
      "content": "rgetobject,\nwecanqueryGCwhetherthetargetobjectisgraspablebycomparingitsoutputtoapreset\nthresholdR∗. DetailsaboutGC’simplementationandtraininginsimulationcanbefoundin\nc\ntheonlinesupplementarymaterial. NotethatGNisstillusedaftereachfullMCTSrunfor\npotentiallygraspingthetargetobject,asdescribedinsubsection6.3.1. 6.3.3 ParallelMCTSwithBatchedSimulation\nGiventheavailabilityofpowerfulGPU-basedphysicssimulatorsincludingIsaacGym[135]\nandBrax[136],whichenablesthesimulationofalargenumberofsystemsindependently\n=== 페이지 93 ===\n75\nand simultaneously, a naturalroute for speedingup long-horizonepisodic robotplanning\ntasks is to introduce parallelism into the MCTS pipeline outlined in subsection 6.3.1, to\nperformmanysimultaneoussimulations. However,itischallengingtointroduceparallelism\nintoMCTSbecauseoptimalnodeselectiondependsontherewardofallpreviousrounds.",
      "size": 852,
      "sentences": 5
    },
    {
      "id": 203,
      "content": "S pipeline outlined in subsection 6.3.1, to\nperformmanysimultaneoussimulations. However,itischallengingtointroduceparallelism\nintoMCTSbecauseoptimalnodeselectiondependsontherewardofallpreviousrounds. To enable parallelism in MCTS for object retrieval from clutter and harness GPU-based\nsimulation, we introduce the following modifications to the MCTS procedure outlined in\nsubsection6.3.1. Weassumethenumberofparallelenvironmentsinthesimulatorisfixedto\nsomenumberN . Eachparallelenvironmentcontainsanidenticalvirtualrobotandobjects. e\nSelectionwithVirtualLoss. ByobservingtheoperationsofMCTS,itisnotdifficultto\nseethattheparallelismofMCTSrequiresmodifyingtheUCBformula. Otherwise,thesame\nnodeinasearchtreewillbeselectedforexpansioninmultipleparallelenvironments,leading\nto redundancy and poor performance. To address this issue, we adopt the idea of virtual\nloss [141], which has shown to give good results in multiple application domains [119],\n[142],[143].",
      "size": 958,
      "sentences": 9
    },
    {
      "id": 204,
      "content": "ts,leading\nto redundancy and poor performance. To address this issue, we adopt the idea of virtual\nloss [141], which has shown to give good results in multiple application domains [119],\n[142],[143]. VirtuallossisusedtoadjustthecalculationofUCBvaluesforthenodesthat\nhavebeenselectedbutnotyetexpanded[111],[141],\n(cid:115)\nQ(n′) 2ln(N(n)+N ˆ (n))\nargmax +c , (6.2)\nn′∈childrenofn N(n′)+N ˆ (n′) N(n′)+N ˆ (n′)\nwheretheN ˆ (n)isthe numberofselected butnotyet expandednodesunder noden. N ˆ (n)\nwill be reset to zero once the selection phase of parallel MCTS is completed. Basically,\nEquation6.2penalizesselectingnodesthathavealreadybeenselectedinsomeotherparallel\nenvironment but for which expansion and simulation have not yet been completed. With\nEq. (Equation6.2),itisstillpossibleforanodentobeselectedmultipletimes,whichmay\nleadtoredundantsimulations.",
      "size": 852,
      "sentences": 7
    },
    {
      "id": 205,
      "content": "arallel\nenvironment but for which expansion and simulation have not yet been completed. With\nEq. (Equation6.2),itisstillpossibleforanodentobeselectedmultipletimes,whichmay\nleadtoredundantsimulations. Toavoidthisandensurethatnoredundantsimulationsare\ncarried out, we markall selected actions ofnode nand sharethis information across allthe\nparallelenvironments. Acollectionofstate-actionpairsisreturnedfromtheselectionphase. Thesamestate\n=== 페이지 94 ===\n76\ncould be selectedmany times, but allstate-action pairs in theselected collection are unique. For example, in Figure 6.4, upper left, (s1 ,a1),...,(s4 ,a4) are four such state-action\nt+2 t+1\npairs. Batch-mode expansion/simulation on this collection is then performed in parallel\nusingGPU. Batch Parallel Simulation\n(reallocate idle environments)\nSelection (Virtual loss)\nBackpropagation\nBatch Parallel Expansion\n(keep the maximum reward)\nFigure6.4: StepsinPMBS,ourparallelMCTSwithbatchedoperation. Batch Expansion.",
      "size": 968,
      "sentences": 10
    },
    {
      "id": 206,
      "content": "(reallocate idle environments)\nSelection (Virtual loss)\nBackpropagation\nBatch Parallel Expansion\n(keep the maximum reward)\nFigure6.4: StepsinPMBS,ourparallelMCTSwithbatchedoperation. Batch Expansion. After a batch of state-action pairs ({(s,a)}) has been selected,\nthe expansion step is carried out for all of these pairs simultaneously. For this purpose,\n=== 페이지 95 ===\n77\nenvironmentsinthesimulator(IsaacGym)willbeloadedwiththeappropriatelyselected\nstates,afterwhichexpansion(transition)iscarriedoutinparallel. Abatchexpansioncreates\na set of new nodes added to the tree, each of which is different. In Figure 6.4, lower left,\ns1 ,...,s4 aretheresultofexpanding(s1 ,a1),...,(s4 ,a4),respectively.",
      "size": 698,
      "sentences": 6
    },
    {
      "id": 207,
      "content": "utinparallel. Abatchexpansioncreates\na set of new nodes added to the tree, each of which is different. In Figure 6.4, lower left,\ns1 ,...,s4 aretheresultofexpanding(s1 ,a1),...,(s4 ,a4),respectively. t+3 t+2 t+2 t+1\nAlgorithm4: ParallelMCTSwithBatchedSimulation\n1 FunctionMain(s ,o )\nt t\n2 whilethereisatargetobjectinworkspacedo\n3 if thetargetobjectcanbegrasped(queryGN)then\n4 Executegraspofthetargetobject\n5 elseExecuteParallel-MCTS(s ) //Push\nt\n6 FunctionParallel-MCTS(s)\n7 Createrootnoden withstates\n0\n8 es level ← 1 //Earlystoplevel\n9 graspable nodes ← ∅\n10 while(withintimebudget)and(depthsofallgraspable nodesaregreaterthanes level)\ndo\n11 [(n1,a1),...,(nNe,aNe)] ← Selection(n )\n0\n12\nResetallNˆ(n)to0\n13 [n′1,...,n′Ne] ← Expansion([(n1,a1),...,(nNe,aNe)])\n14 forn′ in[n′1,...,n′Ne]do\n15 if GC(n′(o)) > R∗ then\nc\n16 graspable nodes ← graspable nodes∪{n′}\n17 if allnodesates level−1arefullyexpandedorterminalthen\n18 es level ← es level+1\n19 [r1,...,rNe] ← Simulation([n′,...,n′ ])\n1 Ne\n20\nBackpropagation([(n′1,r1),...,(n′Ne,rNe)])\n21 returntheap thatleadstobestchildnodeofroot,rankedbyEquation6.2\nBatch Simulation.",
      "size": 1119,
      "sentences": 4
    },
    {
      "id": 208,
      "content": "es level ← es level+1\n19 [r1,...,rNe] ← Simulation([n′,...,n′ ])\n1 Ne\n20\nBackpropagation([(n′1,r1),...,(n′Ne,rNe)])\n21 returntheap thatleadstobestchildnodeofroot,rankedbyEquation6.2\nBatch Simulation. The batch simulation step of our parallel MCTS implementation\nis similar to the batch expansion step, but with additional steps inserted before and after. Beforeapushsimulation,randomactionsmustfirstbeselected,usingtheactionsampling\nmethod outlined in subsection 6.3.2. After each push simulation, GC, as described in\nsubsection 6.3.2 is applied to evaluate the outcome. As we can see, to best exploit the\nparallelismfromthesimulator,actionsamplingandGCshouldbecarriedoutasefficiently\naspossible,sothattheydonotbecomesignificantcomputationalbottlenecks. === 페이지 96 ===\n78\nDuring simulation, we also perform leaf parallelization [141] when the number of\nsimulation environments is more than the number of states for which MCTS simulations are\ntobecarriedout.",
      "size": 957,
      "sentences": 6
    },
    {
      "id": 209,
      "content": "페이지 96 ===\n78\nDuring simulation, we also perform leaf parallelization [141] when the number of\nsimulation environments is more than the number of states for which MCTS simulations are\ntobecarriedout. ThisisreflectedinFigure6.4,upperright,wherethefirsttwostateseach\nare simulated twice initially. If some environments, after a push, are predicted by GC as\ngraspable, thenfurther simulationon theseenvironments willnot becarried out,and these\nenvironments can be re-purposed. For example, in Figure 6.4, upper right, a simulation\nunder the second state terminates early, and the associated environment can be used to\nperformadditionalsimulationforthefirststate. Backpropagation. Thebackpropagationphaseisstraightforwardtoexecute,asitsimply\nbackpropagatestherewardsto therootofthetree. Wenotethat, forasingle stateforwhich\nmultiple simulations are carried out, it is natural to select the maximum reward obtained\ninsteadoftakingaverages(seeFigure6.4,lowerright).",
      "size": 959,
      "sentences": 7
    },
    {
      "id": 210,
      "content": "dsto therootofthetree. Wenotethat, forasingle stateforwhich\nmultiple simulations are carried out, it is natural to select the maximum reward obtained\ninsteadoftakingaverages(seeFigure6.4,lowerright). Thepseudo-codeofPMBSisgiveninAlgorithm4withtheselectionsubroutinegiven\ninAlgorithm5. OthersubroutinesofPMBSaremostlystraightforward. Algorithm5: SelectionwithVirtualLoss\n1 FunctionSelection(n )\n0\n2 Pairs ← ∅\n3 whilelen(Pairs) < Ne do\n4 ni ← traversetreeuntilaleafnodeusingEquation6.2\n5 ai ← onesampledactionofni\n6 Removeai fromthesampledactionsofni\n7 Pairs ← Pairs∪{(ni,ai)}\n8 Nˆ(ni) ← Nˆ(ni)+1\n9 incrementvirtualcountsofancestornodesofni\n10 returnPairs\n6.4 ExperimentalEvaluation\nWe evaluated the proposed system (PMBS) in a physics simulator (Isaac Gym) and on a\nreal robot on adversarial test cases.",
      "size": 802,
      "sentences": 5
    },
    {
      "id": 211,
      "content": "entvirtualcountsofancestornodesofni\n10 returnPairs\n6.4 ExperimentalEvaluation\nWe evaluated the proposed system (PMBS) in a physics simulator (Isaac Gym) and on a\nreal robot on adversarial test cases. In comparisons to baseline and ablation studies, we\nobserve significant improvements using the GPU-based physics simulator together with\n=== 페이지 97 ===\n79\nparallelMCTS,whichbringsepisodicdecisionmakingforrealrobotsclosertoreal-time,\ni.e.,asinglecomplexdecisionismadeinafewseconds. Allexperimentswereconductedon\nadesktopwithanNvidiaRTX2080TiGPU,anInteli7-9700KCPU,and32GBofmemory. 6.4.1 SimulationStudies\nIn this work, the simulated environment is built with Isaac Gym [135], consisting of a\nUniversal Robot UR5e with a two-finger gripper Robotiq 2F-85, and an Intel RealSense\nD455RGB-DcameraoverlookingatabletopworkspaceasshowninFigure6.1.",
      "size": 839,
      "sentences": 4
    },
    {
      "id": 212,
      "content": "ent is built with Isaac Gym [135], consisting of a\nUniversal Robot UR5e with a two-finger gripper Robotiq 2F-85, and an Intel RealSense\nD455RGB-DcameraoverlookingatabletopworkspaceasshowninFigure6.1. Therobot\nis in position-controlmode; pushand grasp actions controlthe end-effector’s position, and\nInverseKinematics(IK)[144],[145]isappliedtoconvertthesetojointspacecommands. Theeffectiveworkspaceisatasizeof0.288×0.288m,discretizedasagridof144×144cells\nwhereeachcellisonepixelintheimage(orthographicprojection)takenbythecamera. The\nworkspace,incomparisontopreviousChapter4andChapter5,issignificantlysmaller(only\nabout45%in terms of area),making thesetting much morechallenging. We intentionally\nselectedthesettingtodemonstratethepowerofPMBS. Allobjectsshouldreside intheworkspace. 20casesfromChapter4 usedforevaluation\ncanbefoundinFigure6.5,wheretheredlinesdenotetheboundarytowhichobjectscenters\nmustbeconfinedatalltimes.",
      "size": 922,
      "sentences": 7
    },
    {
      "id": 213,
      "content": "ratethepowerofPMBS. Allobjectsshouldreside intheworkspace. 20casesfromChapter4 usedforevaluation\ncanbefoundinFigure6.5,wheretheredlinesdenotetheboundarytowhichobjectscenters\nmustbeconfinedatalltimes. The push distance of a push action is fixed at 5cm (10cm in previous Chapter 4 and\nChapter5,theeffectivepushdistanceisaround3cm(thedistanceobjectsaremoved). Metrics. Fourmetricsareusedtoevaluateoursystems:\n1. thenumberofactionsusedtoretrievethetargetobject\n2. thetotalplanningtimeusedforretrievingthetargetobject(buildthetree)\n3. thecompletionrateinretrievingthetargetobjectwithin16actions\n4. thegraspsuccess rate,whichisthe numberofsuccessfulgraspsdivided bythetotal\nnumberofgraspingattempts\n=== 페이지 98 ===\n80\nFigure6.5: 20casesfromChapter4usedinsimulationexperiments,wherethetargetobject\nhasabluemask. Noobjectshouldexceedtheboundary(redlines). Baseline. WeuseanoptimizedserialMCTSimplementationasthebaseline,wherethe\nnumberof environmentsused for MCTSisone.",
      "size": 960,
      "sentences": 9
    },
    {
      "id": 214,
      "content": "riments,wherethetargetobject\nhasabluemask. Noobjectshouldexceedtheboundary(redlines). Baseline. WeuseanoptimizedserialMCTSimplementationasthebaseline,wherethe\nnumberof environmentsused for MCTSisone. The followinghyperparameters areused\nacrossallmethodsinbenchmarkunlessotherwisementioned. Thediscountfactorγ = 0.8. The default maximum tree depth is d = 7, and the default simulation (rollout) depth is\nT\nd = 3. The threshold of GC is R∗ = 0.9. The UCB exploration term c in Equation 6.1\ns c\nand Equation 6.2 is 0.3. The time limit (budget) T for one step planning is 60 seconds. max\n1000robots(environments)inIsaacGymareusedinourPMBS;ittakesaround2.2seconds\nforallrobotstocompleteonepushaction. WeevaluatetheperformanceofPMBSandthebaselineserialMCTSoverall20cases,\nrunning each case five times. For the evaluation, we set a time budget T = 60s and\nmax\ndenotedthetwomethodsasPMBS-60andMCTS-60,respectively.",
      "size": 906,
      "sentences": 13
    },
    {
      "id": 215,
      "content": "heperformanceofPMBSandthebaselineserialMCTSoverall20cases,\nrunning each case five times. For the evaluation, we set a time budget T = 60s and\nmax\ndenotedthetwomethodsasPMBS-60andMCTS-60,respectively. Thesummarybenchmark\nforthesetwomethodscanbefoundinthefirsttworowsofTable6.1;individualresultsfor\neachcasecanbefoundinFigure6.6andFigure6.7. Wemakesomeobservationsbasedontheresults. First,PMBSoutperformstheserial\nMCTSversionintermsofnumberofactionsandcomputationtimeacrossallcases,which\nis as expected because PMBS engages many environments to facilitate its search effort. === 페이지 99 ===\n81\nOntheotherhand,whenweviewthesolutionqualityandcomputationtimetogether,the\nadvantageofPMBSoverserialMCTSissignificant: PMBS-60uses35secondsonaverage\nfor planning, whereas MCTS-60 uses over 300 seconds. This along translates to an 8.6×\nspeedup. Atthesametime,PMBS-60uses70%feweractionsinsolvingthetasks. Afurther\ndatapointregardingthespeedupatthesamesolutionisgivenabitlaterinFigure6.8.",
      "size": 975,
      "sentences": 9
    },
    {
      "id": 216,
      "content": "er 300 seconds. This along translates to an 8.6×\nspeedup. Atthesametime,PMBS-60uses70%feweractionsinsolvingthetasks. Afurther\ndatapointregardingthespeedupatthesamesolutionisgivenabitlaterinFigure6.8. A second observation is that, despite the fact that we are dealing with a difficult long-\nhorizonplanning problem, PMBS isable to achieve planning thatis close tobeing able to\nperform reasoning in real-time, as it takes an average of 35/3.91 < 9 seconds to make a\nsingledecision. Withfurtheroptimizationand/orbetterhardware,webelievethatPMBS\nwillachievereal-timedecision-makingcapabilityforthecurrentsetofobjectretrievaltasks. 10\n8\n6\n4\n2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nIndex of cases\nsnoitca\nfo\nrebmuN\n16 PMBS-60\n13.4 10\nMCTS-60\n8.8\nFigure6.6: Theaveragenumber(overfiveindependenttrials)ofactionspercaseneededfor\nsolvingthetwentycases,givenatimebudgetof60seconds.",
      "size": 883,
      "sentences": 7
    },
    {
      "id": 217,
      "content": "9 20\nIndex of cases\nsnoitca\nfo\nrebmuN\n16 PMBS-60\n13.4 10\nMCTS-60\n8.8\nFigure6.6: Theaveragenumber(overfiveindependenttrials)ofactionspercaseneededfor\nsolvingthetwentycases,givenatimebudgetof60seconds. 600\n400\n200\n0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nIndex of cases\n)s(\nemiT\n1013 PMBS-60\n793\nMCTS-60\n417\nFigure6.7: Theaveragetime(overfiveindependenttrials)percaseneededforsolvingthe\ntwentycases,givenatimebudgetof60seconds. [표 데이터 감지됨]\n\n=== 페이지 100 ===\n82\nNum. ofActions Time Completion GraspSuccess\nPMBS-60 3.91 35s 100% 98.3%\nMCTS-60 6.67 301s 93.0% 96.4%\nPMBS-60(c = 0) 4.03 113s 100.0% 99.2%\nPMBS-60(c = ∞) 12.71 147s 42.0% 96.7%\nTable 6.1: Simulation experiment results for 20 cases. Time budgets are limited up to 60\nseconds. Ablation Study. The time budget T is one of the main factors that influence the\nmax\nsolutionquality andplanning time. Tounderstandits role, several timebudgetsare usedto\nevaluateourmethod,asshowninFigure6.8.",
      "size": 953,
      "sentences": 8
    },
    {
      "id": 218,
      "content": "Study. The time budget T is one of the main factors that influence the\nmax\nsolutionquality andplanning time. Tounderstandits role, several timebudgetsare usedto\nevaluateourmethod,asshowninFigure6.8. Givenmoretimefortreesearch,serialMCTS\nandPMBScouldimprovethesolutionquality,leadingtofewerrequiredactions. Thetrends\nof serialMCTS (number of action) are steep, as itis highly possible that it couldnot find\na solution given a limited time. The trend of PMBS (planning time) is more gradual, as\nthemosttime-consumingsearchhappensinthefirstfewiterations,whichusuallyusesall\nthe time budget. While serial MCTS never achieves the same solution quality as PMBS,\ncomparing the first PMBS data point and the last serial MCTS data point, we observe a\n855/28 = 30×speedupwithPMBSstillhassomequalityadvantage. On the flip side, we note that the speed-up of 30× seems small considering that\nwe used 1000 environments. This is due to two factors.",
      "size": 933,
      "sentences": 9
    },
    {
      "id": 219,
      "content": "serve a\n855/28 = 30×speedupwithPMBSstillhassomequalityadvantage. On the flip side, we note that the speed-up of 30× seems small considering that\nwe used 1000 environments. This is due to two factors. First, MCTS is itself a serial\nprocess; parallelization will incur performance loss. Second, while we have improved\nmanybottlenecks, e.g.,onactionsamplingandgraspclassification,theobjectretrievaltask\ncontainsmanyelementsthatcannotbereadilyparallelized. Wealsoevaluatedtheimpactoftheexplorationandexploittrade-offonPMBS.Ifthecin\nEquation6.2issetto0,i.e.,pureexploitation,PMBS-60uses4.03actionsand113seconds\n(planning time) on average on 20 cases. The performance is worse than when c = 0.3,\nas shown in Table 6.1. This is expected as the greedy approach could be stuck in a local\noptimum. PMBS-60 is also tested by setting c to be a large number in Equation 6.2, i.e.,\npureexploration,whichuses12.71actionsand147seconds(planningtime)onaverages;the\ncompletionratehasasteepdropto42.0%.",
      "size": 982,
      "sentences": 9
    },
    {
      "id": 220,
      "content": "mum. PMBS-60 is also tested by setting c to be a large number in Equation 6.2, i.e.,\npureexploration,whichuses12.71actionsand147seconds(planningtime)onaverages;the\ncompletionratehasasteepdropto42.0%. [표 데이터 감지됨]\n\n=== 페이지 101 ===\n83\n8\n7\n6\n5\n4\n3\n15 30 60 120 240 480\nTime Budget (s)\nsnoitca\nfo\nrebmuN\n800\n7.63\n6.67 600\n5.67 400\n5.39\n4.67\n200\n4.24\n4.05 3.91 3.88\n0\n15 30 60 120 240 480\nTime Budget (s)\n)s(\nemiT\nPMBS\n855\nMCTS 675\n418\n301\n195\n28 32 35 46\nFigure 6.8: PMBSand serial MCTSevaluatedwithdifferent timebudgets. Thereported\nvaluesareaveragesoverall20cases. 6.4.2 RealRobotExperiments\nFor experiments on the physical UR-5e, the input to PMBS is a single RGB-D image. A\n1280×720RGB-Dimage istaken, thenit isorthogonally projectedoverthe workspaceof\nresolution of144×144(with cropping). Since the same robot andobjects are used in both\nsimulationandtherealworld,wecanobserveandactonarealrobotbutplaninasimulator.",
      "size": 914,
      "sentences": 7
    },
    {
      "id": 221,
      "content": "gonally projectedoverthe workspaceof\nresolution of144×144(with cropping). Since the same robot andobjects are used in both\nsimulationandtherealworld,wecanobserveandactonarealrobotbutplaninasimulator. Foreachobject,simpleposeestimationisperformedtotransfertheperceivedscenefrom\nrealimagestothephysicssimulatorenvironments. Theposeestimationisdonebyfirstlyextractingmasksforobjectsfromtheimage,then\na brute-force matching between detected mask and recorded mask is performed for each\nobject. Wecouldachieveitat0.15secondsforoneimage(around10objects). SerialMCTS\nandPMBSwereevaluatedthesamewayasinsimulationexperiments,exceptweonlyrun\ntestsonthesixmostchallengingcases. Individualbenchmarksonsixcasescanbefound\nin Figure 6.9. Average statistics are listed in Table 6.2. We observe minimal sim-to-real\nperformance loss; A small gap exists between the real and the simulation experiments,\nmainlyduetoposeestimationerrorsandmismatchofphysicsproperties. AdditionalExperimentalDetails.",
      "size": 977,
      "sentences": 10
    },
    {
      "id": 222,
      "content": "minimal sim-to-real\nperformance loss; A small gap exists between the real and the simulation experiments,\nmainlyduetoposeestimationerrorsandmismatchofphysicsproperties. AdditionalExperimentalDetails. Curiousreadersmayfindintheonlinesupplementary\nmaterial additional experimental details including complete, actual execution snapshots\nof PMBS and MCTS for all 20 cases, as well as the execution snapshots for real robot\n[표 데이터 감지됨]\n\n=== 페이지 102 ===\n84\n16\n14\n12\n10\n8\n6\n4\n2 4 12 13 18 20\nIndex of cases\nsnoitca\nfo\nrebmuN\n600\n500\n400\n300\n200\n100\n0\n2 4 12 13 18 20\nIndex of cases\n)s(\nemiT\n16 PMBS-60 1027\nMCTS-60\nFigure6.9: Thenumberofactionsandtimeusedforsolvingthesixmostchallengingcases\nonthephysicalrobot. Thetimebudgetis60seconds. Num. ofActions Time Completion GraspSuccess\nPMBS-60 5.72 73s 100% 100%\nMCTS-60 10.45 529s 83.3% 87.0%\nPMBS-60(sim) 5.03 81s 100% 97.2%\nMCTS-60(sim) 10.77 587s 76.7% 96.6%\nTable 6.2: Realrobotexperimentresultson thesixmost difficultcases.",
      "size": 968,
      "sentences": 6
    },
    {
      "id": 223,
      "content": "ccess\nPMBS-60 5.72 73s 100% 100%\nMCTS-60 10.45 529s 83.3% 87.0%\nPMBS-60(sim) 5.03 81s 100% 97.2%\nMCTS-60(sim) 10.77 587s 76.7% 96.6%\nTable 6.2: Realrobotexperimentresultson thesixmost difficultcases. Time budgetsare\nlimitedto60secondspercase. experiments. 6.5 Summary\nIn this chapter, we proposed PMBS, a novel parallel Monte Carlo tree search technique\nwith GPU-enabled batched simulations for accelerating long-horizon, episodic robotic\nplanningtasks. Throughaseriesofcarefuldesignchoicestoovercomemajorparallelization\nbottlenecks,PMBSachievesanover30×speedupcomparedtoanoptimizedserialMCTS\nimplementation while also delivering better solution quality, using identical computing\nhardware. Real robot experiments show that PMBS directly transfers from simulation to\nthe realphysical world toachievenear real-timeplanning performancein solving complex\nlong-horizonepisodicrobotplanningtasks.",
      "size": 891,
      "sentences": 6
    },
    {
      "id": 224,
      "content": "eal robot experiments show that PMBS directly transfers from simulation to\nthe realphysical world toachievenear real-timeplanning performancein solving complex\nlong-horizonepisodicrobotplanningtasks. [표 데이터 감지됨]\n\n=== 페이지 103 ===\n85\nCHAPTER7\nTOWARDOPTIMALTABLETOPREARRANGEMENTWITHMULTIPLE\nMANIPULATIONPRIMITIVES\n7.1 Introduction\nReal-worldmanipulationtasks,e.g.,rearrangingamessytabletoporfurnitureinthehouse,\noftenrequiremultiplemanipulationprimitives(e.g.,pick-n-place,pushing,toppling,etc.) to\naccomplish. Whenrearrangingsmall/lightobjects,e.g.,acellphoneonatableorasmall\nchairinaroom,itisconvenienttodo apick-n-place,i.e.,topickup theobject,liftitabove\notherobjects,moveitacrossthespacetoaboveitsdestinationon thetable,andthenplaceit. On the other hand, for handling large/heavy objects, e.g., a thick book or a heavy couch,\npushing or draggingclose tothe space’ssurface is more commonly adopted, executed with\naddedcaution.",
      "size": 927,
      "sentences": 5
    },
    {
      "id": 225,
      "content": "nplaceit. On the other hand, for handling large/heavy objects, e.g., a thick book or a heavy couch,\npushing or draggingclose tothe space’ssurface is more commonly adopted, executed with\naddedcaution. Inthiscase,planningtheobject’smotiontrajectorymustconsideravoiding\ncolliding with other objects more carefully. Solving such long-horizon task-and-motion\nplanning tasks efficiently and optimally is highly challenging, as it involves not only an\nextendedhorizonbutalsoselectingamongmultipletypesofmanipulationprimitivesateach\nstep,bothofwhichaddtothecombinatorialexplosionofthesearchspace. Towardquicklyandoptimallysolvingrearrangementtasksusingmultiplemanipulation\nprimitives,wefocusonatabletopsettingwherebothpick-n-placeandpushingareemployed\ntorearrangeobjects(seeFigure 7.1). Manyobjects,suchasthoseshown inFigure7.1(e),\ncannotbeeasilypickedupandmovedaroundwithoutdamagingordisassemblingtheobject.",
      "size": 900,
      "sentences": 6
    },
    {
      "id": 226,
      "content": "wherebothpick-n-placeandpushingareemployed\ntorearrangeobjects(seeFigure 7.1). Manyobjects,suchasthoseshown inFigure7.1(e),\ncannotbeeasilypickedupandmovedaroundwithoutdamagingordisassemblingtheobject. Forexample,asshowninFigure7.1(f)(g),booksandcertainboxescannotbemovedaround\nusingsuction-basedpick-n-placemanipulationprimitive(notethatitisalsodifficulttodo\npick-n-placeusing fingeredgrippers). However, theseobjectscan beeffectively rearranged\nusing a pushing manipulation primitive in which the suction-based end-effector holds the\n=== 페이지 104 ===\n86\n(b)Startstate\n(c)Push\n(a)Hardwaresetup (d)Goalstate\n(e)Objectsrequirepush (f)Book (g)Box\nFigure 7.1: (a) Overview of system setup, a camera is mounted on the end-effector for\nperception. (b)-(d) An example case and an intermediate step in solving it. (e) Example\nobjectsrequiringapush. (f)pick-n-placemaybreakthebook. (g)pick-n-placewillseparate\nabox,failingtopickitup. objectonorclosetothetabletopandpushes/dragstheobjectaround(seeFigure7.1(c)).",
      "size": 999,
      "sentences": 9
    },
    {
      "id": 227,
      "content": "it. (e) Example\nobjectsrequiringapush. (f)pick-n-placemaybreakthebook. (g)pick-n-placewillseparate\nabox,failingtopickitup. objectonorclosetothetabletopandpushes/dragstheobjectaround(seeFigure7.1(c)). We\ncallthefrequentlyencounteredyetlargelyunaddressedproblemrearrangementwithmultiple\nmanipulationprimitives(REMP).ThisstudyonREMPbringsthefollowingcontributions:\n=== 페이지 105 ===\n87\n• Withtheformulation of REMP,we proposeafirstformalstudy ofsolvinglong-horizon\nrearrangementtasksutilizingmultipledistinctiveprecisionmanipulationprimitiveswith\nthe goal of computing an optimized manipulation sequence. Due to its high practical\nrelevance,REMPconstitutesanimportantspecializedtaskandmotionplanningproblem. • We developed two novel algorithms for REMP, the first of which is a fast rule-based\nsolution capable of effectively and quickly solving non-trivial REMP instances.",
      "size": 868,
      "sentences": 8
    },
    {
      "id": 228,
      "content": "izedtaskandmotionplanningproblem. • We developed two novel algorithms for REMP, the first of which is a fast rule-based\nsolution capable of effectively and quickly solving non-trivial REMP instances. The\nsecond, leveragingMonte Carlo treesearch(MCTS) [122]and parallelismto lookfurther\nintotheplanninghorizon,deliversamuchhighersuccessrateformorechallengingtasks,\nprovidinghigher-qualitysolutionssimultaneously. • Wethoroughlyevaluateourmethodsinsimulationandextensiverealrobotexperiments. In particular, our real robot experiments with integrated vision solutions, demonstrate\nthatouralgorithmscanbereadilyappliedtointeractwitheverydayhouseholdobjectsin\nreal-worldscenarios. 7.2 ProblemFormulation\n7.2.1 RearrangementwithMultipleManipulationPrimitives\nWe nowspecify theconcreterearrangementwithmultiple manipulationprimitives(REMP)\nstudiedinthis work. Lettheworkspacebe W a2Drectangle.",
      "size": 886,
      "sentences": 7
    },
    {
      "id": 229,
      "content": "Formulation\n7.2.1 RearrangementwithMultipleManipulationPrimitives\nWe nowspecify theconcreterearrangementwithmultiple manipulationprimitives(REMP)\nstudiedinthis work. Lettheworkspacebe W a2Drectangle. Therobotis provided witha\nstart image (state) s and a goal image (state) s containing the initial and desired object\ns g\narrangements. Therobotmustrearrangetheobjectstomatchtheconfigurationsspecifiedin\ns . Twomanipulationprimitivesarepermitted: pick-n-placeA andpush(fromtop)A . g pp pt\nThe robot’s objective is tocomplete the task efficiently in terms of theexecution time. The\nstartandgoalstatesandtheobjects’transportationshouldbecollision-free,andallobjects\nshouldremain withintheworkspace. It isassumedthatall tasksarefeasible,i.e., thereis\nalwaysaviablesolutionP = {a ,a ,...,a }leadingfromthestartstates tothegoalstate\n1 2 n s\ns ,wherea ∈ {A ,A }. g pp pt\n=== 페이지 106 ===\n88\nAstate s representstheposeofobjectsattimet.",
      "size": 925,
      "sentences": 9
    },
    {
      "id": 230,
      "content": "sible,i.e., thereis\nalwaysaviablesolutionP = {a ,a ,...,a }leadingfromthestartstates tothegoalstate\n1 2 n s\ns ,wherea ∈ {A ,A }. g pp pt\n=== 페이지 106 ===\n88\nAstate s representstheposeofobjectsattimet. Apick-n-placeactionisspecifiedbya\nt\npickpose(x ,y ,θ )andaplacepose(x ,y ,θ ). Apushactionisspecifiedbytrajectories\n0 0 0 1 1 1\n{(x ,y ,θ ),...,(x ,y ,θ )}, where the robot holds the object against the tabletop at the\n0 0 0 n n n\ninitialpose,andthenpushesitfollowingthewaypoints,endingatthefinalpose. One assumption is that the object should be capable of being stably positioned on the\nworkspace,asallprimitivesareconsideredtobequasi-static. 7.2.2 MonteCarloTreeSearch\nThe Monte Carlo tree search (MCTS) algorithm has broad applications. It is prevalent in\nturn-basedtaskssuchasthegameofGo[146],butitsusageextendsbeyondsuchcontexts. MCTSplaysacrucialroleinsolvingrearrangementtasks[21],[147],[148].",
      "size": 899,
      "sentences": 8
    },
    {
      "id": 231,
      "content": "CTS) algorithm has broad applications. It is prevalent in\nturn-basedtaskssuchasthegameofGo[146],butitsusageextendsbeyondsuchcontexts. MCTSplaysacrucialroleinsolvingrearrangementtasks[21],[147],[148]. Asananytime\ntreesearchalgorithm,MCTSisdesignedtorunforafixedamountoftime,eachconsistingof\nfourstages: selection,expansion,simulation,andbackpropagation. Fundamentally,MCTS\npreferentially exploits nodes that yield superior outcomes. To strike a balance between\nexplorationandexploitationintheselectionphase,anupperconfidencebound (UCB)[138]\nformula is utilized (see Equation 6.2), where n is the parent node of n′, and Q(n′) is the\ntotalrewardn′ receivedafterN(n′)visits. 7.3 Methodology\nThis work addresses the primary challenge of synergistic integration of pick-n-place and\npushactions. Becauseplanningapushinvolvescollision-freepathplanninginSE(2),which\nistime-consuming,itposesasignificantchallengeifmanypushactionsareexplored.",
      "size": 931,
      "sentences": 8
    },
    {
      "id": 232,
      "content": "synergistic integration of pick-n-place and\npushactions. Becauseplanningapushinvolvescollision-freepathplanninginSE(2),which\nistime-consuming,itposesasignificantchallengeifmanypushactionsareexplored. MCTS\noffersasolutioncapableofelegantlynegotiatingbetweenthetwodisparateactionswhile\nmaintainingoptimality,givenampleplanningtime. === 페이지 107 ===\n89\n7.3.1 ActionSpaceDesign\nPlanning requires searching through candidate manipulation actions, which must first be\nsampled. Action space design refers to action sampling, which plays a critical role in\ndictatingtheexpansionofthetreesearchbecausethereareanuncountablyinfinitenumber\nofpossiblemanipulationactions. Insamplingpick-n-placeactions,wemustensuretheplace\nposeiscollision-free. Thesameappliestoapushaction’sfinalpose(though,inaddition,\ntheentiretrajectoryconnectingallwaypointsforapushactionmustbecollision-free). Four\ncriteriaareappliedtosampletheplace/finalposesforpick-n-place/pushactionsatthecurrent\nstates :\nt\n1. Random.",
      "size": 978,
      "sentences": 9
    },
    {
      "id": 233,
      "content": "addition,\ntheentiretrajectoryconnectingallwaypointsforapushactionmustbecollision-free). Four\ncriteriaareappliedtosampletheplace/finalposesforpick-n-place/pushactionsatthecurrent\nstates :\nt\n1. Random. A naive approach randomly samples collision-free place/final poses for\npick-n-place/pushactions. 2. AroundCurrent andGoal. Random samplingisnot alwaysefficient. Forobject o ,\ni\nfavoringregionsaroundthecurrentandgoalposesofo ins canbehelpful. i g\n3. Grid. Additionally,weadoptagrid-basedsamplingstrategy. Bymodelinganobject\nasa2Dpolygon,weencapsulateitwithinarotatedboundingbox. Thisallowsusto\ngenerate a tiled representation in the workspace, denoted as W, resembling a grid\nstructure. Thismethodprovesadvantageousforcoveringboundaryareas,whichcan\nbehardtosamplethroughrandommethods. 4. Direct to Goal. If o can be directly placed at its goal, this pose will be prioritized\ni\novertheabovethreesamplingsinthesearchprocess. Once the place/finalposes have beensampled, a A sample is obtained.",
      "size": 989,
      "sentences": 18
    },
    {
      "id": 234,
      "content": "ect to Goal. If o can be directly placed at its goal, this pose will be prioritized\ni\novertheabovethreesamplingsinthesearchprocess. Once the place/finalposes have beensampled, a A sample is obtained. However,A\npp pt\nrequiresanadditionalstep-generatingatrajectoryfromthecurrentposetotheplacepose\nforo . WeemployRRT-connect[149]duringthetreesearchandLazyRRT[150]forrobot\ni\nexecutiontoproducesuchcollision-freetrajectorieswithinasettimelimit. Anexampleof\nsamplingthefinalposeforapushactionisshowninFigure7.2. Thecriteriaforsamplingare\n=== 페이지 108 ===\n90\n0\n1\n3\n2\nFigure7.2: Consideractionsamplingforlabeled3tobemanipulatedusingpush(therearea\ntotal of four objects). The absence of sampled actions in the right region is attributed to\nobstructionsposedbyobjects0,1,and2,preventingthemovementofobject3tothatarea. crucialinsolvingtheproblem. Relyingsolelyonstandarduniformsamplingoftenproves\ninefficient, particularly when sampling around boundaries and certain edge cases.",
      "size": 966,
      "sentences": 10
    },
    {
      "id": 235,
      "content": "eventingthemovementofobject3tothatarea. crucialinsolvingtheproblem. Relyingsolelyonstandarduniformsamplingoftenproves\ninefficient, particularly when sampling around boundaries and certain edge cases. While\none mightconsider increasingthe sample sizeto cover these edgecases, this inadvertently\nleadstomanyredundantactionsthataretime-consumingtoprocess. 7.3.2 HierarchicalBest-FirstSearch\nThefirstalgorithmwedesignedfor REMPisa(greedy)best-firstsearchalgorithmcalled\nhierarchical best-first search (HBFS). HBFS is outlined in Algorithm 6 and operates\naccordingtothefollowingsequenceofsteps:\n• (Lines3-4)Whenobjectscanbedirectlymovedtotheirgoalposes,anactioncostis\ncomputedforeach. Theactionyieldingthesmallestcostisthenapplied. • (Lines 5-9) For each object o , HBFS identifies which objects occupy o ’s goal and\ni i\nattemptstodisplacetheseobstructingobjectsinthedirectionoftheirrespectivegoals. If no actions are feasible in this direction, a random action is sampled.",
      "size": 968,
      "sentences": 9
    },
    {
      "id": 236,
      "content": "ntifies which objects occupy o ’s goal and\ni i\nattemptstodisplacetheseobstructingobjectsinthedirectionoftheirrespectivegoals. If no actions are feasible in this direction, a random action is sampled. Again, the\nactionassociatedwiththesmallestcostisselectedandimplemented. === 페이지 109 ===\n91\n• (Line10)Iftheabovestepsdonotyieldaviableaction,anactionisrandomlyselected\nforexecution. The above three phases of HBFS may best be viewed as a three-level hierarchical search. To boost its performance and solution optimality, HBFS is implemented by leveraging\nmulti-core capabilities of modern CPUs. This is realized by executing multiple HBFS in\nparallelandchoosingthebestactionamongthereturnedsolutions.",
      "size": 698,
      "sentences": 7
    },
    {
      "id": 237,
      "content": "olution optimality, HBFS is implemented by leveraging\nmulti-core capabilities of modern CPUs. This is realized by executing multiple HBFS in\nparallelandchoosingthebestactionamongthereturnedsolutions. Algorithm6:HierarchicalBest-FirstSearch(HBFS)\n1 FunctionHBFS(s ,s )\ns g\n2 s ← s ,A ← ∅\ns\n3 foro insdo A ← A∪{moveo toitsgoal}\ni i\n4 if |A| > 0then returnthelowestcostactionfromA\n5 foro insdo\ni\n6 obs ← objectsoccupythegoalposeofo\ni\n7 foro inobsdo\nj\n8 A ← A∪{moveo towardsitsgoal,otherwiseatrandom}\nj\n9 if |A| > 0then returnthelowestcostactionfromA\n10 returnarandomlysampledaction\n7.3.3 SpeedingupMCTSwithParallelism\nStandardMCTSismorestraightforwardtoimplement,butitdoesnotfullyutilizemulti-core\nprocessingcapabilitiesofmodernhardware. Weintroduceparallelismtotheexpansionand\nsimulationstagesofMCTS,leveragingtreeparallelizationtechniques[141]. Theapplication\nof parallelism allows for decoupling the select and expand stages from the simulation stage\ninanMCTSiteration.",
      "size": 969,
      "sentences": 5
    },
    {
      "id": 238,
      "content": "nand\nsimulationstagesofMCTS,leveragingtreeparallelizationtechniques[141]. Theapplication\nof parallelism allows for decoupling the select and expand stages from the simulation stage\ninanMCTSiteration. ThedecouplingallowsmultipleMCTSiterationstobecarriedout\nsimultaneously, limited only by the number of CPU cores. The standard UCB formula\nused in MCTS is updated as Equation 6.2, where the idea of virtual loss [141] is applied\nby adding one extra virtual visit counts N ˆ that indicates a node has been selected but not\nyetsimulatedandbackpropagated. Sincethesimulationandsubsequentbackpropagation\nstagesarenotyetcompleted,thetreesearchalgorithmmustbenotifiedtoupdatetheQand\nN ofthenode. Thisadjustmentminimizesthelikelihoodofrevisitingthenodeinthenext\n=== 페이지 110 ===\n92\niteration,implementingaconservativeapproachinanticipationofapotentiallypoorreward. Once the simulation stage has concluded, Q is updated in backpropagation with returned\nrewardfromsimulationresult,N incrementsandN ˆ decrements.",
      "size": 999,
      "sentences": 7
    },
    {
      "id": 239,
      "content": "rvativeapproachinanticipationofapotentiallypoorreward. Once the simulation stage has concluded, Q is updated in backpropagation with returned\nrewardfromsimulationresult,N incrementsandN ˆ decrements. 7.3.4 AdaptingMCTSforREMP\nGiven REMP’s extremely large search space due to push actions’ trajectory planning\nrequirements,modificationsareintroducedtobestapplyMCTStoREMP. ActionSpaceBias. Theactionspaceusedinthesimulationstageisasubsetofthatused\nintheexpansionstage. Giventhatoneiterationofthesimulationstageconstitutesacoarse\nestimationofactionandstate,reducingthenumberofactionsleadstoalargernumberof\ntotaliterations. BiasedSimulation. AstraightforwardimplementationofthesimulationstageinMCTS\nisarandompolicythatindiscriminatelyselectsanactionforexecution,ultimatelyobtaining\narewardattheterminalstatereached. Inourimplementation,weadoptaheuristictoguide\nthe action selection in the simulation stage towards the ultimate goal of a given object.",
      "size": 946,
      "sentences": 9
    },
    {
      "id": 240,
      "content": "rexecution,ultimatelyobtaining\narewardattheterminalstatereached. Inourimplementation,weadoptaheuristictoguide\nthe action selection in the simulation stage towards the ultimate goal of a given object. Specifically,withaprobabilityofθ (dynamicallychangedbasedondepthofthesearch),\nsim\narandomactionisselected;otherwise,anattemptismadetoselectanactionthatwillmove\nanobjecttowardsitsgoalpose. Thisintroducesa biasinthesimulationstage,whichoffsets\nthe drawback of limited iterations due to the time-consuming nature of motion planning\nandcollision checking. Wenote thatwedidnot adoptrecent advancementsin MCTSfor\nlong-horizonplanningthatinjectsadata-drivenelementtopartiallylearnthereward,e.g.,a\nneuralnetworkcanbetrainedtoevaluatethequalityofanaction-statepair[19],[79],[151]. RewardShaping. Westructuretherewardtofavorthegoalstatebutwithoutintroducing\nundue bias. The reward function plays a critical role as it steers the tree search and is\ncomposedofthreecomponents.",
      "size": 964,
      "sentences": 8
    },
    {
      "id": 241,
      "content": "[151]. RewardShaping. Westructuretherewardtofavorthegoalstatebutwithoutintroducing\nundue bias. The reward function plays a critical role as it steers the tree search and is\ncomposedofthreecomponents. Firstly, ifthetaskisaccomplished, withallobjectsplaced\nat their goal poses, a reward of R is awarded. Secondly, if an object o is located at\ng i\nits goal pose, a reward r is given. The cumulative reward from all objects, denoted as\no\n=== 페이지 111 ===\n93\nR , is computed as R = (cid:80) r . Lastly, the reward structure also takes into account the\no o i oi\ncost associated with the movement of objects. For the pick-n-place action (A ), the cost\npp\ncorrespondstotheEuclideandistancebetweenthepick-and-placeposes,withanadditional\nfixed cost factored in. For the push action (A ), the cost is determined by the Euclidean\npt\ndistance of the path, also supplemented by a fixed cost.",
      "size": 876,
      "sentences": 10
    },
    {
      "id": 242,
      "content": "ncebetweenthepick-and-placeposes,withanadditional\nfixed cost factored in. For the push action (A ), the cost is determined by the Euclidean\npt\ndistance of the path, also supplemented by a fixed cost. Additionally, a base reward is\ncomputed R = R (s ) from initial state s , which is used to normalize the final reward\nb o 0 0\nduring the search. For each iteration, a reward is returned by the simulation stage and is\nupdatedduringthebackpropagationstage. \n  max(0,R\ng\n−cost−R\nb\n), ifs\ni\nisthegoalstate\nR =\ni\n  max(0,R\no\n(s\ni\n)−cost−R\nb\n), otherwise\nTraditionally, Q retains the average reward values derived from simulation results,\nproviding a robust estimation for action over millions of iterations. However, in our case,\nweaimtomaintaintheplanningtimewithinreasonablelimits. Therefore,weintroduceda\npriorityqueuetostoresimulationresults,whichservesastheQvalueinthealgorithm. For\nthe purpose of calculation in Equation 6.2, we only preserve the top k rewards, similarly\nin the Chapter 5.",
      "size": 997,
      "sentences": 8
    },
    {
      "id": 243,
      "content": "introduceda\npriorityqueuetostoresimulationresults,whichservesastheQvalueinthealgorithm. For\nthe purpose of calculation in Equation 6.2, we only preserve the top k rewards, similarly\nin the Chapter 5. There is a possibility that during a simulation, a subsequent action may\ntransitionthestatetoonewithlowerrewards,therebynegatingthebenefitsofa preceding\nbeneficialactionwithinthatsimulation. Tolimitthe searchtime,wechoose toreturnthe\nmaximumreward encounteredattheintermediatestepsduringthesimulation instead ofthe\nfinalreward. Therefore,thereturnedrewardfromasimulationisgivenby\nmax(β · max(R ),R )·γm,\ni m\ni∈m−1\nwhereβ ∈ (0,1]isascaling parameterandmisthetotal stepsusedinthesimulation. γ is\nthediscountfactor,encouragingtheproblemtobesolvedintheearlystageifpossible. Duetolimitedspace,weomitthepseudo-codeoftheparallelMCTSalgorithmbutnote\n=== 페이지 112 ===\n94\nthatalldetailsforreproducingthealgorithmhavebeenfullyspecified.",
      "size": 924,
      "sentences": 7
    },
    {
      "id": 244,
      "content": "eproblemtobesolvedintheearlystageifpossible. Duetolimitedspace,weomitthepseudo-codeoftheparallelMCTSalgorithmbutnote\n=== 페이지 112 ===\n94\nthatalldetailsforreproducingthealgorithmhavebeenfullyspecified. Wecalltheresulting\nalgorithmparallelMonteCarlotreesearchformulti-primitiverearrangement or PMMR. 7.4 ExperimentalEvaluation\nWeevaluatedHBFSandPMMRmethodsforREMPinsimulatedenvironmentsandonareal\nrobot. Regardlessofwhetheritisasimulationorarealrobotexperiment,bothalgorithms\nperformpercept-plan-act loopsuntilthetaskissolvedorthebudgetedtimeormaximum\nnumber of actions is exhausted. All experiments were conducted on an Intel i9-10900K\n(10 CPUcores) desktopPC andimplementedin Python. Asa note,limited testingshows\nthat using Intel i9-13900K (24 CPU cores) reduces the planning time by roughly half,\ndemonstratingtheeffectivenessandscalabilityofemployingparallelism(codeinPython). S-4.1 S-r.7.3 S-r.8.3\nG-4.1 G-r.7.3 G-r.8.3\nFigure 7.3: Example cases.",
      "size": 949,
      "sentences": 8
    },
    {
      "id": 245,
      "content": "res) reduces the planning time by roughly half,\ndemonstratingtheeffectivenessandscalabilityofemployingparallelism(codeinPython). S-4.1 S-r.7.3 S-r.8.3\nG-4.1 G-r.7.3 G-r.8.3\nFigure 7.3: Example cases. The top row shows the start states and the bottom goal states. Lightlyshadedobjectscanbepick-n-placed;heavilyshadedobjectsmustbemanipulated\nusingpush. Cases4.1,r.7.3,andr.8.3areevaluatedandpresentedinFigure7.4. Objectsare\ndistinguishedbycolor. 7.4.1 SimulationStudies\nSimulationsare conductedinPyBullet[152]. Arealrobotsetup, consistingofa Universal\nRobotUR5e+OnRobotVGC-10vacuumgripper,isreplicated. Therobotoperatesunder\n[표 데이터 감지됨]\n\n=== 페이지 113 ===\n95\nend-effectorpositioncontrol;theworkspacemeasures0.78×0.52m2. 25feasiblescenarios\narecreatedwhereallobjectsareconfinedwithintheworkspace. Objectsizes,shapes,and\nposesarerandomlydeterminedineachscenario (seeFigure7.3forsomeexamples). Cases\nthataretrivialtosolve(e.g.,objectsthathappentobemostlysmall)arefiltered.",
      "size": 965,
      "sentences": 12
    },
    {
      "id": 246,
      "content": "nfinedwithintheworkspace. Objectsizes,shapes,and\nposesarerandomlydeterminedineachscenario (seeFigure7.3forsomeexamples). Cases\nthataretrivialtosolve(e.g.,objectsthathappentobemostlysmall)arefiltered. Thenumber\nof objects ranges from four to eight; five distinct cases are generated for each specified\nnumberofobjects. 20.0\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n4.1 4.2 4.3 4.4 4.5 5.1 5.2 5.3 5.4 5.5 6.1 6.2 6.3 6.4 6.5 7.1 7.2 7.3 7.4 7.5 8.1 8.2 8.3 8.4 8.5 r.4.1r.4.2r.4.3r.5.1r.5.2r.5.3r.6.1r.6.2r.6.3r.7.1r.7.2r.7.3r.8.1r.8.2r.8.3\nIndex of cases\nsnoitca\nfo\nrebmuN\nPMMR-40\nHBFS\n70\n60\n50\n40\n30\n20\n10\n4.1 4.2 4.3 4.4 4.5 5.1 5.2 5.3 5.4 5.5 6.1 6.2 6.3 6.4 6.5 7.1 7.2 7.3 7.4 7.5 8.1 8.2 8.3 8.4 8.5 r.4.1r.4.2r.4.3r.5.1r.5.2r.5.3r.6.1r.6.2r.6.3r.7.1r.7.2r.7.3r.8.1r.8.2r.8.3\nIndex of cases\n)s(\nemit\ntoboR\nPMMR-40\nHBFS\nFigure 7.4: As an expanded illustration of Table 7.1, the upper plot lists the number of\nactions the robot executes to resolve individual cases.",
      "size": 957,
      "sentences": 5
    },
    {
      "id": 247,
      "content": "1r.8.2r.8.3\nIndex of cases\n)s(\nemit\ntoboR\nPMMR-40\nHBFS\nFigure 7.4: As an expanded illustration of Table 7.1, the upper plot lists the number of\nactions the robot executes to resolve individual cases. The lower plot lists the robot’s\nexecutiontimesinsolvingtheindividualcasesfollowingthecomputedplan. Forthelabels\nonthehorizontalaxis,thefirstdigitindicatesthenumberofobjectscontainedwithineach\ncase, while the second digit represents the index of the cases. Cases beginning with the\nprefix’r’aretheonesthatareconstructedforandexecutedbythereal-robotsetup. In evaluating PMMR, each percept-plan-act loop runs for a predetermined duration\nto identify the best next action until the problem is resolved or the maximum number of\nactionshasbeenexhausted. Ifthelatteroccurs,itistreatedasafailedcase. Wedenotethe\ncorrespondingPMMRmethodasPMMR-X,whereXisthemaximumnumberofseconds\nallowed in a single iteration of the loop. We settled on PMMR-40 as the main PMMR\nmethodusedintheevaluation.",
      "size": 979,
      "sentences": 8
    },
    {
      "id": 248,
      "content": "afailedcase. Wedenotethe\ncorrespondingPMMRmethodasPMMR-X,whereXisthemaximumnumberofseconds\nallowed in a single iteration of the loop. We settled on PMMR-40 as the main PMMR\nmethodusedintheevaluation. Inbothsimulationandrealrobotexperiments,forPMMR-40,\nwe keep the top k = 100 for Q value, c = 1.5 in Equation 6.2. The maximum depth of\nthe MCTS tree D is based on the number of objects N: D = 2N +2. θ is based on\nsim\nthedepthdofthenodeθ = max(−0.106+0.231d−0.013d2,0.2). Thesenumbersare\nsim\n[표 데이터 감지됨]\n\n=== 페이지 114 ===\n96\nhandpicked,representingthatasthetreegoesdeeper,theprobabilityofselectingarandom\naction should be increased. r = 0.7 for object can be operated by A , the R = 2r N.\no pp g 0\nForanobjectthatcanbeoperatedbyA ,therewardisgivento1.1r . Wesetβ = 0.5and\npt o\nγ = 0.9toscalethereward. RobotTime Completion Num.",
      "size": 825,
      "sentences": 10
    },
    {
      "id": 249,
      "content": "sed. r = 0.7 for object can be operated by A , the R = 2r N.\no pp g 0\nForanobjectthatcanbeoperatedbyA ,therewardisgivento1.1r . Wesetβ = 0.5and\npt o\nγ = 0.9toscalethereward. RobotTime Completion Num. ofActions PlanTime\nPMMR-40 29.17s 98.00% 8.90 264.99s\nHBFS 36.22s 54.50% 13.72 30.04s\nTable7.1: Summaryof simulationresults(25cases) andreal-robotexperiments (15cases)\nfor HBFSandPMMR-40. IndividualexperimentresultsforallcasesareshowninFigure7.4(whichalsoincludes\ncasesusedforreal-robotexperiments,tobedetailedinsubsection7.4.3). Detailedexperiment\nresultsarepresentedinTable7.1. Here,robottimereferstothecumulativetimerequiredfor\ntherobot toexecute allactions, whilecompletionrefersto thesuccess rate. The numberof\nactionsquantifiestheexecutionofatomicactions,representedbyA ,A . Theplantimeis\npp pt\nthetotalplanningtime. Eachcaseunderwentfiveindependenttrials. Failure often happens because the case requires more than 15actions to solve (even for\nhumans).",
      "size": 958,
      "sentences": 12
    },
    {
      "id": 250,
      "content": "s,representedbyA ,A . Theplantimeis\npp pt\nthetotalplanningtime. Eachcaseunderwentfiveindependenttrials. Failure often happens because the case requires more than 15actions to solve (even for\nhumans). Afailuremaybeduetosampledactionsnotcontainingasolutionorthesearch\nnot being deep enough. Sometimes, the algorithm can recover from an early bad choice,\nbutnotalwayssincethenumberofiterationsiscapped. Weobservethat,whileHBFSruns\nrelatively fast in comparison to PMMR-40, it frequently fails (55% success vs. 98% for\nPMMR-40) and uses many more actions (13.7 vs. 8.9 for PMMR-40). Visually, as can\nbe seen in the accompanying video, the actions generated by PMMR-40 are much more\nhuman-likethanthosebyHBFS(thesameholdsforrealrobotexperiments). 7.4.2 AblationStudies\nWeinvestigatedtheimpactoftimebudgets,thedepthoftreesearch,andthebaserewardR\nb\non solving REMP. The time budget is critical; an extended search duration tends to yield\nbetter results.",
      "size": 946,
      "sentences": 10
    },
    {
      "id": 251,
      "content": "AblationStudies\nWeinvestigatedtheimpactoftimebudgets,thedepthoftreesearch,andthebaserewardR\nb\non solving REMP. The time budget is critical; an extended search duration tends to yield\nbetter results. However, it is necessary to balance planning time and solution quality. An\n[표 데이터 감지됨]\n\n=== 페이지 115 ===\n97\ninsufficientsearchmightsamplehighlysuboptimalpaths,leadingtolocallyoptimalactions. AsdepictedinFigure7.5andTable7.2showsthecorrelationbetweenplanningtimeand\nsolutionquality,leadingustoselect PMMR-40forourmainevaluation. A shallow MCTS (PMMR-40 (D = 3), max MCTS tree depth of 3) was included\nspecificallytocomparewithHBFS,whichhasthree“depthlevels”periteration. In terms of reward design, as detailed in section 7.3, we introduced a base reward R ,\nb\nwhichservesto normalizethe rewardto0atroot. Withoutthisadjustment,thetreesearch\nmightcommencewitha non-zerorewardsignal,whereadisadvantageous branchmaystill\nreturn a reward, causing the search to frequently explore such branches.",
      "size": 986,
      "sentences": 8
    },
    {
      "id": 252,
      "content": "ardto0atroot. Withoutthisadjustment,thetreesearch\nmightcommencewitha non-zerorewardsignal,whereadisadvantageous branchmaystill\nreturn a reward, causing the search to frequently explore such branches. By comparing\nPMMR-40(no-R )inTable7.2withPMMR-40inTable7.1,weobservethattheintroduction\nb\nofR aidsthetreesearch. b\nRobotTime Completion Num. ofActions PlanTime\nPMMR-10 35.60s 94.00% 10.51 103.61s\nPMMR-20 32.46s 96.00% 9.86 155.68s\nPMMR-60 27.93s 99.50% 8.53 358.44s\nPMMR-40(D = 3) 57.83s 32.50% 16.62 641.76s\nPMMR-40(no-R ) 32.92s 94.00% 9.83 270.65s\nb\nTable7.2: Ablationstudyresults(averagedover40cases),forcomparisonwithTable7.1. 100\n99\n98\n97\n96\n95\n94\n93\n10 20 40 60\nTime Budget (s)\n)%(\nnoitelpmoC\n36\nPMMR\n99.5\n34\n98.0\n32\n30\n96.0\n28\n94.0\n26\n10 20 40 60\nTime Budget (s)\n)s(\nemit\ntoboR\n35.60\n32.46\n29.17\n27.93\nFigure 7.5: PMMR is evaluated with different time budgets. The reported values are\naveragedover40cases.",
      "size": 913,
      "sentences": 7
    },
    {
      "id": 253,
      "content": "4\n98.0\n32\n30\n96.0\n28\n94.0\n26\n10 20 40 60\nTime Budget (s)\n)s(\nemit\ntoboR\n35.60\n32.46\n29.17\n27.93\nFigure 7.5: PMMR is evaluated with different time budgets. The reported values are\naveragedover40cases. [표 데이터 감지됨]\n\n=== 페이지 116 ===\n98\n7.4.3 RealRobotExperiments\nInsimulation,weemulatethevacuumfunctionbyattachingtheobjecttotheend-effector\nusinganextralinkviaafixedjoint. Weemploytwovacuumcupstoprovidesufficientsuction\npower to ensure a robust connection in the real-world setup. The point of suction on the\nobjectis taken asitscenter, assumingthiscentralarea isflat. Similartosimulation studies,\nthenumberofobjects rangesfromfourtoeight,and threedistinctcasesaregeneratedfor\neachnumberofobjects. Figure7.6: Thefullsetofobjectsusedinourreal-robotexperiments. A RealSense D455 camera is affixed to the robot’s wrist, capturing the scene from a\ntop-down perspective, andanorthogonalviewisrendered fromthepoint cloud.",
      "size": 911,
      "sentences": 8
    },
    {
      "id": 254,
      "content": "setofobjectsusedinourreal-robotexperiments. A RealSense D455 camera is affixed to the robot’s wrist, capturing the scene from a\ntop-down perspective, andanorthogonalviewisrendered fromthepoint cloud. Weemploy\nthe Segment Anything Model [153] to extract masks of the objects present on the table. Subsequently,OpenCV[154]isappliedtodeterminethecontoursandapproximatethem\nintopolygons,whicharethenusedforplanning. Anadditionalstepdetermineseachobject’s\nSE(2)poses. Foreachcase,theexperimentisrepeatedatleastthreetimes. Due to the small sim-to-real gap, we let the algorithm plan the entire sequence of\n=== 페이지 117 ===\n99\nmanipulation actions at the beginning, which generally works well. If no solution is\nfound using 20 actions, we mark it as a failure; otherwise, the robot executes the planned\nactions. Robot time is not recorded for failure cases, hence its absence in Table 7.3.",
      "size": 881,
      "sentences": 9
    },
    {
      "id": 255,
      "content": "ll. If no solution is\nfound using 20 actions, we mark it as a failure; otherwise, the robot executes the planned\nactions. Robot time is not recorded for failure cases, hence its absence in Table 7.3. For\ncompleteness,incaseswhere bothPMMRandHBFSsucceedatleastonce, HBFSaverages\n15.15 actions and PMMR 9.56, with robot (execution) times of 96.62 seconds and 95.75\nseconds, respectively (but note that HBFS fails much more frequently). Results from\nindividualbenchmarksacross15casesarepresentedinFigure7.7. Eachcasewassubjected\nto three independent trials. In real-robot experiments, the cases were intentionally designed\ntobechallenging. Agreedyactionmayexacerbatetheproblem,makingitevenmoredifficult\ntoresolve. Consequently,thecompletionrateof HBFSissignificantlyreduced. RobotTime Completion Num.",
      "size": 797,
      "sentences": 10
    },
    {
      "id": 256,
      "content": "ntentionally designed\ntobechallenging. Agreedyactionmayexacerbatetheproblem,makingitevenmoredifficult\ntoresolve. Consequently,thecompletionrateof HBFSissignificantlyreduced. RobotTime Completion Num. ofActions PlanTime\nPMMR-40 95.75s* 96.44% 9.56 292.02s\nHBFS 96.62s* 38.33% 15.15 29.05s\nPMMR-40(Sim) − 94.67% 10.44 306.41s\nHBFS(Sim) − 45.33% 14.99 22.18s\nTable 7.3: Experiment results of real robot trials across 15 cases, with time budgets\nconstrained to a maximum of 40 seconds for a single MCTS run. The robot time is only\nconsideredincaseswhereboth methods succeedatleastonce. Additionally, benchmarks\nfromsimulationscovering15casesareincludedforsim-to-realgapcomparisons. Therobot\ntime for PMMR-40 and HBFS, denoted with an asterisk, is recorded only for successful\ncases.",
      "size": 778,
      "sentences": 8
    },
    {
      "id": 257,
      "content": "e. Additionally, benchmarks\nfromsimulationscovering15casesareincludedforsim-to-realgapcomparisons. Therobot\ntime for PMMR-40 and HBFS, denoted with an asterisk, is recorded only for successful\ncases. 20\n15\n10\n5\nr.4.1 r.4.2 r.4.3 r.5.1 r.5.2 r.5.3 r.6.1 r.6.2 r.6.3 r.7.1 r.7.2 r.7.3 r.8.1 r.8.2 r.8.3\nIndex of cases\nsnoitca\nfo\nrebmuN\nPMMR-40\nHBFS\nFigure 7.7: As an expanded illustration of Table 7.3, this plot illustrates the number of\nactionstherobotexecutestoresolveindividualcases. [표 데이터 감지됨]\n\n=== 페이지 118 ===\n100\n7.5 Summary\nWemaketheobservationthathumansfrequentlysolvemanipulationchallengesusingmultiple\ntypesofmanipulationactions. Incontrast,therehasbeenrelativelylimitedresearchtackling\nplanninghigh-qualityresolutionsforlong-horizonmanipulationtasksexploringthesynergy\nof multiple manipulation actions. Inspired by how humans solve everyday manipulation\ntasks,inthispaper,weproposedandstudiedtheRearrangementwithMultipleManipulation\nPrimitives(REMP)problem.",
      "size": 968,
      "sentences": 6
    },
    {
      "id": 258,
      "content": "thesynergy\nof multiple manipulation actions. Inspired by how humans solve everyday manipulation\ntasks,inthispaper,weproposedandstudiedtheRearrangementwithMultipleManipulation\nPrimitives(REMP)problem. TooptimallysolveREMP,wedevelopedtwoeffectivemethods,\nHBFSandPMMR.PMMRisespeciallyadeptatsolvingdifficultREMPinstanceswithhigh\nsuccessratesandproducinghigh-qualitysolutionsequences,capabilitiesconfirmedthrough\nthoroughsimulationandreal-robotexperimentsthatincludedfullpercept-plan-actloops. === 페이지 119 ===\n101\nCHAPTER8\nCONCLUSION\nInthisdissertation,wehaveexploredaspectrumofapproachesforroboticmanipulationin\ncluttered and long-horizon scenarios, with an emphasis on efficiently combining learned\npredictivemodels,search-basedplanning,andparallelizationtechniques. Acentraltheme\nacrossthepresented workisthepursuit ofaccurateinteractionpredictionand high-quality\nplanningunderuncertainty,groundedinbothsimulationandreal-worldexperimentation.",
      "size": 941,
      "sentences": 5
    },
    {
      "id": 259,
      "content": "llelizationtechniques. Acentraltheme\nacrossthepresented workisthepursuit ofaccurateinteractionpredictionand high-quality\nplanningunderuncertainty,groundedinbothsimulationandreal-worldexperimentation. WiththeintroductionofDeepInteractionPredictionNetwork(DIPN)forpush-and-grasp\nchallenges, we established the importance of generating clear and reliable intermediate\npredictionsthat canbeeffectively usedbydownstreamnetworkssuchas theGraspNetwork\n(GN). This integrated method (DIPN+GN) exhibits strong generalization capabilities,\ndemonstrates excellent sample efficiency, and outperforms prior state-of-the-art learning-\nbasedapproaches. Notably,themethodstraininaself-supervisedmanner,requirenomanual\nlabelingorhumaninput,andarerobusttovariationsinobjectsize,shape,color,andfriction. Buildingontheselearnedpredictivemodels,theVisualForesightTrees(VFT)framework\nintroducesasynergybetweenDIPNandMonteCarloTreeSearch(MCTS)forlong-horizon\nplanning in object retrieval tasks from dense clutter.",
      "size": 989,
      "sentences": 6
    },
    {
      "id": 260,
      "content": "ngontheselearnedpredictivemodels,theVisualForesightTrees(VFT)framework\nintroducesasynergybetweenDIPNandMonteCarloTreeSearch(MCTS)forlong-horizon\nplanning in object retrieval tasks from dense clutter. VFT has been shown to generate\nhigh-quality multi-step plans, though its performance is constrained by the substantial\ncomputational overhead. This drawback can be mitigatedbyparallelization—an approach\nthathasbeenpartiallyaddressedbydevelopingparallelMCTSstrategiesandwillbeessential\nforachievingreal-timeperformanceinroboticapplications. In parallel, we explored techniques for model-based simulation, including MORE (a\nsimulator-driven approach), which uses a search-and-learn philosophy. Despite showing\npromising results, MORE and similar planning algorithms require either explicit object\nmodels or learned surrogates to simulate push outcomes.",
      "size": 850,
      "sentences": 5
    },
    {
      "id": 261,
      "content": "hich uses a search-and-learn philosophy. Despite showing\npromising results, MORE and similar planning algorithms require either explicit object\nmodels or learned surrogates to simulate push outcomes. These requirements impose\n=== 페이지 120 ===\n102\npotentiallimitationsongeneralizationtonovelobjectsandintroduceadditionaluncertainties\nwhenlearnedcomponentsareusedinplaceofaphysicsengine. To address the computational bottleneck inherent in MCTS for robotic tasks, we\nintroduced PMBS, a novel parallel MCTS method that leverages GPU-enabled batched\nsimulations, achieving an over 30× speedup relative to a strong serial MCTS baseline. Real-worldrobotexperimentsconfirmthatPMBStransferseffectivelyfromsimulationto\nphysicalsystems,enablingnearreal-timeperformanceincomplexlong-horizonplanning. Finally, we proposed the rearrangement with multiple manipulation primitives (REMP)\nframework,inspired by howhumans tackle dailymanipulation tasksusing diverse actions.",
      "size": 956,
      "sentences": 6
    },
    {
      "id": 262,
      "content": "incomplexlong-horizonplanning. Finally, we proposed the rearrangement with multiple manipulation primitives (REMP)\nframework,inspired by howhumans tackle dailymanipulation tasksusing diverse actions. Ourmethods,HBFSandPMMR,facilitateplanningthatencompassesmultipleactiontypes\n(e.g.,pushing,grasping)andachievehighsuccessrateswithhigh-qualitysolutionsequences\ninbothsimulationandreal-robottrials. Across all these methods, a unifying message is the feasibility and effectiveness of\nintegratinglearning,search,andparallelizationtosolvechallengingroboticmanipulation\ntasks. Nonetheless, data efficiency, robustness to model inaccuracies, and computational\nscalabilityremainongoingconcernsthatwarrantfurtherattention.",
      "size": 713,
      "sentences": 5
    },
    {
      "id": 263,
      "content": "allelizationtosolvechallengingroboticmanipulation\ntasks. Nonetheless, data efficiency, robustness to model inaccuracies, and computational\nscalabilityremainongoingconcernsthatwarrantfurtherattention. In future research, we will (1) extend the range of manipulation actions to include\nnon-horizontalpushesandnon-verticalgrasps(arbitrary6Dend-effectorposes),enabling\nmore versatile rearrangement; (2) further optimize parallelization to achieve near real-\ntime planning, focusing on multi-threaded or GPU-accelerated strategies; and (3) pursue\nintegrated learning-based frameworks for rollout policies and reward estimation, aiming\nto reduce reliance on explicit simulation and improve overall efficiency and robustness. These directions promise to enhance the adaptability and performance of robotic systems in\nincreasinglycomplex,real-worldscenarios.",
      "size": 850,
      "sentences": 4
    },
    {
      "id": 264,
      "content": "explicit simulation and improve overall efficiency and robustness. These directions promise to enhance the adaptability and performance of robotic systems in\nincreasinglycomplex,real-worldscenarios. === 페이지 121 ===\nAppendices\n=== 페이지 122 ===\n104\nAPPENDIXA\nCHAPTER6-PMBSSUPPLEMENTARY\nA.0.1 GraspClassifierImplementationDetails\nFor our implementation of the grasp classifier (GC), we used Isaac Gym to collect grasp\ntrainingdata. Randomobjectsarefirstsampledontheworkspace,andthenwediscretizethe\nworkspaceintoagrid,whereeachpointisthe(x,y)ofagraspactionag. Wealsodiscretize\nrotation into K angles uniformly. All robots in simulator will pick one grasp action and\ncheckthe distancebetweentwo fingers asa signalof successfulgrasping. Foreach depth\nimage, it is associated with hundreds of grasps. If the target can be grasped in at least n\nattempts,thenthelabelis1,and0otherwise(n=5).",
      "size": 880,
      "sentences": 8
    },
    {
      "id": 265,
      "content": "weentwo fingers asa signalof successfulgrasping. Foreach depth\nimage, it is associated with hundreds of grasps. If the target can be grasped in at least n\nattempts,thenthelabelis1,and0otherwise(n=5). Weusedtwodaysofgenerating20000\ntraining data (a depth image focus centered on the target object and a label of can it be\ngrasped) without human annotation. It is evaluated on test data with 93.45% accuracy if\nthe R∗ equals 0.7. The batch size is 256, learning rate is 0.1, epochs is 90, momentum is\nc\n0.9. Wehavesuccessfullyreducedthegraspevaluationtimefrom0.26to0.003perimage,\nmakingtheparallelMCTSpossible. A.0.2 GraspNetworkImplementationDetails\nFordeciding whethertoperformfurther pushactionsorto makeanattemptto retrievethe\ntargetobject, we resorttoa graspnetwork (GN)thatis fastandamenableto parallelization. GNisbasedonfullyconvolutionalnetworks(FCNs)[12],[27]andcustomizedtoestimate\nthe grasp probability for the target object [16], [19].",
      "size": 946,
      "sentences": 9
    },
    {
      "id": 266,
      "content": "esorttoa graspnetwork (GN)thatis fastandamenableto parallelization. GNisbasedonfullyconvolutionalnetworks(FCNs)[12],[27]andcustomizedtoestimate\nthe grasp probability for the target object [16], [19]. It takes an RGB-D image o as input\nt\nand outputs dense pixel-wise values P(o ) ∈ [0,1]H×W×K. H and W are the height and\nt\nwidthoftheo . Toaccountthegripperorientation,wediscretizeθ ofag intoK = 16angles,\nt\ninmultiplesof(22.5◦),soo hasbeenrotatedK times. GNpresentedin[12]istrainedto\nt\nestimate the grasp success rate for all objects. Further, binary mask of target object M is\n=== 페이지 123 ===\n105\nimposedusingMaskR-CNN[101],totruncatethevaluesasP (o ) = P(o )∩M(o ). In\nm t t t\nproposed system, we only interested the highest grasp probability. If max P (o ) is\nh,w,k m t\ngreaterthanthe presetthresholdP∗ (itis 0.75inour case),then, therobotshouldgrasp at\nlocationh,w withk asorientationofgripper.",
      "size": 897,
      "sentences": 9
    },
    {
      "id": 267,
      "content": "only interested the highest grasp probability. If max P (o ) is\nh,w,k m t\ngreaterthanthe presetthresholdP∗ (itis 0.75inour case),then, therobotshouldgrasp at\nlocationh,w withk asorientationofgripper. ThebackboneofGNisResNet-18FPN[102],\n[110], withconvolution layersand bilinear-upsampling layers as describedin [16], [19]. We\nusedthepre-trainedGraspNetworkfrom[19]. TheGraspNetworkisaplug-incomponent\nfor the system, can be replaced by other advanced methods as if a grasp probability and\ngraspactioncanbeprovided. A.0.3 Real-to-Sim-to-RealComparison\nWesolvethetask usingaphysicssimulator. Whilethereexistsa real-to-simandsim-to-real\ngaps, it is sufficiently small for this type of task, even when considering pose estimation\nerrorsthataffectobjectlocalizationinboththerealandsimulatedenvironments.",
      "size": 798,
      "sentences": 7
    },
    {
      "id": 268,
      "content": "eexistsa real-to-simandsim-to-real\ngaps, it is sufficiently small for this type of task, even when considering pose estimation\nerrorsthataffectobjectlocalizationinboththerealandsimulatedenvironments. From the Figure A.1 and Figure A.2, in the first row and first column, we capture an\nimage of the real-world scene, perform pose estimation, and reconstruct the scene in the\nsimulator (first column, second row). Planning is conducted within the simulator, where\nthe estimated push action and its resulting state are shown in the first-row images. This\nprocessisiterativelyrepeateduntilthetargetobjectissuccessfullygrasped. Thediscrepancy\nbetween the third-row images and the first-row images in the next column illustrates the\ndifferencebetweentheplanned/estimatedresultsand theactualexecution resultsofthereal\nrobot. Thesimulatoriscapableofprovidinghighlyaccuratephysicssimulations. However,in\nsomecases,itmayyieldnon-accurateyetreasonablephysicsapproximations,whicharestill\nusefulfortaskexecution.",
      "size": 999,
      "sentences": 7
    },
    {
      "id": 269,
      "content": "softhereal\nrobot. Thesimulatoriscapableofprovidinghighlyaccuratephysicssimulations. However,in\nsomecases,itmayyieldnon-accurateyetreasonablephysicsapproximations,whicharestill\nusefulfortaskexecution. === 페이지 124 ===\n106\nFigureA.1: Casestudyoneofrealtosimtorealgap. Simulatorprovidesaccuratephysics\nsimulations. FigureA.2: Casestudytwoofrealtosimtorealgap. Simulatorprovidenon-accuratebut\nreasonablephysicssimulations. === 페이지 125 ===\n107\nAPPENDIXB\nCHAPTER7-REMPSUPPLEMENTARY\nStart image Goal image\nStart image Goal image\nStart image Goal image\nStart image Goal image\nFigureB.1: Somecasesinrealworldsetup. === 페이지 126 ===\n108\nACKNOWLEDGMENTOFPREVIOUSPUBLICATIONS\nP1 Baichuan Huang, Shuai D. Han, Abdeslam Boularias, and Jingjin Yu. ”Dipn: Deep\ninteraction prediction network with application to clutter removal.” In 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 4694–4701, 2021. IEEE. P2 BaichuanHuang,ShuaiD.Han,JingjinYu,andAbdeslamBoularias.",
      "size": 971,
      "sentences": 13
    },
    {
      "id": 270,
      "content": "k with application to clutter removal.” In 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 4694–4701, 2021. IEEE. P2 BaichuanHuang,ShuaiD.Han,JingjinYu,andAbdeslamBoularias. ”Visualforesight\ntreesforobjectretrievalfromclutterwithnonprehensilerearrangement.”IEEERobotics\nandAutomationLetters,vol. 7,no. 1,pp. 231–238,2021. IEEE. P3 BaichuanHuang,TengGuo,AbdeslamBoularias,andJingjinYu. ”Interleavingmonte\ncarlo tree search and self-supervised learning for object retrieval in clutter.” In 2022\nInternational Conference on Robotics and Automation (ICRA), pp. 625–632, 2022. IEEE. P4 Baichuan Huang, Abdeslam Boularias, and Jingjin Yu. ”Parallel monte carlo tree\nsearchwith batchedrigid-body simulationsforspeeding uplong-horizonepisodic robot\nplanning.”In2022IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems\n(IROS),pp. 1153–1160,2022. IEEE. P5 BaichuanHuang,XujiaZhang,andJingjinYu.",
      "size": 918,
      "sentences": 18
    },
    {
      "id": 271,
      "content": "lationsforspeeding uplong-horizonepisodic robot\nplanning.”In2022IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems\n(IROS),pp. 1153–1160,2022. IEEE. P5 BaichuanHuang,XujiaZhang,andJingjinYu. ”Towardoptimaltabletoprearrangement\nwith multiple manipulation primitives.” In 2024 IEEE International Conference on\nRoboticsandAutomation(ICRA),pp. 10860–10866,2024. IEEE. P6 BaichuanHuang,JingjinYu,andSiddarthJain. ”EARL:Eye-on-handreinforcement\nlearner for dynamic grasping with active pose estimation.” In 2023 IEEE/RSJ Inter-\nnationalConferenceonIntelligentRobotsandSystems (IROS),pp. 2963–2970,2023. === 페이지 127 ===\n109\nIEEE. This work, while part of the author’s PhD research efforts, is not discussed in\ndetailwithinthisdissertation. === 페이지 128 ===\n110\nREFERENCES\n[1] M.T.Mason,“Towardroboticmanipulation,”AnnualReviewofControl,Robotics,\nandAutonomousSystems,vol.1,no.1,pp.1–28,2018.",
      "size": 892,
      "sentences": 13
    },
    {
      "id": 272,
      "content": "s not discussed in\ndetailwithinthisdissertation. === 페이지 128 ===\n110\nREFERENCES\n[1] M.T.Mason,“Towardroboticmanipulation,”AnnualReviewofControl,Robotics,\nandAutonomousSystems,vol.1,no.1,pp.1–28,2018. [2] K.Azadeh,R.DeKoster,andD.Roy,“Robotizedandautomatedwarehousesystems:\nReview and recent developments,” Transportation Science, vol. 53, no. 4, pp. 917–\n945,2019. [3] R.H.Taylor,A.Menciassi,G.Fichtinger,P.Fiorini,andP.Dario,“Medicalrobotics\nandcomputer-integratedsurgery,”Springerhandbookofrobotics,pp.1657–1684,\n2016. [4] A. Ajoudani, A. M. Zanchettin, S. Ivaldi, A. Albu-Scha¨ffer, K. Kosuge, and O.\nKhatib, “Progress and prospects of the human–robot collaboration,” Autonomous\nrobots,vol.42,pp.957–975,2018. [5] J.S.Jennings,G.Whelan,and W.F.Evans,“Cooperativesearch andrescuewitha\nteamofmobilerobots,”in19978thInternationalConferenceonAdvancedRobotics. Proceedings.ICAR’97,IEEE,1997,pp.193–200.",
      "size": 900,
      "sentences": 10
    },
    {
      "id": 273,
      "content": "7–975,2018. [5] J.S.Jennings,G.Whelan,and W.F.Evans,“Cooperativesearch andrescuewitha\nteamofmobilerobots,”in19978thInternationalConferenceonAdvancedRobotics. Proceedings.ICAR’97,IEEE,1997,pp.193–200. [6] J. Mahler, M. Matl, X. Liu, A. Li, D. Gealy, and K. Goldberg, “Dex-net 3.0:\nComputingrobustvacuumsuctiongrasptargetsinpointcloudsusinganewanalytic\nmodelanddeeplearning,”in2018IEEEInternationalConferenceonroboticsand\nautomation(ICRA),IEEE,2018,pp.5620–5627. [7] T.Boroushaki,L.Dodds,N.Naeem,andF.Adib,“Fusebot:Rf-visualmechanical\nsearch,”Robotics:ScienceandSystems2022,2022. [8] A.KrontirisandK.E.Bekris,“Dealingwithdifficultinstancesofobjectrearrange-\nment.,”inRobotics:ScienceandSystems,vol.1123,2015. [9] N. Marturi, M. Kopicki, A. Rastegarpanah, et al., “Dynamic grasp and trajectory\nplanningformovingobjects,”AutonomousRobots,vol.43,pp.1241–1256,2019.",
      "size": 859,
      "sentences": 7
    },
    {
      "id": 274,
      "content": ".,”inRobotics:ScienceandSystems,vol.1123,2015. [9] N. Marturi, M. Kopicki, A. Rastegarpanah, et al., “Dynamic grasp and trajectory\nplanningformovingobjects,”AutonomousRobots,vol.43,pp.1241–1256,2019. [10] O. M. Andrychowicz, B. Baker, M. Chociej, et al., “Learning dexterous in-hand\nmanipulation,” The International Journal of Robotics Research, vol. 39, no. 1,\npp.3–20,2020. [11] A.Rodriguez,M.T.Mason,andS.Ferry,“Fromcagingtograsping,”TheInterna-\ntionalJournalofRoboticsResearch,vol.31,no.7,pp.886–900,2012. === 페이지 129 ===\n111\n[12] B.Huang,S.D.Han,A.Boularias,andJ.Yu,“Dipn:Deepinteractionprediction\nnetworkwithapplicationtoclutterremoval,”in2021IEEEInternationalConference\nonRoboticsandAutomation(ICRA),IEEE,2021,pp.4694–4701. [13] K.Gao,S.W.Feng,B.Huang,andJ.Yu,“Minimizingrunningbuffersfortabletopob-\njectrearrangement:Complexity,fastalgorithms,andapplications,”TheInternational\nJournalofRoboticsResearch,vol.42,no.10,pp.755–776,2023.",
      "size": 940,
      "sentences": 8
    },
    {
      "id": 275,
      "content": ",S.W.Feng,B.Huang,andJ.Yu,“Minimizingrunningbuffersfortabletopob-\njectrearrangement:Complexity,fastalgorithms,andapplications,”TheInternational\nJournalofRoboticsResearch,vol.42,no.10,pp.755–776,2023. [14] B. Huang, X. Zhang, and J. Yu, “Toward optimal tabletop rearrangement with\nmultiple manipulation primitives,” in 2024 IEEE International Conference on\nRoboticsandAutomation(ICRA),IEEE,2024,pp.10860–10866. [15] H. Chang, K. Gao, K. Boyalakuntla, et al., “Lgmcts: Language-guided monte-\ncarlotreesearch forexecutablesemanticobjectrearrangement,”in 2024IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), IEEE, 2024,\npp.13607–13612. [16] B. Huang, S. D. Han, J. Yu, and A. Boularias, “Visual foresight trees for ob-\nject retrieval from clutter with nonprehensile rearrangement,” IEEE Robotics and\nAutomationLetters,vol.7,no.1,pp.231–238,2021. [17] S.D. Han,B. Huang,S.",
      "size": 894,
      "sentences": 7
    },
    {
      "id": 276,
      "content": "A. Boularias, “Visual foresight trees for ob-\nject retrieval from clutter with nonprehensile rearrangement,” IEEE Robotics and\nAutomationLetters,vol.7,no.1,pp.231–238,2021. [17] S.D. Han,B. Huang,S. Ding, etal.,“Toward fully automatedmetal recycling using\ncomputervisionandnon-prehensilemanipulation,”in2021IEEE17thInternational\nConference on Automation Science and Engineering (CASE), IEEE, 2021, pp. 891–\n898. [18] K. Gao, D. Lau, B. Huang, K. E. Bekris, and J. Yu, “Fast high-quality tabletop\nrearrangementinboundedworkspace,”in2022InternationalConferenceonRobotics\nandAutomation(ICRA),IEEE,2022,pp.1961–1967. [19] B. Huang, T. Guo, A. Boularias, and J. Yu, “Interleaving monte carlo tree search\nand self-supervised learning for object retrieval in clutter,” in 2022 International\nConferenceonRoboticsandAutomation(ICRA),IEEE,2022,pp.625–632.",
      "size": 845,
      "sentences": 8
    },
    {
      "id": 277,
      "content": "as, and J. Yu, “Interleaving monte carlo tree search\nand self-supervised learning for object retrieval in clutter,” in 2022 International\nConferenceonRoboticsandAutomation(ICRA),IEEE,2022,pp.625–632. [20] Y.Zhao,B.Huang,J.Yu,andQ.Zhu,“Stackelbergstrategicguidanceforhetero-\ngeneous robots collaboration,” in 2022 International Conference on Robotics and\nAutomation(ICRA),IEEE,2022,pp.4922–4928. [21] B.Huang,A.Boularias,andJ.Yu,“Parallelmontecarlotreesearchwithbatched\nrigid-body simulations for speeding up long-horizon episodic robot planning,” in\n2022IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),\nIEEE,2022,pp.1153–1160. === 페이지 130 ===\n112\n[22] B.Huang, J.Yu, andS.Jain, “Earl: Eye-on-hand reinforcement learnerfordynamic\ngrasping with active pose estimation,” in 2023 IEEE/RSJ International Conference\nonIntelligentRobotsandSystems(IROS),IEEE,2023,pp.2963–2970.",
      "size": 888,
      "sentences": 4
    },
    {
      "id": 278,
      "content": "S.Jain, “Earl: Eye-on-hand reinforcement learnerfordynamic\ngrasping with active pose estimation,” in 2023 IEEE/RSJ International Conference\nonIntelligentRobotsandSystems(IROS),IEEE,2023,pp.2963–2970. [23] X.Zhang,S.Jain,B.Huang,M.Tomizuka,andD.Romeres,“Learninggeneralizable\npivotingskills,”in2023IEEEInternationalConferenceonRoboticsandAutomation\n(ICRA),IEEE,2023,pp.5865–5871. [24] A. Bicchi and V. Kumar, “Robotic grasping and contact: A review,” in Proceedings\n2000 ICRA. Millennium conference. IEEE international conference on robotics\nandautomation.Symposiaproceedings(Cat.No.00CH37065),IEEE,vol.1,2000,\npp.348–353. [25] M.BauzaandA.Rodriguez,“Aprobabilisticdata-drivenmodelforplanarpushing,”\nin2017IEEEInternationalConferenceonRoboticsandAutomation(ICRA),IEEE,\n2017,pp.3008–3015. [26] R. Shome, W. N. Tang, C. Song, et al., “Towards robust product packing with\na minimalistic end-effector,” in 2019 International Conference on Robotics and\nAutomation(ICRA),IEEE,2019,pp.9007–9013.",
      "size": 987,
      "sentences": 7
    },
    {
      "id": 279,
      "content": "26] R. Shome, W. N. Tang, C. Song, et al., “Towards robust product packing with\na minimalistic end-effector,” in 2019 International Conference on Robotics and\nAutomation(ICRA),IEEE,2019,pp.9007–9013. [27] A.Zeng,S.Song,S.Welker,J.Lee,A.Rodriguez,andT.Funkhouser, “Learning\nsynergiesbetweenpushingandgraspingwithself-superviseddeepreinforcement\nlearning,” in 2018 IEEE/RSJ International Conference on Intelligent Robots and\nSystems(IROS),IEEE,2018,pp.4238–4245. [28] K. Hang, A. S. Morgan, and A. M. Dollar, “Pre-grasp sliding manipulation of\nthin objects using soft, compliant, or underactuated hands,” IEEE Robotics and\nAutomationLetters,vol.4,no.2,pp.662–669,2019. [29] M.R.Dogar,M.C.Koval,A.Tallavajhula,andS.S.Srinivasa,“Objectsearchby\nmanipulation,”AutonomousRobots,vol.36,pp.153–167,2014. [30] Y.Hou,Z.",
      "size": 808,
      "sentences": 5
    },
    {
      "id": 280,
      "content": "obotics and\nAutomationLetters,vol.4,no.2,pp.662–669,2019. [29] M.R.Dogar,M.C.Koval,A.Tallavajhula,andS.S.Srinivasa,“Objectsearchby\nmanipulation,”AutonomousRobots,vol.36,pp.153–167,2014. [30] Y.Hou,Z. Jia,A.M.Johnson,and M.T.Mason,“Robustplanar dynamicpivoting\nbyregulatinginertialandgripforces,”inAlgorithmicFoundationsofRoboticsXII:\nProceedingsoftheTwelfthWorkshopontheAlgorithmicFoundationsofRobotics,\nSpringer,2020,pp.464–479. [31] N. Doshi, O. Taylor, and A. Rodriguez, “Manipulation of unknown objects via\ncontactconfigurationregulation,”in2022InternationalConferenceonRoboticsand\nAutomation(ICRA),IEEE,2022,pp.2693–2699. [32] J. Bohg, A. Morales, T. Asfour, and D. Kragic, “Data-driven grasp synthesis—a\nsurvey,”IEEETransactionsonrobotics,vol.30,no.2,pp.289–309,2013.",
      "size": 773,
      "sentences": 6
    },
    {
      "id": 281,
      "content": "Roboticsand\nAutomation(ICRA),IEEE,2022,pp.2693–2699. [32] J. Bohg, A. Morales, T. Asfour, and D. Kragic, “Data-driven grasp synthesis—a\nsurvey,”IEEETransactionsonrobotics,vol.30,no.2,pp.289–309,2013. === 페이지 131 ===\n113\n[33] R.Detry,C.H.Ek,M.Madry,andD.Kragic,“Learningadictionaryofprototypical\ngrasp-predicting parts from grasping experience,” in 2013 IEEE International\nConferenceonRoboticsandAutomation,IEEE,2013,pp.601–608. [34] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting robotic grasps,” The\nInternationalJournalofRoboticsResearch,vol.34,no.4-5,pp.705–724,2015. [35] D. Kappler, J. Bohg, and S. Schaal, “Leveraging big data for grasp planning,” in\n2015 IEEE international conference on robotics and automation (ICRA), IEEE,\n2015,pp.4304–4311. [36] X.Yan,J.Hsu,M.Khansari,etal.,“Learning6-dofgraspinginteractionviadeep3d\ngeometry-aware representations,” in Proceedings of IEEE International Conference\nonRoboticsandAutomation(ICRA2018),2018.",
      "size": 963,
      "sentences": 6
    },
    {
      "id": 282,
      "content": ". [36] X.Yan,J.Hsu,M.Khansari,etal.,“Learning6-dofgraspinginteractionviadeep3d\ngeometry-aware representations,” in Proceedings of IEEE International Conference\nonRoboticsandAutomation(ICRA2018),2018. [37] A.Mousavian,C.Eppner,andD.Fox,“6-dofgraspnet:Variationalgraspgeneration\nforobjectmanipulation,”inProceedingsof the IEEE/CVFinternational conference\noncomputervision,2019,pp.2901–2910. [38] H. Liang, X. Ma, S. Li, et al., “Pointnetgpd: Detecting grasp configurations from\npointsets,”in2019InternationalConferenceonRoboticsandAutomation(ICRA),\nIEEE,2019,pp.3629–3635. [39] A.TenPasandR.Platt,“Usinggeometrytodetectgraspposesin3dpointclouds,”\nRoboticsResearch:Volume1,pp.307–324,2018. [40] L.PintoandA.Gupta,“Supersizingself-supervision:Learningtograspfrom50k\ntriesand700robothours,”in 2016IEEEinternationalconferenceonroboticsand\nautomation(ICRA),IEEE,2016,pp.3406–3413.",
      "size": 873,
      "sentences": 6
    },
    {
      "id": 283,
      "content": "07–324,2018. [40] L.PintoandA.Gupta,“Supersizingself-supervision:Learningtograspfrom50k\ntriesand700robothours,”in 2016IEEEinternationalconferenceonroboticsand\nautomation(ICRA),IEEE,2016,pp.3406–3413. [41] J. Mahler and K. Goldberg, “Learning deep policies for robot bin picking by\nsimulating robust grasping sequences,” in Conference on robot learning, PMLR,\n2017,pp.515–524. [42] H.-S.Fang,C.Wang,M.Gou,andC.Lu,“Graspnet-1billion:Alarge-scalebench-\nmarkforgeneralobjectgrasping,”in ProceedingsoftheIEEE/CVFConferenceon\nComputerVisionandPatternRecognition,2020,pp.11444–11453. [43] A. Boularias, J. Bagnell, and A. Stentz, “Efficient optimization for autonomous\nrobotic manipulation of natural objects,” in Proceedings of the AAAI Conference on\nArtificialIntelligence,vol.28,2014.",
      "size": 780,
      "sentences": 5
    },
    {
      "id": 284,
      "content": "A. Boularias, J. Bagnell, and A. Stentz, “Efficient optimization for autonomous\nrobotic manipulation of natural objects,” in Proceedings of the AAAI Conference on\nArtificialIntelligence,vol.28,2014. [44] B. Wen, W. Lian, K. Bekris, and S. Schaal, “Catgrasp: Learning category-level\ntask-relevant grasping in clutter from simulation,” in 2022 International Conference\nonRoboticsandAutomation(ICRA),IEEE,2022,pp.6401–6408. === 페이지 132 ===\n114\n[45] J. Mahler, J. Liang, S. Niyaz, et al., “Dex-net 2.0: Deep learning to plan robust\ngrasps with synthetic point clouds and analytic grasp metrics,” arXiv preprint\narXiv:1703.09312,2017. [46] J.Mahler,M.Matl,V.Satish,etal.,“Learningambidextrousrobotgraspingpolicies,”\nScienceRobotics,vol.4,no.26,eaau4984,2019. [47] Y.Deng,X.Guo,Y.Wei,etal.,“Deepreinforcementlearningforroboticpushing\nand picking in cluttered environment,” in 2019 IEEE/RSJ International Conference\nonIntelligentRobotsandSystems(IROS),Ieee,2019,pp.619–626.",
      "size": 966,
      "sentences": 5
    },
    {
      "id": 285,
      "content": ".Guo,Y.Wei,etal.,“Deepreinforcementlearningforroboticpushing\nand picking in cluttered environment,” in 2019 IEEE/RSJ International Conference\nonIntelligentRobotsandSystems(IROS),Ieee,2019,pp.619–626. [48] K.Xu,H.Yu,Q.Lai,Y.Wang,andR.Xiong,“Efficientlearningofgoal-oriented\npush-grasping synergy in clutter,” IEEE Robotics and Automation Letters, vol. 6,\nno.4,pp.6337–6344,2021. [49] J. Stu¨ber, C. Zito, and R. Stolkin, “Let’s push things forward: A survey on robot\npushing,”FrontiersinRoboticsandAI,vol.7,p.8,2020. [50] K.M.Lynch,“Estimatingthefrictionparametersofpushedobjects,”inProceedings\nof 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS’93),IEEE,vol.1,1993,pp.186–193. [51] M. T. Mason, “Mechanics and planning of manipulator pushing operations,” The\nInternationalJournalofRoboticsResearch,vol.5,no.3,pp.53–71,1986.",
      "size": 854,
      "sentences": 6
    },
    {
      "id": 286,
      "content": "ts and Systems\n(IROS’93),IEEE,vol.1,1993,pp.186–193. [51] M. T. Mason, “Mechanics and planning of manipulator pushing operations,” The\nInternationalJournalofRoboticsResearch,vol.5,no.3,pp.53–71,1986. [52] K. M. Lynch and M. T. Mason, “Stable pushing: Mechanics, controllability, and\nplanning,”Theinternationaljournalofroboticsresearch,vol.15,no.6,pp.533–556,\n1996. [53] K.M.LynchandM.T.Mason,“Dynamicnonprehensilemanipulation:Controllabil-\nity, planning, and experiments,” The International Journal of Robotics Research,\nvol.18,no.1,pp.64–92,1999. [54] S.AkellaandM.T.Mason,“Posingpolygonalobjectsintheplanebypushing,”The\nInternationalJournalofRoboticsResearch,vol.17,no.1,pp.70–88,1998. [55] M. T. Mason, “On the scope of quasi-static pushing,” inInternational Symposium\nonRoboticsResearch,1986,1986,pp.229–233.",
      "size": 812,
      "sentences": 6
    },
    {
      "id": 287,
      "content": "ing,”The\nInternationalJournalofRoboticsResearch,vol.17,no.1,pp.70–88,1998. [55] M. T. Mason, “On the scope of quasi-static pushing,” inInternational Symposium\nonRoboticsResearch,1986,1986,pp.229–233. [56] T.YoshikawaandM.Kurisu,“Indentificationofthecenteroffrictionfrompushing\nan object by a mobile robot,” in Proceedings IROS’91: IEEE/RSJ International\nWorkshoponIntelligentRobotsandSystems’91,IEEE,1991,pp.449–454. === 페이지 133 ===\n115\n[57] R. D. Howe and M. R. Cutkosky, “Practical force-motion models for sliding\nmanipulation,” The International Journal of Robotics Research, vol. 15, no. 6,\npp.557–572,1996. [58] J.Zhou,R.Paolini,J.A.Bagnell,andM.T.Mason,“Aconvexpolynomialforce-\nmotion model for planar sliding: Identification and application,” in 2016 IEEE\nInternationalConferenceonRoboticsandAutomation(ICRA),IEEE,2016,pp.372–\n377. [59] J.Zhou, M.T.Mason,R. Paolini,andD.",
      "size": 878,
      "sentences": 9
    },
    {
      "id": 288,
      "content": "rce-\nmotion model for planar sliding: Identification and application,” in 2016 IEEE\nInternationalConferenceonRoboticsandAutomation(ICRA),IEEE,2016,pp.372–\n377. [59] J.Zhou, M.T.Mason,R. Paolini,andD. Bagnell,“A convexpolynomialmodel for\nplanar sliding mechanics: Theory, application, and experimental validation,” The\nInternationalJournalofRoboticsResearch,vol.37,no.2-3,pp.249–265,2018. [60] J.Zhou,Y.Hou,andM.T.Mason,“Pushingrevisited:Differentialflatness,trajectory\nplanning,andstabilization,”TheInternationalJournalofRoboticsResearch,vol.38,\nno.12-13,pp.1477–1489,2019. [61] M. R. Dogar and S. S. Srinivasa, “A framework for push-grasping in clutter.,” in\nRobotics:Scienceandsystems,vol.2,2011. [62] C.Finn,I.Goodfellow,andS.Levine,“Unsupervisedlearningforphysicalinteraction\nthrough video prediction,” in Advances in neural information processing systems,\n2016,pp.64–72.",
      "size": 875,
      "sentences": 7
    },
    {
      "id": 289,
      "content": "andsystems,vol.2,2011. [62] C.Finn,I.Goodfellow,andS.Levine,“Unsupervisedlearningforphysicalinteraction\nthrough video prediction,” in Advances in neural information processing systems,\n2016,pp.64–72. [63] A.Byravanand D.Fox,“Se3-nets: Learning rigid bodymotion usingdeepneural\nnetworks,” in 2017 IEEE International Conference on Robotics and Automation\n(ICRA),IEEE,2017,pp.173–180. [64] N.Watters,D.Zoran, T. Weber,P.Battaglia,R.Pascanu,and A.Tacchetti,“Visual\ninteraction networks: Learning a physics simulator from video,” in Advances in\nneuralinformationprocessingsystems,2017,pp.4539–4547. [65] L.Sergey,N.Wagener,andP.Abbeel,“Learningcontact-richmanipulationskillswith\nguided policy search,” in Proceedings of the 2015 IEEE International Conference\nonRoboticsandAutomation(ICRA),Seattle,WA,USA,2015,pp.26–30. [66] S.Levine,C.Finn,T.Darrell,andP.Abbeel,“End-to-endtrainingofdeepvisuomotor\npolicies,”JournalofMachineLearningResearch,vol.17,no.39,pp.1–40,2016.",
      "size": 962,
      "sentences": 6
    },
    {
      "id": 290,
      "content": "sandAutomation(ICRA),Seattle,WA,USA,2015,pp.26–30. [66] S.Levine,C.Finn,T.Darrell,andP.Abbeel,“End-to-endtrainingofdeepvisuomotor\npolicies,”JournalofMachineLearningResearch,vol.17,no.39,pp.1–40,2016. [67] C.Finn andS.Levine, “Deepvisualforesightfor planningrobotmotion,”in2017\nIEEEInternationalConferenceonRoboticsandAutomation(ICRA),IEEE,2017,\npp.2786–2793. === 페이지 134 ===\n116\n[68] A. Ghadirzadeh, A. Maki, D. Kragic, and M. Bjo¨rkman, “Deep predictive policy\ntraining using reinforcement learning,” in 2017 IEEE/RSJ InternationalConference\nonIntelligentRobotsandSystems(IROS),IEEE,2017,pp.2351–2358. [69] M.R.Dogar and S.S.Srinivasa,“Push-graspingwith dexteroushands:Mechanics\nand a method,” in 2010 IEEE/RSJ International Conference on Intelligent Robots\nandSystems,IEEE,2010,pp.2123–2130. [70] M.R.Dogar,K.Hsiao,M.T.Ciocarlie,andS.S.Srinivasa,“Physics-basedgrasp\nplanningthroughclutter.,”inRobotics:Scienceandsystems,vol.8,2012,pp.57–64.",
      "size": 942,
      "sentences": 6
    },
    {
      "id": 291,
      "content": "elligent Robots\nandSystems,IEEE,2010,pp.2123–2130. [70] M.R.Dogar,K.Hsiao,M.T.Ciocarlie,andS.S.Srinivasa,“Physics-basedgrasp\nplanningthroughclutter.,”inRobotics:Scienceandsystems,vol.8,2012,pp.57–64. [71] J.E.King,M.Klingensmith,C.M.Dellin,etal.,“Pregraspmanipulationastrajectory\noptimization.,”inRobotics:ScienceandSystems,Berlin,2013. [72] L. Chang, J. R. Smith, and D. Fox, “Interactive singulation of objects from a pile,”\nin2012IEEEInternationalConferenceonRoboticsandAutomation,IEEE,2012,\npp.3875–3882. [73] A. Eitel, N. Hauff, and W. Burgard, “Learning to singulate objects using a push\nproposalnetwork,”inRoboticsResearch:The18thInternationalSymposiumISRR,\nSpringer,2020,pp.405–419. [74] M. Danielczuk, J. Mahler, C. Correa, and K. Goldberg, “Linear push policies\nto increase grasp access for robot bin picking,” in 2018 IEEE 14th international\nconferenceonautomationscienceandengineering(CASE),IEEE,2018,pp.1249–\n1256.",
      "size": 927,
      "sentences": 6
    },
    {
      "id": 292,
      "content": "rea, and K. Goldberg, “Linear push policies\nto increase grasp access for robot bin picking,” in 2018 IEEE 14th international\nconferenceonautomationscienceandengineering(CASE),IEEE,2018,pp.1249–\n1256. [75] B.Tang,M.Corsaro,G.Konidaris,S.Nikolaidis,andS.Tellex,“Learningcollabo-\nrativepushingandgraspingpoliciesindenseclutter,”in2021IEEEInternational\nConferenceonRoboticsandAutomation(ICRA),IEEE,2021,pp.6177–6184. [76] Y. Xiao, S. Katt, A. ten Pas, S. Chen, and C. Amato, “Online planning for target\nobjectsearchinclutterunderpartialobservability,”in2019InternationalConference\nonRoboticsandAutomation(ICRA),IEEE,2019,pp.8241–8247. [77] M.Danielczuk,A.Kurenkov,A.Balakrishna,etal.,“Mechanicalsearch:Multi-step\nretrievalofatargetobjectoccludedbyclutter,”in2019InternationalConferenceon\nRoboticsandAutomation(ICRA),IEEE,2019,pp.1614–1621. [78] A.Kurenkov,J.Taglic,R.Kulkarni,etal.,“Visuomotormechanicalsearch:Learning\nto retrieve target objects in clutter,” in IEEE/RSJ Int. Conference.",
      "size": 983,
      "sentences": 6
    },
    {
      "id": 293,
      "content": "RoboticsandAutomation(ICRA),IEEE,2019,pp.1614–1621. [78] A.Kurenkov,J.Taglic,R.Kulkarni,etal.,“Visuomotormechanicalsearch:Learning\nto retrieve target objects in clutter,” in IEEE/RSJ Int. Conference. on Intelligent\nRobotsandSystems(IROS),2020. [79] H.Song, J.A.Haustein, W.Yuan,et al.,“Multi-object rearrangementwithmonte\ncarlotreesearch:Acasestudyonplanarnonprehensilesorting,”in2020IEEE/RSJ\n=== 페이지 135 ===\n117\ninternational conference on intelligent robots and systems (IROS), IEEE, 2020,\npp.9433–9440. [80] S. D. Han, N. M. Stiffler, A. Krontiris, K. E. Bekris, and J. Yu, “High-quality\ntabletoprearrangementwithoverhandgrasps:Hardnessresultsandfastmethods,”in\nRobotics:SciencesandSystems,2017. [81] S.D.Han,N.M.Stiffler,A.Krontiris,K.E.Bekris,andJ.Yu,“Complexityresults\nand fast methods for optimal tabletop rearrangement with overhand grasps,” The\nInternationalJournalofRoboticsResearch,vol.37,no.13-14,pp.1775–1795,2018.",
      "size": 927,
      "sentences": 7
    },
    {
      "id": 294,
      "content": "rontiris,K.E.Bekris,andJ.Yu,“Complexityresults\nand fast methods for optimal tabletop rearrangement with overhand grasps,” The\nInternationalJournalofRoboticsResearch,vol.37,no.13-14,pp.1775–1795,2018. [82] K.Gao,S.W.Feng,andJ.Yu,“Onminimizingthenumberofrunningbuffersfor\ntabletoprearrangement,”inRobotics:SciencesandSystems,2021. [83] J. Yu, “Rearrangement on lattices with swaps: Optimality structures and efficient\nalgorithms,”inRobotics:SciencesandSystems,2021. [84] J. Yu, “Rearrangement on lattices with pick-n-swaps: Optimality structures and\nefficientalgorithms,”TheInternationalJournalofRoboticsResearch,vol.42,no.10,\npp.957–973,2023. [85] K.Gao,S.W.Feng,B.Huang,andJ.Yu,“Minimizingrunningbuffersfortabletopob-\njectrearrangement:Complexity,fastalgorithms,andapplications,”TheInternational\nJournalofRoboticsResearch,vol.42,no.10,pp.755–776,2023. [86] B.TangandG.S.Sukhatme,“Selectiveobjectrearrangementinclutter,”inConfer-\nenceonRobotLearning,PMLR,2023,pp.1001–1010. [87] J. Ahn,C.",
      "size": 987,
      "sentences": 7
    },
    {
      "id": 295,
      "content": "ational\nJournalofRoboticsResearch,vol.42,no.10,pp.755–776,2023. [86] B.TangandG.S.Sukhatme,“Selectiveobjectrearrangementinclutter,”inConfer-\nenceonRobotLearning,PMLR,2023,pp.1001–1010. [87] J. Ahn,C. Kim, and C.Nam, “Coordination oftwo robotic manipulatorsfor object\nretrieval inclutter,” in 2022 InternationalConference onRobotics andAutomation\n(ICRA),IEEE,2022,pp.1039–1045. [88] M.Moll,L.Kavraki,J.Rosell,etal.,“Randomizedphysics-basedmotionplanningfor\ngraspinginclutteredanduncertainenvironments,”IEEERoboticsandAutomation\nLetters,vol.3,no.2,pp.712–719,2017. [89] R. Wang, Y. Miao, and K. E. Bekris, “Efficient and high-quality prehensile rear-\nrangementinclutteredandconfinedspaces,”in2022InternationalConferenceon\nRoboticsandAutomation(ICRA),IEEE,2022,pp.1968–1975. [90] L.P.KaelblingandT.Lozano-Pe´rez,“Hierarchicaltaskandmotionplanninginthe\nnow,”in2011IEEEInternational Conferenceon RoboticsandAutomation,IEEE,\n2011,pp.1470–1477.",
      "size": 937,
      "sentences": 7
    },
    {
      "id": 296,
      "content": "ion(ICRA),IEEE,2022,pp.1968–1975. [90] L.P.KaelblingandT.Lozano-Pe´rez,“Hierarchicaltaskandmotionplanninginthe\nnow,”in2011IEEEInternational Conferenceon RoboticsandAutomation,IEEE,\n2011,pp.1470–1477. === 페이지 136 ===\n118\n[91] S.Srivastava,E.Fang,L.Riano,R.Chitnis,S.Russell,andP.Abbeel,“Combined\ntaskandmotionplanningthroughanextensibleplanner-independentinterfacelayer,”\nin2014 IEEEinternational conference on robotics andautomation (ICRA), IEEE,\n2014,pp.639–646. [92] M.Toussaint,“Logic-geometricprogramming:Anoptimization-basedapproachto\ncombinedtaskandmotionplanning.,”inIJCAI,2015,pp.1930–1936. [93] N. T. Dantam, Z. K. Kingston, S. Chaudhuri, and L. E. Kavraki, “Incremental\ntask and motion planning: A constraint-based approach.,” in Robotics: Science and\nsystems,AnnArbor,MI,USA,vol.12,2016,p.00052.",
      "size": 806,
      "sentences": 5
    },
    {
      "id": 297,
      "content": "T. Dantam, Z. K. Kingston, S. Chaudhuri, and L. E. Kavraki, “Incremental\ntask and motion planning: A constraint-based approach.,” in Robotics: Science and\nsystems,AnnArbor,MI,USA,vol.12,2016,p.00052. [94] C. R. Garrett, T. Lozano-Pe´rez, and L. P. Kaelbling, “Pddlstream: Integrating\nsymbolic planners and blackbox samplers via optimistic adaptive planning,” in\nProceedings of the international conference on automated planning and scheduling,\nvol.30,2020,pp.440–448. [95] T. Migimatsu and J. Bohg, “Object-centric task and motion planning in dynamic\nenvironments,” IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 844–851,\n2020. [96] R.Chitnis,D.Hadfield-Menell,A.Gupta,etal.,“Guidedsearchfortaskandmotion\nplans using learned heuristics,” in 2016 IEEE International Conference on Robotics\nandAutomation(ICRA),IEEE,2016,pp.447–454.",
      "size": 839,
      "sentences": 7
    },
    {
      "id": 298,
      "content": "6] R.Chitnis,D.Hadfield-Menell,A.Gupta,etal.,“Guidedsearchfortaskandmotion\nplans using learned heuristics,” in 2016 IEEE International Conference on Robotics\nandAutomation(ICRA),IEEE,2016,pp.447–454. [97] B.Kim,Z.Wang,L.P.Kaelbling,andT.Lozano-Pe´rez,“Learningtoguidetaskand\nmotion planning using score-space representation,” The International Journal of\nRoboticsResearch,vol.38,no.7,pp.793–812,2019. [98] D. Driess,J.-S. Ha,and M. Toussaint,“Deep visual reasoning:Learning topredict\nactionsequencesfortaskandmotionplanningfromaninitialsceneimage,”arXiv\npreprintarXiv:2006.05398,2020. [99] A.M.Wells,N.T.Dantam,A.Shrivastava,andL.E.Kavraki,“Learningfeasibilityfor\ntask and motion planning in tabletop environments,” IEEE robotics and automation\nletters,vol.4(2),pp.1255–1262,2019. [100] N.Dengler,D.Großklaus,andM.Bennewitz,“Learninggoal-orientednon-prehensile\npushing in cluttered scenes,” in 2022 IEEE/RSJ International Conference on Intelli-\ngentRobotsandSystems(IROS),IEEE,2022,pp.1116–1122.",
      "size": 995,
      "sentences": 5
    },
    {
      "id": 299,
      "content": ",D.Großklaus,andM.Bennewitz,“Learninggoal-orientednon-prehensile\npushing in cluttered scenes,” in 2022 IEEE/RSJ International Conference on Intelli-\ngentRobotsandSystems(IROS),IEEE,2022,pp.1116–1122. [101] K. He, G. Gkioxari, P. Dolla´r, and R. Girshick, “Mask r-cnn,” in Proceedings of the\nIEEEinternationalconferenceoncomputervision,2017,pp.2961–2969. === 페이지 137 ===\n119\n[102] T.Lin,P.Dolla´r,R.B.Girshick,K.He,B.Hariharan,andS.J.Belongie,“Feature\npyramid networks for object detection,” CoRR, vol. abs/1612.03144, 2016. arXiv:\n1612.03144. [103] L. Yen-Chen, A. Zeng, S. Song, P. Isola, and T.-Y. Lin, “Learning to see before\nlearningtoact:Visualpre-trainingformanipulation,”in2020IEEEInternational\nConferenceonRoboticsandAutomation(ICRA),IEEE,2020,pp.7286–7293. [104] E. Rohmer, S. P. Singh, and M. Freese, “V-rep: A versatile and scalable robot\nsimulationframework,”in2013IEEE/RSJInternationalConferenceonIntelligent\nRobotsandSystems,IEEE,2013,pp.1321–1326.",
      "size": 962,
      "sentences": 7
    },
    {
      "id": 300,
      "content": "3. [104] E. Rohmer, S. P. Singh, and M. Freese, “V-rep: A versatile and scalable robot\nsimulationframework,”in2013IEEE/RSJInternationalConferenceonIntelligent\nRobotsandSystems,IEEE,2013,pp.1321–1326. [105] D.Hafner,T.Lillicrap,J.Ba,andM.Norouzi,“Dreamtocontrol:Learningbehaviors\nby latent imagination,” in International Conference on Learning Representations,\n2020. [106] F. Ebert, C. Finn, S. Dasari, A. Xie, A. X. Lee, and S. Levine, “Visual foresight:\nModel-baseddeepreinforcementlearningforvision-basedroboticcontrol,”CoRR,\nvol.abs/1812.00568,2018.arXiv:1812.00568. [107] Muhayyuddin, M. Moll, L. Kavraki, and J. Rosell, “Randomized physics-based\nmotion planning for grasping in cluttered and uncertain environments,” IEEE\nRoboticsandAutomationLetters,vol.3,no.2,pp.712–719,Apr.2018. [108] V. Mnih, K. Kavukcuoglu, D. Silver, et al., “Human-level control through deep\nreinforcementlearning,”nature,vol.518,no.7540,pp.529–533,2015.",
      "size": 934,
      "sentences": 6
    },
    {
      "id": 301,
      "content": "andAutomationLetters,vol.3,no.2,pp.712–719,Apr.2018. [108] V. Mnih, K. Kavukcuoglu, D. Silver, et al., “Human-level control through deep\nreinforcementlearning,”nature,vol.518,no.7540,pp.529–533,2015. [109] M.Andrychowicz,F.Wolski,A.Ray,etal.,“Hindsightexperiencereplay,”Advances\ninneuralinformationprocessingsystems,vol.30,2017. [110] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimagerecognition,”\ninProceedingsof theIEEEconferenceon computervision andpattern recognition,\n2016,pp.770–778. [111] C.B.Browne,E.Powley,D.Whitehouse,etal.,“Asurveyofmontecarlotreesearch\nmethods,”IEEETransactionsonComputationalIntelligenceandAIinGames,vol.4,\nno.1,pp.1–43,2012. [112] I.LoshchilovandF.Hutter,“Sgdr:Stochasticgradientdescentwithwarmrestarts,”\narXivpreprintarXiv:1608.03983,2016. [113] M. Andrychowicz, F. Wolski, A. Ray, et al., “Hindsight experience replay,” in\nAdvances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S.\nBengio,etal.,Eds.,vol.30,2017.",
      "size": 974,
      "sentences": 7
    },
    {
      "id": 302,
      "content": "2016. [113] M. Andrychowicz, F. Wolski, A. Ray, et al., “Hindsight experience replay,” in\nAdvances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S.\nBengio,etal.,Eds.,vol.30,2017. === 페이지 138 ===\n120\n[114] E.CoumansandY.Bai,Pybullet,apythonmoduleforphysicssimulationforgames,\nroboticsandmachinelearning,http://pybullet.org,2016–2019. [115] J. Schrittwieser, I. Antonoglou, T. Hubert, et al., “Mastering atari, go, chess and\nshogibyplanningwithalearnedmodel,”Nature,vol.588,no.7839,pp.604–609,\n2020. [116] D.Kahneman,Thinking,fastandslow.Macmillan,2011. [117] Y.Bengio,“Theconsciousnessprior,”arXivpreprintarXiv:1709.08568,2017. [118] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction.MITpress,\n2018. [119] D. Silver, T. Hubert, J. Schrittwieser, et al., “A general reinforcement learning\nalgorithm that masters chess, shogi, and go through self-play,” Science, vol. 362,\nno.6419,pp.1140–1144,2018. [120] H. Song, J.",
      "size": 945,
      "sentences": 10
    },
    {
      "id": 303,
      "content": "T. Hubert, J. Schrittwieser, et al., “A general reinforcement learning\nalgorithm that masters chess, shogi, and go through self-play,” Science, vol. 362,\nno.6419,pp.1140–1144,2018. [120] H. Song, J. A. Haustein, W. Yuan, et al., “Multi-object rearrangement with\nmonte carlo tree search: A case study on planar nonprehensile sorting,” CoRR,\nvol.abs/1912.07024,2019.arXiv:1912.07024. [121] R. Boney, N. Di Palo, M. Berglund, et al., “Regularizing trajectory optimization\nwith denoising autoencoders,” Advances in Neural Information Processing Systems,\nvol.32,pp.2859–2869,2019. [122] R.Coulom,“Efficientselectivityandbackupoperatorsinmonte-carlotreesearch,”\ninInternationalconferenceoncomputersandgames,Springer,2006,pp.72–83. [123] A.Paszke,S.Gross,F.Massa,etal.,“Pytorch:Animperativestyle,high-performance\ndeeplearninglibrary,”in Advances inNeuralInformationProcessing Systems 32,\nH.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alche´-Buc,E.Fox,andR.Garnett,\nEds.,CurranAssociates,Inc.,2019,pp.8024–8035.",
      "size": 998,
      "sentences": 7
    },
    {
      "id": 304,
      "content": "rmance\ndeeplearninglibrary,”in Advances inNeuralInformationProcessing Systems 32,\nH.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alche´-Buc,E.Fox,andR.Garnett,\nEds.,CurranAssociates,Inc.,2019,pp.8024–8035. [124] J.B.Hamrick,V.Bapst,A.Sanchez-Gonzalez,etal.,“Combiningq-learningand\nsearch with amortized value estimates,” in International Conference on Learning\nRepresentationsICLR,2019. [125] E.CoumansandY.Bai,Pybullet,apythonmoduleforphysicssimulationforgames,\nroboticsandmachinelearning,http://pybullet.org,2016–2021. [126] B.Wen,C.Mitash,B.Ren,andK.E.Bekris,“Se(3)-tracknet:Data-driven6dpose\ntracking by calibrating image residuals in synthetic domains,” in 2020 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), IEEE, 2020,\npp.10367–10373.",
      "size": 764,
      "sentences": 4
    },
    {
      "id": 305,
      "content": "acknet:Data-driven6dpose\ntracking by calibrating image residuals in synthetic domains,” in 2020 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), IEEE, 2020,\npp.10367–10373. === 페이지 139 ===\n121\n[127] B. Wen and K. Bekris, “Bundletrack: 6d pose tracking for novel objects without\ninstance orcategory-level3d models,”in 2021 IEEE/RSJInternational Conference\nonIntelligentRobotsandSystems(IROS),IEEE,2021,pp.8067–8074. [128] C.Mitash,B.Wen,K.Bekris,andA.Boularias,“Scene-levelposeestimationfor\nmultiple instances of densely packed objects,” in Conference on Robot Learning,\nPMLR,2020,pp.1133–1145. [129] X.B.Peng,E.Coumans,T.Zhang,T.-W.E.Lee,J.Tan,andS.Levine,“Learning\nagile robotic locomotion skills by imitating animals,” in Robotics: Science and\nSystems,Jul.2020. [130] J.Hwangbo,J.Lee,A.Dosovitskiy,etal.,“Learningagileanddynamicmotorskills\nforleggedrobots,”ScienceRobotics,vol.4,no.26,eaau5872,2019.",
      "size": 928,
      "sentences": 5
    },
    {
      "id": 306,
      "content": "mitating animals,” in Robotics: Science and\nSystems,Jul.2020. [130] J.Hwangbo,J.Lee,A.Dosovitskiy,etal.,“Learningagileanddynamicmotorskills\nforleggedrobots,”ScienceRobotics,vol.4,no.26,eaau5872,2019. [131] A. Kumar, Z. Fu, D. Pathak, and J. Malik, “RMA: Rapid Motor Adaptation for\nLegged Robots,” in Proceedings of Robotics: Science and Systems, Virtual, Jul. 2021. [132] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassificationwithdeep\nconvolutional neural networks,” Advances in neural information processing systems,\nvol.25,2012. [133] V.Mnih,K.Kavukcuoglu,D.Silver,etal.,“Playingatariwithdeepreinforcement\nlearning,”arXivpreprintarXiv:1312.5602,2013. [134] E. Todorov, T. Erez, and Y. Tassa, “MuJoCo: A physics engine for model-based\ncontrol,” in 2012 IEEE/RSJ international conference on intelligent robots and\nsystems,IEEE,2012,pp.5026–5033.",
      "size": 853,
      "sentences": 7
    },
    {
      "id": 307,
      "content": ",2013. [134] E. Todorov, T. Erez, and Y. Tassa, “MuJoCo: A physics engine for model-based\ncontrol,” in 2012 IEEE/RSJ international conference on intelligent robots and\nsystems,IEEE,2012,pp.5026–5033. [135] V. Makoviychuk, L. Wawrzyniak, Y. Guo, et al., “Isaac gym: High performance\ngpu-basedphysicssimulationforrobotlearning,”arXivpreprintarXiv:2108.10470,\n2021. [136] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem, Brax -\nadifferentiablephysicsengineforlargescalerigidbodysimulation,version0.0.10,\n2021. [137] D.Silver,T.Hubert,J.Schrittwieser,etal.,“Masteringchessandshogibyself-play\nwitha general reinforcementlearningalgorithm,”arXivpreprintarXiv:1712.01815,\n2017. [138] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of the multiarmed\nbanditproblem,”Machinelearning,vol.47,no.2,pp.235–256,2002.",
      "size": 844,
      "sentences": 6
    },
    {
      "id": 308,
      "content": "ningalgorithm,”arXivpreprintarXiv:1712.01815,\n2017. [138] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of the multiarmed\nbanditproblem,”Machinelearning,vol.47,no.2,pp.235–256,2002. === 페이지 140 ===\n122\n[139] L. Kocsis and C. Szepesva´ri, “Bandit based monte-carlo planning,” in European\nconferenceonmachinelearning,Springer,2006,pp.282–293. [140] M. Tan andQ. Le,“Efficientnet: Rethinkingmodel scalingfor convolutional neural\nnetworks,”inInternationalconferenceonmachinelearning,PMLR,2019,pp.6105–\n6114. [141] G.M.-B.Chaslot,M.H.Winands,andH.Herik,“Parallelmonte-carlotreesearch,”\ninInternationalConferenceonComputersandGames,Springer,2008,pp.60–71. [142] A.Liu,J.Chen,M.Yu,Y.Zhai,X.Zhou,andJ.Liu,“Watchtheunobserved:Asimple\napproachtoparallelizingmontecarlotreesearch,”inInternationalConferenceon\nLearningRepresentations,2020.",
      "size": 845,
      "sentences": 7
    },
    {
      "id": 309,
      "content": "ringer,2008,pp.60–71. [142] A.Liu,J.Chen,M.Yu,Y.Zhai,X.Zhou,andJ.Liu,“Watchtheunobserved:Asimple\napproachtoparallelizingmontecarlotreesearch,”inInternationalConferenceon\nLearningRepresentations,2020. [143] X. Yang, T. Aasawat, and K. Yoshizoe, “Practical massively parallel monte-carlo\ntree searchapplied tomolecular design,” inInternational Conferenceon Learning\nRepresentations,2021. [144] K.P.Hawkins,“Analyticinversekinematicsfortheuniversalrobotsur-5/ur-10arms,”\nGeorgiaInstituteofTechnology,Tech.Rep.,2013. [145] S.W.Feng,T.Guo,K.E.Bekris,andJ.Yu,“Teamrubot’sexperiencesandlessons\nfromtheariac,”Roboticsandcomputer-integratedmanufacturing,vol.70,p.102126,\n2021. [146] D. Silver, A. Huang, C. J. Maddison, et al., “Mastering the game of go with deep\nneuralnetworksandtreesearch,”nature,vol.529,no.7587,pp.484–489,2016.",
      "size": 823,
      "sentences": 6
    },
    {
      "id": 310,
      "content": "egratedmanufacturing,vol.70,p.102126,\n2021. [146] D. Silver, A. Huang, C. J. Maddison, et al., “Mastering the game of go with deep\nneuralnetworksandtreesearch,”nature,vol.529,no.7587,pp.484–489,2016. [147] Y.Labbe´,S.Zagoruyko,I.Kalevatykh,etal.,“Monte-carlotreesearchforefficient\nvisually guided rearrangement planning,” IEEE Robotics and Automation Letters,\nvol.5,no.2,pp.3715–3722,2020. [148] K. Gao, J. Yu, T. S. Punjabi, and J. Yu, “Effectively rearranging heterogeneous\nobjects on cluttered tabletops,” in 2023 IEEE/RSJ International Conference on\nIntelligentRobotsandSystems(IROS),IEEE,2023,pp.2057–2064. [149] J. Kuffner and S. LaValle, “Rrt-connect: An efficient approach to single-query\npath planning,” in Proceedings 2000 ICRA. Millennium Conference. IEEE Inter-\nnational Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),vol.2,2000,995–1001vol.2.",
      "size": 888,
      "sentences": 9
    },
    {
      "id": 311,
      "content": "e-query\npath planning,” in Proceedings 2000 ICRA. Millennium Conference. IEEE Inter-\nnational Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),vol.2,2000,995–1001vol.2. [150] R.BohlinandL.E.Kavraki,“Arandomizedapproachtorobotpathplanningbased\nonlazyevaluation,”COMBINATORIALOPTIMIZATION-DORDRECHT-,vol.9,\nno.1,pp.221–249,2001. === 페이지 141 ===\n123\n[151] F.Bai,F.Meng,J.Liu,J.Wang,andM.Q.-H.Meng,“Hierarchicalpolicywithdeep-\nreinforcement learning for nonprehensile multiobject rearrangement,” Biomimetic\nIntelligenceandRobotics,vol.2,no.3,p.100047,2022. [152] E.CoumansandY.Bai,“Pybullet,apythonmoduleforphysicssimulationforgames,\nroboticsandmachinelearning,”2016. [153] A. Kirillov, E. Mintun, N. Ravi, et al., “Segment anything,” in Proceedings of the\nIEEE/CVFinternationalconferenceoncomputervision,2023,pp.4015–4026. [154] G.Bradski,“TheOpenCVLibrary,”Dr.Dobb’sJournalofSoftwareTools,2000.",
      "size": 923,
      "sentences": 10
    }
  ]
}