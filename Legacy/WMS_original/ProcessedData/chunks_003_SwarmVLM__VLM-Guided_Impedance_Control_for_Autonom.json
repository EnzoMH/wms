{
  "source": "ArXiv",
  "filename": "003_SwarmVLM__VLM-Guided_Impedance_Control_for_Autonom.pdf",
  "total_chars": 27979,
  "total_chunks": 41,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nSwarmVLM: VLM-Guided Impedance Control for Autonomous\nNavigation of Heterogeneous Robots in Dynamic Warehousing\nMalaika Zafar, Roohan Ahmed Khan*, Faryal Batool*, Yasheerah Yaqoot*, Ziang Guo,\nMikhail Litvinov, Aleksey Fedoseev, and Dzmitry Tsetserukou\nAbstract—With the growing demand for efficient logistics,\nunmanned aerial vehicles (UAVs) are increasingly being paired\nwith automated guided vehicles (AGVs). While UAVs offer the\nability to navigate through dense environments and varying\naltitudes, they are limited by battery life, payload capacity,\nand flight duration, necessitating coordinated ground support. Focusing on heterogeneous navigation, SwarmVLM ad-\ndressestheselimitationsbyenablingsemanticcollaborationbe-\ntweenUAVsandgroundrobotsthroughimpedancecontrol.The\nsystem leverages the Vision Language Model (VLM) and the\nRetrieval-Augmented Generation (RAG) to adjust impedance\ncontrol parameters in response to environmental changes.",
      "size": 963,
      "sentences": 3
    },
    {
      "id": 2,
      "content": "sthroughimpedancecontrol.The\nsystem leverages the Vision Language Model (VLM) and the\nRetrieval-Augmented Generation (RAG) to adjust impedance\ncontrol parameters in response to environmental changes. In\nthis framework, the UAV acts as a leader using Artificial\nPotential Field (APF) planning for real-time navigation, while\nthe ground robot follows via virtual impedance links with\nadaptive link topology to avoid collisions with short obstacles. Thesystemdemonstrateda92%successrateacross12real-\nworldtrials.Underoptimallightingconditions,theVLM-RAG\nframework achieved 80% accuracy in object detection and\nselectionofimpedanceparameters.Themobilerobotprioritized\nshort obstacle avoidance, occasionally resulting in a lateral\ndeviation of up to 50 cm from the UAV path, which showcases\nsafe navigation in a cluttered setting. Video of SwarmVLM: https://youtu.be/IdlUhQfz8w0\nKeywords: Vision Language Model (VLM), Retrieval\nFig.1.",
      "size": 929,
      "sentences": 4
    },
    {
      "id": 3,
      "content": "ation of up to 50 cm from the UAV path, which showcases\nsafe navigation in a cluttered setting. Video of SwarmVLM: https://youtu.be/IdlUhQfz8w0\nKeywords: Vision Language Model (VLM), Retrieval\nFig.1. Frameworkforadaptiveswarmnavigation.Thesystemprocesses\nAugmented Generation (RAG), Heterogeneous Robots, a top-down view of the environment through the VLM-RAG system to\nPath Planning, Artificial Potential Field, Impedance Con- determineimpedanceparametersbasedonthearrangementsandnumberof\nobstaclesintheenvironment.Experimentswerealsoperformedinthereal-\ntrol\nworld environment to evaluate the system’s robustness and generalization\ncapabilities. I.",
      "size": 649,
      "sentences": 4
    },
    {
      "id": 4,
      "content": "metersbasedonthearrangementsandnumberof\nobstaclesintheenvironment.Experimentswerealsoperformedinthereal-\ntrol\nworld environment to evaluate the system’s robustness and generalization\ncapabilities. I. INTRODUCTION\nTo enable such heterogeneous cooperation, we propose\nAdvancements in automation and artificial intelligence\na leader-follower framework called SwarmVLM, where\nhavesignificantlyimprovedlogisticsandwarehousemanage-\nthe drone leads using an Artificial Potential Field (APF)-\nment by enabling autonomous systems that reduce human\nbased planner, and the mobile robot follows via virtual\nerror and improve operational efficiency [1]. Automated\nimpedance linkages [4]. The mobile robot also forms tem-\nguidedvehicles(AGVs)andmobilerobotshavetraditionally\nporary impedance links with short obstacles undetectable\ndominated indoor and last-mile delivery due to their reliable\nby the drone, while a custom PID controller minimizes its\nlocalization and high payload capacity [2].",
      "size": 981,
      "sentences": 5
    },
    {
      "id": 5,
      "content": "s with short obstacles undetectable\ndominated indoor and last-mile delivery due to their reliable\nby the drone, while a custom PID controller minimizes its\nlocalization and high payload capacity [2]. Unmanned aerial\npath deviation. This architecture ensures reliable operation\nvehicles(UAVs),incontrast,offerfastandflexiblemovement\nin dense and dynamic environments. To further enhance\ninthree-dimensionalspace,makingthemidealfornavigating\nadaptability, we integrate a Vision-Language Model (VLM)\nover obstacles and through constrained environments. For\nwith a Retrieval-Augmented Generation (RAG) framework. instance,Cristianietal. [3]proposedamini-droneswarmfor\nThis VLM-RAG module interprets top-down environmental\ninventory tracking. However, UAVs are limited by payload,\nviews and retrieves context-specific impedance parameters\nflighttime,andsensingrange.IntegratingUAVswithground\nfrom a scenario database.",
      "size": 912,
      "sentences": 8
    },
    {
      "id": 6,
      "content": "ntal\ninventory tracking. However, UAVs are limited by payload,\nviews and retrieves context-specific impedance parameters\nflighttime,andsensingrange.IntegratingUAVswithground\nfrom a scenario database. It enables both agents to adjust\nrobots enables a hybrid system that combines aerial agility\ntheir impedance settings based on obstacle arrangement and\nwith ground-level stability and endurance. proximity, ensuring safe and intelligent navigation. SwarmVLM introduces several key contributions:\nTheauthorsarewiththeIntelligentSpaceRoboticsLaboratory,Skolkovo\nInstituteofScienceandTechnology,Moscow,Russia. {malaika.zafar, • An impedance-based coordination mechanism for\nroohan.khan, faryal.batool, yasheerah.yaqoot, drone–robot cooperation in cluttered environments. ziang.guo, mikhail.litvinov, aleksey.fedoseev,\nd.tsetserukou}@skoltech.ru • Integration of the VLM-RAG framework for un-\n*Theseauthorscontributedequallytothiswork.",
      "size": 930,
      "sentences": 7
    },
    {
      "id": 7,
      "content": "peration in cluttered environments. ziang.guo, mikhail.litvinov, aleksey.fedoseev,\nd.tsetserukou}@skoltech.ru • Integration of the VLM-RAG framework for un-\n*Theseauthorscontributedequallytothiswork. derstanding environmental configuration and adaptive\n5202\nguA\n11\n]OR.sc[\n1v41870.8052:viXra\n=== 페이지 2 ===\nimpedance tuning. mobility. While effective in structured spaces, such systems\n• Real-world validation of the system in dynamic indoor lack aerial reach and adaptability in cluttered or hard-to-\nenvironments. access environments. Further demonstrating the benefits of aerial-ground col-\nII. RELATEDWORK\nlaboration, Salas et al. [16] presented a UAV-AGV system\nLogistics and warehouse management have become criti- forsearchandrescue.TheirUAVprovidedaerialpathfinding\ncalcomponentsofmodernsupplychains,drivingthedemand usingamonocularcamera,whilethegroundrobotperformed\nfor more efficient and autonomous delivery solutions.",
      "size": 928,
      "sentences": 9
    },
    {
      "id": 8,
      "content": "andrescue.TheirUAVprovidedaerialpathfinding\ncalcomponentsofmodernsupplychains,drivingthedemand usingamonocularcamera,whilethegroundrobotperformed\nfor more efficient and autonomous delivery solutions. In close-range inspection and mapping, showing the value of\nresponse, both homogeneous and heterogeneous swarms of layered autonomy in unstructured terrains. robotic agents have been explored to meet these challenges. Paralleltothesedevelopments,vision-basedlearningmod-\nManaging large UAV swarms, however, presents signif- els, particularly Vision Transformers (ViTs), have shown\nicant difficulties in coordination, stability, and real-time promiseinroboticperceptionanddecision-making.Dosovit-\ndecision-making. To address this, several studies have intro- skiyetal. [17]demonstratedtheeffectivenessoftransformers\nduced human-in-the-loop control approaches. For instance, for image recognition, paving the way for their use in aerial\nAbdi et al.",
      "size": 946,
      "sentences": 7
    },
    {
      "id": 9,
      "content": "ave intro- skiyetal. [17]demonstratedtheeffectivenessoftransformers\nduced human-in-the-loop control approaches. For instance, for image recognition, paving the way for their use in aerial\nAbdi et al. [5] leveraged EMG signals to enable gesture- robotics for tasks like object detection and spatial under-\nbasedswarmcontrolviamuscleactivity,whileKhenetal. [6] standing. More recently, Brohan et al. [18] introduced RT-1,\ncombinedgesturerecognitionwithmachinelearningforintu- atask-conditionedtransformercapableofgeneralizingacross\nitive drone manipulation through natural human movements. diverse robotic control tasks in real-world environments. Despite these advances, achieving fully autonomous and Multimodal transformers such as Molmo-7B-D [19] in-\nreliable swarm navigation in complex and dynamic envi- tegrate visual, spatial, and linguistic inputs, enabling rich\nronments remains a major challenge. Many existing UAV contextual awareness for autonomous systems.",
      "size": 968,
      "sentences": 10
    },
    {
      "id": 10,
      "content": "avigation in complex and dynamic envi- tegrate visual, spatial, and linguistic inputs, enabling rich\nronments remains a major challenge. Many existing UAV contextual awareness for autonomous systems. These archi-\nsystems still struggle with robust perception, real-time adap- tectures are especially useful for coordination and decision-\ntation, and decentralized coordination without human over- making in cluttered or uncertain environments. For instance,\nsight.Amongthemostwidelyusedmethodsforautonomous FlockGPT [20] introduced a generative AI interface that\nnavigation is the Artificial Potential Field (APF) technique, allows users to control drone flocks using natural language,\nappreciated for its simplicity and reactive obstacle avoid- achieving high accuracy and strong usability for dynamic\nance capabilities. Batinovic et al. [7] integrated APF with shape formation.",
      "size": 879,
      "sentences": 6
    },
    {
      "id": 11,
      "content": "uage,\nappreciated for its simplicity and reactive obstacle avoid- achieving high accuracy and strong usability for dynamic\nance capabilities. Batinovic et al. [7] integrated APF with shape formation. LiDAR for real-time path planning in unstructured envi-\nBuilding on these foundations, our research proposes a\nronments, demonstrating its suitability for navigating dense\nnovelheterogeneousswarmsystemthatcombinesAPF-based\nand dynamic spaces. Yu et al. [8] proposed a distributed\naerial navigation, impedance-guided ground mobility, and a\nalgorithmthatcouplesAPFwithvirtualleaderformationand\nVLM-RAG-powered framework for adaptive, context-aware\naswitchingcommunicationtopologytoensurerobustswarm\nbehavior. Inspired by prior work on HetSwarm [21] and\nbehavior. ImpedanceGPT [22], this study introduces a new agile and\nTo improve swarm cohesion and control, impedance-\nsafe path planning solution for drone–robot teams operating\nbased strategies have been introduced. Tsykunov et al.",
      "size": 982,
      "sentences": 9
    },
    {
      "id": 12,
      "content": "s study introduces a new agile and\nTo improve swarm cohesion and control, impedance-\nsafe path planning solution for drone–robot teams operating\nbased strategies have been introduced. Tsykunov et al. [9]\nin dynamic and cluttered environments. developed SwarmTouch, which utilized virtual impedance\nlinks [4] for drone swarm coordination. Building on this,\nIII. SWARMVLMTECHNOLOGY\nFedoseev et al. [10] analyzed the influence of impedance\ntopologiesonswarmstability.Khanetal. [11]latercombined The proposed methodology, illustrated in Fig. 2, consists\nAPF-based leader planning with impedance-based formation of two primary components: a VLM-RAG system for esti-\ncontrol,buttheirapproachremainedlimitedtostaticenviron- matingimpedanceparametersandahigh-levelcontrolframe-\nments and did not address challenges like energy efficiency work that integrates APF planner with impedance-based\nor long-term operation. coordination.",
      "size": 921,
      "sentences": 10
    },
    {
      "id": 13,
      "content": "gimpedanceparametersandahigh-levelcontrolframe-\nments and did not address challenges like energy efficiency work that integrates APF planner with impedance-based\nor long-term operation. coordination. The system enables collaboration between a\nTo overcome the limitations of aerial-only or ground-only drone and a mobile robot in dynamic environments. The\nsystems,researchhasincreasinglyfocusedonheterogeneous droneusestheAPFplannertocomputeandupdateitstrajec-\nswarm architectures. Darush et al. [12] employed virtual tory in real-time, focusing on global navigation and obstacle\nimpedance links between a leader octocopter and micro- avoidance. The mobile robot follows this trajectory through\ndrones, while [13] used reinforcement learning to facilitate virtual impedance links, which help maintain formation and\ndocking, transport, and in-air recharging. Chen et al.",
      "size": 868,
      "sentences": 8
    },
    {
      "id": 14,
      "content": "this trajectory through\ndrones, while [13] used reinforcement learning to facilitate virtual impedance links, which help maintain formation and\ndocking, transport, and in-air recharging. Chen et al. [14] allow local navigation around small obstacles not perceived\nproposed a UAV-AGV system for collaborative exploration by the drone. Additionally, the robot can act as a mobile\ninhazardousareas,highlightingtheadvantagesofcombining landing or recharging platform for the drone. agentswithcomplementarycapabilitiestoenhanceflexibility The methodology was first validated in the Gym PyBullet\nand robustness. [23] simulation environment using custom PID controllers\nGroundrobotsalsoplayacrucialroleinwarehouselogis- for both agents to ensure accurate and coordinated path\ntics.Malopolskietal. [15]introducedanautonomousmobile following.",
      "size": 833,
      "sentences": 7
    },
    {
      "id": 15,
      "content": "using custom PID controllers\nGroundrobotsalsoplayacrucialroleinwarehouselogis- for both agents to ensure accurate and coordinated path\ntics.Malopolskietal. [15]introducedanautonomousmobile following. Real-world experiments were then conducted,\nrobot equipped with a hybrid drive system for navigating demonstrating effective drone–robot collaboration in clut-\nflat and rail surfaces, and an integrated elevator for vertical tered environments. Communication between the drone and\n=== 페이지 3 ===\nFig.2. SystemarchitectureoftheproposedSwarmVLM,integratingaVLM-RAGmoduleforimpedanceestimationwithahigh-levelcontrolsystemthat\ncombinesAPF-basedpathplanningandimpedancecontrolforheterogeneousnavigation. the mobile robot is handled via ROS, enabling synchronized k are virtual mass, damping, and stiffness; and F (t) is the\next\nbehavior through a shared information framework. virtual force from the drone. To handle short obstacles undetected by the drone, the\nA.",
      "size": 957,
      "sentences": 8
    },
    {
      "id": 16,
      "content": "are virtual mass, damping, and stiffness; and F (t) is the\next\nbehavior through a shared information framework. virtual force from the drone. To handle short obstacles undetected by the drone, the\nA. Artificial Potential Fields for Global Path Generation\nmobile robot temporarily disengages from the drone and\nInorderfortheleaderdronetonavigateefficientlyaround formslocalimpedancelinkswiththeobstacles.Theresulting\nthe obstacles while setting the path toward the goal, we repulsive displacement is given by:\napplied the APF planning algorithm [24]. This algorithm\nallows the UAV to continuously update its trajectory in\n∆x =k ·r , (3)\nrobot impF imp\nresponse to shifting obstacles, enabling robust navigation in\ncluttered and dynamic environments. The equations for the where k is the velocity-dependent force coefficient and\nimpF\nAPF planner are as follows [11]: r defines the influence radius of the obstacle.",
      "size": 912,
      "sentences": 6
    },
    {
      "id": 17,
      "content": "cluttered and dynamic environments. The equations for the where k is the velocity-dependent force coefficient and\nimpF\nAPF planner are as follows [11]: r defines the influence radius of the obstacle. This mech-\nimp\nanism enables timely, collision-free navigation in cluttered\nF =F +F , (1)\ntotal attraction repulsion environments. This equation ensures a collision-free trajec-\nwhere tory by applying a repulsive displacement proportional to\nthe obstacle’s influence radius r , redirecting the robot\nimp\naway from potential collisions. The coefficient k adjusts\nF (d )=k ·d , impF\nattraction g att g the strength of this deflection based on the robot’s velocity,\n(cid:40)\n0 if d o >d safe ensuring timely and stable avoidance.",
      "size": 726,
      "sentences": 5
    },
    {
      "id": 18,
      "content": "lisions. The coefficient k adjusts\nF (d )=k ·d , impF\nattraction g att g the strength of this deflection based on the robot’s velocity,\n(cid:40)\n0 if d o >d safe ensuring timely and stable avoidance. F (d )= (cid:16) (cid:17)\nrepulsion o k · 1 − 1 if d ≤d ,\nrep do dsafe o safe C. VLM-RAG System\nwhere d g and d o are the distances from the drone to the The VLM-RAG system processes a top-down visual view\ngoal and to the obstacle, respectively, k att and k rep are the oftheenvironmentusingtheMolmo-7B-DBnB4-bitmodel\nattraction and repulsion coefficients, respectively. to extract key features such as the number and spatial distri-\nbution of obstacles. These features are converted into vector\nB. Impedance Controller\nrepresentationsandpassedtotheRetrieval-AugmentedGen-\nTo enable smooth coordination, a virtual impedance con-\neration (RAG) system, which retrieves optimal impedance\ntroller couples the drone and the mobile robot.",
      "size": 932,
      "sentences": 5
    },
    {
      "id": 19,
      "content": "nsandpassedtotheRetrieval-AugmentedGen-\nTo enable smooth coordination, a virtual impedance con-\neration (RAG) system, which retrieves optimal impedance\ntroller couples the drone and the mobile robot. The mobile\ncontrol parameters — namely, virtual mass (m), virtual\nrobot acts as a follower, linked to the drone’s APF-based\nstiffness (k), virtual damping (d), and the impedance force\ntrajectorythroughamassspring dampersystemthatensures\ncoefficient (F). Communication between the VLM-RAG\nstableformationtracking.Thisdynamiccouplingisgoverned\nmodule,runningontheserver,andtheheterogeneousswarm\nby:\nsystem is handled through the ROS framework.",
      "size": 641,
      "sentences": 3
    },
    {
      "id": 20,
      "content": "icient (F). Communication between the VLM-RAG\nstableformationtracking.Thisdynamiccouplingisgoverned\nmodule,runningontheserver,andtheheterogeneousswarm\nby:\nsystem is handled through the ROS framework. 1) IntegratingVLMforObstacleIdentificationandSpatial\nm∆x¨+d∆x˙ +k∆x=F (t), (2)\next Analysis\nwhere ∆x, ∆x˙, and ∆x¨ represent deviations in position, The Vision-Language Model (VLM) uses visual input\nvelocity, and acceleration from the desired state; m, d, and from a ceiling-mounted camera to semantically analyze the\n=== 페이지 4 ===\nenvironment by detecting, and localizing obstacles. For ef- the drone and mobile robot. The setup supports real-time\nficient real-time performance, the lightweight Molmo-7B-D visualization and allows dynamic repositioning of obstacles\nBnB 4-bit model [19] is employed. The extracted obstacle for flexible testing.",
      "size": 845,
      "sentences": 6
    },
    {
      "id": 21,
      "content": "me\nficient real-time performance, the lightweight Molmo-7B-D visualization and allows dynamic repositioning of obstacles\nBnB 4-bit model [19] is employed. The extracted obstacle for flexible testing. The drone navigates using an Artificial\ninformation is then forwarded to the RAG system, which PotentialField(APF)planner,whilethemobilerobotfollows\nuses the spatial configuration to retrieve suitable impedance thedrone’strajectorythroughimpedance-basedcoordination. control parameters, thereby enhancing swarm navigation in\nboth static and dynamic environments. 2) Custom Database for multiple environmental cases\nA custom database of six environmental scenarios was\ncreated to support the RAG system. Each scenario repre-\nsents an indoor space with different obstacle quantities and\nspatialarrangements.Thedatabasestoresempiricallyderived\noptimal impedance parameters obtained through simulation\nexperiments using the APF planner for drone navigation.",
      "size": 953,
      "sentences": 6
    },
    {
      "id": 22,
      "content": "ferent obstacle quantities and\nspatialarrangements.Thedatabasestoresempiricallyderived\noptimal impedance parameters obtained through simulation\nexperiments using the APF planner for drone navigation. Table I summarizes the parameters for all six cases. TABLEI\nDATABASECONTAINSTHEOPTIMALIMPEDANCEPARAMETERS,\nINCLUDINGVIRTUALMASS(M),VIRTUALSTIFFNESS(K),VIRTUAL Fig.3. ExperimentalsetupintheGymPybulletenvironmentshowingthe\ntrajectoryofthedroneandmobilerobotunderadenseenvironment.Where\nDAMPING(D),ANDIMPEDANCEORDEFLECTIONFORCECOEFFICIENT\nthe red line shows the impedance linkages and the black and blue lines\n(F),FORSIXDIFFERENTCASES showthedroneandmobilerobotpaths,respectively. Parameter m(kg) k (N/m) d(N·s/m) F coeff V. REAL-WORLDEXPERIMENTALSETUP\nCaseI 1.0 5.0 2.5 0.68 The real-world setup consists of a drone and a mobile\nCaseII 1.5 3.0 3.5 0.35 robot communicating via ROS over a shared Wi-Fi network.",
      "size": 907,
      "sentences": 5
    },
    {
      "id": 23,
      "content": "m) F coeff V. REAL-WORLDEXPERIMENTALSETUP\nCaseI 1.0 5.0 2.5 0.68 The real-world setup consists of a drone and a mobile\nCaseII 1.5 3.0 3.5 0.35 robot communicating via ROS over a shared Wi-Fi network. CaseIII 1.2 3.5 3.0 0.45 The drone operates as the ROS master using an onboard\nCaseIV 1.3 3.4 3.6 0.45\nOrange Pi, while the mobile robot functions as a ROS slave\nCaseV 1.4 3.3 3.7 0.15\nrunning on an Intel NUC. Initially operating independently,\nCaseVI 1.2 3.8 4.0 0.65\nboth agents are synchronized through ROS for collaborative\n3) Retrieval-AugmentedGeneration(RAG)forImpedance operation. Parameter Generation The drone continuously publishes its planned target posi-\nRetrieval-AugmentedGeneration(RAG)systemstypically tion and the mobile robot’s current position (as tracked by\nenhance Large Language Models (LLMs) by integrating a VICON motion capture system). Upon synchronization,\nexternal knowledge retrieval into the generation process.",
      "size": 942,
      "sentences": 5
    },
    {
      "id": 24,
      "content": "t’s current position (as tracked by\nenhance Large Language Models (LLMs) by integrating a VICON motion capture system). Upon synchronization,\nexternal knowledge retrieval into the generation process. In the mobile robot subscribes to this data and uses a PID\nthis work, a na¨ıve RAG implementation is used. It employs controller to compute velocity commands, enabling it to\na sentence transformer to generate vector embeddings from follow the drone’s trajectory while maintaining formation\ntextual queries and utilizes Facebook AI Similarity Search and avoiding local obstacles. A total of 12 experiments\n(FAISS) for fast and efficient nearest-neighbor retrieval. An were conducted with varying environmental configurations\nexactnearest-neighborsearchalgorithmisadoptedtoensure by altering obstacle positions. precise matching. The RAG pipeline processes the textual\nA.",
      "size": 869,
      "sentences": 8
    },
    {
      "id": 25,
      "content": "e conducted with varying environmental configurations\nexactnearest-neighborsearchalgorithmisadoptedtoensure by altering obstacle positions. precise matching. The RAG pipeline processes the textual\nA. Experimental Result\noutputoftheVLM,embedsitasaqueryvector,andperforms\na similarity search in the database of impedance parameters Trajectoriesfromthreeoftheexperimentsareshown:two\nusing Euclidean distance given by: in static environments, and Case III evaluated under both\nstatic and dynamic conditions. (cid:118)\n(cid:117) n 1) Results in static environment\n(cid:117)(cid:88)\nd(X,Y)=(cid:116) (X\ni\n−Y\ni\n)2, (4)\nFig. 4 demonstrates how the drone leads the mobile robot\ni=1 throughvirtualimpedance-basedcouplingwhilesuccessfully\nwhere X = (X ,X ,...,X ) is the query embedding, Y = avoiding tall obstacles. The mobile robot actively avoids\n1 2 n\n(Y ,Y ,...,Y ) is the stored embedding, n = 384 is the short ground obstacles, which are not perceived or avoided\n1 2 n\nembedding dimension.",
      "size": 985,
      "sentences": 7
    },
    {
      "id": 26,
      "content": "ing tall obstacles. The mobile robot actively avoids\n1 2 n\n(Y ,Y ,...,Y ) is the stored embedding, n = 384 is the short ground obstacles, which are not perceived or avoided\n1 2 n\nembedding dimension. by the drone since they do not interfere with its flight path. The closest matching case is returned, and its associated 2) Results in dynamic environment\nimpedance parameters are used to adjust the behavior of the In Case III (Fig. 5), one of the short obstacles was\nswarm according to the current environmental context. made dynamic to simulate a real-world scenario. As the\nresponsibilityforavoidingshortobstaclesliessolelywiththe\nIV. EXPERIMENTALSETUPINSIMULATION\nmobile robot, the figure shows that while the drone follows\nENVIRONMENT\nthe same planned trajectory in both static and dynamic\nAcustomsimulationenvironmentwasdevelopedusingthe scenarios, the mobile robot adjusts its path in real time to\nGym-PyBullet framework (Fig. 3) to model the dynamics of avoid the approaching obstacle.",
      "size": 993,
      "sentences": 9
    },
    {
      "id": 27,
      "content": "Acustomsimulationenvironmentwasdevelopedusingthe scenarios, the mobile robot adjusts its path in real time to\nGym-PyBullet framework (Fig. 3) to model the dynamics of avoid the approaching obstacle. [표 데이터 감지됨]\n\n=== 페이지 5 ===\nFig.4. Resultsinstaticenvironment.CASEI:Onetallobstacle,oneshort\nobstacle,CASEII:twotallobstacles,oneshortobstacle. Fig.6. Velocityprofilealongthetrajectoryinrealworldscenarios. TABLEIII\nTRAJECTORYLENGTHCOMPARISONBETWEENDRONEANDMOBILE\nFig. 5. Results in a dynamic environment. CASE III: One Tall Obstacle ROBOT\n–TwoShortObstacles. Drone MobileRobot\nCase\n3) Deviation of mobile robot path from drone path trajectory(m) trajectory(m)\nWhile the drone demonstrates minimal deviation from\nCaseI 4.272 5.593\nits planned path due to real-time APF-based planning, the\nmobile robot deviates to avoid short obstacles that are not in CaseII 4.492 5.822\nthe path of the drone, especially in cluttered environments.",
      "size": 928,
      "sentences": 11
    },
    {
      "id": 28,
      "content": "ts planned path due to real-time APF-based planning, the\nmobile robot deviates to avoid short obstacles that are not in CaseII 4.492 5.822\nthe path of the drone, especially in cluttered environments. CaseIII(Static) 4.684 5.634\nThesedeviationshighlightthemobilerobot’sroleinensuring\ncollision avoidance while maintaining formation. The quan- CASEIII(Dynamic) 4.672 5.710\ntitative deviations are summarized in Table II. Experiments were conducted under varying conditions. TABLEII\nOut of 12 trials, 11 of them succeeded. One failure resulted\nMOBILEROBOTPATHOFFSETDUETOSHORTOBSTACLEAVOIDANCE\nfrom synchronization issues between both the agents and\nIII IV limited flight area due to the large mobile robot’s size. The\nCase I II\n(Static) (Dynamic) successratewascomputedtobe92%.Thishighsuccessrate\ndemonstrates the robustness of the system and its capability\nMobileRobotDeviation(m) 0.45 0.45 0.45 0.43\nto adapt to dynamic and diverse real-world conditions.",
      "size": 953,
      "sentences": 7
    },
    {
      "id": 29,
      "content": "ewascomputedtobe92%.Thishighsuccessrate\ndemonstrates the robustness of the system and its capability\nMobileRobotDeviation(m) 0.45 0.45 0.45 0.43\nto adapt to dynamic and diverse real-world conditions. 4) Velocity Distribution along the trajectory\nC. Evaluation of VLM-RAG Framework\nThe velocity profiles in Fig. 6 capture the real-world\nmotion behavior of both agents. The drone maintains a rela- Fig. 7 shows that the VLM-RAG system achieves 80%\ntively high and consistent velocity, with slight accelerations success in detecting and retrieving short and tall objects\nnear tall obstacles followed by gradual deceleration. The under good lighting. In poor lighting, the success rate drops\nmobile robot exhibits a more variable profile, accelerating to 60%, mainly due to difficulty identifying tall obstacles. whensimultaneouslyfollowingthedroneandavoidingshort Despite lighting variations, the system consistently detects\nobstacles, and decelerating as it reaches the final target.",
      "size": 981,
      "sentences": 8
    },
    {
      "id": 30,
      "content": "dentifying tall obstacles. whensimultaneouslyfollowingthedroneandavoidingshort Despite lighting variations, the system consistently detects\nobstacles, and decelerating as it reaches the final target. obstacles regardless of color or placement. B. Trajectory Analysis Across Experimental Cases VI. CONCLUSIONANDFUTUREWORK\nTable III shows the trajectory lengths of the drone and SwarmVLM introduces a heterogeneous robotic system\nmobile robot. The robot consistently takes a longer path due that integrates the aerial agility of a drone with the ground-\nto short obstacle avoidance, while the drone flies directly leveladaptabilityofamobilerobot,enablingresilientnaviga-\nfrom them. Case III highlights performance in both static tionincomplexanddynamicenvironments.Throughtheuse\nand dynamic settings, reflecting system adaptability.",
      "size": 830,
      "sentences": 7
    },
    {
      "id": 31,
      "content": "amobilerobot,enablingresilientnaviga-\nfrom them. Case III highlights performance in both static tionincomplexanddynamicenvironments.Throughtheuse\nand dynamic settings, reflecting system adaptability. ofimpedance-basedcoordination,APF-drivenpathplanning,\n[표 데이터 감지됨]\n\n=== 페이지 6 ===\n[7] A.Batinovic,J.Goricanec,L.Markovic,andS.Bogdan,“Pathplan-\nningwithpotentialfield-basedobstacleavoidanceina3denvironment\nbyanunmannedaerialvehicle,”inProc.IEEEInt.Conf.onUnmanned\nAircraftSystems(ICUAS),June21-24,2022,pp.394–401. [8] Y.Yu,C.Chen,J.Guo,M.Chadli,andZ.Xiang,“Adaptiveformation\ncontrol for unmanned aerial vehicles with collision avoidance and\nswitching communication network,” IEEE Transactions on Fuzzy\nSystems,vol.32,no.3,pp.1435–1445,March2024. [9] E.Tsykunov,R.Agishev,R.Ibrahimov,L.Labazanova,A.Tleugazy,\nand D. Tsetserukou, “SwarmTouch: Guiding a swarm of micro-\nquadrotorswithimpedancecontrolusingawearabletactileinterface,”\nIEEETransactionsonHaptics,vol.12,no.3,pp.363–374,July2019.",
      "size": 987,
      "sentences": 5
    },
    {
      "id": 32,
      "content": "zanova,A.Tleugazy,\nand D. Tsetserukou, “SwarmTouch: Guiding a swarm of micro-\nquadrotorswithimpedancecontrolusingawearabletactileinterface,”\nIEEETransactionsonHaptics,vol.12,no.3,pp.363–374,July2019. [10] A.Fedoseev,A.Baza,A.Gupta,E.Dorzhieva,R.N.Gujarathi,and\nD. Tsetserukou, “DandelionTouch: High fidelity haptic rendering of\nsoftobjectsinvrbyaswarmofdrones,”inProc.IEEEInt.Conf.on\nSystems, Man, and Cybernetics (SMC), Oct. 9-12, 2022, pp. 1078–\n1083. [11] R. A. Khan, M. Zafar, A. Batool, A. Fedoseev, and D. Tsetserukou,\nFig. 7. Performance of the VLM-RAG system under different lighting “SwarmPath:Droneswarmnavigationthroughclutteredenvironments\nconditions. leveraging artificial potential field and impedance control,” in Proc. IEEE Int. Conf. on Robotics and Biomimetics (ROBIO), Dec. 10-14,\nand vision-language perception via VLM-RAG, the system\n2024,pp.402–407.",
      "size": 871,
      "sentences": 10
    },
    {
      "id": 33,
      "content": "ng artificial potential field and impedance control,” in Proc. IEEE Int. Conf. on Robotics and Biomimetics (ROBIO), Dec. 10-14,\nand vision-language perception via VLM-RAG, the system\n2024,pp.402–407. achieved a navigation success rate of approximately 92% in [12] Z. Darush, M. Martynov, A. Fedoseev, A. Shcherbak, and D. Tset-\nreal-world scenarios. The drone serves as the path leader, serukou, “SwarmGear: Heterogeneous swarm of drones with mor-\nphogenetic leader drone and virtual impedance links for multi-agent\nwhile the mobile robot ensures continuous tracking and\ninspection,”in Proc.IEEEInt. Conf.onUnmannedAircraftSystems\nobstacle avoidance. Moreover, due to its focus on bypassing (ICUAS),June6-9,2023,pp.557–563. short obstacles, the mobile robot exhibited a deviation of up [13] S. Karaf, A. Fedoseev, M. Martynov, Z. Darush, A. Shcherbak,\nand D. Tsetserukou, “MorphoLander: Reinforcement learning based\nto 50cm from the drone’s trajectory.",
      "size": 952,
      "sentences": 9
    },
    {
      "id": 34,
      "content": "obot exhibited a deviation of up [13] S. Karaf, A. Fedoseev, M. Martynov, Z. Darush, A. Shcherbak,\nand D. Tsetserukou, “MorphoLander: Reinforcement learning based\nto 50cm from the drone’s trajectory. Furthermore, real-time\nlandingofagroupofdronesontheadaptivemorphogeneticuav,”in\nimpedance parameter tuning enhances the system’s adapt- Proc.IEEEInt.Conf.onSystems,Man,andCybernetics(SMC),Oct.\nability to environmental changes. The VLM-RAG module 1-4,2023,pp.2507–2512. [14] Y.ChenandJ.Xiao,“Targetsearchandnavigationinheterogeneous\nalso achieved 80% accuracy in detecting relevant objects\nrobotsystemswithdeepreinforcementlearning,”MachineIntelligence\nand retrieving appropriate control parameters under optimal Research,vol.22,no.1,p.79–90,Jan2025. lighting conditions.",
      "size": 770,
      "sentences": 5
    },
    {
      "id": 35,
      "content": "relevant objects\nrobotsystemswithdeepreinforcementlearning,”MachineIntelligence\nand retrieving appropriate control parameters under optimal Research,vol.22,no.1,p.79–90,Jan2025. lighting conditions. Therefore, these combined capabilities [15] W. Małopolski and S. Skoczypiec, “The concept of an autonomous\nmobilerobotforautomatingtransporttasksinhigh-baywarehouses,”\nenable both agents to adjust their behavior dynamically,\nAdvances in Science and Technology Research Journal, vol. 18, pp. ensuring intelligent and dependable operation across varied 1–10,April2024. environments. [16] W. L. Salas, L. M. Valent´ın-Coronado, I. Becerra, and A. Ram´ırez-\nPedraza, “Collaborative object search using heterogeneous mobile\nIn future work, this research can be extended to incor-\nrobots,” in Proc. IEEE Int. Autumn Meeting on Power, Electronics\nporate multiple drones and mobile robots operating collabo- andComputing(ROPEC),vol.5,Nov.10-12,2021,pp.1–6. ratively.",
      "size": 957,
      "sentences": 10
    },
    {
      "id": 36,
      "content": "ended to incor-\nrobots,” in Proc. IEEE Int. Autumn Meeting on Power, Electronics\nporate multiple drones and mobile robots operating collabo- andComputing(ROPEC),vol.5,Nov.10-12,2021,pp.1–6. ratively. Additionally, integrating advanced computer vision [17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT.Unterthineretal.,“Animageisworth16x16words:Transformers\ntechniquesmayfurtherenhancereal-timeimpedanceparam-\nforimagerecognitionatscale,”2021,arXiv:2010.11929.\neter tuning by enabling more accurate scene interpretation [18] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn\nand contextual awareness. et al., “RT-1: Robotics transformer for real-world control at scale,”\n2023,arXiv:2212.06817.",
      "size": 730,
      "sentences": 6
    },
    {
      "id": 37,
      "content": "rpretation [18] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn\nand contextual awareness. et al., “RT-1: Robotics transformer for real-world control at scale,”\n2023,arXiv:2212.06817. [19] M. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang, J. S. Park et al.,\nREFERENCES\n“MolmoandPixMo:Openweightsandopendataforstate-of-the-art\n[1] S.Lo´pez-SorianoandR.Pous,“Inventoryrobots:Performanceeval- multimodalmodels,”2024,arXiv:2409.17146.\nuation of an rfid-based navigation strategy,” IEEE Sensors Journal, [20] A. Lykov, S. Karaf, M. Martynov, V. Serpiva, A. Fedoseev, M. Ko-\nvol.23,no.14,pp.16210–16218,July2023.",
      "size": 623,
      "sentences": 3
    },
    {
      "id": 38,
      "content": "”2024,arXiv:2409.17146.\nuation of an rfid-based navigation strategy,” IEEE Sensors Journal, [20] A. Lykov, S. Karaf, M. Martynov, V. Serpiva, A. Fedoseev, M. Ko-\nvol.23,no.14,pp.16210–16218,July2023. nenkov, and D. Tsetserukou, “Flockgpt: Guiding uav flocking with\n[2] A.Motroni,S.D’Avella,A.Buffi,P.Tripicchio,M.Unetti,G.Cecchi, linguistic orchestration,” in 2024 IEEE International Symposium on\nand P. Nepa, “Advanced rfid-robot with rotating antennas for smart MixedandAugmentedRealityAdjunct(ISMAR-Adjunct),Oct.21-15,\ninventory in high-density shelving systems,” IEEE Journal of Radio 2024,pp.485–488. FrequencyIdentification,vol.8,pp.559–570,Feb.2024.",
      "size": 656,
      "sentences": 3
    },
    {
      "id": 39,
      "content": "smart MixedandAugmentedRealityAdjunct(ISMAR-Adjunct),Oct.21-15,\ninventory in high-density shelving systems,” IEEE Journal of Radio 2024,pp.485–488. FrequencyIdentification,vol.8,pp.559–570,Feb.2024. [21] M. Zafar, R. A. Khan, A. Fedoseev, K. K. Jaiswal, P. B. Sujit, and\n[3] D. Cristiani, F. Bottonelli, A. Trotta, and M. Di Felice, “Inventory D.Tsetserukou,“HetSwarm:Cooperativenavigationofheterogeneous\nmanagementthroughmini-drones:Architectureandproof-of-concept swarmindynamicanddenseenvironmentsthroughimpedance-based\nimplementation,” in Proc. IEEE Int. Symposium on ”A World of guidance,” in Proc. IEEE Int. Conf. on Unmanned Aircraft Systems\nWireless, Mobile and Multimedia Networks” (WoWMoM), Aug. 31- (ICUAS),May14-17,2025,pp.309–315. Sept.3,2020,pp.317–322. [22] F. Batool, M. Zafar, Y. Yaqoot, R. A. Khan, M. H. Khan, A. Fe-\n[4] N.Hogan,“Impedancecontrol:Anapproachtomanipulation,”inProc.",
      "size": 899,
      "sentences": 10
    },
    {
      "id": 40,
      "content": "Aug. 31- (ICUAS),May14-17,2025,pp.309–315. Sept.3,2020,pp.317–322. [22] F. Batool, M. Zafar, Y. Yaqoot, R. A. Khan, M. H. Khan, A. Fe-\n[4] N.Hogan,“Impedancecontrol:Anapproachtomanipulation,”inProc. doseev,andD.Tsetserukou,“ImpedanceGPT:Vlm-drivenimpedance\nAmericanControlConference,June6-8,1984,pp.304–313. controlofswarmofmini-dronesforintelligentnavigationindynamic\n[5] S.S.AbdiandD.A.Paley,“Safeoperationsofanaerialswarmviaa environment,”2025,arxiv:2503.02723.\ncobothumanswarminterface,”inProc.IEEEInt.Conf.onRobotics [23] J.Panerati,H.Zheng,S.Zhou,J.Xu,A.Prorok,andA.P.Schoellig,\nandAutomation(ICRA),May29-June2,2023,pp.1701–1707.",
      "size": 635,
      "sentences": 5
    },
    {
      "id": 41,
      "content": "ment,”2025,arxiv:2503.02723.\ncobothumanswarminterface,”inProc.IEEEInt.Conf.onRobotics [23] J.Panerati,H.Zheng,S.Zhou,J.Xu,A.Prorok,andA.P.Schoellig,\nandAutomation(ICRA),May29-June2,2023,pp.1701–1707. “Learning to fly—a gym environment with pybullet physics for\n[6] G. Khen, D. Zhao, and J. Baca, “Intuitive human-swarm interaction reinforcement learning of multi-agent quadcopter control,” in 2021\nwith gesture recognition and machine learning,” in Association for IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems\nComputing Machinery, New York, NY, USA, Oct. 23-26, 2023, p. (IROS),2021,pp.7512–7519. 453–456. [24] H. Li, “Robotic path planning strategy based on improved artificial\npotential field,” in Proc. Int. Conf. on Artificial Intelligence and\nComputerEngineering(ICAICE),Oct.23-25,2020,pp.67–71.",
      "size": 816,
      "sentences": 7
    }
  ]
}