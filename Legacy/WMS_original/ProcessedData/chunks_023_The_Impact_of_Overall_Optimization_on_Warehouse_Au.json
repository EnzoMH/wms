{
  "source": "ArXiv",
  "filename": "023_The_Impact_of_Overall_Optimization_on_Warehouse_Au.pdf",
  "total_chars": 45379,
  "total_chunks": 68,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\n3202\nguA\n11\n]OR.sc[\n1v63060.8032:viXra\nThe Impact of Overall Optimization on Warehouse Automation\nHiroshi Yoshitake1 and Pieter Abbeel2\nAbstract—In this study, we propose a novel approach for howmuchtheoveralloptimizationofmultipleprocesseswill\ninvestigating optimization performance by flexible robot co- affect the automated system. ordination in automated warehouses with multi-agent rein-\nMulti-agent reinforcement learning (MARL) can provide\nforcement learning(MARL)-basedcontrol. Automated systems\npractical overall optimization for controlling industrial au-\nusing robots are expected to achieve efficient operations com-\npared with manual systems in terms of overall optimization tomated systems. A control method of robot coordination\nperformance. However, the impact of overall optimization on for the overall optimization has not yet been established.",
      "size": 876,
      "sentences": 5
    },
    {
      "id": 2,
      "content": "of overall optimization tomated systems. A control method of robot coordination\nperformance. However, the impact of overall optimization on for the overall optimization has not yet been established. performance remains unclear in most automated systems due However, centralized training with decentralized execution\nto a lack of suitable control methods. Thus, we proposed a\n(CTDE), one of the major MARL frameworks in which\ncentralizedtraining-and-decentralizedexecutionMARLframe-\nagents make decisions in a decentralized manner and learn\nworkasapracticaloverall optimizationcontrolmethod.Inthe\nproposed framework, we also proposed a single shared critic, the coordination using centralized estimators [4], would\ntrained with global states and rewards, applicable to a case be a solution for it: cyber-physical modeling enables the\nin which heterogeneous agents make decisions asynchronously. training of reinforcementlearning (RL) agents centrally [5].",
      "size": 954,
      "sentences": 6
    },
    {
      "id": 3,
      "content": "pplicable to a case be a solution for it: cyber-physical modeling enables the\nin which heterogeneous agents make decisions asynchronously. training of reinforcementlearning (RL) agents centrally [5]. Our proposed MARL framework was applied to the task\nFurthermore,a decentralizedRL policywouldbereasonable\nselection of material handling equipment through automated\nfor practical implementation: a centralized controller (e.g.,\norder picking simulation, and its performance was evaluated\nto determine how far overall optimization outperforms partial warehousemanagementsystem)wouldnotcontrolthousands\noptimization by comparing it with other MARL frameworks of robots one by one in terms of system load [6]. In this\nand rule-based control methods. case, it is more natural that the robot fleet is controlled\nby a decentralized sub-system operating the corresponding\nI.",
      "size": 866,
      "sentences": 5
    },
    {
      "id": 4,
      "content": "ne by one in terms of system load [6]. In this\nand rule-based control methods. case, it is more natural that the robot fleet is controlled\nby a decentralized sub-system operating the corresponding\nI. INTRODUCTION\nprocess.Furthermore,thereis currentlynounifiedcontroller\nInrecentyears,industrialautomationhasrapidlyadvanced that can handle various types of MHE and robots released\nwith the active installation of robots. Many industrial sites, from different vendors. suchaslogisticswarehousesandproductionlines,areexperi- In this study, we investigate the impact of overall op-\nencinglaborshortages.Theyalsorequirehighlyefficientand timization on automated systems, particularly warehouse\nlong-hour operations to deal with bulk orders promoted by automation using different MARL frameworks. There are\nE-commerce[1].",
      "size": 815,
      "sentences": 7
    },
    {
      "id": 5,
      "content": "ghlyefficientand timization on automated systems, particularly warehouse\nlong-hour operations to deal with bulk orders promoted by automation using different MARL frameworks. There are\nE-commerce[1]. To addressthese issues, manualoperations two mainchallengesforthe MARL-basedapproachin robot\nare automated by replacing human laborers with industrial coordination, which is essential for the overall optimization\nrobots.Theinstallationofindustrialrobotsincreasesatanan- of industrial automation. The first one is a long-horizon\nnualrateof∼10%,androboticautomationhelpscompensate task under extremely sparse reward settings: evaluation in-\nfor the workforce shortage [2]. More efficient automation dices of system performance such as task completion time\ntechniques are also required to improve the effectiveness of (makespan) and productivity can be estimated only at the\nrobot installation. terminal state when all tasks are completed.",
      "size": 936,
      "sentences": 6
    },
    {
      "id": 6,
      "content": "completion time\ntechniques are also required to improve the effectiveness of (makespan) and productivity can be estimated only at the\nrobot installation. terminal state when all tasks are completed. The second\nOne of the reasons why the installation of automated one is the asynchronous decision making of heterogeneous\nsystems is expected to increase efficiency is the possibility agents: when various types of robots execute tasks, they\nof further efficient operationsbased on overalloptimization. do not make decisions under the synchronous settings that\nIn conventional manual operations, detailed control and majorMARL algorithmsassume. We, therefore,examinean\nmodeling are challenging due to the several uncertainties effectiveMARLframeworkthatmaximizestheperformance\nassociated with human actions and decision making. If au- of automatedsystems underthese challengesby introducing\ntomationprogressesusingindustrialrobots,theirpredictable a simplified automated warehouse simulator.",
      "size": 988,
      "sentences": 6
    },
    {
      "id": 7,
      "content": "human actions and decision making. If au- of automatedsystems underthese challengesby introducing\ntomationprogressesusingindustrialrobots,theirpredictable a simplified automated warehouse simulator. anddeterministicbehaviorswillincreasethecontrolaccuracy\nThe purpose of this study is to answer the following\nin operations. By constructing such highly accurate control\nresearch questions (RQs). schemesinvariousprocesses,thereisa prospectofoptimiz-\n• RQ1: How can MARL agents acquire coordinated be-\ning theentire system. However,mostofthe previousstudies\nhaviors in industrial automated environments? have only addressed partial optimization for limited cases,\n• RQ2: Which coordination condition is important for\nsuch as a part of an automated system or material handling\noverall optimization? equipment (MHE) for a certain process [3].",
      "size": 837,
      "sentences": 8
    },
    {
      "id": 8,
      "content": "ion for limited cases,\n• RQ2: Which coordination condition is important for\nsuch as a part of an automated system or material handling\noverall optimization? equipment (MHE) for a certain process [3]. It is unclear\n• RQ3: What advantagesdoes overalloptimization bring\n1H.Yoshitake iswithR&DGroup,Hitachi Ltd.,Kokubunji-shi, Tokyo, to automated systems compared with partial optimiza-\nJapan hiroshi.yoshitake.nt@hitachi.com, 2P. Abbeel is tion? with Department of EECS, University of California, Berkeley, CA, USA\npabbeel@cs.berkeley.edu The contributions of this study are as follows. === 페이지 2 ===\n• Proposal of a CTDE-based practical MARL algorithm MARL-based control with a decentralized policy would\napplicable to industrial automated environments. be reasonable for automated systems due to its scalability. • Building a simulation environment of an automated The simplest framework of such control is independent\norder picking system1.",
      "size": 940,
      "sentences": 8
    },
    {
      "id": 9,
      "content": "ironments. be reasonable for automated systems due to its scalability. • Building a simulation environment of an automated The simplest framework of such control is independent\norder picking system1. learning(IL),whereeachagentistrainedinthesamemanner\n• Clarifying effective coordination conditions and the as in single-agent RL: an agent independently learns the\nimpactofoveralloptimizationwhenusingtheproposed policy from its local observations and behaviors [10] even\nMARL algorithm and simulation environment. though it suffers from instability of the training environ-\nTheremainderofthispaperisorganizedasfollows.In§II, ment caused by the policy updates of other agents. Recent\nwe explain related works. We provide a detailed description MARLstudieshavefocusedontheCTDEframeworkwhere\nof the proposed MARL frameworks and algorithms in §III. decentralized policies are trained by centralized critics that\nIn §IV, we evaluate the proposed methods.",
      "size": 949,
      "sentences": 8
    },
    {
      "id": 10,
      "content": "dieshavefocusedontheCTDEframeworkwhere\nof the proposed MARL frameworks and algorithms in §III. decentralized policies are trained by centralized critics that\nIn §IV, we evaluate the proposed methods. We conclude the estimate the contributions of all agents. One of the most\nstudy and discuss future work in §V. widelyreferencedCTDEframeworksisthemulti-agentdeep\ndeterministic policy gradient (MADDPG), which uses joint\nII. RELATEDWORKS critics of the states and actions of all agents [11]. We apply\nA. Optimization of Logistics Warehouse Operation both IL and CTDE frameworks to automated warehouse\noperations and clarify how much they can contribute to the\nThe operational purpose of logistics warehouses is to\noptimization.",
      "size": 725,
      "sentences": 7
    },
    {
      "id": 11,
      "content": "s Warehouse Operation both IL and CTDE frameworks to automated warehouse\noperations and clarify how much they can contribute to the\nThe operational purpose of logistics warehouses is to\noptimization. collect and ship inventory items according to orders re-\nceivedfrom customers[7].To achievethis order fulfillment, C. Applications of MARL in Industrial Automation\nwarehouseoperationsconsistofseveralprocesses:receiving,\nApplications of MARL are frequently studied in many\ninventory control, order picking, inspection, packing, and\nindustrialfieldssuchasmanufacturing,logistics,networking,\nshipping. Each process also includes the workforce such as\nand automotive [12], [13]. Most industrial systems require\nhuman laborers and MHE, as well as their tasks to handle\nintelligentcontrollersthatcreateoperationschedulesorsend\ntheordereditems.Optimizingtheseprocesseshelpsmaintain\ninstructions to the control object to maximize performance.",
      "size": 934,
      "sentences": 4
    },
    {
      "id": 12,
      "content": "ell as their tasks to handle\nintelligentcontrollersthatcreateoperationschedulesorsend\ntheordereditems.Optimizingtheseprocesseshelpsmaintain\ninstructions to the control object to maximize performance. efficient warehouse operations, and thus, minimizing the\nRL provides reasonable solutions to these problems com-\nmakespan and maximizing the system throughput are key\npared with other optimization techniques [14] and has the\nresearch topics for warehouse optimization [8]. Because\nadvantageof flexibility:RL agentsare trained by experienc-\nwarehouse automation has made operation control more ac-\ningvarioussituationsina system,andthe trainedpolicycan\ncuratebyreplacinghumanlaborerswithrobots,optimization\noutput optimal actions for any states. Because unexpected\nhas become even more crucial for efficiency [9].",
      "size": 812,
      "sentences": 4
    },
    {
      "id": 13,
      "content": "ionsina system,andthe trainedpolicycan\ncuratebyreplacinghumanlaborerswithrobots,optimization\noutput optimal actions for any states. Because unexpected\nhas become even more crucial for efficiency [9]. variable factors such as noises and disturbances tend to\nMaximizing the efficiency of warehouse operations re-\noccur in industrial systems, the control of agents, resource\nquirescontroltechnologythatcomprehensivelyoptimizesall\nallocation, and task planning is expected to be flexible in\nrelated processes. Despite the widely recognized need for\nresponse. The goal of MARL research in the industrial\nsuchtechnology,previousstudieshavebeenlimitedtopartial\nfield is to incorporate the above RL features into systems\noptimizationwithinacertainprocesssuchasoptimalrouting\nconsisting of several autonomous agents.",
      "size": 807,
      "sentences": 5
    },
    {
      "id": 14,
      "content": "ology,previousstudieshavebeenlimitedtopartial\nfield is to incorporate the above RL features into systems\noptimizationwithinacertainprocesssuchasoptimalrouting\nconsisting of several autonomous agents. and order batching for manual/robotized order picking and\nOneofthemostpopularMARLapplicationsisthecontrol\noptimal storage assignment based on order frequency for\nof MHE used for automating order picking in logistics\ninventory control [3]. Therefore, an overall optimization\nwarehousesandfactories[15].Orderpickingisanoperation,\nmethod that considers the operation of multiple processes\nwhere ordered items are collected for shipping destinations\nhas not yet been established. In this study, we introduce\nfrom warehouse storage. The transfer of items during order\na state-of-the-art (SOTA) MARL algorithm as an overall\npicking has been recently automated by MHE consisting\noptimization method for warehouse operations. of several automated guided vehicles (AGVs) [3].",
      "size": 966,
      "sentences": 6
    },
    {
      "id": 15,
      "content": "ate-of-the-art (SOTA) MARL algorithm as an overall\npicking has been recently automated by MHE consisting\noptimization method for warehouse operations. of several automated guided vehicles (AGVs) [3]. The task\nB. Multi-Agent Reinforcement Learning allocation or path planning of AGVs has been successfully\nexecuted by introducingrecent CDTE algorithms[15], [16]. MARL, an extensionof the RL framework,where several\nHowever, such MARL-based control has only been applied\nagents execute different tasks, is extensively studied for\nto the process of item transfer by homogeneous robots,\nacquiring the optimal policies of agents in a multi-agent\nwhereas order picking includes other processes involving\nsystem(MAS).Agentsneedtolearnbyconsideringnotonly\nheterogeneous robots such as picking and placing items by\ninformation about their local environments but also other\npickingrobotsandsubsequenttransferofitemsbyconveyors. learning agents.",
      "size": 934,
      "sentences": 5
    },
    {
      "id": 16,
      "content": "eringnotonly\nheterogeneous robots such as picking and placing items by\ninformation about their local environments but also other\npickingrobotsandsubsequenttransferofitemsbyconveyors. learning agents. Many MARL frameworks have been pro-\nFewpreviousstudieshaveaddressedmorecomplicatedcases\nposed depending on various conditions such as operational\nthat allow training heterogeneous agents with asynchronous\nsettings,whereagentsarecompletelycontrolledbyacentral\ndecision making [17]–[19]; however, they used the MAD-\nunit or operate autonomously in decentralized settings, and\nDPG (i.e., off-policy MARL algorithm), making it difficult\nsituationsin whichthe tasks of agentsare stationaryor non-\ntoapplythemtothesparserewardsetting,oneofthetypical\nstationary [4]. featuresinindustrialautomation.Inthisstudy,wedevelopan\nMARL framework that can be applied to several processes\n1The source code is available at https://github.com/\n16444take/aope-sim.git with heterogeneousagents. === 페이지 3 ===\nIII.",
      "size": 991,
      "sentences": 5
    },
    {
      "id": 17,
      "content": "nthisstudy,wedevelopan\nMARL framework that can be applied to several processes\n1The source code is available at https://github.com/\n16444take/aope-sim.git with heterogeneousagents. === 페이지 3 ===\nIII. METHODOLOGY 2) Algorithm Design: to address the LTESR issue, we\nused an on-policy AC algorithm in the proposed MARL\nA. Problem Settings\nframework: The LTESR makes state values, estimated as\nAlthoughthe operationalstatus of automatedsystems can TD errors frequently used in off-policy RL algorithms,\nbe disturbed by noise and stochastic factors, it depends on uncertain because of many non-rewarded state transitions. the decision making for controlling MHE and robots that We, therefore, used the on-policyalgorithm in our proposed\nperformthetasksineachprocess.Byregardingthecontroller MARL frameworks to estimate state values as reward-to-\nof MHE and robots as an autonomous agent, state changes go using the Monte Carlo (MC) method.",
      "size": 934,
      "sentences": 5
    },
    {
      "id": 18,
      "content": "formthetasksineachprocess.Byregardingthecontroller MARL frameworks to estimate state values as reward-to-\nof MHE and robots as an autonomous agent, state changes go using the Monte Carlo (MC) method. As a SOTA RL\nin an automated system can be described as a Markov algorithm, proximal policy optimization (PPO), which can\ndecision process: hS,A,P ,Ri, where s∈S denotes a set trainthepolicyofanagentconservatively,wasimplemented\nT\nof system states, a ∈ A denotes a set of agents’ actions, inourframework[21].PPOalsoachievesgoodperformance\nP : S × A × S → [0,1] denotes the state transition in the MA domain when implemented for CTDE (multi-\nT\nprobability organizedby the system operations, and r ∈R: agent PPO, MAPPO) [22]. Hence, we applied MAPPO to\nS ×A×S → R denotes the reward function, respectively.",
      "size": 804,
      "sentences": 3
    },
    {
      "id": 19,
      "content": "n when implemented for CTDE (multi-\nT\nprobability organizedby the system operations, and r ∈R: agent PPO, MAPPO) [22]. Hence, we applied MAPPO to\nS ×A×S → R denotes the reward function, respectively. CDSC, whereas the effectiveness of the PPO-based MARL\nAssuming a situation, where the controller makes decisions framework is not well verified for our unique problem\nsolely based on the states of its corresponding process, we settings explained in §III.A. further study the decentralized partially observable MDP as\nhS,{Ai},O,P ,R,Ni, where oi ∈ O denotes the local\nT\nobservation for agent i ∈ N at global state s, and ai ∈ Ai Algorithm 1: Episodic Training in CDSC\ndenotes an action set of each agent [20].",
      "size": 708,
      "sentences": 4
    },
    {
      "id": 20,
      "content": "MDP as\nhS,{Ai},O,P ,R,Ni, where oi ∈ O denotes the local\nT\nobservation for agent i ∈ N at global state s, and ai ∈ Ai Algorithm 1: Episodic Training in CDSC\ndenotes an action set of each agent [20]. In a finite horizon 1: Initialize policies {πi}N parametrized by {θi}, and\nsetting with length T, agent i is trained with its policy πi shared critic Vˆ parame i t = e 1 rized by φ\nt t o co m m ax p i u m te i d ze as a J d π i is = cou E n π ted a T τ c = − cu 0 t m γτ u r la τ i t + ed t , re w w h a e r r d e a γ t d ti e m n e o - t s e t s ep a 2 3 : : for Se e t pi r s o o l d lo e u = t b 1 uf to fer N B ep ← do ∅\ndiscount factor for accu (cid:2) m P ulating the rew (cid:3) ards. 4: for rollout =1 to N ℓ do\nTherearetwomajorchallengeswhenweapplytheMARL 5: Set rollout R←∅, and time t=0\nframework to the flexible coordination of agents in an 6: while flag = True do\nindustrial automated environment.",
      "size": 909,
      "sentences": 3
    },
    {
      "id": 21,
      "content": "o N ℓ do\nTherearetwomajorchallengeswhenweapplytheMARL 5: Set rollout R←∅, and time t=0\nframework to the flexible coordination of agents in an 6: while flag = True do\nindustrial automated environment. 7: for Agent i=1 to N do\n• Long trajectory and extremely sparse reward (LTESR): 8: if Agent i needs to take action then\nan automated system is expected to maximize perfor- 9: Make decision ai =πi(ai|oi)\nt θ t t\nmance based on evaluation indices such as makespan 10: end if\nand productivity estimated at the end of operations. 11: end for\nAgents are, therefore, rewarded only at the last state 12: if {ai}6=∅ then\nt\ntransition eventhoughthey experiencemany state tran- 13: Execute actions {ai}\nt\nsitions to complete all tasks. 14: end if\n• Asynchronized multi-agent (MA) decision making 15: Count up t+=1\n(AMADM): each agent, which corresponds to an au- 16: for Agent i=1 to N do\ntomated process or robot, performs its tasks asyn- 17: if Agent i took action at t−1 then\nchronously.",
      "size": 980,
      "sentences": 4
    },
    {
      "id": 22,
      "content": "king 15: Count up t+=1\n(AMADM): each agent, which corresponds to an au- 16: for Agent i=1 to N do\ntomated process or robot, performs its tasks asyn- 17: if Agent i took action at t−1 then\nchronously. Most CTDE frameworks assume synchro- 18: Observe oi t ,s t , and r t i\nnized behaviors among agents for estimating joint 19: R+=[oi t ,s t ,ai t ,i,r t i,oi t+1 ,s t+1 ]\nstate(-action)values, and few previousstudies on asyn- 20: end if\nchronoussettingsdescribedin§II.Ccanonlybeapplied 21: end for\nto off-policyRL algorithmswith densereward settings.",
      "size": 549,
      "sentences": 2
    },
    {
      "id": 23,
      "content": "t i,oi t+1 ,s t+1 ]\nstate(-action)values, and few previousstudies on asyn- 20: end if\nchronoussettingsdescribedin§II.Ccanonlybeapplied 21: end for\nto off-policyRL algorithmswith densereward settings. 22: if All tasks have been completed then\n23: flag= False\nB. Proposalof MARLFramework for IndustrialAutomation 24: end if\n25: end while\na C 1 T ) D F E ou f n r d a a m ti e o w n o a r l k F w ra i m th ew a o s r h k a : re t d oa c n ri s t w ic e , r te R rm Q e 1 d ,w C e D p S r C op : o w s e e 26: G Co A m E p f u r t o e m Vˆ τ τ (s = τ ) 0 , t { o C t τ i} a , nd an a d dd {A to i τ } R by MC or\nadopt an actor-critic (AC) architecture for CDSC, where an\n27:\nB+=R\nactorpolicyπ andacriticVˆ aremodeledbydifferentneural\n28: end for\nnetworks.",
      "size": 752,
      "sentences": 2
    },
    {
      "id": 24,
      "content": "t { o C t τ i} a , nd an a d dd {A to i τ } R by MC or\nadopt an actor-critic (AC) architecture for CDSC, where an\n27:\nB+=R\nactorpolicyπ andacriticVˆ aremodeledbydifferentneural\n28: end for\nnetworks. Although widely referenced CTDE frameworks\n29: end for\nsuch as the MADDPG introduce individual joint critics that\ncan be trained cooperatively with global state s = N oi 30: for epoch =1 to N k do\nt i=1 t 31: for mini-batch b=1 to B do\nandglobalrewardr t g, theycanonlytrain networksb S asedon 32: Make mini-batch b sampled from B\nstate transitions caused by themselves under the AMADM\n33: Update {θi},φ with data b\nsettings. Thus, CDSC can provide more globalized training\n34: end for\nconditions for agents by aggregating all transition histories\n35: end for\ninto a single critic. === 페이지 4 ===\nA pseudocode for training agents using the proposed inthecaseoftheAMADMofheterogeneousagentsbecause\nMARL framework is shown in Algorithm 1.",
      "size": 934,
      "sentences": 4
    },
    {
      "id": 25,
      "content": "n histories\n35: end for\ninto a single critic. === 페이지 4 ===\nA pseudocode for training agents using the proposed inthecaseoftheAMADMofheterogeneousagentsbecause\nMARL framework is shown in Algorithm 1. Because in- the input of the value function includes the agent ID i. We\ndustrial operations frequently involve batch processes (for useboththeMCmethodandGAEtoestimatetheadvantage\ninstance, in logistics warehouses, shipping orders received in CDSC to maximize performance. from customers are processed hourly in batches), we as-\nsume an episodic training scheme for the proposed MARL C. Other MARL Frameworks for Comparison\nframework. In this scheme, agent-environment interactions\nTo answer RQ2 and RQ3, we further introduced three\ncontinueuntilallagentshavecompletedalltasks(Algorithm\ndifferent MARL frameworks to compare their coordination\n1, lines 22–24). To address the AMADM issue, an agent\nperformance with that of CDSC.",
      "size": 926,
      "sentences": 7
    },
    {
      "id": 26,
      "content": "hree\ncontinueuntilallagentshavecompletedalltasks(Algorithm\ndifferent MARL frameworks to compare their coordination\n1, lines 22–24). To address the AMADM issue, an agent\nperformance with that of CDSC. Table I summarizes the\nmakesa decisionandstoresthestate transitioninthe rollout\nfeaturesofcriticsintheseframeworks,includingCDSCfrom\nbuffer only when an action is required (lines 7–11 and\nthe viewpoint of information globalization. These frame-\n16–21). The PPO-based policy is trained conservatively by\nworks assume the same functional policies {πi(ai|oi)}N\nmaximizing the following clipped objective function: t t i=1\nfor decentralized execution.",
      "size": 647,
      "sentences": 5
    },
    {
      "id": 27,
      "content": "21). The PPO-based policy is trained conservatively by\nworks assume the same functional policies {πi(ai|oi)}N\nmaximizing the following clipped objective function: t t i=1\nfor decentralized execution. Detailed training conditions of\nLi(θi)=E min ρi(θi)Ai,clip ρi(θi),1±ǫ Ai , (1) agents in the proposed MARL frameworks are described as\nb t t t t\nfollows:\nwhere ρi(θi)(cid:2)=πi(cid:0)(ai|oi)/πi (a(cid:0)i|oi) is a poli(cid:1)cy r(cid:1)a(cid:3)tio with\nt θ t t θold t t 1) ILwithLocalReward(ILLR): anIL-basedframework,\nthe old policy prior to the update parametrized by θ o i ld , Ai t where individual critic Vi using the local state of an agent\nis an advantage, and ǫ is the clipping range of the policy oi as input is trained with the local reward ri that can\n(line 33).Here, the expectationE [∗] indicatesthe empirical t t\nb be estimated by itself. Because all information required for\naverage over a finite batch b of samples.",
      "size": 931,
      "sentences": 4
    },
    {
      "id": 28,
      "content": "h the local reward ri that can\n(line 33).Here, the expectationE [∗] indicatesthe empirical t t\nb be estimated by itself. Because all information required for\naverage over a finite batch b of samples. The advantage can\nagent training is based on local observations, ILLR is the\nbe estimated using the MC method as Ai =Ci−Vˆ (s ,i ),\nt t φ t t most localized training framework. A policy trained with\nwhereC =\nT−tγτr\ndenotesareward-to-gofromtime\nt τ=0 τ+t ILLR corresponds to a short-sighted strategy, where robots\nttothehorizonT whenallagentshavecompletedtheirtasks. are controlled to complete their tasks at hand as quickly\nThe CDSC c P ritic Vˆ takes as inputs not only state variables\nas possible such as first-in-first-out and greedy heuristic\nbut also the ID of the agent i to identify which agent’s\nalgorithms [23], [24]. action contributes a corresponding state transition (line 26).",
      "size": 889,
      "sentences": 6
    },
    {
      "id": 29,
      "content": "possible such as first-in-first-out and greedy heuristic\nbut also the ID of the agent i to identify which agent’s\nalgorithms [23], [24]. action contributes a corresponding state transition (line 26). 2) ILwithGlobalReward(ILGR): analternativeIL-based\nThe critic was trained in the following supervised learning\nframework with the same learning structure as ILLR, but\nmanner as L(φ)=E b [(Vˆ φ −C t )2]. the critic is trained with the global reward rg shared by all\n3) Generalized Advantage Estimation for Asynchronous t\nagents. Introduction of the global reward to the MAS is a\nHeterogeneous Multi-agent Settings: the feature of a CDSC\ntypical cooperative setting in MARL [25]. framework is that each agent can be trained with a shared\n3) CTDE with Individual Critic (CDIC): a CTDE-based\ncritic aggregating all state transitions caused by different\nframework, where the individual critic Vi can access infor-\nagents in a given rollout.",
      "size": 935,
      "sentences": 6
    },
    {
      "id": 30,
      "content": "3) CTDE with Individual Critic (CDIC): a CTDE-based\ncritic aggregating all state transitions caused by different\nframework, where the individual critic Vi can access infor-\nagents in a given rollout. Thus, the installation of a shared\nmation about the global state s . All agents are also trained\ncritic brings alternative methods of advantage estimation t\nwith rg to promote coordination. based on a TD error. Furthermore, generalized advantage t\nWe applied independent PPO (IPPO) to ILLR and ILGR\nestimation(GAE),whichadjuststhebias-variancetradeoffof\nand MAPPO to CDIC, respectively [22], [26]. Compared\npolicygradientestimatesbetweenMC andTD methods,can\nwith these three frameworks, CDSC uses the most global\nbe used for computing Ai in CDSC. Although a truncated\nt information (data) for agent training. By comparing the\nversion of GAE has already been proposed for episodic\ntrainingperformanceoftheseframeworks,wecandetermine\ntraining by Schulman et al.",
      "size": 959,
      "sentences": 8
    },
    {
      "id": 31,
      "content": "uncated\nt information (data) for agent training. By comparing the\nversion of GAE has already been proposed for episodic\ntrainingperformanceoftheseframeworks,wecandetermine\ntraining by Schulman et al. [21], we need to modify it to\nwhich global information is effective for achieving coordi-\naddress the AMADM issue. Because the decision making\nnation in industrial settings. of asynchronous agents would have irregular time intervals,\nGAE for AMADM in a finite episode is calculated as\nIV. EVALUATION\nfollows:\nAi t =δ t +(γλ)∆tδ t+1 +···+(γλ)∆(T−1)δ T−1 , (2) wo T r o ks an to sw th er e R fo Q ll s o , w w in e g ap a p u l t i o e m d a t t h e e d p o r r o d p e o r s -p ed ick M in A g R s L im f u ra la m to e r -\nwhere λ denotes a bias-variance tradeoff parameter [0,1], T and evaluated the performance of trained policies.",
      "size": 833,
      "sentences": 6
    },
    {
      "id": 32,
      "content": "m d a t t h e e d p o r r o d p e o r s -p ed ick M in A g R s L im f u ra la m to e r -\nwhere λ denotes a bias-variance tradeoff parameter [0,1], T and evaluated the performance of trained policies. represents the episode length, ∆t denotes a time difference\nbetween t and t+1, and δ denotes the following modified\nt TABLEI\nTD error:\nMARLFRAMEWORKSWITHDIFFERENTCRITICS\nδ =r +γ∆tVˆ(s ,i )−Vˆ(s ,i ) . (3)\nt t t+1 t+1 t t MARLframework\nCritic feature\nILLR ILGC CDIC CDSC\nCompared with the original one, we can apply the proposed\nState Local Global\nGAE to AMADM by simply extending the discounting Reward Local Global\ncoefficients (γλ) and γ in Eqs. (2) and (3) to an irregular Architecture Individual (N) Shared(1)\ninterval setting.",
      "size": 731,
      "sentences": 4
    },
    {
      "id": 33,
      "content": "e Local Global\nGAE to AMADM by simply extending the discounting Reward Local Global\ncoefficients (γλ) and γ in Eqs. (2) and (3) to an irregular Architecture Individual (N) Shared(1)\ninterval setting. In addition, we can use the proposed GAE Statevalue Localized ⇐⇒ Globalized\n=== 페이지 5 ===\nTABLEII\nA. Simulation Environment\nPICKINGORDERDATA\n1) Fully Automated Order Picking: the simulation of an\nautomated order picking environment (AOPE) in a logistics Pickingorder Orders Items Types Shippings\nwarehouse was performed for quantitative evaluation of the LowMixed(LM) 179 201 16 42\nHighMixed(HM) 186 200 95 41\nproposedmethod.TheoverviewoftheAOPEisillustratedon\nthe left side of Fig. 1. Ordered items are arranged from left\nto right in the figure. The AOPE consists of four types of\nthere are in the upstream process, and the more difficult it\nMHE whose controller makes decisions for task selection,\nis to optimize operations. To investigate the performance of\nas shown on the right side of Fig. 1.",
      "size": 998,
      "sentences": 8
    },
    {
      "id": 34,
      "content": "upstream process, and the more difficult it\nMHE whose controller makes decisions for task selection,\nis to optimize operations. To investigate the performance of\nas shown on the right side of Fig. 1. The first one is a\nourproposedalgorithmfordifferentpickingorders,theitem\nflow rack (FR) in which sorting boxes are allocated. Each\ntypes are different, but items and shipping boxes are nearly\nsorting box corresponds to a shipping destination. When all\nthe same. These picking orders have reasonable characteris-\nordered items are sorted into a box, the box is transported\ntics compared with previous studies in terms of the number\nto the next working area, and FR replaces it with a new\nof orders per type [27], [29]. one. The FR controller selects a shipping box to start order\npicking tasks for its required items.",
      "size": 816,
      "sentences": 9
    },
    {
      "id": 35,
      "content": "of the number\nto the next working area, and FR replaces it with a new\nof orders per type [27], [29]. one. The FR controller selects a shipping box to start order\npicking tasks for its required items. The second one is a B. RL Settings\nset of parallel conveyors(PC) that transportsitems from the\n1) Training Condition: the proposed MARL frameworks\ninventory area to allocated shipping boxes in FR. There are\nwere applied to train the four process controllers in an\nthree conveyors in parallel, and items are loaded from the\nAOPE.EachprocesscontrollerwasregardedasanRLagent\ninventoryarea bytype.Each conveyorhassix loadingports. and trained in the advantage AC manner [30]. Table III\nThe PC controller selects an item type to be loaded on the\nsummarizes the hyperparametersused in this evaluation. conveyorfromthe inventoryarea. Thethird oneis a picking\n2) AgentStates: thestatesofallagentsaresummarizedin\nrobot (PR1) that picks up items from the PC and places\nTable IV.",
      "size": 968,
      "sentences": 9
    },
    {
      "id": 36,
      "content": "rsused in this evaluation. conveyorfromthe inventoryarea. Thethird oneis a picking\n2) AgentStates: thestatesofallagentsaresummarizedin\nrobot (PR1) that picks up items from the PC and places\nTable IV. The number in brackets represents the number of\nthemonacarouselconveyoronebyone.PR1canalsomove\nstatesmultipliedbythenumberofcomponents.Asdescribed\namongthe conveyorsto pickitemsfromeachconveyor.The\nin §III, all agents were assumed to make decisions in a\nPR1 controller selects a conveyor to pick items: conveyor\ndecentralized manner. selectionoccurswheneverthepickingofthesame item type\n3) Action Mask: we introduced an action mask to elim-\nis completed. The last one is another picking robot (PR2)\ninate invalid actions at every decision making [32]. The\nthatpicksup itemsfroma carouselconveyorandsorts them\nmasked stochastic policy πˆi computes the probability of a\ninto shipping boxes one by one.",
      "size": 899,
      "sentences": 7
    },
    {
      "id": 37,
      "content": "nate invalid actions at every decision making [32]. The\nthatpicksup itemsfroma carouselconveyorandsorts them\nmasked stochastic policy πˆi computes the probability of a\ninto shipping boxes one by one. The PR2 controller selects θ\nvalid action ai,k as follows:\nan allocated shipping box to sort items from the carousel t\nconveyor.To preventthe simulation from being fixed to one πˆi(ai,k|oi)=πi(ai,k|oi)/P m(ai,j)πi(ai,j|oi), (4)\nwork scenario, the item loading and replacement times of\nθ t t θ t t ai\nt\n,j∈Ai t θ t t\nPCs and the picking time of both PR1 and PR2 are given where m(ai,j) denotes the action mask of an agent i that\nt\nnormal distribution variations. Similar configurations were outputs 0/1 when the j-th action is valid/invalid at t. The\ndesigned as a type of parts-to-picker systems [27], [28], action mask reduces ρi(θi) and stabilize gradient updates.",
      "size": 866,
      "sentences": 4
    },
    {
      "id": 38,
      "content": "ar configurations were outputs 0/1 when the j-th action is valid/invalid at t. The\ndesigned as a type of parts-to-picker systems [27], [28], action mask reduces ρi(θi) and stabilize gradient updates. t\nwhereastheAOPEwassimplifiedtocompletethesimulation 4) Reward Design: the objective of agents in AOPE was\nquickly as a training environment. tominimizethemakespanT ofallorderpickingtasks.Thus,\nc\n2) Picking Order Data: we evaluated the performanceof thelocalrewardri forILLRwassettothenegativevalueof\nt\nthe proposed MARL frameworks with two different picking the elapsed time until an agent selects the next task: the RL\norder datasets in warehouse operations. The characteristics\nof these datasets are summarized in Table II. The “Orders”\nTABLEIII\nin the table shows the number of unique sets of item types\nHYPERPARAMETERSINMARLSETTINGS\n(“Types”) and shipping boxes (“Shippings”).",
      "size": 881,
      "sentences": 5
    },
    {
      "id": 39,
      "content": "f these datasets are summarized in Table II. The “Orders”\nTABLEIII\nin the table shows the number of unique sets of item types\nHYPERPARAMETERSINMARLSETTINGS\n(“Types”) and shipping boxes (“Shippings”). In logistics\nwarehouses,itemsare typicallystoredbytype[7].Thus,the Setting Hyperparameter Value\nmore item types in a picking order, the more item transfers Clipping rangeǫ 0.2\nDiscountfactorγ 0.99\nScalingfactor ζ 800\nIPPO,\nCarousel conveyor FR Shipping box Controller decisions MAPPO Rollouts Nℓ 64\nEpochsNk 5\nFR: Select shipping box\nRotate\nEpisodesNep 5000\nPC: Select item type to load on PC Minibatch size 64\nP1 Network MLP\nPR1: Select conveyor to pick items Hiddenlayers 2\nItem flow Item PR1 Network Hiddenunits 128/layer\nPR2: Select shipping box to sort item\nPC PR2 Activation Tanh()\nOptimizer Adam[31]\nFig. 1. Simulation environment.",
      "size": 838,
      "sentences": 5
    },
    {
      "id": 40,
      "content": "onveyor to pick items Hiddenlayers 2\nItem flow Item PR1 Network Hiddenunits 128/layer\nPR2: Select shipping box to sort item\nPC PR2 Activation Tanh()\nOptimizer Adam[31]\nFig. 1. Simulation environment. Left side: overview ofAOPE composed Optimizer Learningrates 0.001(θ),0.0003(φ)\noffourautomatedprocesses (FR,PC,PR1,andPR2).Itemswithdifferent Decayrate 0.8/250episodes\nshapes(squareandcircle)andcolorsdenotedifferentitemtypes.Rightside: Initialization Orthogonal\ndecisions taken bythecontroller ofeachprocess.",
      "size": 508,
      "sentences": 4
    },
    {
      "id": 41,
      "content": "C,PR1,andPR2).Itemswithdifferent Decayrate 0.8/250episodes\nshapes(squareandcircle)andcolorsdenotedifferentitemtypes.Rightside: Initialization Orthogonal\ndecisions taken bythecontroller ofeachprocess. Reward tofs 6.6(LM),6.4(HM)\n=== 페이지 6 ===\nTABLEIV TABLEV\nSTATESOFAGENTS CONTROLRULESOFFOURPROCESSES\nAgent States Process Controlrules fortaskselection\nNumbersofunsorteditemsandtypesinallocatedshipping Ashippingboxwiththemost/fewestitems (2)\nboxes (2), numbers of items and types waiting to be FR Ashippingboxwiththemost/fewestitem types(2)\nFR loaded on PC in allocated shipping boxes (2), number Ashippingboxselected byseedalgorithm (4)\nofunallocated shipping boxes (1),and numbers ofitems Anitem typewiththemost/fewest items,or\nPC\nandtypes inunallocated shippingboxes(2) farthest/closest toPR1(8)\nIDofconveyorontowhichitemsareloaded(1),numbers Aconveyor loadingaheaditemtypewiththe\nof loaded items (1×3), locations of the first and last PR1 most/fewest items,orfarthest/closest toPR1(8)\nPC l t o o a b d e ed lo i a t d e e m d s a ( t 2 l × oa 3 d ) i , ng nu p m o b rt e s rs (2 o × f 6 i ) te , m an s d a n n u d m t b y e p r e s s o w f a it i e ti m ng s PR2 A ite s m hi s p , p o in r g fa b r o th x es w t/ i c t l h os th es e t m to os P t R /fe 2 w ( e 8 s ) tunsorted\nloading onto conveyors and location ofwork-in-progress\nloadingports(2×3)\nLocation and direction of PR1 (2), numbers of items on\ncarousel conveyor (1), numbers of items on PC (1×3), • Each process contains stochastic uncertainties (e.g.,\nlocations of the first items on PC and the numbers of picking speeds of PR1 and PR2).",
      "size": 1607,
      "sentences": 2
    },
    {
      "id": 42,
      "content": "s on\ncarousel conveyor (1), numbers of items on PC (1×3), • Each process contains stochastic uncertainties (e.g.,\nlocations of the first items on PC and the numbers of picking speeds of PR1 and PR2). items whose types are the same as the first ones (2×3),\nPR1 location ofthelastitem onPC(1×3),numbers ofitems We have found that there is no prior approach using math-\nwaitingtobeloadedatloadingports(1×6),andnumbers ematical optimizations, metaheuristics, or artificial intelli-\nofitems loading into conveyors andlocation ofwork-in- gence that can comprehensively handle the above four fea-\nprogressloading ports(2×3)\ntures eventhoughrecentstudies have coveredsome of them\nLocationanddirectionofPR2(2),numberofsorteditems\n(1),arrayofitemsoncarouselconveyorwhethertheycan [28], [33]–[35].",
      "size": 786,
      "sentences": 2
    },
    {
      "id": 43,
      "content": "a-\nprogressloading ports(2×3)\ntures eventhoughrecentstudies have coveredsome of them\nLocationanddirectionofPR2(2),numberofsorteditems\n(1),arrayofitemsoncarouselconveyorwhethertheycan [28], [33]–[35]. Thus, the rule-based control, still actively\nPR2 or cannot be sorted into shipping boxes (28×4), and researched for warehouse optimization, was used for our\nnumberofunsorteditemsofshippingboxes (1×4)\nevaluation [36]. The sets of control rules of all processes\nCommon Time-step(1)andaction mask(numberofactions)\nare summarized in Table V, where a number in parentheses\nindicates the number of control rules for each process. We\npolicy was expected to learn to complete the current task appliedaseedalgorithmtofouroftheeightrulesofFR[37]. as quickly as possible. Agents in ILLR received ri as an The seed algorithm can select a shipping box with the best\nt\nimmediate reward for every decision they made.",
      "size": 901,
      "sentences": 6
    },
    {
      "id": 44,
      "content": "ithmtofouroftheeightrulesofFR[37]. as quickly as possible. Agents in ILLR received ri as an The seed algorithm can select a shipping box with the best\nt\nimmediate reward for every decision they made. In contrast, scoreobtainedbycomparingitemsincandidateandallocated\nagents in the other three MARL frameworks were trained boxes.Inthisevaluation,we estimated the scoreof shipping\nin the LTESR setting: the global reward was only given at boxesasthesimilarityofallocatedboxes.Thesimilaritywas\ntheterminalstatetransition.Theterminalglobalrewardrg calculated by accumulating weights (w\ns\n,w\nd\n), where w\ns\nis\ntml\nwas estimated as follows: added to the score if one item type in the candidate box\nis required for the allocated boxes, whereas w is added\nd\nrg = 2·[(tk c s−t ofs )4−1] if t c ≥t ofs , (5) if the item type is excluded from the allocated boxes.",
      "size": 851,
      "sentences": 4
    },
    {
      "id": 45,
      "content": "if one item type in the candidate box\nis required for the allocated boxes, whereas w is added\nd\nrg = 2·[(tk c s−t ofs )4−1] if t c ≥t ofs , (5) if the item type is excluded from the allocated boxes. To\ntml (−2·[(tk c s−t ofs )4+1] otherwise expand the choices of shipping boxes, we established seed\nalgorithm-based rules by changing weights, as (w ,w ) =\nwhere tks denotes the ksec-unit T when the last item is s d\nc c (1,0),(0,1),(1,−1), and (−1,1). sorted into the last shipping box, and t denotes an offset\nofs\ndependingonthepickingorderdataset.Becauset represents D. Results and Discussions\nc\na ksec-order value and the AOPE was computed at 10 1) Improving Training Performance with GAE in CDSC:\nfps, agents exhibit a long trajectory through the simulation.",
      "size": 761,
      "sentences": 3
    },
    {
      "id": 46,
      "content": "sents D. Results and Discussions\nc\na ksec-order value and the AOPE was computed at 10 1) Improving Training Performance with GAE in CDSC:\nfps, agents exhibit a long trajectory through the simulation. Figure 2 (a) shows training curves of CDSC with different\nTherefore, if a well-known γ ∼0.99 is used in the training, advantage estimations in AOPE with LM and HM pick-\nthe impact of r t g ml on C t will be significantly decayed ing orders. Each curve shows averaged performance with\nbecauseofthelongtrajectory.Topropagaterg throughthe a standard deviation over three random seeds. TD(0) has\ntml\ntrajectory, we introduced a scaling factor ζ listed in Table the worst performance among all methods, suggesting the\nIII into the discounting coefficients in Eqs. (2) and (3) as difficultyofapplyingtheTDmethodinLTESRasexplained\n(γλ)∆t/ζ and γ∆t/ζ.",
      "size": 843,
      "sentences": 5
    },
    {
      "id": 47,
      "content": "ted in Table the worst performance among all methods, suggesting the\nIII into the discounting coefficients in Eqs. (2) and (3) as difficultyofapplyingtheTDmethodinLTESRasexplained\n(γλ)∆t/ζ and γ∆t/ζ. in§III.B.2.ComparedwiththeMCmethod,GAEaccelerated\nthe training and achievedsignificantly better performancein\nC. Rule-based Control\ntheearlyphase.Thisperformancesuperioritywasmaintained\nTo compare the performanceof MARL-based control,we until the end of training for λ = 0.5 ∼ 0.95. Hence,\nestablished a set of controlrules for each process controller. the proposed GAE is an effective advantage estimation for\nControl features in AOPE are summarized as follows: CDSC in AOPE. • Decentralized control decisions with hierarchical struc- 2) Comparison of Different Control Methods: we sum-\nture (FR→PC→PR1→PR2). marize the training results of four MARL frameworks in\n• Task execution can be parallelized in a process (pro- Fig. 2 (b) and simulation results of AOPE obtained from\ncesses in FR and PC).",
      "size": 998,
      "sentences": 8
    },
    {
      "id": 48,
      "content": "→PR1→PR2). marize the training results of four MARL frameworks in\n• Task execution can be parallelized in a process (pro- Fig. 2 (b) and simulation results of AOPE obtained from\ncesses in FR and PC). different control methods in Table VI. Each value shows\n• Each process has different task granularities (shipping an average and standard deviation of T sampled from 192\nc\ndestinations for FR, item types for PC, and items for rollouts. As the baseline of each simulation, we added the\nPR1 and PR2). results, where all controllers selected their tasks at random,\n=== 페이지 7 ===\nFR PC PR1 PR2\n0\n-5\n-10\n-15\nLM HM\nFR 146.7 203.8\n-20 P P R C 1 1 1 0 7 5 6 . . 9 2 1 1 2 7 9 4 . . 2 2\nLM HM PR2 104.6 123.0\n-25 (2302 d.o.f.) Fig. 2. Training results of proposed MARL frameworks in AOPE with\nLM and HM picking orders. (a) learning curves of CDSC with different\nadvantage estimations. (b) Comparison of learning curves among four\nMARLframeworks.",
      "size": 936,
      "sentences": 16
    },
    {
      "id": 49,
      "content": "sults of proposed MARL frameworks in AOPE with\nLM and HM picking orders. (a) learning curves of CDSC with different\nadvantage estimations. (b) Comparison of learning curves among four\nMARLframeworks. (c)Comparisonofexplained variances ofeachAOPE\nagentamongfourMARLframeworks. as “Randomchoice.” The performanceof rule-basedcontrol\nrepresents the result of the best combination rules (4096 in\ntotal) listed in Table V. ILLR, the most localized MARL\nframework, achieved comparative performance to the rule-\nbased control for the LM picking order, outperforming the\nHMone.Becausethesemethodscontrolrobotswithashort-\nsighted plan to complete their tasks at hand as quickly as\npossible, the advantage of ILLR over rule-based control for\nthe HM picking order can be attributed to the flexibility in\ntask selection.",
      "size": 808,
      "sentences": 5
    },
    {
      "id": 50,
      "content": "rt-\nsighted plan to complete their tasks at hand as quickly as\npossible, the advantage of ILLR over rule-based control for\nthe HM picking order can be attributed to the flexibility in\ntask selection. The ILLR-trained policy can flexibly select\na task depending on the input oi reflecting the operation\nt\nstatus compared with the rule-based policy whose selection\nis solely based on the implemented rule. Such flexibility is\nmoreeffectivefortheHMpickingorderwhosetaskselection\ninPCsismorefrequentduetothelargevarietyofitemtypes. Thus, the MARL-based control can provide more efficient\noperations in industrial automated systems than the rule-\nbased control, even with localized training. We answer the research questions as follows.",
      "size": 731,
      "sentences": 5
    },
    {
      "id": 51,
      "content": ", the MARL-based control can provide more efficient\noperations in industrial automated systems than the rule-\nbased control, even with localized training. We answer the research questions as follows. RQ1: as shown in Table VI, CDSC-based control with\nGAE (λ = 0.5,0.75 for LM, HM picking orders) achieved\ntheshortestmakespansamongallmethods.Thus,theMARL\nagents achieve coordination by introducing both the global-\nization of training information and the unification of state\ntransitions to the shared critic. RQ2:themosteffectiveglobalizationfromILLRisthere-\nward setting causedby ILGR, where agentsshare the global\nreward.",
      "size": 623,
      "sentences": 4
    },
    {
      "id": 52,
      "content": "training information and the unification of state\ntransitions to the shared critic. RQ2:themosteffectiveglobalizationfromILLRisthere-\nward setting causedby ILGR, where agentsshare the global\nreward. Figure 2 (c) shows smoothed explained variance\nof state values averaged over different trials computed as\nTABLEVI\nCOMPARISONOFMAKESPANSAMONGDIFFERENTCONTROLMETHODS\nPicking order\nControlmethod\nLM HM\nRandomchoice 5385.0±188.9s 5227.8±180.6s\nRule-based 3273.4±110.6s 3599.5±120.1s\nILLR 3278.2±120.2s 3334.2±103.9s\nILGR 2941.9±62.9s 2975.9±90.8s\nMARL CDIC 2914.9±87.5s 2891.3±102.8s\nCDSC(MC) 2884.7±94.9s 2793.1±123.2s\nCDSC(GAE) 2834.3±69.4s 2638.2±101.3s\n%\nni\nsegnahC\nFig.3. PercentagechangeinmakespanwhenswitchingfromCDSC-trained\ntoILLR-trainedpoliciesforeachagent.StatisticsofWelch’st-testbetween\nCDSCandswitchedresultsarelistedintheinsertedtable:allcorresponding\np-values satisfyp<0.001(95%confidence interval).",
      "size": 910,
      "sentences": 4
    },
    {
      "id": 53,
      "content": "romCDSC-trained\ntoILLR-trainedpoliciesforeachagent.StatisticsofWelch’st-testbetween\nCDSCandswitchedresultsarelistedintheinsertedtable:allcorresponding\np-values satisfyp<0.001(95%confidence interval). Vi = 1−Var(Vi −Vi)/Var(Vi), where Vi denotes a exp φ φ exp\ntrue state value obtained from experiments, and Vi denotes\nφ\nthepredictionbythecritic.VPR1 andVPR2 ofILLRrapidly\nconverged to 1 and almost fully predicted the actual state\nvalues compared with the other three frameworks, whereas\nVFR and VPC of ILLR yield poor prediction. This result\nsuggests that ILLR agents were trained so locally that the\nagentsindownstreamprocessescouldeasilyinferindividual\nenvironmentalchanges,whereasonesinupstreamprocesses,\nsignificantlyaffectedbydownstreamperformance,couldnot\nachieve reasonable prediction. This localized training ten-\ndencywassignificantlyeliminated byintroducingthe global\nrewards:VFRandVPCweresignificantlyimprovedbyILGR\nin exchange for a slight decrease in VPR1 and VPR2.",
      "size": 979,
      "sentences": 4
    },
    {
      "id": 54,
      "content": "e prediction. This localized training ten-\ndencywassignificantlyeliminated byintroducingthe global\nrewards:VFRandVPCweresignificantlyimprovedbyILGR\nin exchange for a slight decrease in VPR1 and VPR2. The\nrelatively marginal contribution of state globalization via\nCDIC may be attributed to the difficulty in critic training\ndueto the increasein the numberofinputstates, aslisted in\nTable IV. In support of this consideration, CDSC with the\nMCmethodandthesameadvantageestimationoutperformed\nCDIC. Although the representation ability of CDIC individ-\nualcriticswaslost,thisdrawbackmayhavebeenresolvedby\naCDSCsinglecritic.Furthermore,theperformanceofCDSC\nwas improved by the proposed GAE, and thus, it achieved\nthe shortest T among all control methods. c\nRQ3: we evaluated performance degradation caused by\nchanging the policy for each process from CDSC to ILLR. Figure 3 shows the percentage change in the makespan\nwhenswitchingfromCDSC-trainedtoILLR-trainedpolicies\nfor each agent.",
      "size": 980,
      "sentences": 7
    },
    {
      "id": 55,
      "content": "degradation caused by\nchanging the policy for each process from CDSC to ILLR. Figure 3 shows the percentage change in the makespan\nwhenswitchingfromCDSC-trainedtoILLR-trainedpolicies\nfor each agent. The performance of CDSC was degraded\neven if its downstream policies (namely, PR1 and PR2)\nwere replaced with ILLR-trained policies. The performance\ndegradation in downstream processes suggests the coordi-\nnation throughout the overall processes, and such overall\ncoordination may be the first benefit of overall optimization\nviaCDSC. Furthermore,theILLR-trainedpolicyofFR(and\nPC)resultsinsignificantperformancedegradation.Thistrend\ncan be seen more prominently with the HM picking order,\nwhere PCs make decisions more frequently due to many\nitem types.",
      "size": 752,
      "sentences": 5
    },
    {
      "id": 56,
      "content": "-trainedpolicyofFR(and\nPC)resultsinsignificantperformancedegradation.Thistrend\ncan be seen more prominently with the HM picking order,\nwhere PCs make decisions more frequently due to many\nitem types. Such results are consistent with the comparison\nresults of explained variances, where ILLR causes poor\nstate value predictions in upstream processes, as described\nabove.Theupstreamprocessesmakedecisionsmoresparsely\nbecause their task granularity is bulkier, such as shipping\nbox (FR) and item type (PC). This sparse decision making\nis more susceptible to environmentalchanges caused by the\n[표 데이터 감지됨]\n\n=== 페이지 8 ===\ndownstream processes; thus, the ILLR framework fails to [13] L.Canese,G.C.Cardarilli, L.DiNunzio,R.Fazzolari, D.Giardino,\noptimizethecontrolofoperations.Hence,thesecondbenefit M.Re,andS.Spano`,“Multi-agent reinforcement learning: Areview\nof challenges and applications,” Appl. Sci., vol. 11, no.",
      "size": 912,
      "sentences": 5
    },
    {
      "id": 57,
      "content": ",R.Fazzolari, D.Giardino,\noptimizethecontrolofoperations.Hence,thesecondbenefit M.Re,andS.Spano`,“Multi-agent reinforcement learning: Areview\nof challenges and applications,” Appl. Sci., vol. 11, no. 11, p. 4948,\nof overall optimization in warehouse automation via CDSC\n2021.\nmaybetheimprovementinefficiencyinupstreamprocesses. [14] I.Bello,H.Pham,Q.V.Le,M.Norouzi,andS.Bengio,“Neuralcom-\nbinatorial optimization with reinforcement learning,” arXiv preprint\nV. CONCLUSION 1611.09940,2016. [15] G. Shen, R. Ma, Z. Tang, and L. Chang, “A deep reinforcement\nTo clarify the impact of overalloptimization on industrial\nlearningalgorithmforwarehousingmulti-agvpathplanning,”inProc. automation, we explored an efficient MARL framework oftheInt.Conf.onNetCIT. IEEE,2021,pp.421–429. that enables practical robot coordination.",
      "size": 816,
      "sentences": 9
    },
    {
      "id": 58,
      "content": "rningalgorithmforwarehousingmulti-agvpathplanning,”inProc. automation, we explored an efficient MARL framework oftheInt.Conf.onNetCIT. IEEE,2021,pp.421–429. that enables practical robot coordination. In the proposed [16] M.Li,B.Guo, J.Zhang,J.Liu,S.Liu,Z.Yu,Z.Li,and L.Xiang,\n“Decentralized multi-agv task allocation based on multi-agent rein-\nframework, agents were trained in CDSC, a CTDE man-\nforcementlearningwithinformationpotential fieldrewards,”inProc. ner using both globalized rewards and single shared critic. oftheIEEE18thInt.Conf.onMASS,2021,pp.482–489. Furthermore, we proposed the modified GAE for policy [17] Y. Xiao, J. Hoffman, T. Xia, and C. Amato, “Learning multi-robot\ndecentralized macro-action-based policies via acentralized q-net,” in\nupdate to improve the performance of CDSC to address\nProc.oftheIEEEInt.Conf.onICRA,2020,pp.10695–10701. the two major issues in typical industrial settings: LTESR [18] J. Wang and L. Sun, “Reducing bus bunching with asynchronous\nand AMADM.",
      "size": 998,
      "sentences": 9
    },
    {
      "id": 59,
      "content": "C to address\nProc.oftheIEEEInt.Conf.onICRA,2020,pp.10695–10701. the two major issues in typical industrial settings: LTESR [18] J. Wang and L. Sun, “Reducing bus bunching with asynchronous\nand AMADM. The evaluation results show that the CDSC- multi-agentreinforcementlearning,”arXivpreprint2105.00376,2021. [19] Y.Xiao,W.Tan,andC.Amato,“Asynchronousactor-criticformulti-\nbased control applied to task selections of MHE in AOPE\nagentreinforcement learning,” arXivpreprint2209.10113,2022. can achieve the shortest makespan compared with other [20] F.A.OliehoekandC.Amato,Aconciseintroductiontodecentralized\nMARL frameworks and rule-based controls. The results POMDPs. Springer, 2016.",
      "size": 681,
      "sentences": 7
    },
    {
      "id": 60,
      "content": "022. can achieve the shortest makespan compared with other [20] F.A.OliehoekandC.Amato,Aconciseintroductiontodecentralized\nMARL frameworks and rule-based controls. The results POMDPs. Springer, 2016. [21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\nalso suggest that the overall optimization has the following\n“Proximalpolicyoptimizationalgorithms,”arXivpreprint1707.06347,\nadvantages for warehouse automation: bottom-up efficiency 2017.\nthrough process coordination and further efficiency of up- [22] C. Yu, A. Velu, E. Vinitsky, Y. Wang, A. Bayen, and Y. Wu, “The\nsurprising effectiveness of ppo in cooperative multi-agent games,”\nstream process control.",
      "size": 676,
      "sentences": 4
    },
    {
      "id": 61,
      "content": "rdination and further efficiency of up- [22] C. Yu, A. Velu, E. Vinitsky, Y. Wang, A. Bayen, and Y. Wu, “The\nsurprising effectiveness of ppo in cooperative multi-agent games,”\nstream process control. Although the obtained knowledge\narXivpreprint2105.00376, 2021.\nis limited to our experimental design, we believe that we [23] N.Ascheuer,M.Gro¨tschel,andA.Abdel-AzizAbdel-Hamid,“Order\ncan qualitatively and quantitatively clarify the impact of pickinginanautomatic warehouse: Solvingonlineasymmetrictsps,”\nMath.Oper.Res.,vol.49,pp.501–515, 1999.\noverall optimization on various types of automated systems\n[24] J. M. Framinan and R. Leisten, “Total tardiness minimization in\nwith different layouts and configurationsby introducing our permutationflowshops:asimpleapproachbasedonavariablegreedy\nproposed MARL frameworks for future work. algorithm,” Int.J.Prod.Res.,vol.46,no.22,pp.6479–6498, 2008.",
      "size": 894,
      "sentences": 3
    },
    {
      "id": 62,
      "content": "and configurationsby introducing our permutationflowshops:asimpleapproachbasedonavariablegreedy\nproposed MARL frameworks for future work. algorithm,” Int.J.Prod.Res.,vol.46,no.22,pp.6479–6498, 2008. [25] A.OroojlooyJadidandD.Hajinezhad,“Areviewofcooperativemulti-\nREFERENCES agentdeepreinforcementlearning,”arXivpreprint1908.03963,2019. [26] C.S.deWitt,T.Gupta,D.Makoviichuk,V.Makoviychuk,P.H.Torr,\n[1] L. Custodio and R. Machado, “Flexible automated warehouse: a M.Sun,andS.Whiteson,“Isindependentlearningallyouneedinthe\nliterature review and an innovative framework,” Int. J. Adv. Manuf. starcraft multi-agent challenge?” arXivpreprint2011.09533,2020. Technol., vol.106,no.1,pp.533–558,2020. [27] L.Xie, N.Thieme, R. Krenzler, andH. Li,“Introducing split orders\n[2] “Welcome to the presentation of World Robotics 2021,” presented and optimizing operational policies in robotic mobile fulfillment\nat the World Robotics.",
      "size": 920,
      "sentences": 10
    },
    {
      "id": 63,
      "content": "enzler, andH. Li,“Introducing split orders\n[2] “Welcome to the presentation of World Robotics 2021,” presented and optimizing operational policies in robotic mobile fulfillment\nat the World Robotics. International Federation of Robotics, 2021. systems,”Eur.J.Oper.Res.,vol.288,no.1,pp.80–97,2021. [Online]. Available: https://ifr.org/downloads/press2018/2021 1028 [28] I. Suemitsu, H. K. Bhamgara, K. Utsugi, J. Hashizume, and K. Ito,\nWRPKPresentation longversion.pdf\n“Fast simulation-based order sequence optimization assisted by pre-\n[3] K. Azadeh, R. De Koster, and D. Roy, “Robotized and automated trainedbayesianrecurrentneuralnetwork,”IEEERobot.Autom.Lett.,\nwarehouse systems: Review and recent developments,” Transp. Sci.,\nvol.7,no.3,pp.7818–7825, 2022.\nvol.53,no.4,pp.917–945, 2019. [29] D. Fu¨ßler and N. Boysen, “High-performance order processing in\n[4] T.T.Nguyen,N.D.Nguyen,andS.Nahavandi,“Deepreinforcement picking workstations,” EURO J. Transp. Logist., vol. 8, no. 1, pp.",
      "size": 986,
      "sentences": 10
    },
    {
      "id": 64,
      "content": "19. [29] D. Fu¨ßler and N. Boysen, “High-performance order processing in\n[4] T.T.Nguyen,N.D.Nguyen,andS.Nahavandi,“Deepreinforcement picking workstations,” EURO J. Transp. Logist., vol. 8, no. 1, pp. learning for multiagent systems: A review of challenges, solutions,\n65–90,2019. andapplications,”IEEETrans.Cybern.,vol.50,no.9,pp.3826–3839,\n[30] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\n2020. D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\n[5] NVIDIA,“DeveloponNVIDIAOmniverse,”https://developer.nvidia. reinforcement learning,” inProc.ofthe33rdICML,2016,pp.1928–\ncom/nvidia-omniverse-platform, accessed: 2023-02-20. 1937. [6] P.R.Wurman,R.D’Andrea,andM.Mountz,“Coordinatinghundreds\n[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\nofcooperative,autonomousvehiclesinwarehouses,”AIMag.,vol.29,\ntion,”inProc.ofthe3rdInt.Conf.onLearningRepresentations,2015. no.1,pp.9–20,Mar.2008.",
      "size": 940,
      "sentences": 12
    },
    {
      "id": 65,
      "content": "a and J. Ba, “Adam: A method for stochastic optimiza-\nofcooperative,autonomousvehiclesinwarehouses,”AIMag.,vol.29,\ntion,”inProc.ofthe3rdInt.Conf.onLearningRepresentations,2015. no.1,pp.9–20,Mar.2008. [32] S.HuangandS.Ontan˜o´n,“Acloserlookatinvalidactionmaskingin\n[7] J. J. Bartholdi and S. T. Hackman, Warehouse & Distribution policygradient algorithms,” arXivpreprint2006.14171, 2020. Science, Release 0.98.1. Atlanta, GA: Georgia Tech.,\n[33] S. A. B. Rasmi, Y. Wang, and H. Charkhgard, “Wave order picking\n2017. [Online]. Available: https://www.warehouse-science.com/book/ under the mixed-shelves storage strategy: A solution method and\neditions/wh-sci-0.98.1.pdf advantages,” Comput.Oper.Res.,vol.137,p.105556,2022.",
      "size": 719,
      "sentences": 8
    },
    {
      "id": 66,
      "content": "ne]. Available: https://www.warehouse-science.com/book/ under the mixed-shelves storage strategy: A solution method and\neditions/wh-sci-0.98.1.pdf advantages,” Comput.Oper.Res.,vol.137,p.105556,2022. [8] Y. Jaghbeer, R. Hanson, and M. I. Johansson, “Automated order\n[34] M. Kordos, J. Boryczko, M. Blachnik, and S. Golak, “Optimization\npicking systems and the links between design and performance: a ofwarehouseoperationswithgeneticalgorithms,”Appl.Sci.,vol.10,\nsystematic literature review,” Int. J. Prod. Res., vol. 58, no. 15, pp. no.14,p.4817,2020. 4489–4505, 2020. [35] J.Park,J.Chun,S.H.Kim,Y.Kim,andJ.Park,“Learningtoschedule\n[9] L.ZhenandH.Li,“Aliteraturereviewofsmartwarehouseoperations\njob-shop problems: representation and policy learning using graph\nmanagement,” Front.Eng.Manag.,pp.1–25,2022.",
      "size": 805,
      "sentences": 11
    },
    {
      "id": 67,
      "content": "J.Park,“Learningtoschedule\n[9] L.ZhenandH.Li,“Aliteraturereviewofsmartwarehouseoperations\njob-shop problems: representation and policy learning using graph\nmanagement,” Front.Eng.Manag.,pp.1–25,2022. neuralnetworkandreinforcementlearning,”Int.J.Prod.Res.,vol.59,\n[10] M.Tan,“Multi-agentreinforcementlearning:Independentvs.cooper-\nno.11,pp.3360–3377, 2021.\native agents,”inProc.ofthe10thICML,1993,pp.330–337. [36] Y.A.BozerandC.Eamrungroj,“Throughputanalysisofmulti-device\n[11] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,\ntrip-basedmaterialhandlingsystemsoperatingunderthemodified-fcfs\n“Multi-agent actor-critic for mixed cooperative-competitive environ- dispatching rule,” Int. J. Prod. Res., vol. 56, no. 4, pp. 1486–1503,\nments,”inProc.ofthe31stInt.Conf.onNeurIPS,2017,p.6382–6393. 2018.",
      "size": 808,
      "sentences": 10
    },
    {
      "id": 68,
      "content": "i-agent actor-critic for mixed cooperative-competitive environ- dispatching rule,” Int. J. Prod. Res., vol. 56, no. 4, pp. 1486–1503,\nments,”inProc.ofthe31stInt.Conf.onNeurIPS,2017,p.6382–6393. 2018. [12] T. Pulikottil, L. A. Estrada-Jimenez, H. U. Rehman, J. Barata,\n[37] E. Elsayed, “Algorithms for optimal material handling in automatic\nS.Nikghadam-Hojjati, andL.Zarzycki,“Multi-agent basedmanufac- warehousingsystems,”Int.J.Prod.Res.,vol.19,no.5,pp.525–535,\nturing: current trends andchallenges,” inProc.ofthe26thIEEEInt. 1981. Conf.onETFA,2021,pp.1–7.",
      "size": 556,
      "sentences": 11
    }
  ]
}