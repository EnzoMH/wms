{
  "source": "ArXiv",
  "filename": "048_F-LOAM__Fast_LiDAR_Odometry_And_Mapping.pdf",
  "total_chars": 33297,
  "total_chunks": 47,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nF-LOAM : Fast LiDAR Odometry and Mapping\nHan Wang, Chen Wang, Chun-Lin Chen, and Lihua Xie, Fellow, IEEE\nAbstract—Simultaneous Localization and Mapping (SLAM)\nhas wide robotic applications such as autonomous driving (a) (b)\nand unmanned aerial vehicles. Both computational efficiency\nand localization accuracy are of great importance towards a\ngood SLAM system. Existing works on LiDAR based SLAM\noften formulate the problem as two modules: scan-to-scan\nmatch and scan-to-map refinement. Both modules are solved\nbyiterativecalculationwhicharecomputationallyexpensive.In\n(c)\nthis paper, we propose a general solution that aims to provide\nacomputationallyefficientandaccurateframeworkforLiDAR\nbased SLAM. Specifically, we adopt a non-iterative two-stage\ndistortion compensation method to reduce the computational\ncost.",
      "size": 830,
      "sentences": 5
    },
    {
      "id": 2,
      "content": "hat aims to provide\nacomputationallyefficientandaccurateframeworkforLiDAR\nbased SLAM. Specifically, we adopt a non-iterative two-stage\ndistortion compensation method to reduce the computational\ncost. For each scan input, the edge and planar features are\nextracted and matched to a local edge map and a local plane\nmap separately, where the local smoothness is also considered\nfor iterative pose optimization. Thorough experiments are\nperformedtoevaluateitsperformanceinchallengingscenarios, Fig. 1: Example of the proposed method on KITTI dataset. including localization for a warehouse Automated Guided (a) shows the mapping result on sequence 05. (b) is the\nVehicle(AGV)andapublicdatasetonautonomousdriving.The reconstructed 3D road scenery by integrating camera view. proposed method achieves a competitive localization accuracy (c) plots the trajectory from F-LOAM and the ground truth.",
      "size": 890,
      "sentences": 8
    },
    {
      "id": 3,
      "content": "autonomousdriving.The reconstructed 3D road scenery by integrating camera view. proposed method achieves a competitive localization accuracy (c) plots the trajectory from F-LOAM and the ground truth. with a processing rate of more than 10 Hz in the public\ndataset evaluation, which provides a good trade-off between\nperformanceandcomputationalcostforpracticalapplications. It is one of the most accurate and fastest open-sourced SLAM\nsystems1 in KITTI dataset ranking. to dynamic environments. For example, [5] showed good\nperformanceinindoorscenario,butthelocalizationaccuracy\nI. INTRODUCTION\ndrops a lot in outdoor environment. Another challenge is\nSimultaneous Localization and Mapping (SLAM) is one the computational cost. In many robotic platforms such\nof the most fundamental research topics in robotics.",
      "size": 810,
      "sentences": 9
    },
    {
      "id": 4,
      "content": "tdoor environment. Another challenge is\nSimultaneous Localization and Mapping (SLAM) is one the computational cost. In many robotic platforms such\nof the most fundamental research topics in robotics. It is the as UAVs, the computational resources are limited, where\ntask to localize the robot and build the surrounding map in the on-board processing unit is supposed to perform high\nan unknown or partially unknown environment based on on- frequency localization and path planning at the same time\nboardsensors.Accordingtotheperceptualdevices,itcanbe [6]. Moreover, many works are not open-sourced thus it is\nroughlycategorizedasLiDARandVisualSLAM.Compared difficult to retrieve the same result. to visual SLAM, LiDAR SLAM is often more accurate in The most classical approach to estimate the transform\npose estimation and is robust to environmental variations between two scans is Iterative Closest Point (ICP) [7],\nsuch as illumination and weather change [1].",
      "size": 961,
      "sentences": 6
    },
    {
      "id": 5,
      "content": "sical approach to estimate the transform\npose estimation and is robust to environmental variations between two scans is Iterative Closest Point (ICP) [7],\nsuch as illumination and weather change [1]. Therefore, Li- where the two scans are aligned iteratively by minimizing\nDARSLAMiswidelyadoptedbymanyroboticapplications point cloud distance. However, a large number of points\nsuch as autonomous driving [2], drone inspection [3], and are involved in optimization, which is computationally in-\nwarehouse manipulation [4]. efficient. Another approach is to match features that is\nAlthoughexistingworksonLiDARSLAMhaveachieved more computationally efficient. A typical example is Lidar\ngood performance in public dataset evaluation, there are Odometry And Mapping (LOAM) [8] that extracts edge and\nstill some limitations in practical applications.",
      "size": 844,
      "sentences": 6
    },
    {
      "id": 6,
      "content": "efficient. A typical example is Lidar\ngood performance in public dataset evaluation, there are Odometry And Mapping (LOAM) [8] that extracts edge and\nstill some limitations in practical applications. One of the planar features and calculates the pose by minimizing point-\nlimitations is the robustness over different environments, to-planeandpoint-to-edgedistance.However,bothdistortion\ne.g., from indoor to outdoor environments and from static compensationandlaserodometryrequireiterativecalculation\nwhich are still computationally expensive. The work is supported by Delta-NTU Corporate Laboratory for Cyber-\nIn this paper, we introduce a lightweight LiDAR SLAM\nPhysical Systems under the National Research Foundation Corporate Lab\n@UniversityScheme.Thedemonstrationvideocanbefoundathttps: that targets to provide a practical real-time LiDAR SLAM\n//youtu.be/QvXN5XhAYYw.",
      "size": 872,
      "sentences": 4
    },
    {
      "id": 7,
      "content": "Systems under the National Research Foundation Corporate Lab\n@UniversityScheme.Thedemonstrationvideocanbefoundathttps: that targets to provide a practical real-time LiDAR SLAM\n//youtu.be/QvXN5XhAYYw. solutiontopublic.Anovelframeworkispresentedthatcom-\nHan Wang, Chun-Lin Chen, and Lihua Xie are with the School\nbines feature extraction, distortion compensation, pose opti-\nof Electrical and Electronic Engineering, Nanyang Technological\nUniversity, 50 Nanyang Avenue, Singapore 639798. e-mail: mization, and mapping. Compared to traditional method, we\n{wang.han,ChenCL,elhxie}@ntu.edu.sg\nuseanon-iterativetwo-stagedistortioncompensationmethod\nChen Wang is with the Robotics Institute, Carnegie Mellon University,\nPittsburgh,PA15213,USA.e-mail: chenwang@dr.com to replace the computationally inefficient iterative distortion\n1https://github.com/wh200720041/floam compensation method.",
      "size": 882,
      "sentences": 3
    },
    {
      "id": 8,
      "content": ", Carnegie Mellon University,\nPittsburgh,PA15213,USA.e-mail: chenwang@dr.com to replace the computationally inefficient iterative distortion\n1https://github.com/wh200720041/floam compensation method. It is observed that edge features with\n1202\nluJ\n2\n]OR.sc[\n1v22800.7012:viXra\n=== 페이지 2 ===\nhigher local smoothness and planar features with lower component in SLAM. The LiDAR odometry often comes\nsmoothnessareoftenconsistentlyextractedoverconsecutive with inevitable drifts which can be significant in long-term\nscans.Thosepoints aremoreimportantfor matching.Hence SLAM.Henceinsomelargescalescenarios,theloopclosure\nthelocalgeometryfeatureisalsoconsideredfortheiterative serves in the back-end to identify repetitive places. For\npose estimation to improve the localization accuracy. It is example, in LiDAR-only Odometry and Localization (LOL)\nable to achieve real time performance up to 20 Hz on a the author extends LiDAR odometry into a full SLAM with\nlow power embedded computing unit.",
      "size": 989,
      "sentences": 5
    },
    {
      "id": 9,
      "content": "xample, in LiDAR-only Odometry and Localization (LOL)\nable to achieve real time performance up to 20 Hz on a the author extends LiDAR odometry into a full SLAM with\nlow power embedded computing unit. To demonstrate the loop closure [12]. By closing the loop, odometry drifting\nrobustness, a thorough evaluation of the proposed method is error can be corrected. Fusion with Inertial Measurement\npresented, including both indoor and outdoor experiments. Unit (IMU) is an alternative solution. Shan et al propose\nCompared to existing state-of-the-art methods, our method a fusion method with IMU and GPS named LIO-SLAM\nis able to achieve competitive localization accuracy at a [13]. Different sensor inputs are synchronized and tightly-\nlow computational cost, which is a good trade-off between coupled. The fused SLAM system is proven to be more\nperformanceandspeed.Itisworthynotingthattheproposed accurate than LOAM in outdoor environment.",
      "size": 938,
      "sentences": 8
    },
    {
      "id": 10,
      "content": "omputational cost, which is a good trade-off between coupled. The fused SLAM system is proven to be more\nperformanceandspeed.Itisworthynotingthattheproposed accurate than LOAM in outdoor environment. Fusion-based\nmethod is one of the most accurate and fastest open-sourced approaches have shown improvement on the localization\nmethods in the KITTI benchmark. accuracy.However,multi-sensorfusionrequiressynchroniza-\nThis paper is organized as follows: Section II reviews tion and comprehensive calibration. the related work on existing LiDAR SLAM approaches. In the past few years there are also some deep learning\nSection III describes the details of the proposed approach, related works that attempt to use Convolution Neural Net-\nincluding feature point selection, laser points alignment, work (CNN) for point cloud processing. Rather than hand-\nlaser odometry estimation, and mapping. Section IV shows crafted features, features are extracted by CNN training.",
      "size": 962,
      "sentences": 8
    },
    {
      "id": 11,
      "content": "on, laser points alignment, work (CNN) for point cloud processing. Rather than hand-\nlaser odometry estimation, and mapping. Section IV shows crafted features, features are extracted by CNN training. experiment results, followed by conclusion in Section V. For example, in CAE-LO [14], the author proposes a deep-\nlearning-based feature point selection rules via unsupervised\nII. RELATEDWORK\nConvolutional Auto-Encoder (CAE) [15], while the LiDAR\nThe most essential step in LiDAR SLAM is the matching odometryestimationisstillbasedonfeaturepointsmatching. of point clouds. Existing works mainly leverage on finding The experiment result shows that the learnt features can\nthecorrespondencebetweenpointcloud,includingtwomain improve the matching success rate.",
      "size": 758,
      "sentences": 7
    },
    {
      "id": 12,
      "content": "point clouds. Existing works mainly leverage on finding The experiment result shows that the learnt features can\nthecorrespondencebetweenpointcloud,includingtwomain improve the matching success rate. Although deep learn-\ncategories: raw point cloud matching and feature point pairs ing approaches have shown good performance in public\nmatching.Forrawpointcloudmatching,theIterativeClosest dataset, it is not robust in different environment such as\nPoints (ICP) [7] method is the most classic method. ICP indoor/outdoor and city/rural scenarios. For example, [16]\nmeasures the correspondent points by finding the closest hasagoodperformanceinsemanticKITTIdataset.However,\npoint in Euclidean space. By iteratively minimizing the thesamealgorithmfailswhenmovingtoindoorenvironment\ndistance residual between corresponding points, the pose or in some complex environment. transition between two point clouds converges to the final\nIII.",
      "size": 930,
      "sentences": 7
    },
    {
      "id": 13,
      "content": "amealgorithmfailswhenmovingtoindoorenvironment\ndistance residual between corresponding points, the pose or in some complex environment. transition between two point clouds converges to the final\nIII. METHODOLOGY\nlocation.AnimprovedpointcloudmatchingisIMLS-SLAM\n[9], where a weight value is assigned to each point. Each In this section, the proposed method is described in\nweight is derived by IMplicit Least Square (IMLS) method details. We firstly present the sensor model and feature\nbased on the local surface normal of target point. However, extraction. The extracted features are then calibrated with\nit is often computationally costly to apply raw point cloud distortion compensation. Then, we present general feature\nmatching,e.g.,ittakes1.2stoestimateoneframeinIMLS- extraction and global feature estimation. Lastly we explain\nSLAM that is far from real-time performance requirement. the calculation of laser odometry as well as mapping.",
      "size": 945,
      "sentences": 10
    },
    {
      "id": 14,
      "content": ".2stoestimateoneframeinIMLS- extraction and global feature estimation. Lastly we explain\nSLAM that is far from real-time performance requirement. the calculation of laser odometry as well as mapping. The feature-based methods mainly leverage on point-to-\nA. Sensor Model and Feature Extraction\nsurface/edgematchingandarepopularlyused.Introducedby\nLOAM [10], this has become the standard for the following A mechanical 3D LiDAR perceives the surrounding envi-\nwork.Theedgesandsurfacesareextractedinadvancebased ronment by rotating a vertically aligned laser beam array\nonlocalsmoothnessanalysis,andthepointsfromthecurrent of size M. It scans vertical planes with M readings in\npoint cloud are matched to the edges and surfaces from the parallel. The laser array is rotating in a horizontal plane\nmap. The distance cost is formulated as the euclidean dis- at constant speed during each scan interval, while the laser\ntance between the points and edges and surfaces.",
      "size": 963,
      "sentences": 6
    },
    {
      "id": 15,
      "content": "rotating in a horizontal plane\nmap. The distance cost is formulated as the euclidean dis- at constant speed during each scan interval, while the laser\ntance between the points and edges and surfaces. There are measurements are taken in clockwise or counterclockwise\nalso many works that extends LOAM to achieve better per- order sequentially [17]. For a mechanical LiDAR sensor\nformance, e.g., LeGO-LOAM separates ground optimization model,wedenotethekth LiDARscanasP ,andeachpoint\nk\nbefore feature extraction. For Autonomous Guided Vehicles as p(m,n), where m ∈ [1,M] and n ∈ [1,N]. Raw point\nk\n(AGVs)[11],thez-axisisnotimportantandmostofcasewe cloud matching methods such as ICP are sensitive to noise\ncanassumerobotmovingin2Dspace.Hence,groundpoints anddynamicobjectssuchashumanforautonomousdriving. are not contributing to the localization of x-y plane. This Moreover,aLiDARscancontainstensofthousandsofpoints\napproach is also widely implemented in ground vehicles.",
      "size": 969,
      "sentences": 8
    },
    {
      "id": 16,
      "content": "suchashumanforautonomousdriving. are not contributing to the localization of x-y plane. This Moreover,aLiDARscancontainstensofthousandsofpoints\napproach is also widely implemented in ground vehicles. which makes ICP computationally inefficient. Compared\nSomeworksattempttoimprovetheperformancebyintro- to raw point cloud matching method such as ICP, feature\nducing extra modules. Loop closure detection is another key point matching is more robust and efficient in practice. To\n=== 페이지 3 ===\nimprove the matching accuracy and matching efficiency, we can be corrected by:\nleverage surface features and edge features, while discard\nthose noisy or less significant points. As mentioned above, P˜ k ={T k (δt)p( k m,n) | p( k m,n) ∈P k }. (4)\nthe point cloud returned by a 3D mechanical LiDAR is\nsparseinverticaldirectionanddenseinhorizontaldirection. The undistorted features will be used to find the robot pose\nTherefore, horizontal features are more distinctive and it in the next section.",
      "size": 988,
      "sentences": 10
    },
    {
      "id": 17,
      "content": "is\nsparseinverticaldirectionanddenseinhorizontaldirection. The undistorted features will be used to find the robot pose\nTherefore, horizontal features are more distinctive and it in the next section. is less likely to have false feature detection in horizontal\nplane. For each point cloud, we focus on horizontal plane C. Pose Estimation\nand evaluate the smoothness of local surface by\nThe pose estimation aligns the current undistorted edge\nσ k (m,n) = (cid:12) (cid:12)S(m 1 ,n) (cid:12) (cid:12) (cid:88) (||p( k m,j)−p( k m,n)||), (1) f m ea a t p u . re T s h E e ˜ k g a lo n b d al pl f a e n a a t r ur f e ea m tu a re p s c S o ˜ k ns w is i t t s h o th f e a g e lo d b g a e l f f e e a a t t u u r r e e\n(cid:12) k (cid:12)p( k m,j)∈S k (m,n) map and a planar feature map and they are updated and\nmaintainedseparately.Toreducethesearchingcomputational\nwhere S(m,n) is the adjacent points of p(m,n) in horizontal\nk k cost, both edge and planar map are stored in 3D KD-trees.",
      "size": 987,
      "sentences": 5
    },
    {
      "id": 18,
      "content": "d they are updated and\nmaintainedseparately.Toreducethesearchingcomputational\nwhere S(m,n) is the adjacent points of p(m,n) in horizontal\nk k cost, both edge and planar map are stored in 3D KD-trees. direction and |S(m,n)| is the number of points in local\nk Similar to [19], the global line and plane are estimated by\npoint cloud. S k (m,n) can be easily collected based on point collecting nearby points from the edge and planar feature\nID n, which can reduce the computational cost compared map. For each edge feature point p ∈ E˜, we compute\nE k\nto local searching. In experiment we pick 5 points along the covariance matrix of its nearby points from the global\nthe clockwise and counterclockwise, respectively. For a flat edge feature map. When the points are distributed in a\nsurface such as walls, the smoothness value is small while line, the covariance matrix contains one eigenvalue that is\nfor corner or edge point, the smoothness value is large. For much larger.",
      "size": 973,
      "sentences": 8
    },
    {
      "id": 19,
      "content": "d in a\nsurface such as walls, the smoothness value is small while line, the covariance matrix contains one eigenvalue that is\nfor corner or edge point, the smoothness value is large. For much larger. The eigenvector ng associated with the largest\nE\neach scan plane P k , edge feature points are selected with eigenvalue is considered as the line orientation and the\nhigh σ, and surface feature points are selected with low σ. position of the line pg is taken as the geometric center of\nE\nTherefore, we can formulate the edge feature set as E k and the nearby points. Similarly, for each planar feature point\nthe surface feature set as S k . p S ∈ S˜ k , we can get a global plane with position pg S and\nsurface norm ng. Note that different from global edge, the\nB. Motion Estimation and Distortion Compensation S\nnorm of global plane is taken as the eigenvector associated\nIn existing works such as LOAM [8] and LeGO-LOAM with the smallest eigenvalue.",
      "size": 951,
      "sentences": 7
    },
    {
      "id": 20,
      "content": "edge, the\nB. Motion Estimation and Distortion Compensation S\nnorm of global plane is taken as the eigenvector associated\nIn existing works such as LOAM [8] and LeGO-LOAM with the smallest eigenvalue. [11],distortioniscorrectedbyscan-to-scanmatchwhichiter- Once the corresponding optimal edges/planes are derived,\nativelyestimatesthetransformationbetweentwoconsecutive wecanfindthelinkedglobaledgesorplanesforeachfeature\nlaser scans. However, iterative calculation is required to find pointsfromP˜ .Suchcorrespondencecanbeusedtoestimate\nk\nthe transformation matrix which is computationally ineffi- the optimal pose between current frame and global map by\ncient. In this paper, we propose to use two-stage distortion minimizing the distance between feature points and global\ncompensation to reduce the computational cost. Note that edges/planes.",
      "size": 843,
      "sentences": 6
    },
    {
      "id": 21,
      "content": "al map by\ncient. In this paper, we propose to use two-stage distortion minimizing the distance between feature points and global\ncompensation to reduce the computational cost. Note that edges/planes. The distance between the edges features and\nmostexisting3DLiDARsareabletorunatmorethan10Hz global edge is:\nand the time elapsed between two consecutive LiDAR scans\nis often very short. Hence we can firstly assume constant f\nE\n(p\nE\n)=p\nn\n•((T\nk\np\nE\n−pg\nE\n)×ng\nE\n), (5)\nangular velocity and linear velocity during short period to\npredict the motion and correct the distortion. In the second where symbol • is the dot product and p is the unit vector\nn\nstage, the distortion will be re-computed after the pose esti- given by\nmationprocessandthere-computedundistortedfeatureswill (T p −pg)×ng\np = k E E E . (6)\nbe updated to the final map.",
      "size": 835,
      "sentences": 7
    },
    {
      "id": 22,
      "content": "unit vector\nn\nstage, the distortion will be re-computed after the pose esti- given by\nmationprocessandthere-computedundistortedfeatureswill (T p −pg)×ng\np = k E E E . (6)\nbe updated to the final map. In the experiment we find that n ||(T p −pg)×ng||\nk E E E\nthetwo-stagedistortioncompensationcanachievethesimilar\nlocalization accuracy but at much less computational cost. The distance between the planar features and global plane:\nDenote the robot’s pose at kth scan as a 4x4 homogeneous\ntransformation matrix T k , the 6-DoF transform between two f S (p S )=(T k p S −pg S )•ng S . (7)\nconsecutive frames k−1 and k can be estimated by:\nTraditional feature matching only optimizes the geometry\nξk =log (cid:0) T −1T (cid:1) , (2) distance mentioned above, while the local geometry distri-\nk−1 k−2 k−1\nbutionofeachfeaturepointisnotconsidered.However,Itis\nwhere ξ ∈se(3).",
      "size": 869,
      "sentences": 5
    },
    {
      "id": 23,
      "content": "optimizes the geometry\nξk =log (cid:0) T −1T (cid:1) , (2) distance mentioned above, while the local geometry distri-\nk−1 k−2 k−1\nbutionofeachfeaturepointisnotconsidered.However,Itis\nwhere ξ ∈se(3). The transform of small period δt between\nobservedthatedgefeatureswithhigherlocalsmoothnessand\nthe consecutive scans can be estimated by linear interpola-\nplanar features with lower smoothness are often consistently\ntion:\n(cid:18) (cid:19) extracted over consecutive scans, which is more important\nN −n\nT k (δt)=T k−1 exp N ξ k k −1 , (3) for matching. Therefore, a weight function is introduced\nto further balance the matching process. To reduce the\nwhere function exp(ξ) transforms a Lie algebra into Lie computational cost, the local smoothness defined previously\ngroup defined by [18]. The distortion of current scan P is re-used to determine the weight function. For each edge\nk\n=== 페이지 4 ===\n(a) (b) (c) (d) (e)\n(f) (g) (h) (i) (j)\nFig. 2: Result of proposed F-LOAM on KITTI dataset.",
      "size": 987,
      "sentences": 7
    },
    {
      "id": 24,
      "content": "The distortion of current scan P is re-used to determine the weight function. For each edge\nk\n=== 페이지 4 ===\n(a) (b) (c) (d) (e)\n(f) (g) (h) (i) (j)\nFig. 2: Result of proposed F-LOAM on KITTI dataset. The estimated trajectory and ground truth are plotted in green and\nred color respectively. (a)-(e) sequence 00-04. (f)-(j): sequence 06-10.\npoint p with local smoothness σ and each plane point p odometry. The current pose estimation T∗ can be solved\nE E S k\nwith local smoothness σ , the weight is defined by: by iterative pose optimization until it converges. S\nW(p )= exp(−σ E ) D. Mapping Building & Distortion Compensation Update\nE (cid:80) p(i,j)∈E˜ k exp (cid:16) −σ k (i,j) (cid:17) (8) glo T b h a e l p g l l a o n b a a r l m m ap ap an c d on is si u st p s da o t f ed a b g as lo e b d a o l n e k d e g y e fr m am ap es.",
      "size": 835,
      "sentences": 8
    },
    {
      "id": 25,
      "content": "(cid:80) p(i,j)∈E˜ k exp (cid:16) −σ k (i,j) (cid:17) (8) glo T b h a e l p g l l a o n b a a r l m m ap ap an c d on is si u st p s da o t f ed a b g as lo e b d a o l n e k d e g y e fr m am ap es. a E nd ach a\nexp(σ )\nW(p S )= (cid:80) ex S p (cid:16) σ(i,j) (cid:17) , keyframeisselectedwhenthetranslationalchangeisgreater\np(i,j)∈S˜ k k than a predefined translation threshold, or rotational change\nisgreaterthanapredefinedrotationthreshold.Thekeyframe-\nwhere E and S are the edge features and plane features at\nk k\nbased map update can reduce the computational cost com-\nkth scan.Thenewposeisestimatedbyminimizingweighted\npared to frame-by-frame update. As mentioned in Section\nsum of the point-to-edge and point-to-planar distance:\nIII-B,thedistortioncompensationisperformedbasedonthe\n(cid:88) (cid:88)\nmin W(p )f (p )+ W(p )f (p ). (9) constantvelocitymodelinsteadofiterativemotionestimation\nE E E S S S\nTk in order to reduce the computational cost.",
      "size": 956,
      "sentences": 4
    },
    {
      "id": 26,
      "content": "compensationisperformedbasedonthe\n(cid:88) (cid:88)\nmin W(p )f (p )+ W(p )f (p ). (9) constantvelocitymodelinsteadofiterativemotionestimation\nE E E S S S\nTk in order to reduce the computational cost. However, this\nThe optimal pose estimation can be derived by solving is less accurate than iterative distortion compensation in\nthe non-linear equation through Gauss-Newton method. The LOAM. Hence, in the second stage, the distortion is re-\nJacobian can be estimated by applying left perturbation computed based on the optimization result T∗ from the last\nk\nmodel with δξ ∈se(3) [20]: step by:\nJ p = ∂ ∂ T δξ p = = δ (cid:20) l ξ i I → m 3× 0 3 (ex − p( [ δ T ξ p ) δ ] T × ξ p (cid:21) , −Tp) (10) P˜ k ∗ ={exp (cid:18) N N − ∆ n ξ · ∗ ∆ = ξ∗ lo (cid:19) g p (T ( k m k− ,n 1 ) − | 1 p · ( k T m ∗ k , ) n , ) ∈P k }.",
      "size": 817,
      "sentences": 5
    },
    {
      "id": 27,
      "content": "id:20) l ξ i I → m 3× 0 3 (ex − p( [ δ T ξ p ) δ ] T × ξ p (cid:21) , −Tp) (10) P˜ k ∗ ={exp (cid:18) N N − ∆ n ξ · ∗ ∆ = ξ∗ lo (cid:19) g p (T ( k m k− ,n 1 ) − | 1 p · ( k T m ∗ k , ) n , ) ∈P k }. ( ( 1 1 3 3 b a ) )\n0 0\n1×3 1×3\nThe re-computed undistorted edge features and planar fea-\nwhere [T p ] transforms 4D point expression {x,y,z,1}\nk k × tures will be updated to the global edge map and the global\ninto 3D point expression {x,y,z} and calculates its skew\nplanarmaprespectively.Aftereachupdate,themapisdown-\nsymmetric matrix. The Jacobian matrix of edge residual can\nsampledbyusinga3Dvoxelizedgridapproach[21]inorder\nbe calculated by:\nto prevent memory overflow. ∂f ∂Tp\nJ\nE\n=W(p\nE\n)\n∂T\nE\np ∂δξ\n=W(p\nE\n)p\nn\n•(ng\nE\n×J\np\n), (11) IV. EXPERIMENTEVALUATION\nA. Experiment Setup\nSimilarly, we can derive\nTovalidatethealgorithm,weevaluateF-LOAMonbotha\n∂f ∂Tp\nJ\nS\n=W(p\nS\n)\n∂T\nS\np ∂δξ\n=W(p\nS\n)ng\nE\n•J\np\n.",
      "size": 904,
      "sentences": 6
    },
    {
      "id": 28,
      "content": "=W(p\nE\n)p\nn\n•(ng\nE\n×J\np\n), (11) IV. EXPERIMENTEVALUATION\nA. Experiment Setup\nSimilarly, we can derive\nTovalidatethealgorithm,weevaluateF-LOAMonbotha\n∂f ∂Tp\nJ\nS\n=W(p\nS\n)\n∂T\nS\np ∂δξ\n=W(p\nS\n)ng\nE\n•J\np\n. (12) large scale outdoor environment and a medium scale indoor\nenvironment.Forthelargescaleexperiment,weevaluateour\nBy solving nonlinear optimization we can derive odometry approach on KITTI dataset [22] which is one of the most\nestimation based on above correspondence. Then, we can popular datasets for SLAM evaluation. Then the algorithm\nuse this result to calculate new correspondence and new is integrated into warehouse logistics. It is firstly validated\n=== 페이지 5 ===\nATE & ARE Computing Time (ms)\n500\n1.7\n1.5\n400\n1.3\n1.1 300\n0.9\n200\n0.7\n0.5\n100\n0.3\n0.1 0\nIMLS-SLAM F-LOAM A-LOAM LOAM LeGO-LOAM VINS-MONO HDL-SLAM\nAverage Translational Error (%) Average Rotational Error (x10^-2 deg/m) Computing Time (ms)\nFig.",
      "size": 917,
      "sentences": 7
    },
    {
      "id": 29,
      "content": "00\n1.3\n1.1 300\n0.9\n200\n0.7\n0.5\n100\n0.3\n0.1 0\nIMLS-SLAM F-LOAM A-LOAM LOAM LeGO-LOAM VINS-MONO HDL-SLAM\nAverage Translational Error (%) Average Rotational Error (x10^-2 deg/m) Computing Time (ms)\nFig. 3: Comparison of the different localization approaches on KITTI dataset sequence 00-10.\nin a simulated warehouse environment and then is tested on (a)\nan AGV platform. B. Evaluation on Public Dataset\nWe firstly test our method on KITTI dataset [23] that\nis popularly used for outdoor localization evaluation. The\ndataset is collected from a driving car equipped with Velo-\ndyne HDL-64 LiDAR, cameras, and GPS. Most state-of-\nthe-art SLAM methods have been evaluated on this dataset,\ne.g., ORB-SLAM [24], VINS-Fusion [25], LIMO [26], and\nLSD-SLAM [27]. To validate the robustness, we evaluate (b) (c)\nthe proposed algorithm on all sequences from the KITTI\ndataset,whichincludesdifferentscenariossuchashighway,\ncity center, county road, residential area, etc.",
      "size": 957,
      "sentences": 7
    },
    {
      "id": 30,
      "content": "idate the robustness, we evaluate (b) (c)\nthe proposed algorithm on all sequences from the KITTI\ndataset,whichincludesdifferentscenariossuchashighway,\ncity center, county road, residential area, etc. The ground Lidar Odometry\ntruthofsequence11-21arenotopentopublicandtherefore, Ground Truth\nwemainlyillustrateourperformanceonsequence00-11.We\nFig. 4: Simulated warehouse environment with both static\ncalculatetheAverageTranslationalError(ATE)andAverage\nand dynamic objects. (a) Simulated environment in Gazebo. Rotational Error (ARE) which are defined by the KITTI\n(b) Simulated Pioneer robot and Velodyne LiDAR for eval-\ndataset [23]:\nuation. (c) Trajectory comparison of F-LOAM and ground\nE (F)= 1 (cid:88) ∠[Tˆ Tˆ−1T T−1] truth.",
      "size": 730,
      "sentences": 6
    },
    {
      "id": 31,
      "content": ") which are defined by the KITTI\n(b) Simulated Pioneer robot and Velodyne LiDAR for eval-\ndataset [23]:\nuation. (c) Trajectory comparison of F-LOAM and ground\nE (F)= 1 (cid:88) ∠[Tˆ Tˆ−1T T−1] truth. rot |F| j i i j\ni,j∈F\n(14)\nE (F)= 1 (cid:88) ||Tˆ Tˆ−1T T−1|| ,\ntrans |F| j i i j 2 as LOAM [10], A-LOAM, HDL-Graph-SLAM [28], IMLS-\ni,j∈F\nSLAM[9],andLeGO-LOAM[11].NotedthatIMLS-SLAM\nwhere F is a set of frames (i,j), T and Tˆ are the estimated is not open sourced and the results are collected from the\nandtrueLiDARposesrespectively,∠[·]istherotationangle. original paper [9]. In additional, the visual SLAM such as\nThe results are shown in Fig. 2. Our methods achieves an VINS-MONO [29] is also compared. For consistency, the\naveragetranslationalerrorof0.80%andanaveragerotational IMUinformationisnotusedandtheloopclosuredetectionis\nerror of 0.0048 deg/m over 11 sequences. removed.",
      "size": 883,
      "sentences": 9
    },
    {
      "id": 32,
      "content": "29] is also compared. For consistency, the\naveragetranslationalerrorof0.80%andanaveragerotational IMUinformationisnotusedandtheloopclosuredetectionis\nerror of 0.0048 deg/m over 11 sequences. removed. The testing environment is based on ROS Melodic\nWe also compare our method with state-of-the-art meth- [30]andUbuntu18.04.TheresultisshowninFig.3.IMLS-\nods,includingbothLiDARSLAMandvisualSLAM.Intotal SLAMachievesthehighestaccuracy.However,allpointsare\nthereare23,201framesandtraveledlengthof22kmover11 used in iterative pose estimation and the processing speed\nsequences. The average computing time over all sequences is slow. In terms of computational cost, LeGO-LOAM is\nis also recorded. In order to have precise evaluation on the the fastest LiDAR SLAM since it only applies optimization\ncomputing time, all of the mentioned methods are tested on on non-ground points. Among all methods compared, our\nan Intel i7 3.2GHz processor based computer.",
      "size": 948,
      "sentences": 8
    },
    {
      "id": 33,
      "content": "R SLAM since it only applies optimization\ncomputing time, all of the mentioned methods are tested on on non-ground points. Among all methods compared, our\nan Intel i7 3.2GHz processor based computer. The time is method achieves second highest accuracy at an average\ncalculated from the start of feature extraction till the output processing rate of more than 10 Hz, which is a good trade-\nof estimated odometry. The proposed method is compared off between computational cost and localization accuracy. with the open-sourced state-of-the-art LiDAR methods such Our method is able to provide both fast and accurate SLAM\n[표 데이터 감지됨]\n\n=== 페이지 6 ===\n(a) (b) (c)\n(d) (e)\nFig. 5: F-LOAM in warehouse environment. (a) Automated Guided Vehicle used for experiment. (b-e) Advanced factory\nenvironment built for AGV manipulation, including operating machine, auto charging station, and storage shelves. Center\nimage: F-LOAM result of warehouse localization and mapping. 3 around at the maximum speed of 2 m/s.",
      "size": 998,
      "sentences": 10
    },
    {
      "id": 34,
      "content": "r AGV manipulation, including operating machine, auto charging station, and storage shelves. Center\nimage: F-LOAM result of warehouse localization and mapping. 3 around at the maximum speed of 2 m/s. We record the\ntrajectory from F-LOAM and compare it with the ground\n2 truth. The result is shown in Fig. 4 (c). The F-LOAM\ntrajectoryandgroundtruthareplottedingreenandredcolor\nrespectively.OurmethodisabletotracktheAGVmovingin\n1\nhighspeedunderthedynamicenvironmentwherethehuman\noperators are walking in the warehouse environment. 0\n2) Experiment: To further demonstrate the performance\nof our method, we implement F-LOAM on an actual AGV\n-1\nused for smart manufacturing. As shown in Fig. 5 (b-e), the\nwarehouse environment consists of three main areas: auto\nGround Truth\n-2 charging station, material handling area and manufacturing\nFLOAM\nstation. A fully autonomous factory requires the robot to\n-3 deliver material to manufacturing machines for assembly\n-4 -2 0 2 4 and collect products.",
      "size": 988,
      "sentences": 11
    },
    {
      "id": 35,
      "content": ", material handling area and manufacturing\nFLOAM\nstation. A fully autonomous factory requires the robot to\n-3 deliver material to manufacturing machines for assembly\n-4 -2 0 2 4 and collect products. All these operations require precise\nlocalization to ensure the reliability and safety. The robot\nFig.6:Comparisonoftheproposedmethodandgroundtruth. platform for testing is shown in Fig. 5 (a) and is equipped\nOur method can precisely track robot’s pose and it achieves\nwith an Intel NUC mini computer and a Velodyne VLP-16\nan average localization error of 2 cm\nsensor. The localization and mapping result can be found\nin Fig. 5. In this scenario, the robot automatically explores\nsolution for real time robotic applications and is competitive the warehouse and builds the map simultaneously using the\namong state-of-the-art approaches. proposed method.",
      "size": 852,
      "sentences": 10
    },
    {
      "id": 36,
      "content": "bot automatically explores\nsolution for real time robotic applications and is competitive the warehouse and builds the map simultaneously using the\namong state-of-the-art approaches. proposed method. 3) Performance evaluation: The proposed method is also\nC. Experiment on warehouse logistics tested in an indoor room equipped with a VICON system\nIn this experiment, we target to build an autonomous to evaluate its localization accuracy. The robot is remotely\nwarehouse robot to replace human-labour-dominated man- controlledtomoveinthetestingarea.Theresultsareshown\nufacturing. The AGV is designed to carry out daily tasks inFig.6,wheretheF-LOAMtrajectoryandthegroundtruth\nsuch as transportation. This requires the robot platform to trajectory are plotted in green and red color respectively. actively localize itself in a complex environment. It can be seen that our method can accurately track the\n1) Simulation: Wefirstlyvalidateouralgorithminasim- robot’s pose.",
      "size": 966,
      "sentences": 8
    },
    {
      "id": 37,
      "content": "nd red color respectively. actively localize itself in a complex environment. It can be seen that our method can accurately track the\n1) Simulation: Wefirstlyvalidateouralgorithminasim- robot’s pose. It achieves an average localization accuracy of\nulated environment. The simulation environment is built up 2 cm compared to the ground truth provided by the VICON\nonGazeboandLinuxUbuntu18.04.AsshowninFig.4(a), system. weuseavirtualPioneerrobotandavirtualVelodyneVLP-16 4) Ablationstudy: Tofurtherevaluatetheperformanceof\nas the ground vehicle platform. The simulated environment theproposeddistortioncompensationapproach,wecompare\nreconstructs a complex warehouse environment including the results of different approaches in the warehouse envi-\nvarious objects such as moving human workers, shelves, ronment. We firstly remove the distortion compensation in\nmachines, etc.",
      "size": 872,
      "sentences": 8
    },
    {
      "id": 38,
      "content": "nt including the results of different approaches in the warehouse envi-\nvarious objects such as moving human workers, shelves, ronment. We firstly remove the distortion compensation in\nmachines, etc. The robot is controlled by a joystick to move F-LOAM and record both computing time and localization\n=== 페이지 7 ===\n[7] P.J.BeslandN.D.McKay,“Methodforregistrationof3-dshapes,”\nComputingTime Accuracy\nMethods inSensorfusionIV:controlparadigmsanddatastructures,vol.1611. (ms/frame) (cm)\nInternationalSocietyforOpticsandPhotonics,1992,pp.586–606. NoCompensation 66.33 2.132 [8] J. Zhang and S. Singh, “Low-drift and real-time lidar odometry and\nLOAM 84.52 2.052 mapping,”AutonomousRobots,vol.41,no.2,pp.401–416,2017. F-LOAM 69.07 2.037 [9] J.-E. Deschaud, “Imls-slam: scan-to-model matching based on 3d\ndata,” in 2018 IEEE International Conference on Robotics and Au-\nTABLE I: Ablation study of localization accuracy and com- tomation(ICRA). IEEE,2018,pp.2480–2485.",
      "size": 961,
      "sentences": 7
    },
    {
      "id": 39,
      "content": ": scan-to-model matching based on 3d\ndata,” in 2018 IEEE International Conference on Robotics and Au-\nTABLE I: Ablation study of localization accuracy and com- tomation(ICRA). IEEE,2018,pp.2480–2485. [10] J.ZhangandS.Singh,“Loam:Lidarodometryandmappinginreal-\nputational cost. time.”inRobotics:ScienceandSystems,vol.2,no.9,2014. [11] T. Shan and B. Englot, “Lego-loam: Lightweight and ground-\noptimized lidar odometry and mapping on variable terrain,” in 2018\nIEEE/RSJInternationalConferenceonIntelligentRobotsandSystems\naccuracy. Then we add the iterative distortion compensation (IROS),2018,pp.4758–4765. [12] D. Rozenberszki and A. L. Majdik, “Lol: Lidar-only odometry and\nmethod from LOAM into our method and record the result. localization in 3d point cloud maps,” in 2020 IEEE International\nLastlyweusetheproposedmotioncompensationandrecord Conference on Robotics and Automation (ICRA). IEEE, 2020, pp. theresult.TheresultsareshowninTableI.Itcanbeseenthat 4379–4385.",
      "size": 972,
      "sentences": 10
    },
    {
      "id": 40,
      "content": "2020 IEEE International\nLastlyweusetheproposedmotioncompensationandrecord Conference on Robotics and Automation (ICRA). IEEE, 2020, pp. theresult.TheresultsareshowninTableI.Itcanbeseenthat 4379–4385. [13] T. Shan, B. Englot, D. Meyers, W. Wang, C. Ratti, and R. Daniela,\nthe proposed approach is much faster than the LOAM with\n“Lio-sam:Tightly-coupledlidarinertialodometryviasmoothingand\nmotion compensation yet with a slightly better localization mapping,”inIEEE/RSJInternationalConferenceonIntelligentRobots\naccuracy.. andSystems(IROS). IEEE,2020. [14] D. Yin, Q. Zhang, J. Liu, X. Liang, Y. Wang, J. Maanpa¨a¨, H. Ma,\nJ. Hyyppa¨, and R. Chen, “Cae-lo: Lidar odometry leveraging fully\nV. CONCLUSION\nunsupervised convolutional auto-encoder for interest point detection\nIn this paper, we present a computationally efficient Li- andfeaturedescription,”arXivpreprintarXiv:2001.01354,2020.",
      "size": 886,
      "sentences": 6
    },
    {
      "id": 41,
      "content": "V. CONCLUSION\nunsupervised convolutional auto-encoder for interest point detection\nIn this paper, we present a computationally efficient Li- andfeaturedescription,”arXivpreprintarXiv:2001.01354,2020. [15] J. Masci, U. Meier, D. Cires¸an, and J. Schmidhuber, “Stacked\nDAR SLAM framework which targets to provide a public\nconvolutional auto-encoders for hierarchical feature extraction,” in\nsolution to robotic applications with limited computational Internationalconferenceonartificialneuralnetworks. Springer,2011,\nresources. Compared to traditional methods, we propose to pp.52–59. [16] H.Shi,G.Lin,H.Wang,T.-Y.Hung,andZ.Wang,“Spsequencenet:\nuse a non-iterative two stage distortion compensation to\nSemantic segmentation network on 4d point clouds,” in Proceedings\nreduce the computational cost. It is also observed that edge oftheIEEE/CVFConferenceonComputerVisionandPatternRecog-\nfeatures with higher local smoothness and planar features nition,2020,pp.4574–4583.",
      "size": 966,
      "sentences": 6
    },
    {
      "id": 42,
      "content": "educe the computational cost. It is also observed that edge oftheIEEE/CVFConferenceonComputerVisionandPatternRecog-\nfeatures with higher local smoothness and planar features nition,2020,pp.4574–4583. [17] R.Bergelt,O.Khan,andW.Hardt,“Improvingtheintrinsiccalibration\nwith lower smoothness are often consistently extracted over\nof a velodyne lidar sensor,” in 2017 IEEE SENSORS. IEEE, 2017,\nconsecutive scans, which are more important for scan-to- pp.1–3. map matching. Therefore, the local geometry feature is also [18] A. Kirillov Jr, An introduction to Lie groups and Lie algebras. CambridgeUniversityPress,2008,vol.113. considered for iterative pose estimation. To demonstrate the\n[19] L.ZhangandP.N.Suganthan,“Robustvisualtrackingviaco-trained\nrobustness of the proposed method in practical applications, kernelizedcorrelationfilters,”PatternRecognition,vol.69,pp.82–93,\nthoroughexperimentshavebeendonetoevaluatetheperfor- 2017. [20] T.D.",
      "size": 942,
      "sentences": 10
    },
    {
      "id": 43,
      "content": "ined\nrobustness of the proposed method in practical applications, kernelizedcorrelationfilters,”PatternRecognition,vol.69,pp.82–93,\nthoroughexperimentshavebeendonetoevaluatetheperfor- 2017. [20] T.D. Barfoot,“Stateestimation forrobotics:A matrixliegroup ap-\nmance, including simulation, indoor AGV test, and outdoor\nproach,”DraftinpreparationforpublicationbyCambridgeUniversity\nautonomous driving test. Our method achieves an average Press,Cambridge,2016. localizationaccuracyof2cminindoortestandisoneofthe [21] R. B. Rusu and S. Cousins, “3d is here: Point cloud library (pcl),”\nin 2011 IEEE international conference on robotics and automation. most accurate and fastest open-sourced methods in KITTI\nIEEE,2011,pp.1–4. dataset. [22] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,“Visionmeetsrobotics:\nThe kitti dataset,” The International Journal of Robotics Research,\nREFERENCES vol.32,no.11,pp.1231–1237,2013.",
      "size": 905,
      "sentences": 8
    },
    {
      "id": 44,
      "content": ",2011,pp.1–4. dataset. [22] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,“Visionmeetsrobotics:\nThe kitti dataset,” The International Journal of Robotics Research,\nREFERENCES vol.32,no.11,pp.1231–1237,2013. [23] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\n[1] C. Debeunne and D. Vivet, “A review of visual-lidar fusion based\ndriving?thekittivisionbenchmarksuite,”inConferenceonComputer\nsimultaneous localization and mapping,” Sensors, vol. 20, no. 7, p.\nVisionandPatternRecognition(CVPR),2012. 2068,2020. [24] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a\n[2] S.Milz,G.Arbeiter,C.Witt,B.Abdallah,andS.Yogamani,“Visual\nversatileandaccuratemonocularslamsystem,”IEEEtransactionson\nslamforautomateddriving:Exploringtheapplicationsofdeeplearn-\nrobotics,vol.31,no.5,pp.1147–1163,2015. ing,”inProceedingsoftheIEEEConferenceonComputerVisionand\n[25] T. Qin, J. Pan, S. Cao, and S. Shen, “A general optimization-based\nPatternRecognitionWorkshops,2018,pp.247–257.",
      "size": 983,
      "sentences": 9
    },
    {
      "id": 45,
      "content": "1,no.5,pp.1147–1163,2015. ing,”inProceedingsoftheIEEEConferenceonComputerVisionand\n[25] T. Qin, J. Pan, S. Cao, and S. Shen, “A general optimization-based\nPatternRecognitionWorkshops,2018,pp.247–257. frameworkforlocalodometryestimationwithmultiplesensors,”arXiv\n[3] F. Cunha and K. Youcef-Toumi, “Ultra-wideband radar for robust\npreprintarXiv:1901.03638,2019.\ninspectiondroneinundergroundcoalmines,”in2018IEEEInterna-\n[26] J. Graeter, A. Wilczynski, and M. Lauer, “Limo: Lidar-monocular\ntionalConferenceonRoboticsandAutomation(ICRA). IEEE,2018,\nvisual odometry,” in 2018 IEEE/RSJ International Conference on\npp.86–92. IntelligentRobotsandSystems(IROS). IEEE,2018,pp.7872–7879. [4] S.Ito,S.Hiratsuka,M.Ohta,H.Matsubara,andM.Ogawa,“Small\n[27] J. Engel, J. Stu¨ckler, and D. Cremers, “Large-scale direct slam with\nimagingdepthlidaranddcnn-basedlocalizationforautomatedguided\nstereo cameras,” in Intelligent Robots and Systems (IROS), 2015\nvehicle,”Sensors,vol.18,no.1,p.177,2018.",
      "size": 976,
      "sentences": 7
    },
    {
      "id": 46,
      "content": "rs, “Large-scale direct slam with\nimagingdepthlidaranddcnn-basedlocalizationforautomatedguided\nstereo cameras,” in Intelligent Robots and Systems (IROS), 2015\nvehicle,”Sensors,vol.18,no.1,p.177,2018. IEEE/RSJInternationalConferenceon. IEEE,2015,pp.1935–1942. [5] W.Hess,D.Kohler,H.Rapp,andD.Andor,“Real-timeloopclosure\n[28] K.Koide,J.Miura,andE.Menegatti,“Aportablethree-dimensional\nin2dlidarslam,”in2016IEEEInternationalConferenceonRobotics\nlidar-based system for long-term and wide-area people behavior\nandAutomation(ICRA). IEEE,2016,pp.1271–1278. measurement,” International Journal of Advanced Robotic Systems,\n[6] R. Li, J. Liu, L. Zhang, and Y. Hang, “Lidar/mems imu integrated\nvol.16,no.2,p.1729881419841532,2019. navigation(slam)methodforasmalluavinindoorenvironments,”in\n[29] T.Qin,P.Li,andS.Shen,“Vins-mono:Arobustandversatilemonoc-\n2014 DGON Inertial Sensors and Systems (ISS). IEEE, 2014, pp. ular visual-inertial state estimator,” IEEE Transactions on Robotics,\n1–15.",
      "size": 980,
      "sentences": 10
    },
    {
      "id": 47,
      "content": "29] T.Qin,P.Li,andS.Shen,“Vins-mono:Arobustandversatilemonoc-\n2014 DGON Inertial Sensors and Systems (ISS). IEEE, 2014, pp. ular visual-inertial state estimator,” IEEE Transactions on Robotics,\n1–15. vol.34,no.4,pp.1004–1020,2018. [30] J.M.O’Kane,“Agentleintroductiontoros,”2014.",
      "size": 279,
      "sentences": 5
    }
  ]
}