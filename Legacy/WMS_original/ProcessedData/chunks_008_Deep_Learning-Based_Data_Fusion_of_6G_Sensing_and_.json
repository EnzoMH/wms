{
  "source": "ArXiv",
  "filename": "008_Deep_Learning-Based_Data_Fusion_of_6G_Sensing_and_.pdf",
  "total_chars": 30616,
  "total_chunks": 46,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nDeep Learning-Based Data Fusion of 6G Sensing\nand Inertial Information for Target Positioning:\nExperimental Validation\nKarthik Muthineni∗‡, Alexander Artemenko∗, Artjom Grudnitsky†, Josep Vidal‡, Montse Na´jar‡\n∗ Corporate Sector Research and Advance Engineering, Robert Bosch GmbH, Renningen, Germany\n† Nokia Bell Labs, Stuttgart, Germany\n‡ Department of Signal Theory and Communications, Universitat Polite`cnica de Catalunya (UPC), Barcelona, Spain\nEmail: ∗ karthik.muthineni, alexander.artemenko @de.bosch.com, ‡ josep.vidal, montse.najar @upc.edu\n{ } { }\nAbstract—The sixth-generation (6G) cellular technology will\nbe deployed with a key feature of Integrated Sensing and\nCommunication (ISAC), allowing the cellular network to map\nthe environment through radar sensing on top of providing gNBRU\ncommunication services.",
      "size": 837,
      "sentences": 1
    },
    {
      "id": 2,
      "content": "eployed with a key feature of Integrated Sensing and\nCommunication (ISAC), allowing the cellular network to map\nthe environment through radar sensing on top of providing gNBRU\ncommunication services. In this regard, the entire network can (TX)\nbe considered as a sensor with a broader Field of View (FoV) of\nthe environment, assisting in both the positioning of active and\ndetection of passive targets. On the other hand, the non-3GPP\nsensorsavailableonthetargetcanprovideadditionalinformation\nspecifictothetargetthatcanbebeneficiallycombinedwithISAC\nsensinginformationtoenhancetheoverallachievablepositioning\naccuracy. In this paper, we first study the performance of the\nISAC system in terms of its achievable accuracy in positioning\nthe mobile target in an indoor scenario. Second, we study the\nperformancegainachievedintheISACpositioningaccuracyafter\nfusing the information from the target’s non-3GPP sensors.",
      "size": 913,
      "sentences": 5
    },
    {
      "id": 3,
      "content": "curacy in positioning\nthe mobile target in an indoor scenario. Second, we study the\nperformancegainachievedintheISACpositioningaccuracyafter\nfusing the information from the target’s non-3GPP sensors. To\nthis end, we propose a novel data fusion solution based on the\ndeeplearningframeworktofusetheinformationfromISACand\nnon-3GPP sensors. 5 © In R te o rn b a e l rt | B B o o s s c c h h G R m es b e H a r 2 c 0 h 1 9. All rights reserved, also regarding any disposal, exploitation, reproduction, editing, distribution, as well as in the event of applications for industrial property rights. Wevalidateourproposeddatafusionandpositioningsolution\nwith a real-world ISAC Proof-of-Concept (PoC) as the wireless\ninfrastructure,anAutomatedGuidedVehicle(AGV)asthetarget,\nand the Inertial Measurement Unit (IMU) sensor on the target\nas the non-3GPP sensor. The experimental results show that our\nproposed solution achieves an average positioning error of 3 cm,\noutperforming the considered baselines.",
      "size": 993,
      "sentences": 7
    },
    {
      "id": 4,
      "content": "Unit (IMU) sensor on the target\nas the non-3GPP sensor. The experimental results show that our\nproposed solution achieves an average positioning error of 3 cm,\noutperforming the considered baselines. Index Terms—6G, integrated sensing and communication, po-\nsitioning, non-3GPP sensors, deep learning. I. INTRODUCTION\nThe upcoming sixth-generation (6G) mobile communica-\ntion technology is expected to extend its footprint from the\ntraditional consumer market to vertical industries by promis-\ning to deliver advanced functionalities that open new use\ncases [1]. The 6G technology envisions taking the network\ncapability beyond communications by enabling “radar-like”\nsensing to detect objects and their positions without having\nthem equipped with a radio transceiver.",
      "size": 768,
      "sentences": 6
    },
    {
      "id": 5,
      "content": "technology envisions taking the network\ncapability beyond communications by enabling “radar-like”\nsensing to detect objects and their positions without having\nthem equipped with a radio transceiver. This is achieved with\na functionality called Integrated Sensing and Communication\n(ISAC), with integration performed at different stages, such\nas spectrum sharing between radar sensing and communi-\ncation functions, use of the same hardware, and reuse of\nthe communication waveform and radio resources for sensing\n[2], [3]. Nevertheless, this integration increases the system’s\nm\n9.1\n=h\nWireless Infrastructure\nz\nx (azimuth= 0o)\nelevation= -6o\ny z|\nTarget\n{a, a, v, v}\nx y x y\np = (p, p) p = (?, ?) ISAC x y AGV\nTransmittedsignal\nSignal echo\nm\n5.1\n=h\nSnifferRU\n(RX)\n^\nFig. 1: Block diagram of our ISAC sensing/positioning setup. complexity and opens new challenges. The development of\ncellular networks relied heavily on foundation models and\ntheories.",
      "size": 951,
      "sentences": 7
    },
    {
      "id": 6,
      "content": "ifferRU\n(RX)\n^\nFig. 1: Block diagram of our ISAC sensing/positioning setup. complexity and opens new challenges. The development of\ncellular networks relied heavily on foundation models and\ntheories. However, these approaches face limitations due to\nincreased network complexity as we move from generation\nto generation. Recent years have witnessed the enormous\ncapabilitiesofArtificialIntelligence(AI)algorithmsinsolving\ncomplex models and optimization problems in various appli-\ncations [4], [5]. The role of AI can also be witnessed across\na broad range of communication topics like signal detection\n[6],channelencoding[7],channelestimation[8],andresource\nallocation [9]. Moreover, the future 6G cellular network is\nexpected to natively support AI processing by adding suitable\nhardwarecomponentsintothenetworkdesign[10].Therefore,\nAI will be key in addressing certain challenges of 6G and the\nISAC system [11].",
      "size": 914,
      "sentences": 8
    },
    {
      "id": 7,
      "content": "is\nexpected to natively support AI processing by adding suitable\nhardwarecomponentsintothenetworkdesign[10].Therefore,\nAI will be key in addressing certain challenges of 6G and the\nISAC system [11]. The standardization body 3GPP has conducted a feasibility\nstudy on ISAC in Release 19 and identified 32 potential\nuse cases [12]. This paper evaluates the sensing/positioning\nperformance of the ISAC system introduced in [13] and the\npotential of AI models in enhancing the ISAC-assisted posi-\ntioningaccuracyforoneofthe3GPP’susecases,i.e.,tracking\nAutomated Guided Vehicles (AGVs) in complex scenarios. EarlierworksonISAC-assistedtrackingfocusedonevaluating\nthe ISAC system performance in simulated environments.",
      "size": 711,
      "sentences": 4
    },
    {
      "id": 8,
      "content": "eofthe3GPP’susecases,i.e.,tracking\nAutomated Guided Vehicles (AGVs) in complex scenarios. EarlierworksonISAC-assistedtrackingfocusedonevaluating\nthe ISAC system performance in simulated environments. In\n[14],authorsconsideredmultiplemono-staticradarsforfusing\nsensing information corresponding to a single target and com-\n5202\nraM\n13\n]PS.ssee[\n1v35242.3052:viXra\n=== 페이지 2 ===\nparedthetrackingperformanceagainstdifferentalgorithms.A AGVtrajectoryestimatedbyISAC\njointframeworkforestimatingthetargetlocationanditsclock\ndrift in the 6G ISAC system was presented in [15]. In [16],\nthe authors proposed using an AI model for positioning the 1020 +1.7200 9910×\n1000\ntarget by fusing the sensing information collected by multi-\n980\nstatic radars in an indoor scenario.",
      "size": 762,
      "sentences": 4
    },
    {
      "id": 9,
      "content": "[15]. In [16],\nthe authors proposed using an AI model for positioning the 1020 +1.7200 9910×\n1000\ntarget by fusing the sensing information collected by multi-\n980\nstatic radars in an indoor scenario. 9\n9\n4\n6\n0\n0 me[s]\nThere is still a research gap in ISAC literature investigating 920 Ti\nhow to fuse ISAC sensing information with the data obtained 900\n880\nfrom other sensor modalities, such as the non-3GPP sensors 860\navailable on the target. We contribute to ISAC literature\n1.48\nby (i) evaluating the achievable positioning accuracy of the 1.46\n1.44\nreal mobile target in an indoor scenario using a Proof-of- −0.75 −0.50 −0.25 0.00 1.40 1.4 Y 2 [m]\nConcept (PoC) of ISAC, (ii) demonstrating the value of non- 0.25 0.50 1.38\nX[m] 0.75 1.00 1.36\n3GPPsensorsandtheirprovidedinformationinenhancingthe 1.25 1.34\nISAC-assisted positioning accuracy by fusing the information Fig.",
      "size": 875,
      "sentences": 4
    },
    {
      "id": 10,
      "content": "ii) demonstrating the value of non- 0.25 0.50 1.38\nX[m] 0.75 1.00 1.36\n3GPPsensorsandtheirprovidedinformationinenhancingthe 1.25 1.34\nISAC-assisted positioning accuracy by fusing the information Fig. 2: The trajectory of the target AGV estimated by ISAC\nfrom these two systems, (iii) we propose a novel data-driven PoC using the geometric approach (discussed in II-B). Deep Neural Network (DNN) based data fusion approach for\nfusing the non-3GPP data with ISAC sensing information.",
      "size": 481,
      "sentences": 3
    },
    {
      "id": 11,
      "content": ") we propose a novel data-driven PoC using the geometric approach (discussed in II-B). Deep Neural Network (DNN) based data fusion approach for\nfusing the non-3GPP data with ISAC sensing information. AGVtrajectoryestimatedbyLiDAR(Ground-Truth)\nFurthermore, we validate our approach through real-world\nexperiments with an ISAC PoC as the wireless infrastructure,\nthe AGV serving as the passive target for positioning, and an\n1020 +1.7200\n9910×\nInertial Measurement Unit (IMU) sensor on the target as the 1000\n980\nnon-3GPPsensor.Tothebestofourknowledge,thisisthefirst\nstudy focusing on enhancing ISAC-assisted target positioning 9\n9\n4\n6\n0\n0 me[s]\n920\nTi\naccuracy with non-3GPP data and demonstrating the results 900\nthrough real-world experiments. 880\n860\n2.25\nII. BACKGROUND 2.00\n1.75\ncom\nTh\nm\ne\nun\nIS\nic\nA\na\nC\ntion\nP\ns\noC\nhar\nu\nd\ns\nw\ne\na\nd\nre\nin\n[1\nt\n3\nh\n]\ni\n.",
      "size": 860,
      "sentences": 5
    },
    {
      "id": 12,
      "content": "with non-3GPP data and demonstrating the results 900\nthrough real-world experiments. 880\n860\n2.25\nII. BACKGROUND 2.00\n1.75\ncom\nTh\nm\ne\nun\nIS\nic\nA\na\nC\ntion\nP\ns\noC\nhar\nu\nd\ns\nw\ne\na\nd\nre\nin\n[1\nt\n3\nh\n]\ni\n. s\nIn\nw\nt\no\nh\nr\ni\nk\ns s\ni\ne\ns\nct\nb\nio\nu\nn\nil\n,\nt\nw\nu\ne\npo\np\nn\nres\nt\ne\nh\nn\ne\nt\n5\nth\nG\ne\n−0.25 0.00\n0.25 0.50 1.00\n1.25 1\nY\n.5 [m 0 ]\ndetailsofISACPoC,theachievablepositioningaccuracywith X[m] 0.75 1.00 1.25 0.50 0.75\nISAC, and the problem under investigation. Fig. 3: The trajectory of the target AGV estimated by the\nonboard LiDAR sensor (ground-truth). A. ISAC System\nWe consider a quasi-monostatic1 ISAC PoC, which uses\nin the following experiments. For each target thus obtained,\nbase station-centric sensing, i.e., transmission and reception\nwe extract the parameters range r and Doppler velocity v . of sensing-relevant signals, and their processing does not D\nThe PoC realizing this system consists of 2 Radio Units\ninvolve User Equipment (UE).",
      "size": 949,
      "sentences": 9
    },
    {
      "id": 13,
      "content": "tract the parameters range r and Doppler velocity v . of sensing-relevant signals, and their processing does not D\nThe PoC realizing this system consists of 2 Radio Units\ninvolve User Equipment (UE). Sensing is performed using the\n(RUs): a gNodeB (gNB) RU used for transmission of both\nOrthogonal Frequency Division Multiplexing (OFDM) radar\nsensing and communication signals and an additional RU of\nalgorithm [17], where a transmitted OFDM radio frame with\nthe same type used only for receiving reflections of the trans-\nGsymbolsandHsubcarriers,formingamatrixU,isreflected\nmitted signal, called “Sniffer RU”. Both RUs are rectangular\nby the environment. The reflections are received by the ISAC\narrays, each consisting of 2 sub-arrays with 8 12 antenna\nantennas as an OFDM radio frame forming the matrix V, ×\nelements.",
      "size": 819,
      "sentences": 5
    },
    {
      "id": 14,
      "content": "re rectangular\nby the environment. The reflections are received by the ISAC\narrays, each consisting of 2 sub-arrays with 8 12 antenna\nantennas as an OFDM radio frame forming the matrix V, ×\nelements. The RUs are configured for a center frequency of\nallowing computation of the Channel State Information (CSI)\nf =27.4GHzandabandwidthof200MHzwithasubcarrier\nmatrix Z by element-wise division of V by U. The OFDM c\nspacing of ∆ = 120 KHz. Sensing processing is performed\nradar algorithm then performs H Fast Fourier Transforms f\nby the Sensing Processing Unit (SPU), in the PoC a separate\n(FFTs) of length G and G Inverse Fast Fourier Transforms\nmachinethatmonitorsthefronthaulconnectiontothegNBRU\n(IFFTs) of length H on the CSI matrix Z, to obtain a range-\nanddirectlyinterfacesviaafronthaullinkwiththeSnifferRU. doppler matrix [18].",
      "size": 831,
      "sentences": 6
    },
    {
      "id": 15,
      "content": "Transforms\nmachinethatmonitorsthefronthaulconnectiontothegNBRU\n(IFFTs) of length H on the CSI matrix Z, to obtain a range-\nanddirectlyinterfacesviaafronthaullinkwiththeSnifferRU. doppler matrix [18]. We enhance the OFDM radar algorithm\nThe SPU receives the downlink IQ samples from monitoring\nto improve target detection with the clutter removal approach\nthe gNB RU fronthaul and the reflection IQ samples from the\ndescribed in [19]. The magnitude of each element in the\nSniffer RU fronthaul. It uses them to generate the matrices\nrange-dopplermatrixisusedtodifferentiatebetweennoiseand\nU and V and perform OFDM radar processing as described\npotential targets, with a simple threshold-based approach used\nabove. For the experiments, the RUs were mounted as shown\n1A configuration where transmitter and receiver are co-located but are\nin Fig. 1 with a mechanical elevation of\n−\n6◦ down-tilt and\nslightlyseparatedindistance. azimuth of 0◦.",
      "size": 937,
      "sentences": 8
    },
    {
      "id": 16,
      "content": "Us were mounted as shown\n1A configuration where transmitter and receiver are co-located but are\nin Fig. 1 with a mechanical elevation of\n−\n6◦ down-tilt and\nslightlyseparatedindistance. azimuth of 0◦. [표 데이터 감지됨]\n\n=== 페이지 3 ===\np^^ x t-1 p^^ x t-1 p^ initial Solution to\nx problem (3)\np^^ t-1 p^^ y t-1\nr 2\ny\nD r2D\nGate\np^^\nx\nfinal\np^^\nx\nfinal\nv D vD p^ y initial d imu p^^ y final p^^ y final\nInputs Outputs\nFully connected first\nneural network Fully connected second\nneural network\nFig. 4: Proposed two-stage cascaded DNN data fusion and positioning solution. B. ISAC-Assisted Positioning as Light Detection and Ranging (LiDAR) sensor S3 as the\nUE and/or non-3GPP sensors on the target to be positioned. The range estimated by ISAC PoC towards the target corre-\nThe ISAC PoC provides sensing parameters as an output\nsponds to 3D due to the difference in the heights of the ISAC\ninfrastructure and the target. For 2D positioning (scope of this at a frequency of 33 Hz.",
      "size": 968,
      "sentences": 8
    },
    {
      "id": 17,
      "content": "AC PoC provides sensing parameters as an output\nsponds to 3D due to the difference in the heights of the ISAC\ninfrastructure and the target. For 2D positioning (scope of this at a frequency of 33 Hz. The measurement at i-th time is\nwork), the range in 3D must be projected onto a 2D map for representedasΛ( i S1) = { t i ,x ISAC } ,where,t i isthetimestamp\nfurther integration with sensor fusion. This can be achieved andx ISAC constitutestothestatevectorincorporatingrangeand\nwith r 2D = ((r 3D )2 (∆h)2)1 2, where, ∆h is the difference Dopplervelocityofatarget,x ISAC =[r,v D ]T.TheIMUsensor\nin the heights betwee − n the ISAC infrastructure and the target. provides its motion parameters as an output at a frequency\nThereafter, the 2D position of the target is estimated using the of 50 Hz. The measurement at i-th time is represented as\ngeometric approach as Λ i (S2) = { t i ,x IMU } .",
      "size": 890,
      "sentences": 6
    },
    {
      "id": 18,
      "content": "as an output at a frequency\nThereafter, the 2D position of the target is estimated using the of 50 Hz. The measurement at i-th time is represented as\ngeometric approach as Λ i (S2) = { t i ,x IMU } . The state vector of the IMU sensor\ncontains accelerations and angular velocities in x, y, and z\npˆ x =p x +r 2D . cos(Ψ), (1) axes, x IMU = [a x ,a y ,a z ,g x ,g y ,g z ]T. In addition, LiDAR is\nused as a ground-truth sensor, which provides the 2D position\npˆ y =p y +r 2D . sin(Ψ), (2) of AGV and can be represented as Λ i (S3) = { t i ,x LiDAR } ,\nx = [ptrue,ptrue]T. The problem under investigation is,\nwhere (p x ,p y ) represents the known position coordinates of LiDAR x y\ngiventheISACandtheIMUasynchronousmeasurements,how\nISAC PoC and Ψ indicates the azimuth angle of the sniffer\ncan the position of AGV be obtained by fusing the sensor\nRU. Fig. 2 depicts the target trajectory estimated by ISAC\nmeasurements?",
      "size": 917,
      "sentences": 7
    },
    {
      "id": 19,
      "content": "rements,how\nISAC PoC and Ψ indicates the azimuth angle of the sniffer\ncan the position of AGV be obtained by fusing the sensor\nRU. Fig. 2 depicts the target trajectory estimated by ISAC\nmeasurements? The problem is formulated as\nPoC.TheplotshowsthatISACPoCdeterminesthetargettobe\nmovingonlyacrossthexaxis(1D).However,theground-truth (cid:16) (cid:17)\nLiDAR sensor on the AGV records the trajectory traversed by pˆ AGV =f Λ(S1),Λ(S2) , (3)\nthetargetandisillustratedinFig.3.Observingbothplots,one\nwhere pˆ = [pˆ ,pˆ ]T is the estimated 2D position of AGV\ncan infer the limitation of ISAC PoC in positioning the target AGV x y\nand f() indicates the function that models the relationship\nunder this particular investigation scenario. The achievable\n·\nbetween the measurements of the sensors and the AGVs\npositioning accuracy with ISAC PoC is found to be 2 m. A\nposition.",
      "size": 866,
      "sentences": 5
    },
    {
      "id": 20,
      "content": "the relationship\nunder this particular investigation scenario. The achievable\n·\nbetween the measurements of the sensors and the AGVs\npositioning accuracy with ISAC PoC is found to be 2 m. A\nposition. In this work, the function f() is designed to be a\nhigh positioning error is evident because a single wide beam\n·\ndeep learning model. wasusedtoilluminatethetarget,resultinginreceivingechoes\nreflectedfrommultipleobjects.Asaresult,itbecomesdifficult\nIII. PROPOSEDDNN-BASEDDATAFUSIONARCHITECTURE\nto determine the Line-of-Sight (LoS) path delay, leading to\ninaccurate positioning results. This demands the techniques to This section presents the details of the proposed two-stage\ncomplement ISAC-assisted positioning accuracy. To this end, cascaded DNN data fusion and positioning solution and the\nwe believe that the non-3GPP sensors can be considered an approach used for training the network.",
      "size": 892,
      "sentences": 7
    },
    {
      "id": 21,
      "content": "assisted positioning accuracy. To this end, cascaded DNN data fusion and positioning solution and the\nwe believe that the non-3GPP sensors can be considered an approach used for training the network. additionalsourceofinformation,contributingtoenhancingthe\nA. Two-Stage Cascaded DNN\n3GPP’s system (ISAC) performance. Our proposed data fusion architecture is based on deep\nC. Problem Statement\nlearning. The DNNs can learn the features from the raw data\nThisworkconsidersISACPoCasthewirelessinfrastructure and are known to perform remarkably well with the labeled\nsensor S1. The AGV equipped with IMU sensor S2 as well datasets [20]. Therefore, we propose a two-stage cascaded\n=== 페이지 4 ===\nAlgorithm 1: Proposed algorithm for the problem (3)\nTrainingphase: ISAC (Wireless infrastructure)\n1\nAcquiremeasurementsfromthewirelessinfrastructureISACΛ(S1)andthe\nIMUsensorΛ(S2)throughameasurementcampaign. gNB RU\n2 ConvertaccelerationsfromtheIMUsensortodistancemeasurementdimu.",
      "size": 968,
      "sentences": 8
    },
    {
      "id": 22,
      "content": "ss infrastructure)\n1\nAcquiremeasurementsfromthewirelessinfrastructureISACΛ(S1)andthe\nIMUsensorΛ(S2)throughameasurementcampaign. gNB RU\n2 ConvertaccelerationsfromtheIMUsensortodistancemeasurementdimu. (Transmitter)\n3 UsepositionestimatescreatedbytheLiDARsensorasground-truthandtrain\nboththeDNNstoprovidethepositionoftheAGVˆp astheoutput. AGV\nTestingphase :\n4 fori≥1do\n5 Forthecurrenttimestepti,acquiremeasurementsfromthesensors Sniffer RU\nΛ(S1)andΛ(S2). ti ti (Receiver)\n6 ifISACdataisavailableattithen\n7 Provide\n(cid:104)\npˆˆt x −1,pˆˆ y t−1,r,vD\n(cid:105)T\nasinputstothefirststageofDNN. 8 DeterminetheinitialpositionoftargetAGV[pˆx,pˆy]T. Target AGV\n9\nProvide[pˆx,pˆy]TanddimuasinputstothesecondstageofDNN\ntoestimatethetarget’sfinalposition\n(cid:104)\npˆˆx,pˆˆy\n(cid:105)T\n. Fig. 5: Experimental setup with the wireless infrastructure\n10 else ISAC and the target AGV in an indoor scenario.",
      "size": 889,
      "sentences": 8
    },
    {
      "id": 23,
      "content": "secondstageofDNN\ntoestimatethetarget’sfinalposition\n(cid:104)\npˆˆx,pˆˆy\n(cid:105)T\n. Fig. 5: Experimental setup with the wireless infrastructure\n10 else ISAC and the target AGV in an indoor scenario. 11 Provide (cid:104) pˆˆx ti−1,pˆˆt y i−1(cid:105)T anddimuasinputstothesecond3stag© Ien R te o rn b a e l rt | B B o o s s c c h h G R m es b e H a r 2 c 0 h 1 9. All rights reserved, also regarding any disposal, exploitation, reproduction, editing, distribution, as well as in the event of applications for industrial property rights. ofDNNtoestimatethetarget’sfinalposition. of DNN can provide the target’s initial position as output at a\n12 endif\n13 endfor frequency of 33 Hz. However, the second stage of DNN takes\nthetargetestimatedposition(pˆ ,pˆ )andthedistancetraversed\nx y\nd as inputs, which are provided at different frequency\nimu\nDNN to fuse the measurements from ISAC PoC with the rates.",
      "size": 900,
      "sentences": 8
    },
    {
      "id": 24,
      "content": "stage of DNN takes\nthetargetestimatedposition(pˆ ,pˆ )andthedistancetraversed\nx y\nd as inputs, which are provided at different frequency\nimu\nDNN to fuse the measurements from ISAC PoC with the rates. Therefore, in those time instances where the ISAC\nIMU sensor data to estimate the 2D position of the target, measurements are missing, and the estimated position of the\nas shown in Fig. 4. The first stage of the DNN takes ISAC targetfromthefirststageofDNNisnotavailable,wetakethe\n(cid:16) (cid:17)\nmeasurements as the input and provides the target’s initial previous final estimated position of the target pˆˆt−1,pˆˆt−1 as\nx y\nposition as the output, which acts as input to the second the input to the second stage of DNN. The workflow of the\nstage of the DNN, along with the IMU sensor data.",
      "size": 792,
      "sentences": 5
    },
    {
      "id": 25,
      "content": "f the target pˆˆt−1,pˆˆt−1 as\nx y\nposition as the output, which acts as input to the second the input to the second stage of DNN. The workflow of the\nstage of the DNN, along with the IMU sensor data. Following data fusion architecture is summarized in Algorithm 1.\nthe inputs, the second stage of the DNN provides the final\nestimate of the target position as output, resulting from the B. DNN Training and Testing\nfusion between the two sensor modalities. In the first stage of A pre-defined trajectory was created and sent to the target\nthe DNN, the previous final estimated position of the target AGV to follow the path. The data from the onboard IMU\n(cid:16) (cid:17)\npˆˆt−1,pˆˆt−1 ,ranger,andDopplervelocityv aretheinputs. sensor was recorded as the target continued to traverse along\nx y D\nBy incorporating the temporal data (cid:16) pˆˆt−1,pˆˆt−1 (cid:17) as inputs to the route.",
      "size": 885,
      "sentences": 6
    },
    {
      "id": 26,
      "content": "ranger,andDopplervelocityv aretheinputs. sensor was recorded as the target continued to traverse along\nx y D\nBy incorporating the temporal data (cid:16) pˆˆt−1,pˆˆt−1 (cid:17) as inputs to the route. In addition, ISAC PoC transmits its single wide\nx y beamtowardthetargetviagNBRUandcollectsthereflections\nthe DNN, it helps the network to understand the underlying\nthroughsnifferRUtoderivethesensingparameters.Later,the\npatterns in the data and make predictions that align with the\ndatawasrecordedandcollectedbyISAC,andtheIMUsensors\ndynamic changes. were used to train the two-stage cascaded DNN. The IMU sensor provides the acceleration measurements\nOur data fusion architecture was implemented in Python\nrelativetoitslocalcoordinateframe.Thesemeasurementsneed\nwith the Keras framework. The data collected during the\ntobetranslatedtotheglobalcoordinateframeusingarotation\nmeasurement campaign was divided into training (80%) and\nmatrix as\nvalidation (20%) datasets.",
      "size": 965,
      "sentences": 6
    },
    {
      "id": 27,
      "content": "the Keras framework. The data collected during the\ntobetranslatedtotheglobalcoordinateframeusingarotation\nmeasurement campaign was divided into training (80%) and\nmatrix as\nvalidation (20%) datasets. We use the simplified architecture\n(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21) consisting of four layers in each DNN to keep the model\na cos(Φ) sin(Φ) a\na x = sin(Φ) cos(Φ) a x , (4) lightweight yet powerful. The four layers comprise 1 input\ny g − y l layer, 2 hidden layers, and 1 output layer. The first DNN\nwhere the local and global coordinate frames are represented consists of 4, 32, 16, 2 neurons in corresponding layers. by [a x ,a y ]T l and [a x ,a y ]T g respectively. Furthermore, Φ corre- While, the s { econd DNN c } onsists of 3, 32, 16, 2 neurons in\nsponds to the yaw angle obtained from the gyroscope. Instead corresponding layers.",
      "size": 857,
      "sentences": 8
    },
    {
      "id": 28,
      "content": "d [a x ,a y ]T g respectively. Furthermore, Φ corre- While, the s { econd DNN c } onsists of 3, 32, 16, 2 neurons in\nsponds to the yaw angle obtained from the gyroscope. Instead corresponding layers. We use a batch { size of 32, } representing\nof providing the global accelerations from IMU as input to the number of training samples processed together before\nthe second stage DNN, we translate them into the distance updating the parameters (weights and biases) of the DNN\ntraversedbyAGVasd imu =v∆t+1 2 a g (∆t)2,wherev and∆t model. We consider the Rectified Linear Unit (ReLU) acti-\nreferstothevelocityoftheAGVandthetimeintervalbetween vation function. The Adaptive Moment Estimation (ADAM)\ntwo consecutive IMU measurements. The term a g represents optimizerwithalearningrateof10−3 isusedfortraining.We\n(cid:0) (cid:1)1\ntotal acceleration given by a = (a )2+(a )2 2.",
      "size": 869,
      "sentences": 7
    },
    {
      "id": 29,
      "content": "oment Estimation (ADAM)\ntwo consecutive IMU measurements. The term a g represents optimizerwithalearningrateof10−3 isusedfortraining.We\n(cid:0) (cid:1)1\ntotal acceleration given by a = (a )2+(a )2 2. Therefore, choosethelossfunctionasMeanSquareError(MSE)forboth\ng x y\nin the second stage of DNN, the inputs are the estimated neuralnetworkstominimizethepositioningerrorbetweenthe\nposition of the target (pˆ ,pˆ ) and the distance traversed by estimated target position and the ground-truth, which can be\nx y\nthe target d . It is to be noted that the measurements from formulated as\nimu\nISAC PoC and the IMU sensor are provided at a frequency of\n33Hzand50Hz,respectively.Thisimpliesthatthefirststage MSE(Γ)=E pˆ ptrue 2 , (5)\nL {|| AGV− AGV||2}\n[표 데이터 감지됨]\n\n=== 페이지 5 ===\nDeepNeuralNetwork(Fusion:ISAC+IMU) (ISAC) can benefit from such rich information.",
      "size": 851,
      "sentences": 4
    },
    {
      "id": 30,
      "content": "0Hz,respectively.Thisimpliesthatthefirststage MSE(Γ)=E pˆ ptrue 2 , (5)\nL {|| AGV− AGV||2}\n[표 데이터 감지됨]\n\n=== 페이지 5 ===\nDeepNeuralNetwork(Fusion:ISAC+IMU) (ISAC) can benefit from such rich information. Limiting our\nGround-Truth(LiDAR)\nanalysis to only one non-3GPP sensor, IMU, we can achieve\nan average positioning error of 3 cm for the given AoI,\n1020 +1.7200 9910× corresponding to about 1% relative positioning error. 1000 Our proposed data fusion positioning technique is com-\n980\n9\n9\n9\n2\n4\n6\n0\n0\n0\nTi\nme[s] p\nb\na\na\nr\ns\ne\ne\nd\nlin\nw\ne\ni\n“\nth\nDN\ntw\nN\no\n(I\nb\nS\na\nA\nse\nC\nli\n)\nn\n”\ne\n,\ns\nw\nf\ne\nor\nut\nb\nil\ne\niz\nn\ne\nch\nt\nm\nhe\nark\np\ni\nr\nn\ne\ng\nv\n.",
      "size": 640,
      "sentences": 3
    },
    {
      "id": 31,
      "content": "a fusion positioning technique is com-\n980\n9\n9\n9\n2\n4\n6\n0\n0\n0\nTi\nme[s] p\nb\na\na\nr\ns\ne\ne\nd\nlin\nw\ne\ni\n“\nth\nDN\ntw\nN\no\n(I\nb\nS\na\nA\nse\nC\nli\n)\nn\n”\ne\n,\ns\nw\nf\ne\nor\nut\nb\nil\ne\niz\nn\ne\nch\nt\nm\nhe\nark\np\ni\nr\nn\ne\ng\nv\n. iou\nF\ns\nor\nin\nt\ni\nh\nti\ne\nal\nfi\ne\nr\ns\ns\n-\nt\n900 timated position of the target and ISAC sensing parameters\n880 [pˆt−1,pˆt−1,r,v ]TastheinputstothefirststageofDNNtoes-\n860 x y D\ntimate the target’s trajectory without using the second stage of\n2.25\n2.00 DNN.Thesecondbaseline“EKF(Fusion:ISAC+IMU)”,uses\n1.75\n1.50 the conventional model-driven approach Extended Kalman\n−0.25 0.00\n0.25 1.00\n1.25 Y[m]\nFilter (EKF) to fuse ISAC and IMU measurements. To this\n0.50\nX[m] 0.75 1.00 1.25 0.50 0.75 end, the state vector of the EKF at time t is defined as\nx = [pˆ ,pˆ ,vˆ ,vˆ ]T, where (pˆ , pˆ ) and (vˆ , vˆ ) are the\nFig. 6: Ground-truth and the estimated trajectory of the target t x y x y x y x y\nposition coordinates and velocities of the target in x and y\nby the two-stage cascaded DNN.",
      "size": 981,
      "sentences": 4
    },
    {
      "id": 32,
      "content": ") and (vˆ , vˆ ) are the\nFig. 6: Ground-truth and the estimated trajectory of the target t x y x y x y x y\nposition coordinates and velocities of the target in x and y\nby the two-stage cascaded DNN. directions,respectively.Weusetheaccelerationmeasurements\nwhere pˆ is the estimated position, ptrue is the ground- fromtheIMUsensortopredictthestatevector,whiletheISAC\nAGV AGV\ntruth obtained from the LiDAR sensor of the target AGV, and measurements are used to update the state vector. Therefore,\nΓindicatestheparametersoftheDNN,whichincludeweights the EKF continues to predict the state vector using IMU\nand biases. The models of both the DNNs are validated with measurements,andwhenevertheISACmeasurementsbecome\nthe validation datasets to prevent overfitting. available,theEKF updatesthestatevector.The motionmodel\nis defined as ˆx = Cx + Du + w , where the state\nt+1 t t t\nIV.",
      "size": 877,
      "sentences": 6
    },
    {
      "id": 33,
      "content": "whenevertheISACmeasurementsbecome\nthe validation datasets to prevent overfitting. available,theEKF updatesthestatevector.The motionmodel\nis defined as ˆx = Cx + Du + w , where the state\nt+1 t t t\nIV. PERFORMANCEEVALUATION transitionmatrixC,thecontrolinputmatrixD,andthecontrol\nIn this section, we describe the experimental setup and input vector u are defined as\npresent the analysis results, including the benchmark consid- \n1 0 ∆t 0\n ∆t2\n0\n\nered and the accuracy of the obtained positioning. C=   0 1 0 ∆t , D=   0 2 ∆ 2 t2 , u= (cid:20) a x (cid:21) . A. Experimental Setup 0 0 1 0  ∆t 0  a y g\nThe experiments were conducted in an indoor scenario 0 0 0 1 0 ∆t\n(6)\nwith an Area of Interest (AoI) for sensing and/or positioning\nThetimestepisrepresentedby∆t,andtheIMUaccelerations\nspanning roughly 3.5 m 3.5 m. The ISAC PoC consisting\n× are used as the control inputs u.",
      "size": 887,
      "sentences": 6
    },
    {
      "id": 34,
      "content": "an Area of Interest (AoI) for sensing and/or positioning\nThetimestepisrepresentedby∆t,andtheIMUaccelerations\nspanning roughly 3.5 m 3.5 m. The ISAC PoC consisting\n× are used as the control inputs u. In addition, the process noise\nof the gNB RU and the sniffer RU is mounted at the height\nw is considered Gaussian. The error in the state prediction\nof 1.9 m and 1.5 m, respectively, from the ground level, t\nis computed as Eˆ = CE CT + Q, with Q indicating\nas shown in Fig. 5. Moreover, the RUs are mounted with t+1 t\nan azimuth of 0◦ and elevation of 6◦ to direct the beam the process noise covariance containing IMU measurement\n− uncertainties.TheISACPoCprovidestherangemeasurements\ntoward the AoI. The AGV from Bosch Rexroth is used as a\nof the target, which can be represented through observation\ntarget and is provided with a pre-defined trajectory to traverse\nvector as n = p pˆ + m , with\nin the AoI. The dimensions of the target correspond to be t+1 || ISAC − AGV || t+1\nmeasurement noise m .",
      "size": 999,
      "sentences": 7
    },
    {
      "id": 35,
      "content": "target and is provided with a pre-defined trajectory to traverse\nvector as n = p pˆ + m , with\nin the AoI. The dimensions of the target correspond to be t+1 || ISAC − AGV || t+1\nmeasurement noise m . Subsequently, the EKF computes\n1 m 0.4 m 0.9 m. Furthermore, the target has non-3GPP t+1\n× × theresidual,representingthedifferencebetweentheactualand\nsensors like IMU and LiDAR (ground-truth). predictedmeasurementso =n h(ˆx ).Thepredicted\nt+1 t+1 t+1\n−\nB. Experimental Results measurements can be obtained by computing Jacobian J as\nUsing ISAC PoC, we present the positioning results of the ∂h (cid:104) (cid:105)\ntarget AGV obtained with our proposed two-stage cascaded J t+1 = ∂ˆx t+1 = ||p IS pˆ A x C − − p ˆp x AGV || ||p IS pˆ A y C − − p ˆp y AGV || 0 0 . (7)\nDNN data fusion framework. Fig.",
      "size": 798,
      "sentences": 7
    },
    {
      "id": 36,
      "content": "104) (cid:105)\ntarget AGV obtained with our proposed two-stage cascaded J t+1 = ∂ˆx t+1 = ||p IS pˆ A x C − − p ˆp x AGV || ||p IS pˆ A y C − − p ˆp y AGV || 0 0 . (7)\nDNN data fusion framework. Fig. 6 depicts the ground-truth\ntrajectory of the target AGV obtained from the LiDAR sensor Given the measurement model, the state vector is up-\nand the estimated trajectory computed by fusing the wireless dated using ˆx+ t+1 = ˆx t+1 + K t+1 (n t+1 − h(ˆx t+1 )), where\ninfrastructure ISAC sensing measurements with the data from K t+1 = Eˆ t+1 JT t+1 (J t+1 Eˆ t+1 JT t+1 +R)−1 istheKalmangain\nthe onboard IMU sensor of the target AGV. Looking at the andRisthemeasurementcovariance,indicatinguncertaintyin\ntrajectory plots, one can infer that our proposed positioning rangemeasurementsofISAC.Thereafter,theerrorinthestate\ntechnique estimates the target’s trajectory close to the ground- estimation is also updated as Eˆ+ = (I K J )Eˆ .",
      "size": 932,
      "sentences": 5
    },
    {
      "id": 37,
      "content": "r that our proposed positioning rangemeasurementsofISAC.Thereafter,theerrorinthestate\ntechnique estimates the target’s trajectory close to the ground- estimation is also updated as Eˆ+ = (I K J )Eˆ . t+1 − t+1 t+1 t+1\ntruth and enhances the positioning accuracy compared to the Fig. 7 depicts the Cumulative Distribution Function (CDF)\ngeometric approach (Fig. 2). The improvement in positioning of 2D positioning error for different positioning techniques. accuracy shows the value in the information provided by the LookingattheCDFcurves,onecannotethebenefitsoffusing\nnon-3GPPIMUsensorandalsoexplainshowthe3GPPsystem ISAC and IMU measurements. Our proposed DNN-based\n[표 데이터 감지됨]\n\n=== 페이지 6 ===\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14\nPositioningerror[m]\nFDC\nMarie Sklodowska-Curie grant agreement ID 956670. Also,\npart of this work has been supported by the German Fed-\n90thpercentile\neral Ministry of Education and Research (Foerderkennzeichen\n16KISK116, KOMSENS-6G).",
      "size": 994,
      "sentences": 8
    },
    {
      "id": 38,
      "content": "owska-Curie grant agreement ID 956670. Also,\npart of this work has been supported by the German Fed-\n90thpercentile\neral Ministry of Education and Research (Foerderkennzeichen\n16KISK116, KOMSENS-6G). In addition, this work is part\nof the project ROUTE56 with grant PID2019-104945GB-\nI00 funded by MCIN/AEI/ 10.13039/501100011033 and the\nproject6-SENSESwithgrantPID2022-138648OB-I00funded\nbyMCIN/AEI/10.13039/501100011033andbyERDFAway\nof making Europe. DNN(ISAC) REFERENCES\nEKF(Fusion:ISAC+IMU)\nDNN(Fusion:ISAC+IMU) [1] Bosch, “Bosch drives forward the development of 6G,” Accessed Oct.\n09, 2024. [Online]. Available: https://www.bosch-presse.de/pressportal/\nde/en/bosch-drives-forward-the-development-of-6g-251328.html\n[2] J.A.Zhangetal.,“Enablingjointcommunicationandradarsensingin\nFig. 7: CDF of 2D positioning error for three positioning\nmobile networks—a survey,” IEEE Communications Surveys & Tutori-\ntechniques. als,vol.24,no.1,pp.306–345,2022.",
      "size": 950,
      "sentences": 8
    },
    {
      "id": 39,
      "content": "tcommunicationandradarsensingin\nFig. 7: CDF of 2D positioning error for three positioning\nmobile networks—a survey,” IEEE Communications Surveys & Tutori-\ntechniques. als,vol.24,no.1,pp.306–345,2022. [3] Z.Weietal.,“Integratedsensingandcommunicationsignalstoward5G-\nAand6G:Asurvey,”IEEEInternetofThingsJournal,vol.10,no.13,\nTABLE I: Summary of 2D positioning error. pp.11068–11092,2023. [4] BoschMobility,“Think:Boschmakesautomatedcarssmart,”Accessed\nDNN(ISAC) EKF(Fusion) DNN(Fusion) Oct.12,2024. [Online].Available:https://www.bosch-mobility.com/en/\nmobility-topics/automated-driving-sense-think-act/think/\nAverageerror[cm] 8.2 5.6 3\n[5] Bosch Research, “Bosch to use generative AI\n90thpercentile[cm] 10.5 7 5.5 in manufacturing,” Accessed Oct. 12, 2024. [On-\nline].",
      "size": 768,
      "sentences": 8
    },
    {
      "id": 40,
      "content": "utomated-driving-sense-think-act/think/\nAverageerror[cm] 8.2 5.6 3\n[5] Bosch Research, “Bosch to use generative AI\n90thpercentile[cm] 10.5 7 5.5 in manufacturing,” Accessed Oct. 12, 2024. [On-\nline]. Available: https://www.bosch-presse.de/pressportal/de/en/\nbosch-to-use-generative-ai-in-manufacturing-260806.html\npositioning technique and baseline 2, utilizing the additional\n[6] H.Heetal.,“Model-drivendeeplearningforMIMOdetection,”IEEE\nsource of information from the IMU sensor, outperform the TransactionsonSignalProcessing,vol.68,pp.1702–1715,2020. baseline 1, which uses only ISAC measurements. Looking [7] I.Be’Eryetal.,“Activedeepdecodingoflinearcodes,”IEEETransac-\ntionsonCommunications,vol.68,no.2,pp.728–736,2020.",
      "size": 724,
      "sentences": 5
    },
    {
      "id": 41,
      "content": "ng,vol.68,pp.1702–1715,2020. baseline 1, which uses only ISAC measurements. Looking [7] I.Be’Eryetal.,“Activedeepdecodingoflinearcodes,”IEEETransac-\ntionsonCommunications,vol.68,no.2,pp.728–736,2020. at the 90th percentile, the achievable positioning errors with\n[8] C. Luo et al., “Channel state information prediction for 5G wireless\nbaseline 1, baseline 2, and with our proposed approach are communications: A deep learning approach,” IEEE Transactions on\n10.5 cm, 7 cm, and 5.5 cm, respectively. Fusing ISAC and NetworkScienceandEngineering,vol.7,no.1,pp.227–236,2020. [9] F. Liang et al., “Towards optimal power control via ensembling deep\nIMU measurements with EKF achieves roughly 40% lower\nneuralnetworks,”IEEETransactionsonCommunications,vol.68,no.3,\npositioning errors compared to baseline 1. In addition, our pp.1760–1776,2020.",
      "size": 838,
      "sentences": 7
    },
    {
      "id": 42,
      "content": "deep\nIMU measurements with EKF achieves roughly 40% lower\nneuralnetworks,”IEEETransactionsonCommunications,vol.68,no.3,\npositioning errors compared to baseline 1. In addition, our pp.1760–1776,2020. proposed two-stage cascaded DNN achieves roughly 62% [10] Nokia Bell Labs, “Joint design of communication and sensing for\nbeyond 5G and 6G systems,” Accessed Oct. 12, 2024. [Online]. lower positioning errors compared with the baseline 1. On\nAvailable:https://onestore.nokia.com/asset/210300\nthe other hand, comparing the fusion performance between [11] U. Demirhan and A. Alkhateeb, “Integrated sensing and communica-\nEKF and DNN, our approach achieves roughly 24% lower tion for 6G: Ten key machine learning roles,” IEEE Communications\nMagazine,vol.61,no.5,pp.113–119,2023. positioning errors compared with baseline 2.",
      "size": 818,
      "sentences": 7
    },
    {
      "id": 43,
      "content": "F and DNN, our approach achieves roughly 24% lower tion for 6G: Ten key machine learning roles,” IEEE Communications\nMagazine,vol.61,no.5,pp.113–119,2023. positioning errors compared with baseline 2. Table I presents\n[12] 3GPP,“Feasibilitystudyonintegratedsensingandcommunication,”3rd\nthesummaryofpositioningerrorsw.r.t.averageerrorand90th Generation Partnership Project (3GPP), TR 22.837, V19.4.0 (Rel. 19),\npercentile. 2024. [13] T. Wild, A. Grudnitsky, S. Mandelli, M. Henninger, J. Guan, and\nV. CONCLUSION F.Schaich,“6Gintegratedsensingandcommunication:Fromvisionto\nrealization,”2023. [Online].Available:https://arxiv.org/abs/2305.01978\nThis paper presents the positioning results of a target AGV\n[14] E. Favarelli et al., “Tracking and data fusion in joint sensing and\nin an indoor scenario using the experimental ISAC PoC built communication networks,” in 2022 IEEE Globecom Workshops (GC\non the existing 5G communications hardware with integrated Wkshps),2022,pp.341–346.",
      "size": 978,
      "sentences": 7
    },
    {
      "id": 44,
      "content": "indoor scenario using the experimental ISAC PoC built communication networks,” in 2022 IEEE Globecom Workshops (GC\non the existing 5G communications hardware with integrated Wkshps),2022,pp.341–346. [15] J. Wei et al., “Joint clock synchronization and localization for 6G\nsensing functionality. The measurements from the non-3GPP\nintegrated communication and sensing,” in 2023 IEEE International\nsensorIMUavailableonthetargetAGVwerefusedwithISAC ConferenceonCommunicationsWorkshops(ICCWorkshops),2023,pp. sensing information to enhance the ISAC-assisted positioning 200–206. [16] V. Yajnanarayana and H. Wymeersch, “Multistatic sensing of passive\naccuracy.Atwo-stagecascadedDNNwasproposedasthedata\ntargets using 6G cellular infrastructure,” in 2023 Joint European Con-\nfusion and positioning solution to achieve this. Our proposed ference on Networks and Communications & 6G Summit (EuCNC/6G\npositioning solution avoided more significant errors for the Summit),2023,pp.132–137.",
      "size": 977,
      "sentences": 6
    },
    {
      "id": 45,
      "content": "d positioning solution to achieve this. Our proposed ference on Networks and Communications & 6G Summit (EuCNC/6G\npositioning solution avoided more significant errors for the Summit),2023,pp.132–137. [17] K. M. Braun, “OFDM radar algorithms in mobile communication\nindoor scenario considered in this work. We achieved an\nnetworks,”Ph.D.dissertation,2014. average positioning error of 3 cm, corresponding to around [18] M.Braunetal.,“Maximumlikelihoodspeedanddistanceestimationfor\n1% relative positioning accuracy. For 90% of the cases, the OFDMradar,”in2010IEEERadarConference,2010,pp.256–261. [19] M. Henninger et al., “CRAP: Clutter removal with acquisitions under\nachieved positioning error was less than or equal to 5.5 cm. phasenoise,”in20232ndInternationalConferenceon6GNetworking\n(6GNet),2023,pp.1–8.",
      "size": 807,
      "sentences": 8
    },
    {
      "id": 46,
      "content": "ger et al., “CRAP: Clutter removal with acquisitions under\nachieved positioning error was less than or equal to 5.5 cm. phasenoise,”in20232ndInternationalConferenceon6GNetworking\n(6GNet),2023,pp.1–8. ACKNOWLEDGMENT [20] Z.-Q.Zhaoetal.,“Objectdetectionwithdeeplearning:Areview,”IEEE\nTransactionsonNeuralNetworksandLearningSystems,vol.30,no.11,\nThis work has received funding from the European Union’s\npp.3212–3232,2019. Horizon 2020 research and innovation programme under the\n[표 데이터 감지됨]",
      "size": 487,
      "sentences": 4
    }
  ]
}