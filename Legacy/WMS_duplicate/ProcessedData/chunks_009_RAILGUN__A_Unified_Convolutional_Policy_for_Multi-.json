{
  "source": "ArXiv",
  "filename": "009_RAILGUN__A_Unified_Convolutional_Policy_for_Multi-.pdf",
  "total_chars": 38059,
  "total_chunks": 55,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nRAILGUN: A Unified Convolutional Policy for\nMulti-Agent Path Finding Across Different Environments and Tasks\nYimin Tang1∗, Xiao Xiong2∗, Jingyi Xi2, Jiaoyang Li3, Erdem Bıyık1, Sven Koenig4\nAbstract—Multi-AgentPathFinding(MAPF),whichfocuses communication, to enhance performance. It is important\nonfindingcollision-freepathsformultiplerobots,iscrucialfor to note these approaches focus on individual agents and\napplicationsrangingfromaerialswarmstowarehouseautoma-\nattempt to generate actions based on agent-specific features,\ntion. Solving MAPF is NP-hard so learning-based approaches\nwhich typically do not include global state information. for MAPF have gained attention, particularly those leveraging\ndeep neural networks. Nonetheless, despite the community’s Furthermore, as features are based on the agent itself, these\ncontinued efforts, all learning-based MAPF planners still rely approaches inherently allow the number of agents to vary.",
      "size": 960,
      "sentences": 5
    },
    {
      "id": 2,
      "content": "the community’s Furthermore, as features are based on the agent itself, these\ncontinued efforts, all learning-based MAPF planners still rely approaches inherently allow the number of agents to vary. on decentralized planning due to variability in the number of On the other hand, centralized approaches bring several\nagents and map sizes. We have developed the first centralized\nbenefits, such as the ability to coordinate the movements\nlearning-based policy for MAPF problem called RAILGUN. of multiple agents. However, the literature lacks centralized\nRAILGUN is not an agent-based policy but a map-based\npolicy. By leveraging a CNN-based architecture, RAILGUN MAPF algorithms that are learning-based, since it is chal-\ncan generalize across different maps and handle any number lenging to train a centralized neural network that can handle\nof agents. We collect trajectories from rule-based methods to variability in both the number of agents and map sizes.",
      "size": 960,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "d handle any number lenging to train a centralized neural network that can handle\nof agents. We collect trajectories from rule-based methods to variability in both the number of agents and map sizes. trainourmodelinasupervisedway.Inexperiments,RAILGUN\nWe present RAILGUN the first centralized learning-based\noutperforms most baseline methods and demonstrates great\nmethod for MAPF which generates actions based on maps\nzero-shotgeneralizationcapabilitiesonvarioustasks,mapsand\nagent numbers that were not seen in the training dataset. rather than individual agents. The core idea of RAILGUN is\nto generate a directed graph in which each node has exactly\nI. INTRODUCTION one outgoing edge at every timestep. This design enables\nour method to handle any number of agents on the map.",
      "size": 780,
      "sentences": 7
    },
    {
      "id": 4,
      "content": "f RAILGUN is\nto generate a directed graph in which each node has exactly\nI. INTRODUCTION one outgoing edge at every timestep. This design enables\nour method to handle any number of agents on the map. Multi-Agent Path Finding (MAPF) is an NP-hard prob-\nAdditionally, we use U-Net [16] as the model backbone\nlem [1], [2] which focuses on finding collision-free paths\nwhich produces outputs of the same dimensions as the input\nfor multiple agents to move from start locations to their\nfeatures. This allows RAILGUN to accommodate maps of\ngoal locations in a known environment while optimizing a\nvarying sizes. In summary, our contributions are as follows:\nspecified cost function. This problem could be adapted to\nmany realistic scenarios from aerial swarms to warehouse • We propose the first centralized learning-based MAPF\nalgorithm,RAILGUN,whichgeneratesactionsformap\nautomation which are multi-billion dollar industries. Many\ngrid cells rather than for individual agents.",
      "size": 973,
      "sentences": 8
    },
    {
      "id": 5,
      "content": "ropose the first centralized learning-based MAPF\nalgorithm,RAILGUN,whichgeneratesactionsformap\nautomation which are multi-billion dollar industries. Many\ngrid cells rather than for individual agents. algorithms have been proposed to solve this problem or its\nvariants, such as Conflict-Based Search (CBS) [3], M∗ [4], • We design a CNN-based network enabling RAILGUN\nto handle maps of different sizes. LaCAM [5] and MAPF-LNS2 [6]. Asneuralnetworksdemonstratetheirpowerfulcapabilities • Throughexperimentsindiversetestsettings,wedemon-\nstrate that RAILGUN, trained on data from one map\nin various fields of computer science [7], [8], [9], learning-\ntype, generalizes effectively to new types of maps\nbased MAPF solvers have also garnered significant atten-\nand testing scenarios, and outperforms most baseline\ntion [10]. Currently, all learning-based MAPF solvers adopt\nmethods in POGEMA [17] benchmark.",
      "size": 902,
      "sentences": 6
    },
    {
      "id": 6,
      "content": "d MAPF solvers have also garnered significant atten-\nand testing scenarios, and outperforms most baseline\ntion [10]. Currently, all learning-based MAPF solvers adopt\nmethods in POGEMA [17] benchmark. decentralized approaches, where each agent takes surround-\ning local information as input, typically represented as a II. PROBLEMDEFINITION\nfield-of-view (FOV). These decentralized policies determine\nThe MAPF problem is defined as follows: Let I =\neach agent’s action, either simultaneously or sequentially,\n{1,2,··· ,N} denote a set of N agents. G = (V,E)\nat the current timestep based on the agent’s FOV input. represents an undirected graph, where each vertex v ∈ V\nMany decentralized methods have been proposed, such as\nrepresents a possible location of an agent in the workspace,\nPRIMAL[11],MAPPER[12],MAGAT[13],SCRIMP[14],\nandeachedgee∈E isaunit-costedgebetweentwovertices\nand MAPF-GPT [15]. These methods primarily rely on\nthat moves an agent from one vertex to the other.",
      "size": 979,
      "sentences": 8
    },
    {
      "id": 7,
      "content": "ace,\nPRIMAL[11],MAPPER[12],MAGAT[13],SCRIMP[14],\nandeachedgee∈E isaunit-costedgebetweentwovertices\nand MAPF-GPT [15]. These methods primarily rely on\nthat moves an agent from one vertex to the other. In this\nimitation learning (IL) and reinforcement learning (RL) and\npaper, we focus on 2D grid maps with connections in four\noften incorporate additional components, such as inter-agent\ndirections.Self-loopedgesarealsoallowed,whichrepresent\n“wait-in-place”actions.Eachagenti∈I hasastartlocation\n∗Equalcontribution\n1ThomasLordDepartmentofComputerScience,UniversityofSouthern s i ∈V andagoallocationg i ∈V.Italsoholdsthats i ̸=s j\nCalifornia,yimintan@usc.edu, biyik@usc.edu and g ̸= g when i ̸= j ∀i,j ∈ I. Our task is to plan a\ni j\n2Independent Researcher, xiaoxiong.xx21@gmail.com, collision-free path for each agent i from s to g .",
      "size": 832,
      "sentences": 4
    },
    {
      "id": 8,
      "content": "fornia,yimintan@usc.edu, biyik@usc.edu and g ̸= g when i ̸= j ∀i,j ∈ I. Our task is to plan a\ni j\n2Independent Researcher, xiaoxiong.xx21@gmail.com, collision-free path for each agent i from s to g . flotherxi@gmail.com i i\n3CarnegieMellonUniversity,jiaoyanl@andrew.cmu.edu Each action of agents, either waiting in place or moving\n4UniversityofCalifornia,Irvine,sven.koenig@uci.edu to an adjacent vertex, takes one time unit. Let vi ∈V be the\nt\n5202\nguA\n6\n]OR.sc[\n2v29920.3052:viXra\n=== 페이지 2 ===\nlocation of agent i at timestep t. Let π = [vi,vi,...,vi ] replanning all paths at each specified timestep [31], [32],\ni 0 1 Ti\ndenote a path of agent i from its start location vi to its and replanning only when agents reach their current targets\n0\ntarget vi . We assume that agents rest at their targets after and are assigned new ones [33], [34]. Some algorithms\nTi\ncompleting their paths, i.e., vi = vi ,∀t > Ti.",
      "size": 912,
      "sentences": 6
    },
    {
      "id": 9,
      "content": "ts reach their current targets\n0\ntarget vi . We assume that agents rest at their targets after and are assigned new ones [33], [34]. Some algorithms\nTi\ncompleting their paths, i.e., vi = vi ,∀t > Ti. The cost of can solve LMAPF in an offline setting where all tasks\nt Ti\nagent i’s path is Ti. We refer to the path with the minimum are known in advance. include CBSS [35], which applies\ncost as the shortest path. Traveling Salesman Problem (TSP) methods to plan task\nWe consider two types of agent-agent collisions. The first orders. However, these LMAPF methods also face the same\ntypeisvertexcollision,wheretwoagentsiandj occupythe scalability problem as MAPF methods. same vertex at the same timestep. The second type is edge\nC. Learning-based MAPF\ncollision,wheretwoagentsmoveinoppositedirectionsalong\nthe same edge simultaneously.",
      "size": 835,
      "sentences": 11
    },
    {
      "id": 10,
      "content": "alability problem as MAPF methods. same vertex at the same timestep. The second type is edge\nC. Learning-based MAPF\ncollision,wheretwoagentsmoveinoppositedirectionsalong\nthe same edge simultaneously. We use (i,j,t) to denote a Given the huge success of deep learning, many learning-\nvertex collision between agents i and j at timestep t or an based MAPF methods have been proposed. Compared to\nedge collision between agents i and j at timestep t to t+1. search-basedalgorithms,thesemethodscanusuallycomplete\nThe requirement of being collision-free implies the targets planning in short time and automatically learn heuristic\nassigned to the agents must be distinct from each other. We functions. Some of these methods focus on modifying edge\nuse SoC (flowtime) (cid:80)N Ti as the cost function.",
      "size": 795,
      "sentences": 8
    },
    {
      "id": 11,
      "content": "utomatically learn heuristic\nassigned to the agents must be distinct from each other. We functions. Some of these methods focus on modifying edge\nuse SoC (flowtime) (cid:80)N Ti as the cost function. weightsinthemap,suchasthecongestionmodel[36],which\ni=1\nThe objective of the MAPF problem is to find a set of is a data-driven approach that predicts agents’ movement\npaths {π |i∈I} for all agents such that, for each agent i: delays and uses these delays as movement costs, or On-\ni\n1) Agent i starts from its start location (i.e., vi =s ) and line GGO [37], which optimizes edge weights for Lifelong\n0 i\nstops at its target location g (i.e., vi =g ,∀t≥Ti). MAPF. However, these methods split MAPF planning into\nj t j\nmultiple stages, which can lead to a larger optimization\n2) Everypairofadjacentverticesonpathπ isconnected\ni\nby an edge, i.e., (vi,vi )∈E,∀t∈{0,1,...,Ti}. search space if one considers both edge-weight design and\nt t+1\nthe MAPF solver simultaneously. 3) {π |i∈I} is collision-free.",
      "size": 998,
      "sentences": 8
    },
    {
      "id": 12,
      "content": "ticesonpathπ isconnected\ni\nby an edge, i.e., (vi,vi )∈E,∀t∈{0,1,...,Ti}. search space if one considers both edge-weight design and\nt t+1\nthe MAPF solver simultaneously. 3) {π |i∈I} is collision-free. i\nMost other methods focus on the solver side, using im-\nIII. RELATEDWORK itation learning (IL), reinforcement learning (RL), or both. A. Multi-Agent Path Finding (MAPF) Learning-based solves can make end-to-end decisions using\nall available information and can be trained on various data\nMAPFhasbeenprovedanNP-hardproblemwithoptimal-\ntypes (e.g., MAPF, TAPF, LMAPF). In contrast, search-\nity[2].Ithasinspiredawiderangeofsolutionsforitsrelated\nbased methods often require multi-stage decomposition with\nchallenges. Decoupled strategies, as outlined in [18], [19],\nhand-craftedheuristics.Oneearlylearning-basedmethodfor\n[20], approach the problem by independently planning paths\nMAPF is PRIMAL [11] which is trained by RL and IL.",
      "size": 928,
      "sentences": 8
    },
    {
      "id": 13,
      "content": "tegies, as outlined in [18], [19],\nhand-craftedheuristics.Oneearlylearning-basedmethodfor\n[20], approach the problem by independently planning paths\nMAPF is PRIMAL [11] which is trained by RL and IL. It is\nforeachagentbeforeintegratingthesepaths.Incontrast,cou-\na decentralized algorithm that relies on an FOV around an\npledapproaches[21],[22]deviseaunifiedplanforallagents\nagent to generate the actions of that agent. MAPF-GPT [15]\nsimultaneously. There also exist dynamically coupled meth-\nisaGPT-basedmodelforMAPFproblems,trainedbyILon\nods [3], [23] that consider agents planning independently\nalargedataset.Otherapproachesincorporatecommunication\nat first and then together only when needed for resolving\nmechanisms in a decentralized manner, such as GNN [38]\nagent-agent collisions.",
      "size": 787,
      "sentences": 4
    },
    {
      "id": 14,
      "content": "endently\nalargedataset.Otherapproachesincorporatecommunication\nat first and then together only when needed for resolving\nmechanisms in a decentralized manner, such as GNN [38]\nagent-agent collisions. Among these, Conflict-Based Search\nand MAGAT [13], which employ Graph Neural Networks\n(CBS) algorithm [3] stands out as a centralized and optimal\n(GNNs) for communication, and SCRIMP [14], which uses\nmethodforMAPF,withseveralbounded-suboptimalvariants\na global communication mechanism based on transformers. such as ECBS [24] and EECBS [25]. Some suboptimal\nHowever, all existing learning-based solvers focus on the\nMAPF algorithms, such as Prioritized Planning (PP) [26],\nagents themselves, forcing researchers to design features\n[18], PBS [27], LaCAM [5] and their variant methods [28],\nof agents. This makes it challenging, if not impossible, to\n[6], [29] exhibit better scalability and efficiency.",
      "size": 901,
      "sentences": 5
    },
    {
      "id": 15,
      "content": "searchers to design features\n[18], PBS [27], LaCAM [5] and their variant methods [28],\nof agents. This makes it challenging, if not impossible, to\n[6], [29] exhibit better scalability and efficiency. However,\ndevelopacentralizedpolicythatcanhandlevaryingnumbers\nthese search-based algorithms always face the problem of\nof agents and map sizes. Our method is the first centralized\nsearch space dimensionality explosion as the problem size\nMAPF solver to overcome the challenge of feature design\nincreases, making it difficult to produce a valid solution\nand to integrate edge-weight design ideas [39], [40] into a\nwithinalimitedtime.Learning-basedmethodscanovercome\nneural-network-based solver. the dimensionality issue by learning from large amounts of\ndataandaddressingthetrade-offbetweenlow-costpathsand IV. METHOD\nscalability.",
      "size": 829,
      "sentences": 6
    },
    {
      "id": 16,
      "content": "e.Learning-basedmethodscanovercome\nneural-network-based solver. the dimensionality issue by learning from large amounts of\ndataandaddressingthetrade-offbetweenlow-costpathsand IV. METHOD\nscalability. A. RAILGUN Overview\nB. Lifelong MAPF\nInthissection,weintroduceourRAILGUNmethod.First,\nCompared to the MAPF problem, Lifelong MAPF we discuss why it is difficult to design a learning algorithm\n(LMAPF)continuouslyassignsnewtargetlocationstoagents forcentralizedMAPFwherepoliciesareagent-based.When\nonce they have reached their current targets. In LMAPF, focusing on generating actions based on agent features, we\nagents do not need to arrive at their targets simultaneously. needtoprovideaneuralnetworkwithatleasttheagent’sstart\nTherearethreemainapproachestosolvingLMAPF:solving location, goal location, and additional features, amounting\nthe problem as a whole [30], using MAPF methods but to k scalar variables (k ≥ 4) for one agent.",
      "size": 933,
      "sentences": 6
    },
    {
      "id": 17,
      "content": "ethreemainapproachestosolvingLMAPF:solving location, goal location, and additional features, amounting\nthe problem as a whole [30], using MAPF methods but to k scalar variables (k ≥ 4) for one agent. Then the\n=== 페이지 3 ===\n0 0 0 0\n0 1 0 2 Apply\n1 2 Current Location (! )$* ) 1 2\n0 0 0 0 Sample\n1 1 0 1 2 1 0 0 Input (! !\" )\n0 0 0 0 Goal Location (! + ) Convolutional\nNeural 1 2\n0 0 0 0\nMap (! &'( ) Network Output (! #$% )\n0 0 0 3\nCost-to-goal (! ) )+\nFig. 1: RAILGUN Inference Overview: On the left side (features), there is one current state along with all related input features of size\n(n,m,1). These features are then stacked along the last channel to construct the input feature F of size (n,m,k). In this example,\nin\nwe have n=2, m=4, and k=4. On the right side (inference), the input feature F is fed into a CNN-based neural network, which\nin\noutputsactionprobabilitiesF ofsize(n,m,5).WesamplefromF toobtainactualactionsandthenapplythecorrespondingactions\nout out\nto each agent.",
      "size": 986,
      "sentences": 13
    },
    {
      "id": 18,
      "content": "nput feature F is fed into a CNN-based neural network, which\nin\noutputsactionprobabilitiesF ofsize(n,m,5).WesamplefromF toobtainactualactionsandthenapplythecorrespondingactions\nout out\nto each agent. 1 2 edgeineveryoccupiedgridcell.Oncesuchadirectedgraph\nis given, no MAPF solver is needed, as there is only one\n1 2 1 2 1 2\npossible transition at each timestep. The sequence of these\nspecialized graphs then constitutes a valid MAPF solution. Afterconvertingagent-basedsolutionintoarepresentation\nas a series of specialized graphs, we use a CNN network to\naddress the challenge of generating these specialized graphs\n3 1 4\nand generalizing across different maps, which we discussed\n2 2 1 inthepreviousparagraphs.TheinputfeatureisF in withsize\n(n,m,k), and the output feature is F with size (n,m,5). out\nHere, k represents the number of feature channels based on\nthe feature design, and (n,m) represents the map size. As an example shown in Figure 1, to encode an agent’s\nFig.",
      "size": 975,
      "sentences": 6
    },
    {
      "id": 19,
      "content": "s F with size (n,m,5). out\nHere, k represents the number of feature channels based on\nthe feature design, and (n,m) represents the map size. As an example shown in Figure 1, to encode an agent’s\nFig. 2: This is an example of how an agent-based solution relates\ncurrentlocationasafeature,weconstructatensorF with\nto a series of specialized graphs. The upper-left figure illustrates a cur\ntestcaseweaimtosolve,alongwithagraphwheregreennodesand size (n,m,1). In this tensor, F cur [i][j] = idx if the agent\norangeedgesrepresentmapconnectivity.Theotherfiguresshowa idxisatposition(i,j)inthemap;otherwise,F [i][j]=0. cur\nvalid MAPF solution for this testcase, where agent 1 should yield Stacking all such feature tensors along the last dimension\nto agent 2. At each timestep, each node in the connectivity graph\nforms F with size (n,m,k). F [i][j] represents the\nhas only one outgoing edge.",
      "size": 885,
      "sentences": 9
    },
    {
      "id": 20,
      "content": "king all such feature tensors along the last dimension\nto agent 2. At each timestep, each node in the connectivity graph\nforms F with size (n,m,k). F [i][j] represents the\nhas only one outgoing edge. Here, we draw edges only for nodes in out\nprobability distribution over all possible actions at grid cell\noccupiedbyagents,astheoutgoingedgeforothernodescouldbe\nany of the available edges. position (i,j). We use 5 channels because each agent can\ntake one of up to five different actions at each timestep. total number of features is at least kN. Consider that the\nThus, if an agent is located at grid cell (i,j), its action\nmaximumnumberofagentscouldbeN =|V|≈nm,where\nprobabilities are stored in F [i][j]. Note that the model\nout\nn and m are the 2D map dimensions. If we want to handle\nonly outputs the next action prediction based on the current\nall possible numbers of agents on a specific map, the total\ntimestep. Therefore, to obtain a full trajectory, the trained\nfeature size would be knm.",
      "size": 995,
      "sentences": 11
    },
    {
      "id": 21,
      "content": "ts the next action prediction based on the current\nall possible numbers of agents on a specific map, the total\ntimestep. Therefore, to obtain a full trajectory, the trained\nfeature size would be knm. This dependence on map size\nmodel must be invoked repeatedly. means that we cannot create a policy to cover all different\nmapsifweconstructthefeaturesagentbyagent.Thatiswhy B. Model Architecture\nthereisnocentralizedlearning-basedsolverandalllearning- Inthispaper,weuseU-NetforRAILGUN,asitiswidely\nbased MAPF solvers adopt a decentralized approach with a used in diffusion models [41] and includes transposed con-\nlimited FOV for each agent [11], [12], [13], [14], [15]. volution layers thatallows the network torecover the spatial\nOur insight is that in a valid MAPF solution, there will resolution of the input. That’s why RAILGUN can handle\nbe no collision, which means there can be at most one agent input maps of different sizes during inference. We employ\nin each map grid cell in each timestep.",
      "size": 1000,
      "sentences": 8
    },
    {
      "id": 22,
      "content": "the input. That’s why RAILGUN can handle\nbe no collision, which means there can be at most one agent input maps of different sizes during inference. We employ\nin each map grid cell in each timestep. At any timestep, the standard U-Net architecture comprising five layers in\neach agent chooses one of the five edges of its grid cell as total. The encoder begins with an initial layer containing 64\nits action. Therefore, if we remove all edges that the agents channels. At each subsequent layer, the number of channels\ndo not use at each timestep, we find that a valid MAPF is doubled while the size of the feature maps is halved. In\nsolution can be viewed as a series of specialized graphs. As the decoder, this process is reversed, with the number of\nshowninFigure2,thesespecializedgraphshaveexactlyone channels halved and the spatial resolution doubled at each\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\nlayer.",
      "size": 896,
      "sentences": 9
    },
    {
      "id": 23,
      "content": "decoder, this process is reversed, with the number of\nshowninFigure2,thesespecializedgraphshaveexactlyone channels halved and the spatial resolution doubled at each\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\nlayer. Notably, bilinear interpolation is not employed in the\ndecoder; instead,we use deconvolution asin original U-Net. At the final layer, the number of channels is reduced to 5,\ncorresponding to the maximum number of possible actions. WeshouldalsonotethatsinceU-NetusesCNNlayersinthe\nencoder, which progressively reduce the spatial dimensions\nof the feature maps, there is a minimum required input size\n(a)Maze (b)Random (c)Warehouse\nto ensure valid downsampling operations. For small maps,\npadding is needed. The resulting RAILGUN model contains\napproximately 30 million FP32 parameters.",
      "size": 783,
      "sentences": 6
    },
    {
      "id": 24,
      "content": "ed input size\n(a)Maze (b)Random (c)Warehouse\nto ensure valid downsampling operations. For small maps,\npadding is needed. The resulting RAILGUN model contains\napproximately 30 million FP32 parameters. C. Feature Selection\nAs shown in Figure 1, we construct the input features\nfrommultiplecomponents.Weemployfivetypesoffeatures:\nthe map, current locations, goal locations, cost-to-goal, and (d)Puzzle (e)Cities-tiles (f)Cities\ngradients of cost-to-goal (the last feature is not shown in\nFig. 3: Examples of POGEMA-tested maps. The six met-\nFigure 1 due to space constraints). For the map feature, we rics—Performance, Coordination, Scalability, Cooperation, OOD,\nuse 1 to represent non-traversable grid cells and 0 to repre- and Pathfinding—are evaluated on different map sets. Note that\nsenttraversablegridcells.Forthecurrentandgoallocations, Cities-tiles are 64×64 areas selected from larger Cities maps with\nwe use the agent’s index to indicate which agent occupies dimensions of 256×256.",
      "size": 989,
      "sentences": 8
    },
    {
      "id": 25,
      "content": "traversablegridcells.Forthecurrentandgoallocations, Cities-tiles are 64×64 areas selected from larger Cities maps with\nwe use the agent’s index to indicate which agent occupies dimensions of 256×256. a grid cell; otherwise, the grid cell is set to 0. We also For training data, we randomly generate 180 maze maps\nattemptedencodingagentindicesasbinaryvectors;however, with 32×32 size, each with varying obstacle densities and\nthis produces excessively large input features, rendering the maze shape. For each map, we randomly generate {2, 5, 20,\nmodel too large to train.",
      "size": 570,
      "sentences": 4
    },
    {
      "id": 26,
      "content": "32×32 size, each with varying obstacle densities and\nthis produces excessively large input features, rendering the maze shape. For each map, we randomly generate {2, 5, 20,\nmodel too large to train. 40, 60} MAPF scenarios with {16, 32, 64, 96, 128} agents\nWe also use the precomputed shortest path cost as the respectively,foratotalof127scenariosforeachmap.Weuse\ncost-to-goal feature for each agent which is a widely used LACAM-v1 [5] to compute reference paths for all scenarios\nfeature in learning-based methods, as shown in Figure 1. as training data, primarily due to its fast data generation\nThe gradients of the cost-to-goal, represents the potential speed on large maps. direction of next action, are determined by the changes All experiments1 were conducted on a system running\nof the cost-to-goal distances. Specifically, we define the Ubuntu22.04.1LTSequippedwithanAMDInteli9-12900K\nchanges in cost-to-goal distances from an agent’s current CPU, 128GB RAM and NVIDIA RTX 3080.",
      "size": 986,
      "sentences": 5
    },
    {
      "id": 27,
      "content": "f the cost-to-goal distances. Specifically, we define the Ubuntu22.04.1LTSequippedwithanAMDInteli9-12900K\nchanges in cost-to-goal distances from an agent’s current CPU, 128GB RAM and NVIDIA RTX 3080. For the testing\ncell (i,j) to its adjacent cells as δ left ,δ right ,δ up ,δ down . δ < 0 phase, the POGEMA benchmark provides a total of 3,376\nindicatesthattheagentisapproachingthegoallocation.The test cases featuring six different types of maps shown in\nresulting direction, denoted by g ij =(∆x ij ,∆y ij ), consists Figure3,varyingnumbersofagents,anddifferentmapsizes. of horizontal and vertical components. For the horizontal POGEMA use six metrics, namely, Performance, Coordi-\ncomponent, ∆x ij is computed as shown below: nation, Scalability, Cooperation, Out-of-Distribution (OOD)\n and Pathfinding.",
      "size": 807,
      "sentences": 6
    },
    {
      "id": 28,
      "content": "ts. For the horizontal POGEMA use six metrics, namely, Performance, Coordi-\ncomponent, ∆x ij is computed as shown below: nation, Scalability, Cooperation, Out-of-Distribution (OOD)\n and Pathfinding. The relevant equations are as follows:\n 0\n1\ni\ni\nf\nf\nδ\nδ\nl\nl\ne\ne\nf\nf\nt\nt\n≥\n≥\n0\n0\na\na\nn\nn\nd\nd\nδ\nδ\nr\nr\ni\ni\ng\ng\nh\nh\nt\nt\n≥\n<\n0\n0\n,\n,\n\n\nSoC\nbest\n/SoC if MAPF solved\n∆x ij = Performance= 0 if MAPF not solved\n −\nran\n1\ndom(±1) i\ni\nf\nf\nδ\nδ left\n<\n<\n0\n0\na\na\nn\nn\nd\nd\nδ\nδ right ≥\n<\n0\n0\n,\n,\n\nthr\nt\no\nh\nu\nro\ng\nu\nh\ng\np\nh\nu\np\nt\nb\nu\ne\nt\nst\nif LMAPF\nleft right\nand similarly for the vertical component. The Performance, OOD, and Cooperation metrics share the\nV. EXPERIMENTS&RESULTS same definitions and primarily evaluate solution quality and\nsuccess rate across different maps. SoC represents the\nA. Training and Testing Settings best\nbestSoCperformanceachievedamongalltestedalgorithms.",
      "size": 887,
      "sentences": 6
    },
    {
      "id": 29,
      "content": "efinitions and primarily evaluate solution quality and\nsuccess rate across different maps. SoC represents the\nA. Training and Testing Settings best\nbestSoCperformanceachievedamongalltestedalgorithms. We use the POGEMA [17] benchmark to evaluate our\nmethod, so for comparison, we only include the methods runtime(agents )/runtime(agents )\nScalability= 1 2\navailable in POGEMA. POGEMA includes several different |agents |/|agents |\n1 2\nmetrics, allowing a fair multi-fold comparison. For data col- # of collisions\nCoordination=1−\nlection, our training data is primarily generated by LaCAM-\n|agents|×episode_length\nv1 [5]. The model is trained with cross-entropy loss and (cid:40)\nSoC/SoC\na batch size of 256. We utilize the AdamW optimizer [42] Pathfinding= best\nwithβ valuessetto(0.9,0.999)andaweightdecayof10−3. 0 if path not found\nThetrainingprocessachievesconvergenceinonlysixhours,\nleveraging the power of four NVIDIA A100 GPUs.",
      "size": 931,
      "sentences": 9
    },
    {
      "id": 30,
      "content": "mizer [42] Pathfinding= best\nwithβ valuessetto(0.9,0.999)andaweightdecayof10−3. 0 if path not found\nThetrainingprocessachievesconvergenceinonlysixhours,\nleveraging the power of four NVIDIA A100 GPUs. 1OurcodecanbefoundatGithub:https://github.com/TachikakaMin\n=== 페이지 5 ===\nCoordination\ng n\nhfi n di O O D\nPat\necnamrofreP\nLaCAM RAILGUN VDN QPLEX\nSCRIMP IQL QMIX DCC\nMAMBA\n20\n40\n60\nS ca\nla\nb ility 1\n8\n0\n0\n0\nC\no o p\ner\nati o n\n(a) MAPF: RAILGUN outperforms most baseline models in\nScalability,Performance,Pathfinding,OODandCoordination. Coordination\nPathfinding\nO O\nD\necnamrofreP\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n64 128 192 256\nNumber of Agents\nRHCR RAILGUN QMIX ASwitcher\nFollower IQL QPLEX MATS-LP\nMAMBA VDN\n20\n40\n60\nScalability\n1\n8\n0\n0\n0\nCooperation\n(b) LMAPF: RAILGUN still have good zero-shot LMAPF\nperformace in Pathfinding, Coordination, Cooperation and\nScalabilityjusttrainingonMAPFdataset. Fig.",
      "size": 892,
      "sentences": 5
    },
    {
      "id": 31,
      "content": "A VDN\n20\n40\n60\nScalability\n1\n8\n0\n0\n0\nCooperation\n(b) LMAPF: RAILGUN still have good zero-shot LMAPF\nperformace in Pathfinding, Coordination, Cooperation and\nScalabilityjusttrainingonMAPFdataset. Fig. 4: POGEMA Test Overview: Performance, OOD, Pathfinding\nandCooperationrepresentssolutionSoC/throuputquality.Scalabil-\nityrepresentsruntimerespecttoagentnumbers.Coordinationisthe\nprobability of invalid actions from learning-based methods. Scalability is the ratio of algorithm runtimes with different\nagentnumberswith|agent |<|agent |,providingameasure 1 2\nof how the algorithm’s runtime scales as the agent number\nchanges and higher is better. Coordination focuses on in-\nvalid action frequency produced by learning-based methods. Pathfinding indicates the ability of learning-based methods\nto find the shortest path for a single agent. B. Testing Results\nFigure 4 presents the performance metrics for RAILGUN\nand the baseline methods.",
      "size": 934,
      "sentences": 8
    },
    {
      "id": 32,
      "content": "inding indicates the ability of learning-based methods\nto find the shortest path for a single agent. B. Testing Results\nFigure 4 presents the performance metrics for RAILGUN\nand the baseline methods. The learning-based methods in-\nclude VDN [43], QPLEX [44], SCRIMP [14], IQL [45],\nQMIX [46], DCC [47], MAMBA [48], Switcher [49], Fol-\nRSC\n60000\n50000\n40000\n30000\n20000\n10000\n0\n64 128 192 256\nNumber of Agents\nCoS\nLaCAM SCRIMP MAMBA VDN QPLEX\nDCC RAILGUN IQL QMIX\nFig.5:MAPFtestingonCities-tiles:CSR(thesuccessrateatwhich\nallagentsreachtheirgoallocations;higherisbetter),SoC(Sumof\nall agent arrival time; lower is better). lower [50], and MATS-LP [51]. All these baseline methods\nare decentralized methods. LaCAM-v3 [29] and RHCR [31]\nserve as the search-based algorithm baselines in MAPF and\nLMAPF problems. In Figure 4a, we observe that RAILGUN achieves high\nscores across all six metrics.",
      "size": 890,
      "sentences": 8
    },
    {
      "id": 33,
      "content": "lized methods. LaCAM-v3 [29] and RHCR [31]\nserve as the search-based algorithm baselines in MAPF and\nLMAPF problems. In Figure 4a, we observe that RAILGUN achieves high\nscores across all six metrics. RAILGUN attains the highest\nscore in the Scalability metric because it generates specific\ndirected graphs at each timestep, ensuring that runtime\ndepends only on map size rather than the number of agents\nin theory. However, even though RAILGUN outperforms or\nmatches the scores of other learning-based methods in most\nareas, it still exhibits a significant gap with LaCAM in SoC-\nrelated metrics. This outcome is expected, as RAILGUN\nis trained on data generated by LaCAM-v1, and LaCAM-\nv1 is not designed to achieve the best SoC performance. Mimicking LaCAM-v1 is the top priority of RAILGUN\nrather than producing a valid solution with the lowest SoC.",
      "size": 852,
      "sentences": 7
    },
    {
      "id": 34,
      "content": "generated by LaCAM-v1, and LaCAM-\nv1 is not designed to achieve the best SoC performance. Mimicking LaCAM-v1 is the top priority of RAILGUN\nrather than producing a valid solution with the lowest SoC. As shown in Figure 4b, when testing on LMAPF, a com-\npletely zero-shot task for RAILGUN, RAILGUN achieves\nonly moderate scores in throughput-related metrics since\nnone of the training data was optimized for throughput. However, this zero-shot test also demonstrates RAILGUN’s\nstrong generalization ability across different tasks. RAIL-\nGUN also attains high scores in Pathfinding, Coordination,\nandScalability.Thesestrengthsandweaknessessuggestthat,\nalthough RAILGUN’s overall solution quality remains an\nissue, it can produce valid solutions in a diverse set of\nscenarios. Thus, we believe using a dataset optimized for\nthe cost function of interest, combined with applying a task-\nspecific reward function for fine-tuning via RL after the SL\nprocess, will help improve the overall solution quality.",
      "size": 1000,
      "sentences": 6
    },
    {
      "id": 35,
      "content": "taset optimized for\nthe cost function of interest, combined with applying a task-\nspecific reward function for fine-tuning via RL after the SL\nprocess, will help improve the overall solution quality. Figure 5 presents detailed CSR (see caption) and SoC. Even for unseen maps (Cities-tiles) and larger agent num-\nbers (192 and 256), RAILGUN outperforms other learning-\nbased methods except SCRIMP, achieving up to 60% CSR. ThisalsoshowsRAILGUN’sstrongzero-shotgeneralization\nability in new maps and new agent numbers. Furthermore,\nwe observe that DCC attains a better SoC, despite having a\nlower CSR.",
      "size": 599,
      "sentences": 5
    },
    {
      "id": 36,
      "content": "ieving up to 60% CSR. ThisalsoshowsRAILGUN’sstrongzero-shotgeneralization\nability in new maps and new agent numbers. Furthermore,\nwe observe that DCC attains a better SoC, despite having a\nlower CSR. This indicates that in DCC, only a few agents\n[표 데이터 감지됨]\n\n=== 페이지 6 ===\nCSR SoC(x1000) Makespan\nAgents\n32 64 96 128 160 192 32 64 96 128 160 192 32 64 96 128 160 192\nAlgorithm\nLaCAM 1.00 1.00 1.00 1.00 1.00 1.00 0.98 1.97 3.00 4.07 5.16 6.32 55.34 58.50 60.50 61.59 62.77 64.04\nSCRIMP 1.00 1.00 1.00 0.98 0.98 0.91 1.07 2.34 3.81 5.49 7.45 9.81 56.54 62.48 68.17 75.25 83.43 94.73\nRAILGUN 1.00 0.97 0.73 0.13 0.01 - 1.22 2.92 5.39 9.05 13.17 17.39 63.93 82.52 105.69 126.42 127.91 -\nDCC 0.95 0.86 0.73 0.12 - - 1.10 2.62 4.88 7.82 11.09 14.86 66.06 88.10 111.23 126.75 - -\nMAMBA - - - - - - 2.78 7.06 11.29 15.49 19.58 23.81 - - - - - -\nIQL - - - - - - 4.08 8.15 12.24 16.34 20.45 24.56 - - - - - -\nVDN - - - - - - 3.55 7.44 11.73 16.00 20.21 24.40 - - - - - -\nQMIX - - - - - - 3.67 7.56 11.64 15.85 20.06 24.28 - - - - - -\nQPLEX - - - - - - 3.79 7.68 11.69 15.82 20.03 24.24 - - - - - -\nTABLE I: MAPF Scores on Warehouse: Makespan is the latest agent arrival time.",
      "size": 1166,
      "sentences": 4
    },
    {
      "id": 37,
      "content": "- - - - - - 3.67 7.56 11.64 15.85 20.06 24.28 - - - - - -\nQPLEX - - - - - - 3.79 7.68 11.69 15.82 20.03 24.24 - - - - - -\nTABLE I: MAPF Scores on Warehouse: Makespan is the latest agent arrival time. “-” represents 0 in CSR and 128 in Makespan. 5\n4\n3\n2\n1\n64 128 192 256\nNumber of Agents\ntuphguorhT\n1.75\n1.50\n1.25\n1.00\n0.75\n0.50\n64 128 192 256\nNumber of Agents\nytilibalacS\nRHCR MAMBA IQL QMIX ASwitcher that, rather than predicting actions for individual agents,\nFollower RAILGUN VDN QPLEX MATS-LP predicting edge directions for each map grid cell overcomes\nthe difficulties associated with variable input feature dimen-\nsions. This finding allows RAILGUN to employ a CNN-\nbased architecture capable of handling maps of any size\nand any number of agents.",
      "size": 753,
      "sentences": 4
    },
    {
      "id": 38,
      "content": "es\nthe difficulties associated with variable input feature dimen-\nsions. This finding allows RAILGUN to employ a CNN-\nbased architecture capable of handling maps of any size\nand any number of agents. In our experiments, RAILGUN\ndemonstratesstrongperformanceacrossallsixmetricsinthe\nPOGEMAbenchmark.Furthermore,itsexcellentgeneraliza-\ntion abilities enable it to handle unseen maps, varying agent\nnumbers, and even other tasks such as the LMAPF problem. Infuturework,weplantocollecthigher-qualitydatatotrain\nRAILGUNasafoundationmodelandapplyRLwithatask-\nspecific cost function to fine-tune RAILGUN on specific\ntasks, agent numbers, and map shapes, thereby improving\nFig.6:LMAPFThroughputandScalabilityPerformanceonCities-\nsolution quality and success rate in real-world applications. tiles:Scalabilityiscalculatedbypreviousaverageperagentruntime\ndivide by current one. VII.",
      "size": 872,
      "sentences": 6
    },
    {
      "id": 39,
      "content": "APFThroughputandScalabilityPerformanceonCities-\nsolution quality and success rate in real-world applications. tiles:Scalabilityiscalculatedbypreviousaverageperagentruntime\ndivide by current one. VII. ACKNOWLEDGEMENT\nfail to reach their goal locations and the path lengths are\nThe research at the University of California, Irvine, the\nshorterthanthoseproducedbyRAILGUN,highlightingthat\nCarnegie Mellon University and the University of Southern\ngeneratingvalidsolutionsisahigherpriorityforRAILGUN. CaliforniawassupportedbytheNationalScienceFoundation\nIn Table I, we present the CSR, SoC, and Makespan\n(NSF) under grant numbers 2328671, 2441629, 2434916,\nmetrics (see caption) of different algorithms for the Ware-\n2321786, 2112533, and 2121028, as well as gifts from\nhouse map. We observe a similar pattern where RAILGUN\nAmazon Robotics and the Donald Bren Foundation. The\nachieves a higher CSR score but a lower SoC compared\nviews and conclusions contained in this document are those\nto DCC.",
      "size": 990,
      "sentences": 7
    },
    {
      "id": 40,
      "content": "milar pattern where RAILGUN\nAmazon Robotics and the Donald Bren Foundation. The\nachieves a higher CSR score but a lower SoC compared\nviews and conclusions contained in this document are those\nto DCC. However, when considering Makespan, RAILGUN\nof the authors and should not be interpreted as representing\noutperforms DCC. This indicates that RAILGUN is capable\nthe official policies, either expressed or implied, of the\nof finding relatively short solutions. As Makespan reflects\nsponsoring organizations, agencies, or the U.S. government. the latest arrival time, many agents arriving before the\nlast ones contribute to a higher SoC. This may be due\nREFERENCES\nto congestion situations, where many agents have a dead\nlock, and RAILGUN requires additional time to resolve the [1] R.Stern,N.Sturtevant,A.Felner,S.Koenig,H.Ma,T.Walker,J.Li,\ncongestion2.",
      "size": 851,
      "sentences": 7
    },
    {
      "id": 41,
      "content": "EFERENCES\nto congestion situations, where many agents have a dead\nlock, and RAILGUN requires additional time to resolve the [1] R.Stern,N.Sturtevant,A.Felner,S.Koenig,H.Ma,T.Walker,J.Li,\ncongestion2. SCRIMP achieves the best MAPF performance\nD.Atzmon,L.Cohen,T.Kumaretal.,“Multi-agentpathfinding:Defi-\nnitions,variants,andbenchmarks,”inProceedingsoftheInternational\namong learning-based methods, which may indicate that SymposiumonCombinatorialSearch(SoCS),2019. RAILGUN’s supervised learning is not sufficient and that [2] J. Yu and S. LaValle, “Structure and intractability of optimal multi-\nrobotpathplanningongraphs,”inProceedingsoftheAAAIConference\nreinforcementlearningshouldalsobeinvolved.ForLMAPF,\nonArtificialIntelligence(AAAI),2013.\nasshowninFigure6,RAILGUN’sthroughputscoreisbetter [3] G.Sharon,R.Stern,A.Felner,andN.R.Sturtevant,“Conflict-based\nthan those of VDN, IQL, and MAMBA.",
      "size": 891,
      "sentences": 3
    },
    {
      "id": 42,
      "content": "ved.ForLMAPF,\nonArtificialIntelligence(AAAI),2013.\nasshowninFigure6,RAILGUN’sthroughputscoreisbetter [3] G.Sharon,R.Stern,A.Felner,andN.R.Sturtevant,“Conflict-based\nthan those of VDN, IQL, and MAMBA. Although it does search for optimal multi-agent pathfinding,” Artificial Intelligence,\n2015.\nnot achieve the best throughput score overall, its scalability\n[4] G.WagnerandH.Choset,“M*:Acompletemultirobotpathplanning\nis impressive. Figure 6 also demonstrates that as the number algorithmwithperformancebounds,”inProceedingsoftheIEEE/RSJ\nofagentsincreases,theaverageruntimeperagentdecreases. International Conference on Intelligent Robots and Systems (IROS),\n2011. VI. CONCLUSIONANDFUTUREWORK [5] K. Okumura, “Lacam: Search-based algorithm for quick multi-agent\npathfinding,” in Proceedings of the AAAI Conference on Artificial\nIn this paper, we propose the first centralized learning- Intelligence(AAAI),2023.",
      "size": 908,
      "sentences": 6
    },
    {
      "id": 43,
      "content": "acam: Search-based algorithm for quick multi-agent\npathfinding,” in Proceedings of the AAAI Conference on Artificial\nIn this paper, we propose the first centralized learning- Intelligence(AAAI),2023. [6] J. Li, Z. Chen, D. Harabor, P. J. Stuckey, and S. Koenig, “Mapf-\nbasedmethod,RAILGUN,fortheMAPFproblem.Wefound\nlns2: fast repairing for multi-agent path finding via large neighbor-\nhood search,” in Proceedings of the AAAI Conference on Artificial\n2Wehaveademonstrationinourvideosupplementarymaterial. Intelligence(AAAI),2022. [표 데이터 감지됨]\n\n=== 페이지 7 ===\n[7] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassification ming,” in Proceedings of the International Symposium on Combina-\nwith deep convolutional neural networks,” in Advances in Neural torialSearch(SoCS),2019. InformationProcessingSystems(NeurIPS),2012.",
      "size": 821,
      "sentences": 5
    },
    {
      "id": 44,
      "content": "n ming,” in Proceedings of the International Symposium on Combina-\nwith deep convolutional neural networks,” in Advances in Neural torialSearch(SoCS),2019. InformationProcessingSystems(NeurIPS),2012. [31] J.Li,A.Tinka,S.Kiesel,J.W.Durham,T.S.Kumar,andS.Koenig,\n[8] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van “Lifelong multi-agent path finding in large-scale warehouses,” in\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, ProceedingsoftheAAAIConferenceonArtificialIntelligence(AAAI),\nM. Lanctot et al., “Mastering the game of go with deep neural 2021.\nnetworksandtreesearch,”Nature,2016. [32] K. Okumura, M. Machida, X. Défago, and Y. Tamura, “Priority\n[9] J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman, inheritance with backtracking for iterative multi-agent path finding,”\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 ArtificialIntelligence,2022. technicalreport,”arXivpreprintarXiv:2303.08774,2023.",
      "size": 970,
      "sentences": 5
    },
    {
      "id": 45,
      "content": "cktracking for iterative multi-agent path finding,”\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 ArtificialIntelligence,2022. technicalreport,”arXivpreprintarXiv:2303.08774,2023. [33] M.Cˇáp,J.Vokˇrínek,andA.Kleiner,“Completedecentralizedmethod\n[10] J.-M. Alkazzi and K. Okumura, “A comprehensive review on lever- for on-line multi-robot trajectory planning in well-formed infrastruc-\naging machine learning for multi-agent path finding,” IEEE Access, tures,”inProceedingsoftheInternationalConferenceonAuto-mated\n2024. PlanningandScheduling(ICAPS),2015. [11] G. Sartoretti, J. Kerr, Y. Shi, G. Wagner, T. S. Kumar, S. Koenig, [34] F. Grenouilleau, W.-J. Van Hoeve, and J. N. Hooker, “A multi-\nand H. Choset, “Primal: Pathfinding via reinforcement and imitation labela*algorithmformulti-agentpathfinding,”inProceedingsofthe\nmulti-agentlearning,”IEEERoboticsandAutomationLetters,2019.",
      "size": 903,
      "sentences": 6
    },
    {
      "id": 46,
      "content": "“A multi-\nand H. Choset, “Primal: Pathfinding via reinforcement and imitation labela*algorithmformulti-agentpathfinding,”inProceedingsofthe\nmulti-agentlearning,”IEEERoboticsandAutomationLetters,2019. International Conference on Auto- mated Planning and Scheduling\n[12] Z. Liu, B. Chen, H. Zhou, G. Koushik, M. Hebert, and D. Zhao, (ICAPS),2019. “Mapper:Multi-agentpathplanningwithevolutionaryreinforcement [35] Z. Ren, S. Rathinam, and H. Choset, “CBSS: A new approach\nlearning in mixed dynamic environments,” in Proceedings of the for multiagent combinatorial path finding,” IEEE Transactions on\nIEEE/RSJInternationalConferenceonIntelligentRobotsandSystems Robotics,2023. (IROS),2020. [36] G.YuandM.T.Wolf,“Congestionpredictionforlargefleetsofmobile\n[13] Q.Li,W.Lin,Z.Liu,andA.Prorok,“Message-awaregraphattention robots,”inProceedingsofIEEEInternationalConferenceonRobotics\nnetworks for large-scale multi-robot path planning,” IEEE Robotics andAutomation(ICRA),2023.\nandAutomationLetters,2021.",
      "size": 994,
      "sentences": 5
    },
    {
      "id": 47,
      "content": "waregraphattention robots,”inProceedingsofIEEEInternationalConferenceonRobotics\nnetworks for large-scale multi-robot path planning,” IEEE Robotics andAutomation(ICRA),2023.\nandAutomationLetters,2021. [37] H.Zang,Y.Zhang,H.Jiang,Z.Chen,D.Harabor,P.J.Stuckey,and\n[14] Y. Wang, B. Xiang, S. Huang, and G. Sartoretti, “Scrimp: Scalable J. Li, “Online guidance graph optimization for lifelong multi-agent\ncommunicationforreinforcement-andimitation-learning-basedmulti- path finding,” in Proceedings of the AAAI Conference on Artificial\nagent pathfinding,” in Proceedings of the IEEE/RSJ International Intelligence(AAAI),2025. ConferenceonIntelligentRobotsandSystems(IROS),2023.",
      "size": 672,
      "sentences": 3
    },
    {
      "id": 48,
      "content": "ing,” in Proceedings of the AAAI Conference on Artificial\nagent pathfinding,” in Proceedings of the IEEE/RSJ International Intelligence(AAAI),2025. ConferenceonIntelligentRobotsandSystems(IROS),2023. [38] Q. Li, F. Gama, A. Ribeiro, and A. Prorok, “Graph neural networks\n[15] A.Andreychuk,K.Yakovlev,A.Panov,andA.Skrynnik,“Mapf-gpt: for decentralized multi-robot path planning,” in Proceedings of the\nImitationlearningformulti-agentpathfindingatscale,”arXivpreprint IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems\narXiv:2409.00134,2024. (IROS),2020. [16] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional [39] Y.Zhang,H.Jiang,V.Bhatt,S.Nikolaidis,andJ.Li,“Guidancegraph\nnetworks for biomedical image segmentation,” in International Con- optimization for lifelong multi-agent path finding,” arXiv preprint\nference on Medical Image Computing and Computer-Assisted Inter- arXiv:2402.01446,2024.\nvention(MICCAI),2015.",
      "size": 939,
      "sentences": 5
    },
    {
      "id": 49,
      "content": "in International Con- optimization for lifelong multi-agent path finding,” arXiv preprint\nference on Medical Image Computing and Computer-Assisted Inter- arXiv:2402.01446,2024.\nvention(MICCAI),2015. [40] Z.Chen,D.Harabor,J.Li,andP.J.Stuckey,“Trafficflowoptimisation\n[17] A. Skrynnik, A. Andreychuk, A. Borzilov, A. Chernyavskiy, for lifelong multi-agent path finding,” in Proceedings of the AAAI\nK. Yakovlev, and A. Panov, “POGEMA: A benchmark platform for ConferenceonArtificialIntelligence(AAAI),2024.\ncooperative multi-agent pathfinding,” in International Conference on [41] J.Ho,A.Jain,andP.Abbeel,“Denoisingdiffusionprobabilisticmod-\nLearningRepresentations(ICLR),2025. els,”AdvancesinNeuralInformationProcessingSystems(NeurIPS),\n[18] D.Silver,“Cooperativepathfinding,”inProceedingsoftheAAAICon- 2020.\nferenceonArtificialIntelligenceandInteractiveDigitalEntertainment [42] I.LoshchilovandF.Hutter,“Decoupledweightdecayregularization,”\n(AIIDE),2005. arXivpreprintarXiv:1711.05101,2017.",
      "size": 989,
      "sentences": 2
    },
    {
      "id": 50,
      "content": "heAAAICon- 2020.\nferenceonArtificialIntelligenceandInteractiveDigitalEntertainment [42] I.LoshchilovandF.Hutter,“Decoupledweightdecayregularization,”\n(AIIDE),2005. arXivpreprintarXiv:1711.05101,2017. [43] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,\n[19] K.-H.C.WangandA.Botea,“Fastandmemory-efficientmulti-agent\npathfinding,”inProceedingsoftheInternationalConferenceonAuto- M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,\nmatedPlanningandScheduling(ICAPS),2008.",
      "size": 502,
      "sentences": 3
    },
    {
      "id": 51,
      "content": "dmemory-efficientmulti-agent\npathfinding,”inProceedingsoftheInternationalConferenceonAuto- M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,\nmatedPlanningandScheduling(ICAPS),2008. “Value-decomposition networks for cooperative multi-agent learning\nbasedonteamreward,”inProceedingsoftheInternationalConference\n[20] R. J. Luna and K. E. Bekris, “Push and swap: Fast cooperative\nonAutonomousAgentsandMultiagentSystems(AAMAS),2018.\npath-finding with completeness guarantees,” in Proceedings of the\n[44] J.Wang,Z.Ren,T.Liu,Y.Yu,andC.Zhang,“Qplex:Duplexdueling\nInternationalJointConferenceonArtificialIntelligence(IJCAI),2011.\nmulti-agent q-learning,” in International Conference on Learning\n[21] T. Standley, “Finding optimal solutions to cooperative pathfinding\nRepresentations(ICLR),2020.\nproblems,” in Proceedings of the AAAI Conference on Artificial\n[45] M. Tan, “Multi-agent reinforcement learning: Independent vs. coop-\nIntelligence(AAAI),2010.\nerative agents,” in International Conference on Machine Learning\n[22] T. Standley and R. Korf, “Complete algorithms for cooperative\n(ICML),1993.\npathfindingproblems,”inProceedingsoftheInternationalJointCon-\n[46] T.Rashid,M.Samvelyan,C.S.DeWitt,G.Farquhar,J.Foerster,and\nferenceonArtificialIntelligence(IJCAI),2011.",
      "size": 1279,
      "sentences": 3
    },
    {
      "id": 52,
      "content": "for cooperative\n(ICML),1993.\npathfindingproblems,”inProceedingsoftheInternationalJointCon-\n[46] T.Rashid,M.Samvelyan,C.S.DeWitt,G.Farquhar,J.Foerster,and\nferenceonArtificialIntelligence(IJCAI),2011. S.Whiteson,“Monotonicvaluefunctionfactorisationfordeepmulti-\n[23] G.WagnerandH.Choset,“Subdimensionalexpansionformultirobot\nagentreinforcementlearning,”JournalofMachineLearningResearch,\npathplanning,”Artificialintelligence,2015. 2020. [24] M. Barer, G. Sharon, R. Stern, and A. Felner, “Suboptimal variants\n[47] Z. Ma, Y. Luo, and J. Pan, “Learning selective communication for\noftheconflict-basedsearchalgorithmforthemulti-agentpathfinding\nmulti-agent path finding,” IEEE Robotics and Automation Letters,\nproblem,”inProceedingsoftheInternationalSymposiumonCombi-\n2021.\nnatorialSearch(SoCS),2014.",
      "size": 794,
      "sentences": 4
    },
    {
      "id": 53,
      "content": "searchalgorithmforthemulti-agentpathfinding\nmulti-agent path finding,” IEEE Robotics and Automation Letters,\nproblem,”inProceedingsoftheInternationalSymposiumonCombi-\n2021.\nnatorialSearch(SoCS),2014. [48] V.EgorovandA.Shpilman,“Scalablemulti-agentmodel-basedrein-\n[25] J.Li,W.Ruml,andS.Koenig,“Eecbs:Abounded-suboptimalsearch\nforcement learning,” in Proceedings of the International Conference\nformulti-agentpathfinding,”inProceedingsoftheAAAIConference\nonAutonomousAgentsandMultiagentSystems(AAMAS),2022.\nonArtificialIntelligence(AAAI),2021. [49] A. Skrynnik, A. Andreychuk, K. Yakovlev, and A. I. Panov, “When\n[26] M. Erdmann and T. Lozano-Perez, “On multiple moving objects,”\nto switch: planning and learning for partially observable multi-agent\nAlgorithmica,1987. pathfinding,” IEEE Transactions on Neural Networks and Learning\n[27] H.Ma,D.Harabor,P.J.Stuckey,J.Li,andS.Koenig,“Searchingwith\nSystems,2023.",
      "size": 909,
      "sentences": 4
    },
    {
      "id": 54,
      "content": "g for partially observable multi-agent\nAlgorithmica,1987. pathfinding,” IEEE Transactions on Neural Networks and Learning\n[27] H.Ma,D.Harabor,P.J.Stuckey,J.Li,andS.Koenig,“Searchingwith\nSystems,2023. consistentprioritizationformulti-agentpathfinding,”inProceedings\n[50] A. Skrynnik, A. Andreychuk, M. Nesterova, K. Yakovlev, and\noftheAAAIConferenceonArtificialIntelligence(AAAI),2019. A. Panov, “Learn to follow: Decentralized lifelong multi-agent\n[28] S.-H.Chan,R.Stern,A.Felner,andS.Koenig,“Greedypriority-based\npathfinding via planning and learning,” in Proceedings of the AAAI\nsearchforsuboptimalmulti-agentpathfinding,”inProceedingsofthe\nConferenceonArtificialIntelligence(AAAI),2024. InternationalSymposiumonCombinatorialSearch(SoCS),2023.",
      "size": 745,
      "sentences": 5
    },
    {
      "id": 55,
      "content": "rning,” in Proceedings of the AAAI\nsearchforsuboptimalmulti-agentpathfinding,”inProceedingsofthe\nConferenceonArtificialIntelligence(AAAI),2024. InternationalSymposiumonCombinatorialSearch(SoCS),2023. [51] A. Skrynnik, A. Andreychuk, K. Yakovlev, and A. Panov, “Decen-\n[29] K.Okumura,“Engineeringlacam∗:Towardsreal-time,large-scale,and\ntralized monte carlo tree search for partially observable multi-agent\nnear-optimalmulti-agentpathfinding,”arXivpreprint,2023. pathfinding,” in Proceedings of the AAAI Conference on Artificial\n[30] V. Nguyen, P. Obermeier, T. Son, T. Schaub, and W. Yeoh, “Gener- Intelligence(AAAI),2024.\nalized target assignment and path finding using answer set program-",
      "size": 689,
      "sentences": 4
    }
  ]
}