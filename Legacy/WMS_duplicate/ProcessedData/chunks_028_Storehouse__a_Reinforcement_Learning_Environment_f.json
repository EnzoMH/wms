{
  "source": "ArXiv",
  "filename": "028_Storehouse__a_Reinforcement_Learning_Environment_f.pdf",
  "total_chars": 41931,
  "total_chunks": 64,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nStorehouse: a Reinforcement Learning Environment\nfor Optimizing Warehouse Management\nJulen Cestero Marco Quartulli\nData Intelligence for Energy and Industrial Processes Data Intelligence for Energy and Industrial Processes\nVicomtech Vicomtech\nSan Sebastian, Spain San Sebastian, Spain\n0000-0002-6670-6255 0000-0001-5735-2072\nAlberto Maria Metelli Marcello Restelli\nDipartimento di Elettronica, Informazione e Bioingegneria Dipartimento di Elettronica, Informazione e Bioingegneria\nPolitecnico di Milano Politecnico di Milano\nMilano, Italy Milano, Italy\n0000-0002-3424-5212 0000-0002-6322-1076\nAbstract—Warehouse Management Systems have been evolv- • we formulate warehouse storage management as a\ning and improving thanks to new Data Intelligence techniques. Markov Decision Process (MDP) [Put94].",
      "size": 811,
      "sentences": 2
    },
    {
      "id": 2,
      "content": "act—Warehouse Management Systems have been evolv- • we formulate warehouse storage management as a\ning and improving thanks to new Data Intelligence techniques. Markov Decision Process (MDP) [Put94]. The modeling\nHowever, many current optimizations have been applied to\nis general and can be customized with different ware-\nspecificcasesorareingreatneedofmanualinteraction.Hereis\nhouse topologies and generation processes (Section III);\nwhere Reinforcement Learning techniques come into play, pro-\nviding automatization and adaptability to current optimization • we introduce Storehouse, a customizable environment\npolicies.Inthispaper,wepresentStorehouse,acustomizableenvi- designed to train RL agents to manage the storage of a\nronmentthatgeneralizesthedefinitionofwarehousesimulations warehouse using a FIFO methodology. The environment\nfor Reinforcement Learning.",
      "size": 867,
      "sentences": 4
    },
    {
      "id": 3,
      "content": "eenvi- designed to train RL agents to manage the storage of a\nronmentthatgeneralizesthedefinitionofwarehousesimulations warehouse using a FIFO methodology. The environment\nfor Reinforcement Learning. We also validate this environment is implemented using the gym interface [BCP+16] to\nagainst state-of-the-art reinforcement learning algorithms and\nease connection to popular RL libraries such as stable-\ncompare these results to human and random policies. baselines3 [RHG+21] or RLlib [LLM+17]. Storehouse\nalso includes a customization method that uses a con-\nI. INTRODUCTION\nfiguration file, where developers can try out different\nAdvancements in Warehouse Management Systems can versions of the environment, including different grid\nbe classified into different trends.",
      "size": 771,
      "sentences": 6
    },
    {
      "id": 4,
      "content": "ION\nfiguration file, where developers can try out different\nAdvancements in Warehouse Management Systems can versions of the environment, including different grid\nbe classified into different trends. On the one hand, some sizes, material types, and timers (Section IV);\nadvancements aim to enhance specific processes of the ware- • we provide an experimental evaluation of several RL al-\nhouses,improvingtheeffectivenessandefficiencyofthewhole gorithms, including DQN [MBM+16], PPO [SWD+17],\nactivity.Ontheotherhand,otheradvancesfocusonimproving A2C [MBM+16], against handcrafted human policies,\nmore general processes, using Machine Learning methods showing competitive performance (Section V).1\nsuch as Reinforcement Learning. ReinforcementLearning(RL)[SB18]isamachinelearning II.",
      "size": 782,
      "sentences": 3
    },
    {
      "id": 5,
      "content": "an policies,\nmore general processes, using Machine Learning methods showing competitive performance (Section V).1\nsuch as Reinforcement Learning. ReinforcementLearning(RL)[SB18]isamachinelearning II. CURRENTMETHODOLOGIES\napproach for training agents to solve sequential decision-\nWarehouse management systems follow different method-\nmaking problems by interacting in a trial-and-error fashion\nologies to optimize and balance their workload. Apart from\n[Ach18]. RL is an iterative approach in which an agent\na few very recent facilities, such as Amazon’s Kiva system\nreceivesrewardsbasedontheactionstaken,withtheobjective\n[YJL20], most modern warehouses follow classic approaches. of maximizing the cumulative reward during its experience in\nHowever,withtheriseofIndustry4.0,manywarehouseshave\nthe environment.",
      "size": 810,
      "sentences": 6
    },
    {
      "id": 6,
      "content": "eobjective\n[YJL20], most modern warehouses follow classic approaches. of maximizing the cumulative reward during its experience in\nHowever,withtheriseofIndustry4.0,manywarehouseshave\nthe environment. One of the advantages of RL is that, given\nstarted looking for more data-driven solutions to optimize\na well-defined environment and reward, it can progressively\ntheir long-established processes, which usually depend on the\nadapttonewarrangementswithintheenvlironmenttocontinue\nexperience of senior managers and ad hoc methods. maximizing the cumulative reward. Furthermore, RL is not\nThese methods are largely explored in the literature and\nconstrainedtoaspecifictopologyordynamicsoftheenviron-\ncan be grouped into two trends: a classic trend that aims to\nment, being able to generalize across different configurations.",
      "size": 820,
      "sentences": 5
    },
    {
      "id": 7,
      "content": "n the literature and\nconstrainedtoaspecifictopologyordynamicsoftheenviron-\ncan be grouped into two trends: a classic trend that aims to\nment, being able to generalize across different configurations. enhancecurrentsystemsviaoptimization(SectionII-A),anda\nTherefore, RL turns out to be a potential solution to the\nmoremoderntrend,whichaimstoproposemethods,breaking\nproblem of managing warehouse storage systems. with existing ones (Section II-B). In this paper, we investigate RL approaches to tackle\nwarehousestoragemanagement.Themaincontributionsofthe\n1The code of the environment is available in Github (https://github.com/\npaper can be summarized as follows: JulenCestero/storehouse). 2202\nluJ\n12\n]GL.sc[\n2v15830.7022:viXra\n=== 페이지 2 ===\nA.",
      "size": 743,
      "sentences": 5
    },
    {
      "id": 8,
      "content": "onsofthe\n1The code of the environment is available in Github (https://github.com/\npaper can be summarized as follows: JulenCestero/storehouse). 2202\nluJ\n12\n]GL.sc[\n2v15830.7022:viXra\n=== 페이지 2 ===\nA. Classic Trends address the curse of dimensionality [Bel61] by decreasing the\nnumber of action-state combinations, and apply the algorithm\nThe classic trend leverages advancements in the area of\nsensors and automation, to enhance existing warehouse ad managing to improve performance by 20% compared to a\nhoc methods using real data instead of human experience similar heuristic previously in use. However, this approach is\n[ZHH+18]. These improvements rely on location sensors, limited to the specific layout of the studied warehouse. Dou\net al. [DCY15] proposed a mixed method involving Genetic\nproduct identification tags, motion controllers. Using these\nAlgorithms (GAs) with RL.",
      "size": 882,
      "sentences": 8
    },
    {
      "id": 9,
      "content": "ed to the specific layout of the studied warehouse. Dou\net al. [DCY15] proposed a mixed method involving Genetic\nproduct identification tags, motion controllers. Using these\nAlgorithms (GAs) with RL. GAs are used for the planning\nmethodologies, several authors propose different optimization\npart of the process, involving the macro-action management,\nmethods.Liangetal. [LWZZ20]proposeawave-pickingware-\nwhile the RL algorithms are used for the micro-actions of the\nhouse management that shows a 2% improvement compared\nagents that involve instantaneous movement and path-finding\nwith a baseline item-batching method. Mao et al. [MXZ18]\nthroughoutthegrid.Thus,theGAdefinesthegoaloftheagent,\nprovide a general study of the IT systems in a warehouse\nand the RL algorithm manages the movements to achieve that\nand propose a scheduling function aimed at the interconnec-\ngoal.",
      "size": 873,
      "sentences": 8
    },
    {
      "id": 10,
      "content": "hegoaloftheagent,\nprovide a general study of the IT systems in a warehouse\nand the RL algorithm manages the movements to achieve that\nand propose a scheduling function aimed at the interconnec-\ngoal. The results of this article provide insight that has been\ntion of several agents in an industrial management process,\nverified by other authors [LSK+19], [YJL20], [LJ21]. These\nwhich involves several warehouses, a fleet of vehicles, and\nsome manufacturers and consumers. Zunic et al. [ZHH+18], workscompareRLalgorithmssuchasQ-Learning[WD92]or\n[ZDH+18]proposetwoupgradestothelogisticsofaparticular\nDQN[MBM+16]withclassicpath-findingalgorithms,suchas\nA*[HNR68],andconcludethatRLalgorithmsachieveatleast\nwarehouse.",
      "size": 711,
      "sentences": 5
    },
    {
      "id": 11,
      "content": "rithmssuchasQ-Learning[WD92]or\n[ZDH+18]proposetwoupgradestothelogisticsofaparticular\nDQN[MBM+16]withclassicpath-findingalgorithms,suchas\nA*[HNR68],andconcludethatRLalgorithmsachieveatleast\nwarehouse. The first one consists in managing the distribution\nthe same efficiency as A*, and enhance it in some cases, but\npipeline of supplies and orders of the warehouse, while the\nall these works are limited to address the path-finding part of\nsecond one uses an inner optimization of the ordering and\nthe warehouse management problem, without addressing the\ndistribution of the material on the warehouse shelves. Both\nmacro-planning problem using complex data analysis. have some promising results that improve the previous im-\nplementation close to 12%. Finally, Zunic et al. [ZHH+17] Themainlimitationoftheseapproachesisthatnoneofthem\nanalyze a material reallocation of the warehouse using an proposed a generalized environment.",
      "size": 924,
      "sentences": 6
    },
    {
      "id": 12,
      "content": "mentation close to 12%. Finally, Zunic et al. [ZHH+17] Themainlimitationoftheseapproachesisthatnoneofthem\nanalyze a material reallocation of the warehouse using an proposed a generalized environment. A generalized environ-\nalgorithm that improves previous efficiency by 25 times. ment can be used to train RL agents and, therefore, propose\nAll these methodologies share the same limitations: they a global method that sums up both the micro and the macro\nare aimed at solving specific problems of certain warehouses, management of a real warehouse. theydonotprovideageneralmethodologythatcanbeapplied\nIII. THESTOREHOUSEPROBLEM\nto any kind of management.",
      "size": 653,
      "sentences": 7
    },
    {
      "id": 13,
      "content": "are aimed at solving specific problems of certain warehouses, management of a real warehouse. theydonotprovideageneralmethodologythatcanbeapplied\nIII. THESTOREHOUSEPROBLEM\nto any kind of management. These improvements can be\nAlthough advances in Industry 4.0 have provided new tools\nuseful for micro management optimizations, but we aim to\nfor warehouses optimization, there is still a strong need to\nprovide more general management methodologies that can be\nautomate these improvements and making them adaptable\nadaptable to many kinds of warehouses. to new and unexpected scenarios. Nevertheless, much of\nB. Modern Trends the management is highly dependent on human interaction\nMore recently, approaches are based on machine learning and, in many cases, leading to ad hoc methods originating\nand complex data analysis have been proposed to design from cumulative human experience.",
      "size": 882,
      "sentences": 7
    },
    {
      "id": 14,
      "content": "e recently, approaches are based on machine learning and, in many cases, leading to ad hoc methods originating\nand complex data analysis have been proposed to design from cumulative human experience. In this sense, artificial\nalternatives to classic techniques, taking advantage of the intelligence approaches, and in particular RL, can greatly\naforementioned new developments on warehouse digitaliza- improve warehouse management by devising approaches that\ntion. Among all of them, a relevant role is played by RL. areresponsiveandadaptabletochangesinsidethewarehouse.",
      "size": 570,
      "sentences": 4
    },
    {
      "id": 15,
      "content": "ments on warehouse digitaliza- improve warehouse management by devising approaches that\ntion. Among all of them, a relevant role is played by RL. areresponsiveandadaptabletochangesinsidethewarehouse. These methods can be classified according to which portion In this section, we first provide an informal description of the\nof the process they optimize: the macro actions, planning Storehouse problem by discussing the metrics employed to\nof higher order actions, like, for example, the ordering of evaluatetheperformanceofthewarehousemanagement(Sec-\nnew upcoming materials to the warehouse; and the micro tionIII-A)andthegeneralwarehousefeatures(SectionIII-B). actions, which are based on the specific movements of the Then, according to the previous description, we provide the\nagents inside the warehouse. This latter kind of optimization environment specification as MDP (Section III-C).",
      "size": 891,
      "sentences": 6
    },
    {
      "id": 16,
      "content": "e specific movements of the Then, according to the previous description, we provide the\nagents inside the warehouse. This latter kind of optimization environment specification as MDP (Section III-C). focusesonthepath-findingprocessneededtofindtheoptimal The goal of the Storehouse problem is to micro-manage a\npath to fulfill the macro decisions. Bottani et al. [BMR+15] numberofautonomousagents(e.g.,forklifts),providingthem\ncarried out a very detailed review analyzing the most time- with actions or movements to accomplish. Therefore, we seek\nconsuming activity in a warehouse and providing an exhaus- to design a centralized control unit to manage the logistics of\ntive research encompassing several state-of-the-art types of the warehouse, optimizing the metrics discussed below. algorithms, such as genetic algorithms, ant colony optimiza-\nA. Metrics\ntion, particle swarm optimization, simulated annealing, and\nmethods that use artificial neural networks. Estanjini et al.",
      "size": 978,
      "sentences": 8
    },
    {
      "id": 17,
      "content": "elow. algorithms, such as genetic algorithms, ant colony optimiza-\nA. Metrics\ntion, particle swarm optimization, simulated annealing, and\nmethods that use artificial neural networks. Estanjini et al. Intheliterature,severalmetricsareavailabletoevaluatethe\n[ELP12]proposeanRL-basedmethodusingA2C[MBM+16] performanceofawarehousemanagementsystem.However,in\nto provide high-level actions in the decision-making process, this paper, we mainly focus on three of them:\nsuchasmovingitemsto/fromreserve/pick-up/deportlocations. (i) the number of material or orders delivered in a defined\nThey divide their action space into these macro-activities, time period;\n=== 페이지 3 ===\n(ii) the average age of the material in the warehouse over a\ntime period;\n(iii) the number of times the FIFO criterion has being\nviolated (the FIFO criterion is widely used in current\nwarehouse management systems). The choice of these metrics is justified by the use case of\ninterest.",
      "size": 950,
      "sentences": 6
    },
    {
      "id": 18,
      "content": "umber of times the FIFO criterion has being\nviolated (the FIFO criterion is widely used in current\nwarehouse management systems). The choice of these metrics is justified by the use case of\ninterest. In particular, we focus on the number of materials\nor orders delivered due to the nature of the real warehouse,\nwhich provides industrially manufactured products. If the\nproducts stored in the real warehouse were perishable goods,\nforinstance,othermetricswouldcertainlybemoreimportant,\nas the age of the material. B. Warehouse Description Figure 1: Layout of the warehouse. The blue tiles correspond\nto the area that is always free of material. The entry points\nIn this section, we provide a description of the abstraction\nare green, while the delivery points are red. of the warehouse functioning that we consider in this paper. 1) Warehouse Layout: The warehouse layout is a rectan-\ngular grid with a hollow crown that surrounds the storage\nC. MDP Specification\nroom to ease handling of forklifts.",
      "size": 999,
      "sentences": 10
    },
    {
      "id": 19,
      "content": "at we consider in this paper. 1) Warehouse Layout: The warehouse layout is a rectan-\ngular grid with a hollow crown that surrounds the storage\nC. MDP Specification\nroom to ease handling of forklifts. An example of a squared\nBased on the informal description presented above, in\nlayoutisshowninFigure1.Thewarehousecontainstwoentry\nthis section, we formally define all the elements needed for\npoints (green) and two delivery points (red), where items are\nmodeling the problem as an MDP [Put94]. introduced and delivered to clients. These points are located\n1) States: The state of the environment is composed of\nin an outer crown where no items can be placed with the\nseveral grids, each storing a feature of all the cells in the\nexception of the delivery points, which immediately consume\nwarehouse, easing the training by making it compatible with\nthe items placed therein. Therefore, the items can be placed\nimage processing methods.",
      "size": 934,
      "sentences": 6
    },
    {
      "id": 20,
      "content": "on of the delivery points, which immediately consume\nwarehouse, easing the training by making it compatible with\nthe items placed therein. Therefore, the items can be placed\nimage processing methods. Formally, the state is defined as\nanywhere within the internal storage grid. We assume the size\na 3-dimensional tensor s ∈ Rr×c×d, where r,c ∈ N are\nof the material is equal to one grid tile. Items placed in the\nthe number of rows and columns of the warehouse grid\nstoragecanbepickediftheagentisabletogettothatcellfrom\nrespectively and d ∈ N is the number of feature matrices of\nadjacent horizontal or vertical cells. If a cell is unreachable,\nthe state. Thus, s can be regarded as a list of r×c feature\nthis cell is considered a restricted cell, where no items can be\nmatrices D . We consider d=6+m feature matrices, where\nplaced or picked. i\nm is the number of material types considered:2\n2) ItemGenerationandConsumption: Theitemgeneration\nprocess starts at the delivery points.",
      "size": 980,
      "sentences": 9
    },
    {
      "id": 21,
      "content": "We consider d=6+m feature matrices, where\nplaced or picked. i\nm is the number of material types considered:2\n2) ItemGenerationandConsumption: Theitemgeneration\nprocess starts at the delivery points. By emulating on-demand • BoxgridD 1 :representsthelocationofthedifferentboxes\nwithin the storage. Each box is represented by an integer\nmaterial generation systems, they generate the orders, using\ncorresponding to the type of material it is, mapping each\nrandomtimers,whichthesystemhastofulfill.Theentrypoints\ntype (‘A’, ‘B’, ...) to integers (1, 2, ...). If there is no box\nstart generating the required items, placing them into waiting\nin that grid position, the value is 0.\nqueues with timers that simulate when the different materials\nof the order are ready to be either introduced into the grid • Age grid D 2 : represents the age of the box, which is\nbounded between 0 and 1000 (steps). or delivered directly to the delivery points.",
      "size": 937,
      "sentences": 6
    },
    {
      "id": 22,
      "content": "of the order are ready to be either introduced into the grid • Age grid D 2 : represents the age of the box, which is\nbounded between 0 and 1000 (steps). or delivered directly to the delivery points. In our setting,\ntimers are Poisson processes with a configurable parameter • Restricted grid D 3 : collects the different restricted cells\nin the environment. λ [Wol82]. In summary, the delivery points generate a new\norder with a random distribution, and a random timer with • Agent grid D 4 : used to locate the agent within the grid\nandtoknowiftheagenthaspickedanobjectornot.The\nthe waiting time until ready to collect the order; this order is\ncell in the matrix in which the agent is placed will have\npassed to the entry points and they start creating the material\na value of 255 if the agent has not picked an object, and\ninaqueue,withrandomtimerssimulatingit.Whenthetimers\n128 otherwise.",
      "size": 892,
      "sentences": 5
    },
    {
      "id": 23,
      "content": "will have\npassed to the entry points and they start creating the material\na value of 255 if the agent has not picked an object, and\ninaqueue,withrandomtimerssimulatingit.Whenthetimers\n128 otherwise. The other cells will have a value of 0.\nof the items of the input queues finish, the items are ready to\nbe picked up and moved to the storage or delivered directly • Agent material grid D 5 : the cell where the agent is\nlocated is marked with the type of the material that it\nto the delivery points, and they are collected from the input\nis carrying is represented. If the agent has no items, the\npoints in the order they were generated. value of all cells is 0. 3) Material Types: Different types of material are consid-\neredtomimicactualproductionanddeliverytoclients,named • Entry point grid D 6 : indicates the status of the entry\npoints.",
      "size": 841,
      "sentences": 5
    },
    {
      "id": 24,
      "content": "of all cells is 0. 3) Material Types: Different types of material are consid-\neredtomimicactualproductionanddeliverytoclients,named • Entry point grid D 6 : indicates the status of the entry\npoints. When an item is ready to be picked up, the\nin alphabetical order (e.g., the first material type is ‘A’, the\nposition of the matrix that corresponds to the position of\nsecond‘B’,andsoon).Thegenerationisgovernedbydifferent\nthe corresponding entry point changes its value to 255.\nstochasticprocesses(e.g.,Poissonprocesseswithdifferentλs),\nand the order type is also randomly defined by the delivery\n2Forcompliancewithconvolutionalneuralnetworks,allmatricesaretreated\npoints when a new order is created. asimagesandtheirvaluesarenormalizedbetween0and255.",
      "size": 749,
      "sentences": 4
    },
    {
      "id": 25,
      "content": "ype is also randomly defined by the delivery\n2Forcompliancewithconvolutionalneuralnetworks,allmatricesaretreated\npoints when a new order is created. asimagesandtheirvaluesarenormalizedbetween0and255. === 페이지 4 ===\n• Out point grids: D 7 ,...,D 6+m : the number of matrices state-action pair (s,a):\nvaries with the number of types that the environment \n−1 a∈I (s)\nc\nty\na\np\nn\ne.\ng\nD\nen\ne\ne\np\nr\ne\na\nn\nte\nd\n,\nin\ne\ng\nac\no\nh\nn\nm\nth\na\ne\ntr\nty\nix\npe\nco\no\nr\nf\nre\nm\ns\na\np\nt\no\ne\nn\nri\nd\na\ni\nl\nn\nt\ng\nha\nt\nt\no\nth\na\ne\nd\nd\ni\ne\nff\nli\ne\nv\nr\ne\ne\nr\nn\ny\nt −0.9\na∈N\nnv\n(s)\nR(s,a)= ,\np\nco\no\nr\ni\nr\nn\ne\nt\ns\ns\np\na\no\nr\nn\ne\nd\ne\nin\nx\ng\npe\nt\nc\no\ntin\nth\ng\ne\n, t\nd\nh\ne\ne\nli\nv\nv\na\ne\nl\nr\nu\ny\ne\np\no\no\nf\nin\nth\nts\ne c\nc\ne\nh\nl\na\nl\nn\na\ng\nt\ne\nt\ns\nhe\nto\npo\n2\ns\n5\ni\n5\ntio\ni\nn\nn\n0 min{max 1 { 0 ag 0 e 0 ,0},1000} ·255 a\noth\n∈\ner\nD\nw\n(\ni\ns\ns\n)\ne\nthe corresponding matrix.",
      "size": 837,
      "sentences": 3
    },
    {
      "id": 26,
      "content": "h\ne\ne\nli\nv\nv\na\ne\nl\nr\nu\ny\ne\np\no\no\nf\nin\nth\nts\ne c\nc\ne\nh\nl\na\nl\nn\na\ng\nt\ne\nt\ns\nhe\nto\npo\n2\ns\n5\ni\n5\ntio\ni\nn\nn\n0 min{max 1 { 0 ag 0 e 0 ,0},1000} ·255 a\noth\n∈\ner\nD\nw\n(\ni\ns\ns\n)\ne\nthe corresponding matrix. where:\n2) Actions: The available actions take the form of coordi- • I nv (s) is the set of invalid actions in state s;\nnates of the grid a = (i,j) ∈ {1,...,r}×{1,...,c}. Any • D(s) is the set of valid actions that deliver items to the\naction corresponds to the desired location of the agent. The delivery points in state s;\neffect of the action depends on the status of the target cell: • N(s), is the set of idle actions in state s (i.e., the agent\nmovingfromanemptycelltoanotherwithnoitems,with\nwaiting items at the entry points). • if the agent moves to the same location where a box is\nlocated, it will automatically pick it up; There is no special reward for items collected from entry\n• if the agent has an object and it moves to an empty valid points.",
      "size": 958,
      "sentences": 5
    },
    {
      "id": 27,
      "content": "e same location where a box is\nlocated, it will automatically pick it up; There is no special reward for items collected from entry\n• if the agent has an object and it moves to an empty valid points. If there are items to be delivered, the agent receives\ncell, it will automatically drop the item there. −0.9 reward for avoiding idleness. However, if there are no\nmore effective actions than getting new items, the reward is\nHowever, some actions are considered invalid. For instance, 0.\nthe agent cannot stack items or deposit an item in the outer In this setting, the goal of an RL agent is to find the policy\nring of the environment, unless it is depositing it at an open π, i.e., a mapping from states to actions, that maximizes the\ndelivery point.",
      "size": 752,
      "sentences": 5
    },
    {
      "id": 28,
      "content": "his setting, the goal of an RL agent is to find the policy\nring of the environment, unless it is depositing it at an open π, i.e., a mapping from states to actions, that maximizes the\ndelivery point. If an invalid action is performed, the agent expected discounted sum of the rewards [SB18]:\nwill receive a negative reward and it will stay in the same (cid:34)T−1 (cid:35)\n(cid:88)\ncell as before, even though the environment will advance one J(π)=E γtR(s ,a ) ,\nπ t t\nstepfurtherwithoutregisteringanymovementfromtheagent. t=0\nGiven a state s, these actions belong to the set I (s). nv where γ ∈(0,1) is the discount factor. 3) Reward: We assume that the cost of a movement is IV. ALGORITHMICSOLUTION\nconstant (one step), regardless of the distance involved in the\nTo solve the Storehouse optimization problem, we use RL-\nmovement. These actions result in a reward that values the\nbased strategies and algorithms.",
      "size": 913,
      "sentences": 7
    },
    {
      "id": 29,
      "content": "step), regardless of the distance involved in the\nTo solve the Storehouse optimization problem, we use RL-\nmovement. These actions result in a reward that values the\nbased strategies and algorithms. Our goal is to show that RL\nquality of the outcome of the performed action:\nsolutions are competitive with traditional human-knowledge-\nbased strategies. • if the agent performs an invalid action, the reward will\nalways be −1, with the objective of preventing the agent A. Reinforcement Learning Policies\nfrom performing an invalid action; We tested three state-of-the-art RL algorithms in the Store-\n• when the agent has nothing useful to do, e.g., when house environment: Deep Q-Network (DQN) [MBM+16],\nthe grid is empty and there are no incoming orders, it Asynchronous Actor Critic (A2C) [MBM+16], and Proximal\nwill receive a reward equal to 0, preventing actions from Policy Optimization (PPO) [SWD+17].",
      "size": 907,
      "sentences": 4
    },
    {
      "id": 30,
      "content": "grid is empty and there are no incoming orders, it Asynchronous Actor Critic (A2C) [MBM+16], and Proximal\nwill receive a reward equal to 0, preventing actions from Policy Optimization (PPO) [SWD+17]. In addition, we devel-\nhaving a significant effect in the training; oped a customized version of DQN, in which we filter out\n• if the agent has other significant actions to take (e.g., the invalid actions thus speeding up the training process due\ndelivering ready items) and does not perform them, it to the decreased number of actions available in each step. We\nwill receive a reward of −0.9 to discourage idle actions; called this variant Valid-Action Mask (VAM). • if the agent manages to deliver an item to the delivery\npoints,itwillreceivealessnegativereward,toencourage B. Hyperparameter Definition\nit to this behavior.",
      "size": 825,
      "sentences": 4
    },
    {
      "id": 31,
      "content": "d this variant Valid-Action Mask (VAM). • if the agent manages to deliver an item to the delivery\npoints,itwillreceivealessnegativereward,toencourage B. Hyperparameter Definition\nit to this behavior. This reward is bounded between We also studied the effect of hyperparameter optimiza-\n[−0.5,0]anditwillbecloserto0theolderthedelivered tion, compared to using the default values defined in stable-\nbox is, with the maximum reward being 0 if the box is baselines3, the library we used for the RL training. In this\nthe oldest available and the agent is therefore behaving study, we sought strategies based on Design of Experiments\naccording to a FIFO criterion. The reward will vary (DoE) [Box80] to optimize the training of the RL policies.",
      "size": 738,
      "sentences": 5
    },
    {
      "id": 32,
      "content": "e agent is therefore behaving study, we sought strategies based on Design of Experiments\naccording to a FIFO criterion. The reward will vary (DoE) [Box80] to optimize the training of the RL policies. linearly between the upper and lower bounds, depending Forthispurpose,wequantizedthecontinuoushyperparameters\non the age difference between the delivered box and the and applied a pairwise combination, used in all-pairs testing\nyoungest box in the environment and, additionally, this [Cze06], to reduce the space of possible combinations. This\nage difference is also bounded between 0 and 100 steps, optimizationallowsincreasingthespeedofconvergencetothe\nreturningthehighestandthelowestrewardsrespectively. optimal combination of hyperparameters.",
      "size": 746,
      "sentences": 5
    },
    {
      "id": 33,
      "content": "difference is also bounded between 0 and 100 steps, optimizationallowsincreasingthespeedofconvergencetothe\nreturningthehighestandthelowestrewardsrespectively. optimal combination of hyperparameters. We considered four main hyperparameters, from the ones\nWe can formalize the definition of the rewards for every that stable-baselines3 allows us to tune:\n=== 페이지 5 ===\n• learning rate (α): choosing the learning rate is challeng- the items (thus, the delivery points are open for some items)\ning as too small a value may result in a long training andtheseitemsareinthestorage,itwillprioritizethedelivery\nprocess that could get stuck, whereas too large a value of items following the FIFO order and, therefore, delivering\nmay result in learning a sub-optimal set of weights too the oldest available items.",
      "size": 802,
      "sentences": 3
    },
    {
      "id": 34,
      "content": "rocess that could get stuck, whereas too large a value of items following the FIFO order and, therefore, delivering\nmay result in learning a sub-optimal set of weights too the oldest available items. This means that the priority is to\nfast or an unstable training process; deliver the items already stored, try to empty the warehouse,\n• discount factor (γ ∈ (0,1)): it quantifies the importance and, then, deliver or store the new incoming material from\nwegivetofuturerewards.Ifγiscloserto0,theagentwill the entry point queues.",
      "size": 527,
      "sentences": 2
    },
    {
      "id": 35,
      "content": "house,\n• discount factor (γ ∈ (0,1)): it quantifies the importance and, then, deliver or store the new incoming material from\nwegivetofuturerewards.Ifγiscloserto0,theagentwill the entry point queues. Besides that, the delivery points do\ntendtoconsideronlyimmediaterewards.Ifγ iscloserto not expect any item, this agent will aim to store the incoming\n1, the agent will give similar weights to immediate and itemsintothewarehousestorage.Furthermore,restrictedcells\nfuture rewards, willing to delay the reward; canbecreatediftheincomingflowofnewitemsisgreaterthan\n• Polyak coefficient (τ ∈ [0,1]): it is the soft update theoutputflowofdelivereditems.Ifso,thesetwopoliciesget\ncoefficient, being 1 a hard update. It averages previous the oldest available material in the grid.",
      "size": 771,
      "sentences": 3
    },
    {
      "id": 36,
      "content": "efficient (τ ∈ [0,1]): it is the soft update theoutputflowofdelivereditems.Ifso,thesetwopoliciesget\ncoefficient, being 1 a hard update. It averages previous the oldest available material in the grid. valuesoftheneuralnetworkwiththemostrecentvalues; Wealsoconsiderarandompolicy,whichchoosesuniformly\n• exploration fraction: it is the fractional value for the among all actions defined in the action space of the environ-\nexploration vs. exploitation rate, in this case, an ε- ment at each step. We expect this policy to be the worst, so,\ngreedy strategy. It represents the percentage of steps of wewanttouseitasabaselinefromwhichtheRLpoliciesare\nthe defined maximum number of training steps that the expected to improve. The human policies are also considered\nalgorithm needs to reach the minimum value of (cid:15). For baselinesandweaimforRLpoliciestooutperformthehuman\ninstance, a value of 0.1 means that the value of (cid:15) will policies if possible.",
      "size": 954,
      "sentences": 7
    },
    {
      "id": 37,
      "content": "red\nalgorithm needs to reach the minimum value of (cid:15). For baselinesandweaimforRLpoliciestooutperformthehuman\ninstance, a value of 0.1 means that the value of (cid:15) will policies if possible. vary linearly from its maximum value to its minimum in\ntheinitial10%ofthedefinedmaximumnumberofsteps. B. Experimental Setting\n• Value Function coefficient (vf coef ∈ [0,1]): this coeffi-\nAll results use the 6×6 configuration of the environment\ncient controls the effect of the value function loss in the\n(r = c = 6), where entry points in the upper corners and\ncalculation of the training loss;\ndeliverypointsinthelowercorners.Weconsidertwotypesof\n•\nclip((cid:127)∈[0,1]):thestandardPPOalgorithmhasaclipped\nmaterial, ‘A’ and ‘B’, with a Poisson generation process with\nobjective function and this hyperparameter is used to\nparametersλ=5andλ=10respectivelyforeachitemandan\ndefine the range in which the probability ratio term is\norder delivery parameter of λ = 30 and λ = 50 respectively.",
      "size": 987,
      "sentences": 5
    },
    {
      "id": 38,
      "content": "and this hyperparameter is used to\nparametersλ=5andλ=10respectivelyforeachitemandan\ndefine the range in which the probability ratio term is\norder delivery parameter of λ = 30 and λ = 50 respectively. clipped, meaning that the objective function takes the\nEach order has a random number of items between 2 and 6,\nminimumbetweentheoriginalratioandtheclippedratio,\nand a new order is randomly generated with λ=25. being the clip range [1−(cid:127),1+(cid:127)]. The results were obtained by simulating agents with differ-\nV. RESULTS ent policies in the Storehouse environment, using a maximum\nnumber of steps of 1000 per episode, in 3000 episodes using\nIn this section, we present the results of the methods\na GPU Nvidia Tesla V100 SXM2 32GB, 80 CPUs Intel Xeon\nproposed in the previous sections in terms of the RL training\nGold 6230 @2.16GHz and 755GB of RAM, which results\nscore,whichrelatestothediscountedcumulativerewards,and\ninto a duration of nearly 3.5 hours per trained policy.",
      "size": 982,
      "sentences": 4
    },
    {
      "id": 39,
      "content": "sections in terms of the RL training\nGold 6230 @2.16GHz and 755GB of RAM, which results\nscore,whichrelatestothediscountedcumulativerewards,and\ninto a duration of nearly 3.5 hours per trained policy. Each\nof the different metrics for each defined policy. Furthermore,\npolicy is trained 10 times and, in all figures, the results show\nthe effect of hyperparameter tuning is studied for all the RL\nthe average of the different runs with a darker line, and the\npolicies, but a deeper analysis is shown for the DQN policy. maximum and minimum over the runs with a shaded area. A. Baseline Policies\nC. Results without Hyperparameter Optimization\nWe defined two human policies, i.e., policies defined pro-\ngrammatically that implement expert strategies applicable in Figure 2 shows the evolution of the average return in the\nwarehouses. The first one, named Initial Human Policy (IHP), training process per episode for the different defined policies.",
      "size": 942,
      "sentences": 6
    },
    {
      "id": 40,
      "content": "s applicable in Figure 2 shows the evolution of the average return in the\nwarehouses. The first one, named Initial Human Policy (IHP), training process per episode for the different defined policies. prioritizes retrieving the material from the entry point queues, The RL policies change throughout the episodes, even though\nwhereas the second one, named Enhanced Human Policy some of them stay quite static, but we consider both the\n(EHP), prefers to deliver the available material to the delivery human policies and the random policy as constants because\npoints. Both policies work in such a way that, when a new they do not vary with the episodes. All the policies in this\nobject is generated at the entry points, this is immediately plot have been trained using the default parameters of their\ntaken when the warehouse is empty.",
      "size": 832,
      "sentences": 5
    },
    {
      "id": 41,
      "content": "ith the episodes. All the policies in this\nobject is generated at the entry points, this is immediately plot have been trained using the default parameters of their\ntaken when the warehouse is empty. IHP will always store correspondingalgorithmsinstable-baslines3.Itiscuriousthat\ntheseitemsinthewarehouse,butEHPwillstoretheitemsonly the policies based on DQN evolve successfully, enhancing\niftheagentisunabletodeliverthemdirectly.Whenthestorage their result until achieving near-human performance, but the\ncontains items, IHP will act likewise: prioritizing stocking A2C and PPO policies are not able to learn how to act in\nitems from the entry points and delivering available items the environment. However, it is not surprising that the VAM\nusing a FIFO order, when no items are waiting in the entry policyoutperformstheDQNpolicy,evenenhancingtheinitial\npoint queues.",
      "size": 869,
      "sentences": 4
    },
    {
      "id": 42,
      "content": "ilable items the environment. However, it is not surprising that the VAM\nusing a FIFO order, when no items are waiting in the entry policyoutperformstheDQNpolicy,evenenhancingtheinitial\npoint queues. On the other hand, EHP does not prioritize the human policy, since it is an improved version of the default\nabsence of objects at the entry points. If the agent can deliver DQN algorithms where the invalid actions are suppressed. === 페이지 6 ===\n300\n400\n500\n600\n700\n800\n900\n0 500 1000 1500 2000 2500 3000\nEpisode\nerocS\n300\n400\n500\n600\n700\n800\n900\n0 500 1000 1500 2000 2500 3000\nEpisode\nPPO DQN IHP Random\nA2C VAM EHP\nFigure 2: Score as a function of the number of collected\nepisodes for the tested RL algorithms and baseline policies\nwithout hyperparameter optimization over 10 runs (PPO line\nis behind the A2C line). The shaded area corresponds to the\narea between the maximum and the minimum value between\nthe different runs.",
      "size": 925,
      "sentences": 6
    },
    {
      "id": 43,
      "content": "icies\nwithout hyperparameter optimization over 10 runs (PPO line\nis behind the A2C line). The shaded area corresponds to the\narea between the maximum and the minimum value between\nthe different runs. Score Delivered Boxes\nEHP EHP\nVAM VAM\nIHP DQN\nDQN IHP\nA2C Random\nPPO PPO\nRandom A2C\n800 600 400 200 0 0 50 100 150\nMean box ages FIFO violation\nA2C PPO\nPPO A2C\nEHP EHP\nVAM IHP\nDQN VAM\nIHP DQN\nRandom Random\n0 50 100 150 200 0% 20% 40% 60%\nFigure 3: Environment metric comparison for the defined\npolicies. Figure 3 displays other metrics of interest for the envi-\nronment. These results are obtained by averaging the last\n100 episodes, assuming stability in the results. In terms of\nDelivered Boxes, the results are similar to the score, but\nDQN manages to deliver more boxes than IHP.",
      "size": 783,
      "sentences": 6
    },
    {
      "id": 44,
      "content": "are obtained by averaging the last\n100 episodes, assuming stability in the results. In terms of\nDelivered Boxes, the results are similar to the score, but\nDQN manages to deliver more boxes than IHP. This might\nbebecauseDQNwasabletolearnstrategiesthatcanimprove\nits productivity in terms of delivered boxes, strategies that we\nappliedtobuildtheEHP,infact.However,itscoresworsethan\nIHPbecause,eventhoughitmanagestodelivermoreboxes,it\nperformsothersub-optimalactions.Ontheotherhand,neither\nPPOnorA2Cmanagedtodeliveranyboxes,whichisevident\nby looking at their score.",
      "size": 562,
      "sentences": 3
    },
    {
      "id": 45,
      "content": "tscoresworsethan\nIHPbecause,eventhoughitmanagestodelivermoreboxes,it\nperformsothersub-optimalactions.Ontheotherhand,neither\nPPOnorA2Cmanagedtodeliveranyboxes,whichisevident\nby looking at their score. The consequence of this fact is that\nerocS\n=0.8192, =1, =0.99, explFrac = 0.0125\n=0.8192, =0.1, =0.99, explFrac = 0.1\n=0.8192, =1, =0.2302, explFrac = 0.8\n=1.5625e 06, =1, =0.99, explFrac = 0.0125\n=1.5625e 06, =1, =0.2302, explFrac = 0.1\n=1.5625e 06, =0.1, =0.99, explFrac = 0.8\n=0.0001, =1, =0.99, explFrac = 0.1\n=0.0001, =1, =0.99, explFrac = 0.8\n=0.0001, =0.1, =0.2302, explFrac = 0.0125\nIHP\nEHP\nRandom\nFigure 4: Hyperparameter tuning process for the DQN policy\nover 4 runs. the other metrics (FIFO violation and Mean Box ages) show\napparentlygoodresults,buttheyarenotfeasiblebecausethese\nmetricsarenotvalidforthelownumberofdeliveredmaterial.",
      "size": 845,
      "sentences": 3
    },
    {
      "id": 46,
      "content": "for the DQN policy\nover 4 runs. the other metrics (FIFO violation and Mean Box ages) show\napparentlygoodresults,buttheyarenotfeasiblebecausethese\nmetricsarenotvalidforthelownumberofdeliveredmaterial. Thus, moving on to these metrics, we have the Mean Box\nAges, in which the DQN policies get very close to the EHP,\nenhancingtheIHP.ThisisplausibleasDQNdoesnothavethe\nrestrictions that IHP has, and it is capable of delivering items\ndirectlyfromtheentrypoints.Furthermore,EHPdoesnothave\ntheserestrictionseither,soitisabletofurtherreducethemean\nage.Inaddition,theFIFOViolationmetricshowsthatDQNis\nable to build those solid numbers by not delivering the oldest\nitem 40% of the time, while the VAM policy enhances this\nmetric, achieving a violation rate of 30%. As a side note, the\nsmallpercentageofFIFOviolationbytheIHPpolicyisdueto\nthe fact that, in this policy, the occupancy of the warehouse is\nnearly at its maximum and, as a consequence, restricted cells\nstart to appear.",
      "size": 971,
      "sentences": 4
    },
    {
      "id": 47,
      "content": "the\nsmallpercentageofFIFOviolationbytheIHPpolicyisdueto\nthe fact that, in this policy, the occupancy of the warehouse is\nnearly at its maximum and, as a consequence, restricted cells\nstart to appear. Consequently, in a small number of cases, the\noldestboxesofthewarehousesareinarestrictedpositionand,\ntherefore,theIHPagentmustdeliverotherboxes,violatingthe\nFIFO method. D. Hyperparameter Optimization\nThe aboveresults showthat, eventhough some RLpolicies\nachievenear-human performance,othershaveample roomfor\nimprovement. This is due to the fact that, for the training\nof these policies, we used the default hyperparameters from\nstable-baselines3. In this section, we performed the hyperpa-\nrameter optimization as described in Section IV-B.",
      "size": 741,
      "sentences": 5
    },
    {
      "id": 48,
      "content": "act that, for the training\nof these policies, we used the default hyperparameters from\nstable-baselines3. In this section, we performed the hyperpa-\nrameter optimization as described in Section IV-B. [표 데이터 감지됨]\n\n=== 페이지 7 ===\nα γ vfCoef (cid:127)\n1.5625e-06 0.99 0.1 -\n1.5625e-06 0.2302 0.9 -\n300\n1.5625e-06 0.2302 0.5 -\n0.8192 0.99 0.9 -\n400\nA2C 0.0001 0.2302 0.9 -\n0.8192 0.2302 0.5 -\n500\n0.0001 0.99 0.5 -\n0.8192 0.2302 0.1 - 600\n0.0001 0.2302 0.1 -\n1.5625e-06 0.99 0.1 0.08 700\n1.5625e-06 0.99 0.9 0.6\n0.0001 0.99 0.9 0.2 800\n0.8192 0.2302 0.9 0.08\n0.8192 0.99 0.1 0.6 900\nPPO 1.5625e-06 0.99 0.5 0.6\n0 500 1000 1500 2000 2500 3000\n0.8192 0.99 0.5 0.2 Episode\n0.0001 0.2302 0.1 0.08\n1.5625e-06 0.2302 0.1 0.2\n0.0001 0.2302 0.5 0.6\n1.5625e-06 0.99 0.5 0.08\nTable I: Studied hyperparameter combinations for A2C and\nPPO\nFigure4showstheresultofthetuningprocessforDQN.We\nobserve that some hyperparameter combinations cannot even\ncompetewiththerandompolicy,whileothercombinationscan\nevencompetewiththeIHP,althoughbeingquitefarawayfrom\nthe EHP yet.",
      "size": 1046,
      "sentences": 3
    },
    {
      "id": 49,
      "content": "etuningprocessforDQN.We\nobserve that some hyperparameter combinations cannot even\ncompetewiththerandompolicy,whileothercombinationscan\nevencompetewiththeIHP,althoughbeingquitefarawayfrom\nthe EHP yet. Finally, we have a combination that enhances\nthe IHP and approaches the EHP, the yellow line with the\nhyperparametercombinationofα=0.001,τ =0.1,γ =0.23\nandexplFrac=0.0125.Asareference,thepinkline(withα=\n0.001,τ =1,γ =0.99andexplFrac=0.1)correspondstothe\ndefaulthyperparametersofstable-baselines3.Wecanconclude\nthat these original hyperparameters were a good combination,\nbutwewereabletoenhanceperformancebychoosingamore\nsuitable combination. We performed a similar analysis with the other RL algo-\nrithms,namelyA2CandPPO.TableIshowsthecombinations\nand we highlighted the best combinations for each case. E. Results with Hyperparameter Optimization\nWe re-trained the RL policies using the optimal hyperpa-\nrametercombinationoftheprevioussectionwithDQN,VAM,\nA2C, and PPO. Figure 5 depicts the result.",
      "size": 998,
      "sentences": 5
    },
    {
      "id": 50,
      "content": "ase. E. Results with Hyperparameter Optimization\nWe re-trained the RL policies using the optimal hyperpa-\nrametercombinationoftheprevioussectionwithDQN,VAM,\nA2C, and PPO. Figure 5 depicts the result. We note that the results are\ndrastically enhanced, showing that policies that previously\nreachednear-randomresultsnowperformbetterthantheDQN\npolicyandeventheIHP,gettingclosertotheVAMpolicyand\nto the EHP. Even the VAM policy improves its convergence\nspeed, although its score is quite difficult to enhance. We can\nconclude that, optimizing the hyperparameters, some results\ncan be significantly improved, as now all the RL policies are\nbetter than the IHP, which at first was only surpassed by the\nresults of the VAM policy.",
      "size": 723,
      "sentences": 6
    },
    {
      "id": 51,
      "content": "at, optimizing the hyperparameters, some results\ncan be significantly improved, as now all the RL policies are\nbetter than the IHP, which at first was only surpassed by the\nresults of the VAM policy. As for the metrics, we can observe that the Score metric\nhasimproved,sincealltheRLmetricsarebetterthantheIHP,\nalthoughnotasgoodastheEHP.IntermsofDeliveredBoxes,\neach RL policy is close to the maximum score achieved by\nerocS\nDQN PPO IHP Random\nA2C VAM EHP\nFigure 5: Score as a function of the number of collected\nepisodes for the tested RL algorithms and baseline policies\nafter hyperparameter optimization over 10 runs). Score Delivered Boxes\nEHP EHP\nVAM VAM\nPPO DQN\nA2C PPO\nDQN A2C\nIHP IHP\nRandom Random\n800 600 400 200 0 0 50 100 150\nMean box ages FIFO violation\nDQN EHP\nPPO IHP\nEHP DQN\nVAM PPO\nA2C VAM\nIHP A2C\nRandom Random\n0 50 100 150 200 0% 20% 40% 60%\nFigure 6: Metric comparison for the defined policies with\noptimized hyperparameters.",
      "size": 943,
      "sentences": 3
    },
    {
      "id": 52,
      "content": "ages FIFO violation\nDQN EHP\nPPO IHP\nEHP DQN\nVAM PPO\nA2C VAM\nIHP A2C\nRandom Random\n0 50 100 150 200 0% 20% 40% 60%\nFigure 6: Metric comparison for the defined policies with\noptimized hyperparameters. theEHP,withsmalldivergencesduetothestochasticityofthe\nenvironment. The Mean Box Age metric results are also quite\nenhanced and it can be seen that some RL policies minimize\nthe box ages, even more than the EHP, probably due to the\nfactthatthosepoliciesareallowedtoviolatetheFIFOcriterion\nwhile the human policies are not. VI. DISCUSSION\nIntheprevioussection,weshowedpromisingresults.How-\never, the Storehouse environment shows some potential limi-\ntations, especially when it comes to scalability.",
      "size": 696,
      "sentences": 5
    },
    {
      "id": 53,
      "content": "human policies are not. VI. DISCUSSION\nIntheprevioussection,weshowedpromisingresults.How-\never, the Storehouse environment shows some potential limi-\ntations, especially when it comes to scalability. The previous\nresults were given on the 6×6 configuration, and that leads\ntoamoderatelylargenumberofpossiblestates.However,real\nwarehouses tend to be much larger and, thus, the state space\nwouldincreasesignificantly.Thiscouldbetroublesomeasthe\n[표 데이터 감지됨]\n\n=== 페이지 8 ===\ncomplexity of the problem inherently becomes greater (for [AWR+17] MarcinAndrychowicz,FilipWolski,AlexRay,JonasSchneider,\nexample,a100×100gridwith25differenttypesofmaterials). RachelFong,PeterWelinder,BobMcGrew,JoshTobin,Pieter\nAbbeel, and Wojciech Zaremba. Hindsight experience replay. This training problem impacts the hyperparameter tuning\nNIPS,pages5049–5059,72017. process.",
      "size": 848,
      "sentences": 8
    },
    {
      "id": 54,
      "content": "helFong,PeterWelinder,BobMcGrew,JoshTobin,Pieter\nAbbeel, and Wojciech Zaremba. Hindsight experience replay. This training problem impacts the hyperparameter tuning\nNIPS,pages5049–5059,72017. process. This process, although it was drastically reduced by [BCP+16] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas\nusing the DoE method, is slow, since several runs must be Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.\nperformed to find the optimal combinations, and with larger\n[Bel61] Richard E. Bellman. Adaptive control processes. Adaptive\nstate spaces, this training would be even slower. ControlProcesses,121961. Theimpactofdealingwithlargerstatespacesisunclear.We [BMR+15] EleonoraBottani,RobertoMontanari,MartaRinaldi,Giuseppe\nwould, most likely, have similar results in terms of scoring, Vignali,EBottani,A´ RMontanari,A´ GVignali,RMontanari,\nGVignali,andMRinaldi.",
      "size": 933,
      "sentences": 11
    },
    {
      "id": 55,
      "content": "R+15] EleonoraBottani,RobertoMontanari,MartaRinaldi,Giuseppe\nwould, most likely, have similar results in terms of scoring, Vignali,EBottani,A´ RMontanari,A´ GVignali,RMontanari,\nGVignali,andMRinaldi. Intelligentalgorithmsforwarehouse\nbut the training session would last a lot longer. With the\nmanagement. Intelligent Systems Reference Library, 87:645–\n6 × 6 environment, the training took around 3.5 hours, we 667,2015.\nsuspect this number may be significantly increased. However, [Box80] Joan Fisher Box. R. a. fisher and the design of experiments,\nthe VAM policy would probably be able to maintain its 1922-1926. TheAmericanStatistician,34(1):1–7,1980. [Cze06] JacekCzerwonka.Pairwisetestinginrealworld.In24thPacific\nconvergence speed, since the action space varies depending\nNorthwestSoftwareQualityConference,volume200,2006. on the available actions. Nevertheless, the use of more case- [DCY15] Jiajia Dou, Chunlin Chen, and Pei Yang.",
      "size": 938,
      "sentences": 10
    },
    {
      "id": 56,
      "content": "ince the action space varies depending\nNorthwestSoftwareQualityConference,volume200,2006. on the available actions. Nevertheless, the use of more case- [DCY15] Jiajia Dou, Chunlin Chen, and Pei Yang. Genetic scheduling\nspecific algorithms could overcome this limitation and, thus, andreinforcementlearninginmultirobotsystemsforintelligent\nwarehouses. Mathematical Problems in Engineering, 2015,\nlessentheimpactofthislimitation.Anexampleofpossibleal-\n2015.\ngorithmsthatwouldbeimmunetothecurseofdimensionality\n[ELP12] Reza Moazzez Estanjini, Keyong Li, and Ioannis Ch Pascha-\nis AlphaZero [SHS+18] or adding the Hindsight Experience lidis. Aleastsquarestemporaldifferenceactor–criticalgorithm\nReplay (HER) technique [AWR+17] to an already stated with applications to warehouse management. Naval Research\nLogistics(NRL),59:197–211,42012. algorithm. Regarding hyperparameter tuning, the optimization\n[HNR68] PeterE.Hart,NilsJ.Nilsson,andBertramRaphael.",
      "size": 948,
      "sentences": 9
    },
    {
      "id": 57,
      "content": "applications to warehouse management. Naval Research\nLogistics(NRL),59:197–211,42012. algorithm. Regarding hyperparameter tuning, the optimization\n[HNR68] PeterE.Hart,NilsJ.Nilsson,andBertramRaphael. Aformal\nof these parameters would help to improve the convergence basis for the heuristic determination of minimum cost paths. speed, but we already stated that the time required for this IEEETransactionsonSystemsScienceandCybernetics,4:100–\n107,1968.\nprocess is quite high. However, this tuning process is the last\n[LJ21] HyeoksooLeeandJongpilJeong. Mobilerobotpathoptimiza-\nstep in an RL training, so the impact of this limitation on the tion technique based on reinforcement learning algorithm in\ngeneral scope of the project is minimal. warehouse environment. Applied Sciences 2021, Vol. 11, Page\n1209,11:1209,12021. VII.",
      "size": 825,
      "sentences": 12
    },
    {
      "id": 58,
      "content": "tation on the tion technique based on reinforcement learning algorithm in\ngeneral scope of the project is minimal. warehouse environment. Applied Sciences 2021, Vol. 11, Page\n1209,11:1209,12021. VII. CONCLUSIONS [LLM+17] EricLiang,RichardLiaw,PhilippMoritz,RobertNishihara,Roy\nFox,KenGoldberg,JosephE.Gonzalez,MichaelI.Jordan,and\nIn this contribution, we covered the definition and im- Ion Stoica. Rllib: Abstractions for distributed reinforcement\nlearning. 35th International Conference on Machine Learning,\nplementation of a warehouse management solution for the\nICML2018,7:4768–4780,122017. efficiency problem in the logistics industry. We described the [LSK+19] MaojiaPatrickLi,PrashantSankaran,MichaelE.Kuhl,Amlan\ncurrentmethodologiesusedtosolvethisproblemandfoundan Ganguly,AndresKwasinski,andRaymondPtucha. Simulation\nimprovement opportunity and proposed an RL-based solution analysisofadeepreinforcementlearningapproachfortaskse-\nlectionbyautonomousmaterialhandlingvehicles.",
      "size": 982,
      "sentences": 11
    },
    {
      "id": 59,
      "content": "resKwasinski,andRaymondPtucha. Simulation\nimprovement opportunity and proposed an RL-based solution analysisofadeepreinforcementlearningapproachfortaskse-\nlectionbyautonomousmaterialhandlingvehicles. Proceedings\nthat allows users to simulate a customizable warehouse that\n-WinterSimulationConference,2018-December:1073–1083,1\ncan be used to represent real warehouses. We also proved the 2019.\nfeasibilityofthisenvironmentbytrainingseveralpolicies,and, [LWZZ20] JingranLiang,ZhengningWu,ChenyeZhu,andZhiHaiZhang. An estimation distribution algorithm for wave-picking ware-\nafter comparing the results with human-defined policies and\nhousemanagement.JournalofIntelligentManufacturing,pages\na random policy, we saw that these policies are able to solve 1–14,102020. the problem with near-optimal performance.",
      "size": 805,
      "sentences": 6
    },
    {
      "id": 60,
      "content": "uman-defined policies and\nhousemanagement.JournalofIntelligentManufacturing,pages\na random policy, we saw that these policies are able to solve 1–14,102020. the problem with near-optimal performance. [MBM+16] Volodymyr Mnih, Adria Puigdomenech Badia, Lehdi Mirza,\nTherefore,wecanconcludethattheStorehouseenvironment Alex Graves, Tim Harley, Timothy P. Lillicrap, David Silver,\nandKorayKavukcuoglu. Asynchronousmethodsfordeeprein-\nis a novel tool that can be used for training policies of forcementlearning. 33rdInternationalConferenceonMachine\nwarehouse management systems with customizable layouts Learning,ICML2016,4:2850–2869,22016. and settings. This environment uses the gym interface, so it [MXZ18] JiaMao,HuihuiXing,andXiuzhiZhang. Designofintelligent\nwarehousemanagementsystem.WirelessPersonalCommunica-\nis standardized to be used with almost any RL algorithm. We\ntions,102:1355–1367,92018. also saw that, by training RL policies based on State-of-the- [Put94] Martin L. Puterman.",
      "size": 988,
      "sentences": 10
    },
    {
      "id": 61,
      "content": "lessPersonalCommunica-\nis standardized to be used with almost any RL algorithm. We\ntions,102:1355–1367,92018. also saw that, by training RL policies based on State-of-the- [Put94] Martin L. Puterman. Markov Decision Processes: Discrete\nArt algorithms, this environment is robust and these policies Stochastic Dynamic Programming. Wiley Series in Probability\nandStatistics.Wiley,1994. are able to converge to promising results, and near-optimal\n[RHG+21] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto,\nperformance. Maximilian Ernestus, and Noah Dormann. Stable-baselines3:\nIn future scenarios, we plan to use this environment to Reliable reinforcement learning implementations. Journal of\ntest planning approaches and algorithms like AlphaZero. We MachineLearningResearch,22(268):1–8,2021. [SB18] RichardSSuttonandAndrewGBarto.Reinforcementlearning:\nexpectthattheseapproacheswouldmaintaingoodresultseven\nAnintroduction. MITpress,2018. with larger state spaces.",
      "size": 971,
      "sentences": 13
    },
    {
      "id": 62,
      "content": "ngResearch,22(268):1–8,2021. [SB18] RichardSSuttonandAndrewGBarto.Reinforcementlearning:\nexpectthattheseapproacheswouldmaintaingoodresultseven\nAnintroduction. MITpress,2018. with larger state spaces. [SHS+18] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis\nAntonoglou,MatthewLai,ArthurGuez,MarcLanctot,Laurent\nREFERENCES Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,\nKarenSimonyan,andDemisHassabis.Ageneralreinforcement\n[Ach18] JoshuaAchiam. SpinningUpinDeepReinforcementLearning, learning algorithm that masters chess, shogi, and go through\n2018. self-play. Science,362:1140–1144,122018. === 페이지 9 ===\n[SWD+17] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,\nand Oleg Klimov. Proximal policy optimization algorithms. CoRR,abs/1707.06347,2017. [WD92] ChristopherJCHWatkinsandPeterDayan.Q-learning.Machine\nlearning,8(3-4):279–292,1992. [Wol82] RonaldWWolff.Poissonarrivalsseetimeaverages.Operations\nresearch,30(2):223–231,1982.",
      "size": 957,
      "sentences": 12
    },
    {
      "id": 63,
      "content": "1707.06347,2017. [WD92] ChristopherJCHWatkinsandPeterDayan.Q-learning.Machine\nlearning,8(3-4):279–292,1992. [Wol82] RonaldWWolff.Poissonarrivalsseetimeaverages.Operations\nresearch,30(2):223–231,1982. [YJL20] Yang Yang, Li Juntao, and Peng Lingling. Multi-robot path\nplanningbasedonadeepreinforcementlearningdqnalgorithm. CAAI Transactions on Intelligence Technology, 5:177–183, 9\n2020. [ZDH+18] EmirZunic,SeadDelalic,KerimHodzic,AdmirBesirevic,and\nHarun Hindija. Smart warehouse management system concept\nwithimplementation.201814thSymposiumonNeuralNetworks\nandApplications,NEUREL2018,122018. [ZHH+17] Emir Zunic, Kerim Hodzic, Haris Hasic, Rijad Skrobo, Ad-\nmir Besirevic, and Dzenana Donko. Application of advanced\nanalysis and predictive algorithm for warehouse picking zone\ncapacityandcontentprediction. ICAT2017-26thInternational\nConference on Information, Communication and Automation\nTechnologies,Proceedings,2017-December:1–6,122017.",
      "size": 941,
      "sentences": 11
    },
    {
      "id": 64,
      "content": "lgorithm for warehouse picking zone\ncapacityandcontentprediction. ICAT2017-26thInternational\nConference on Information, Communication and Automation\nTechnologies,Proceedings,2017-December:1–6,122017. [ZHH+18] Emir Zunic, Haris Hasic, Kerim Hodzic, Sead Delalic, and\nAdmirBesirevic.Predictiveanalysisbasedapproachforoptimal\nwarehouseproductpositioning.201841stInternationalConven-\ntiononInformationandCommunicationTechnology,Electronics\nandMicroelectronics,MIPRO2018-Proceedings,pages950–\n954,62018.",
      "size": 498,
      "sentences": 3
    }
  ]
}