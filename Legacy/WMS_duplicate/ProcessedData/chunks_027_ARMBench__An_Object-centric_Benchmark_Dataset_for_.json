{
  "source": "ArXiv",
  "filename": "027_ARMBench__An_Object-centric_Benchmark_Dataset_for_.pdf",
  "total_chars": 44903,
  "total_chunks": 63,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nARMBench: An Object-centric Benchmark Dataset\nfor Robotic Manipulation\nChaitanya Mitash1, Fan Wang1, Shiyang Lu2, Vikedo Terhuja1,\nTyler Garaas1, Felipe Polido1, Manikantan Nambi1\nAbstract—This paper introduces Amazon Robotic Manip-\nulation Benchmark (ARMBench), a large-scale, object-centric\nbenchmark dataset for robotic manipulation in the context of\na warehouse. Automation of operations in modern warehouses\nrequires a robotic manipulator to deal with a wide variety\nof objects, unstructured storage, and dynamically changing\ninventory. Such settings pose challenges in perceiving the\nidentity, physical characteristics, and state of objects during\nmanipulation. Existing datasets for robotic manipulation con-\nsider a limited set of objects or utilize 3D models to gener-\nate synthetic scenes with limitation in capturing the variety\nof object properties, clutter, and interactions.",
      "size": 902,
      "sentences": 4
    },
    {
      "id": 2,
      "content": "robotic manipulation con-\nsider a limited set of objects or utilize 3D models to gener-\nate synthetic scenes with limitation in capturing the variety\nof object properties, clutter, and interactions. We present a\nlarge-scale dataset collected in an Amazon warehouse using\na robotic manipulator performing object singulation from\ncontainers with heterogeneous contents. ARMBench contains\nimages, videos, and metadata that corresponds to 235K+ pick-\nand-place activities on 190K+ unique objects. The data is\ncaptured at different stages of manipulation, i.e., pre-pick,\nduring transfer, and after placement. Benchmark tasks are\nproposed by virtue of high-quality annotations and baseline\nperformance evaluation are presented on three visual percep-\ntion challenges, namely 1) object segmentation in clutter, 2)\nobject identification, and 3) defect detection. ARMBench can\nFig.1.",
      "size": 875,
      "sentences": 6
    },
    {
      "id": 3,
      "content": "baseline\nperformance evaluation are presented on three visual percep-\ntion challenges, namely 1) object segmentation in clutter, 2)\nobject identification, and 3) defect detection. ARMBench can\nFig.1. Alarge-scaleobjectdatasetiscollectedusingaroboticmanipulation\nbe accessed at http://armbench.com\nsystemoperatinginanAmazonwarehouse.Theroboticarmpicksoneobject\natatimefromtheyellowcontainerandplacesitinagraytray(top).The\nI. INTRODUCTION datasetcontainsimagesfordifferentphasesofmanipulationi.e.,imageof\nobjectsintheyellowcontainerbeforepicking(bottom-left),duringtransfer\nRobotic systems for object handling in warehouses can (bottom-mid)andafterplacement(bottom-right).Inadditiontosensordata,\nthe dataset also provides high-quality annotations for tasks such as object\nexpedite fulfillment of customer orders by automating tasks\nsegmentation,objectidentification,anddefectdetection.",
      "size": 883,
      "sentences": 4
    },
    {
      "id": 4,
      "content": "nsordata,\nthe dataset also provides high-quality annotations for tasks such as object\nexpedite fulfillment of customer orders by automating tasks\nsegmentation,objectidentification,anddefectdetection. suchasobjectpicking,sorting,andpacking.However,build-\ningreliableandscalableroboticsystemsforobjectmanipula- Benchmarks and datasets such as ImageNet [56] and MS-\ntioninwarehousesisnottrivial.Modernwarehousesprocess COCO [45] have enabled significant performance improve-\nmillions of unique objects with diverse shapes, materials, ment in computer vision tasks such as image classification\nand other physical properties. These objects are often stored and segmentation. Nosizeable datasetexists thatcapture the\nin unstructured configurations within containers which pose desired variety of objects, configurations, and interactions in\nchallengesforroboticperceptionandplanning.From2015to the context of robotic manipulation.",
      "size": 924,
      "sentences": 4
    },
    {
      "id": 5,
      "content": "d configurations within containers which pose desired variety of objects, configurations, and interactions in\nchallengesforroboticperceptionandplanning.From2015to the context of robotic manipulation. Large repositories of\n2017, the Amazon Robotics Challenge (ARC) helped push 3D shape models [10], [12], [15], [23] enable generating\nthe state-of-the-art for robotic systems in a pick-and-place a variety of scenarios with a rich set of annotations in\ntask representative of a warehouse [14], [17]. Nevertheless, simulation. Nevertheless, they may fail to capture certain\nthe competition could not incorporate challenges of large- physical properties of objects and interactions encountered\nscale operations. Fundamental research still needs to be during manipulation from heterogeneous clutter.",
      "size": 794,
      "sentences": 5
    },
    {
      "id": 6,
      "content": "t incorporate challenges of large- physical properties of objects and interactions encountered\nscale operations. Fundamental research still needs to be during manipulation from heterogeneous clutter. Existing\ncarried out to enable visual perception algorithms such as real-world datasets [36] operate under closed set assumption\nobject segmentation and identification to generalize to a with a small number of object types. Such assumptions\nwidevarietyofunseenobjectsandconfigurations.Additional prevent evaluating algorithms in terms of its generalization\nproblems (such as defect detection) and metrics (measuring capabilitiesovernovelobjectswhichiscriticalinlargescale\nuncertainty in prediction) need to be defined to capture the operations. Additionally, these datasets only deal with static\nscale and high-precision requirements of such systems. scenes with objects in near perfect conditions and do not\nconsider interactions with a robotic manipulator. 1Amazon Robotics, MA, USA.",
      "size": 985,
      "sentences": 7
    },
    {
      "id": 7,
      "content": "th static\nscale and high-precision requirements of such systems. scenes with objects in near perfect conditions and do not\nconsider interactions with a robotic manipulator. 1Amazon Robotics, MA, USA. {cmitash, fanwanf, terhuja, tggaraas, Inthispaper,wepresentARMBench,alarge-scalebench-\npolidof,mnambi}@amazon.com\nmark dataset for a robotic pick-and-place task that captures\n2Computer Science Department, Rutgers University, NJ, USA. shiyang.lu@rutgers.edu.Workdoneduringaco-opatAmazonRobotics. a wide variety of warehouse objects and configurations. The\n3202\nraM\n92\n]OR.sc[\n1v28361.3032:viXra\n[표 데이터 감지됨]\n\n=== 페이지 2 ===\ndataset comprises images and videos for different stages of objects. To obtain data at scale, a common strategy is to\nof robotic manipulation, namely pick, transfer, and place. generatesyntheticdatawithphysicssimulatorsandrendering\nIt includes metadata such as descriptions and reference tools [20], [48], [32], [67], [63].",
      "size": 944,
      "sentences": 9
    },
    {
      "id": 8,
      "content": "otic manipulation, namely pick, transfer, and place. generatesyntheticdatawithphysicssimulatorsandrendering\nIt includes metadata such as descriptions and reference tools [20], [48], [32], [67], [63]. Nevertheless, synthetic\nimages for objects in the container. Each pick-and-place datasets are limited by the availability of high-quality 3D\nactivity is also annotated with the identity of the object object models, and inherently carry a sim2real gap [63]. beingmanipulated,andtheoutcomeofthemanipulationi.e., This work introduces a real-world, large-scale dataset for\nwhether it was successful (a single object was picked and object segmentation that captures a wide variety of objects\nplaced) or if it resulted in a defect. The dataset can be used and configurations relevant to robotic manipulation.",
      "size": 802,
      "sentences": 6
    },
    {
      "id": 9,
      "content": "e object was picked and object segmentation that captures a wide variety of objects\nplaced) or if it resulted in a defect. The dataset can be used and configurations relevant to robotic manipulation. tostudydifferentvisualperceptionproblemsinthecontextof\nroboticmanipulation.Thispaperprovidesnovelbenchmarks C. Object Identification\nwith annotations and baseline performance metrics for:\nObject identification refers to the task of exactly iden-\n• Object Segmentation including 450,000+ high-quality tifying the object specified by an image segment. In an\nmanual labels for object segments on 50,000+ images. open-set setting it is often posed as an image retrieval\nVariations in objects and degree of clutter present a\nproblemi.e.,givenaqueryimageandadatabaseofcandidate\nnovel challenge for instance segmentation algorithms. images, rank them according to their similarity to the query\n• Object Identification presenting an open set object image.",
      "size": 947,
      "sentences": 6
    },
    {
      "id": 10,
      "content": "andadatabaseofcandidate\nnovel challenge for instance segmentation algorithms. images, rank them according to their similarity to the query\n• Object Identification presenting an open set object image. Various approaches have been used to tackle this\nidentification and confidence estimation challenge for\nproblem such as aggregating pre-defined local features [59],\nrobotic manipulation. With 190,000+ unique objects\n[51], [37], [60], computing similarity metrics over features\nin varying configurations, the dataset will be used to\nderived from large-scale image classification training [4],\nbenchmark image retrieval and few-shot classification\n[62], [30], and metric learning with pairs of matching and\nmethods with uncertainty estimation. non-matching images [53], [61].",
      "size": 773,
      "sentences": 5
    },
    {
      "id": 11,
      "content": "ion training [4],\nbenchmark image retrieval and few-shot classification\n[62], [30], and metric learning with pairs of matching and\nmethods with uncertainty estimation. non-matching images [53], [61]. Common benchmarks for\n• Defect Detection with manually assigned labels for image retrieval consider landmark datasets [51], [52], [65]\nrare, but costly, robot-induced defects such as multi-\nand retail datasets such as DeepFashion [47], [31], Online\nobject-pickandpackagingdefects.Thedatasetcontains\nProducts dataset [49], RPC [64], RP2K [50], Products-10K\n19,000+ images and 4,000+ videos of activities with\n[5], and AliProducts [13]. The product images in these\ndefects, and 100,000+ activities without defects. datasets are online store images, customer images or photos\nII. RELATEDWORK from retail stores.",
      "size": 808,
      "sentences": 6
    },
    {
      "id": 12,
      "content": "5], and AliProducts [13]. The product images in these\ndefects, and 100,000+ activities without defects. datasets are online store images, customer images or photos\nII. RELATEDWORK from retail stores. Alternatively, the images in the proposed\ndataset are representative of how objects are stored in a\nA. Benchmarking in Robotic Manipulation\nwarehouse,withdifferenttypesofpackagingandincluttered\nArecentbenchmarkingeffortforroboticmanipulation[9]\nconfigurations. The robotic manipulation context not only\nconsiders challenges in mechanical design, grasp planning\nprovidesauniquesetofchallengesforobjectidentificationin\nand deformable object manipulation but does not focus on\ntermsofocclusionandviewpointvariationsbutalsoimposes\nthe complexities of underlying perception tasks. An annual\nstringent requirements in terms of precision.",
      "size": 831,
      "sentences": 7
    },
    {
      "id": 13,
      "content": "object manipulation but does not focus on\ntermsofocclusionandviewpointvariationsbutalsoimposes\nthe complexities of underlying perception tasks. An annual\nstringent requirements in terms of precision. competition [36] benchmarks performance of relevant per-\nception algorithms such as object detection, segmentation\nD. Defect Detection\nand 6D pose estimation over a collection of datasets [8],\nFew image datasets exist for objects with defects. Prior\n[35],[66].TheAmazonroboticschallenge[18],[24]initiated\nresearch on visual defect detection has focused on surface\nthe development of other relevant datasets [55], [41], [68]. defectsforindividualobjectssuchasLEDchips[44],fabrics\nWhile these datasets present interesting challenges in terms\n[58], and metals [6]. The DAGM 2007 Competition dataset\nof variety of configurations and large occlusions, they are\n[2]comprises6,900syntheticimageswithsixdifferenttypes\nlimited in terms of the number of object instances with a\nof surface defects.",
      "size": 987,
      "sentences": 6
    },
    {
      "id": 14,
      "content": "dataset\nof variety of configurations and large occlusions, they are\n[2]comprises6,900syntheticimageswithsixdifferenttypes\nlimited in terms of the number of object instances with a\nof surface defects. The MVTec Anomaly Detection dataset\nmaximum of 42 unique objects. comprises5,354colorimagescorrespondingto15objectand\nB. Object Segmentation texture categories with 70 different types of defects such as\nObject instance segmentation refers to simultaneously scratches, dents, contaminations, and structural changes [7]. predictingpixel-levelinstance-maskandcorrespondingclass This is the first dataset to capture defects in the context of\nlabels.Thetechniquehasbeenwidelyappliedinautonomous robotic manipulation. driving [69], [21], [54], video surveillance [29], [57], and Datasets for defect detection in videos primarily focus\nrobotics [67], [19], [43].",
      "size": 855,
      "sentences": 6
    },
    {
      "id": 15,
      "content": "ehasbeenwidelyappliedinautonomous robotic manipulation. driving [69], [21], [54], video surveillance [29], [57], and Datasets for defect detection in videos primarily focus\nrobotics [67], [19], [43]. The introduction of large-scale on anomaly detection methods for events such as throwing\nlabeleddatasetsuchasMS-COCO[45],PASCALVOC[25], objects, loitering, running [46], crowded scenes [42], and\nand Cityscapes [16] has significantly advanced the state-of- anomalous pedestrian patterns [40]. These datasets contain\nthe-art in detection and segmentation, particularly for com- a limited number of videos (10-50) per activity. Video\nmon object categories from WordNet [27]. These datasets classificationdatasetsexistsinthedomainofsportsactivities\nserveasastandardbenchmarkforevaluatingcomputervision [38], human actions [39], and holistic video understand-\nmodelsbutarenotrepresentativeofobjectsaroboticmanip- ing [22]. Similar to existing research on using videos for\nulator would interact with.",
      "size": 994,
      "sentences": 7
    },
    {
      "id": 16,
      "content": "on [38], human actions [39], and holistic video understand-\nmodelsbutarenotrepresentativeofobjectsaroboticmanip- ing [22]. Similar to existing research on using videos for\nulator would interact with. Representative datasets such as understanding context and actions, videos can be used to\ntheMVTecD2Sdataset[28]arelimitedinsizeanddiversity understand events in robotic manipulation process such as\n=== 페이지 3 ===\nFig. 2. (left) Benchmark tasks and annotation statistics on the ARMBench dataset. (right) Distribution of product-groups and object dimensions for\n190,00+uniqueobjectsinthedataset. successfulanddefectiveactivities.Thereexistsnolarge-scale • Video: A camera is mounted to capture 720p videos of\nvideo datasets for this purpose. pick-and-place manipulation processes at 30FPS\nAdditionally, the following metadata (Fig. 2 (b)) is available\nIII.",
      "size": 853,
      "sentences": 9
    },
    {
      "id": 17,
      "content": "ideo: A camera is mounted to capture 720p videos of\nvideo datasets for this purpose. pick-and-place manipulation processes at 30FPS\nAdditionally, the following metadata (Fig. 2 (b)) is available\nIII. ARMBENCHDATASET\nby virtue of a warehouse tracking system:\nTheARMBenchdatasetpresents:1)acollectionofsensor • Container-manifest: A list of objects present in the\ndataacquiredbyaroboticmanipulationworkcellperforming container along with data such as product description,\npick-and-place operation, 2) metadata and reference images coarse dimensions, and weight. for objects in containers, 3) a set of annotations acquired • Referenceimages:Oneormoreimagesofobjectsfrom\neither automatically, by virtue of the system design, or via previous operations within the warehouse. manual labeling, and 4) tasks and metrics to benchmark The sensor data and metadata were consumed by perception\nperception algorithms for robotic manipulation. Fig.",
      "size": 934,
      "sentences": 7
    },
    {
      "id": 18,
      "content": "s operations within the warehouse. manual labeling, and 4) tasks and metrics to benchmark The sensor data and metadata were consumed by perception\nperception algorithms for robotic manipulation. Fig. 2 illus- algorithms required to autonomously operate the robotic\ntrates the benchmark tasks and variety of objects captured workcell. Benchmarking against these algorithms would not\nin the dataset. The dataset captures diversity in objects with only optimize a manipulation task such as the one used for\nrespect to Amazon product categories as well as physical datacollectionbutalsoenablemorecomplexandintentional\ncharacteristics such as size, shape, material, deformability, manipulation. This work considers a subset of such percep-\nappearance, fragility, etc. tion tasks namely object segmentation, object identification,\nThe data collection platform is a robotic manipulation and defect detection.",
      "size": 901,
      "sentences": 8
    },
    {
      "id": 19,
      "content": "nsiders a subset of such percep-\nappearance, fragility, etc. tion tasks namely object segmentation, object identification,\nThe data collection platform is a robotic manipulation and defect detection. These are critical not only to make\nworkcell performing pick-and-place operation in a ware- informed grasping and motion decisions but also to track\nhouse [1]. The workcell contains a robotic arm mounted the state of the objects and containers within the warehouse. with a vacuum-based end-effector. It is presented with a The following sections will describe these tasks and present\nheterogeneous collection of objects placed in unstructured the challenges using annotations, baseline algorithms, and\nconfigurations within a container (storage tote). The robotic evaluation metrics. arm is tasked with picking one object at a time (singulation)\nand place it on moving trays until the container is empty. IV.",
      "size": 908,
      "sentences": 9
    },
    {
      "id": 20,
      "content": "rations within a container (storage tote). The robotic evaluation metrics. arm is tasked with picking one object at a time (singulation)\nand place it on moving trays until the container is empty. IV. OBJECTSEGMENTATION\nThe empty container ejects the workcell and is replaced The object instance segmentation task is to identify and\nby a new container. While the operation is completely delineatedistinctobjectsstoredincontainersinawarehouse. autonomous, it includes a human-in-the-loop to monitor the In the context of robotic object manipulation, instance seg-\nstatus of each pick-and-place activity, annotate, and resolve mentation is used to inform downstream robotic processes\nany defects during manipulation. Multiple imaging sensors such as grasp generation, motion planning, and placement. are placed in the workcell to facilitate and validate the pick- Accuracy of instance segmentation can have an impact on\nand-place operation. Following is a list of sensor data (Fig.",
      "size": 978,
      "sentences": 10
    },
    {
      "id": 21,
      "content": "g, and placement. are placed in the workcell to facilitate and validate the pick- Accuracy of instance segmentation can have an impact on\nand-place operation. Following is a list of sensor data (Fig. pickingsuccess,objectidentification,anddefectsintroduced\n1) associated with each pick activity: intheprocess.Forexample,under-segmentationcanresultin\n• Pick-image: A 5MP camera is used to capture a top- picking multiple objects at a time, while over-segmentation\ndown image of the container. can result in a bad choice of grasp leading to damage or\n• Transfer-images: Multiple 5MP cameras are placed on dropping of objects. Fig. 3(a) shows manually annotated\ndifferent sides in the workcell to capture the moving object segments on the pick-image. Presence of deformable\nobject from different viewpoints. andtransparentobjectsincluttermakesthetaskchallenging. • Place-image:Atop-downviewoftheobjectiscaptured Our object instance segmentation dataset contains 50K+\nonce it is placed on the tray.",
      "size": 994,
      "sentences": 10
    },
    {
      "id": 22,
      "content": "ewpoints. andtransparentobjectsincluttermakesthetaskchallenging. • Place-image:Atop-downviewoftheobjectiscaptured Our object instance segmentation dataset contains 50K+\nonce it is placed on the tray. images of objects stored in containers in a warehouse with\n=== 페이지 4 ===\nTABLEI\nMASKR-CNNPERFORMANCEFOROBJECTSEGMENTATIONTASK.THE\nMODELWASTRAINEDONmix-object-toteDATASET\nmix-object-tote zoomed-out-tote- same-object-\ntransfer-set transfer-set\nmAP50 0.72 0.25 0.11\nmAP75 0.61 0.19 0.10\nif feasible at all. The ultimate goal is to readily transfer\nsegmentation to new scenarios with minimal additional an-\nnotations. Fig. 3. (a) Segmentation annotation overlaid on an image from mix-\nWe observe that segmentation performance for our base-\nobject-tote. Each identifiable item is segmented regardless of its size and\nocclusion. Multiple objects in the same package are considered as one line model has a strong correlation to the level of clutter. object and is delineated by the boundary of the package.",
      "size": 999,
      "sentences": 11
    },
    {
      "id": 23,
      "content": "ts size and\nocclusion. Multiple objects in the same package are considered as one line model has a strong correlation to the level of clutter. object and is delineated by the boundary of the package. In particular, Fig. 4 shows that the performance drops significantly as\nitems wrapped in transparent packaging are segmented by the peripheral\nthe number of ground-truth object instances increases in the\nof the package, although other products may be seen through them. (b-\nc) Example images from zoomed-out-tote-transfer-set and same-object- image. The mAP 50 score drops sharply from 0.95 when\ntransfer-setsubsetsrepresentingvariationsinbackground,scale,andclutter. the tote has one to five object instances to a low of 0.38\nwhen there are more than 26 object instances in the image. 500K+ annotations.",
      "size": 804,
      "sentences": 9
    },
    {
      "id": 24,
      "content": "setsubsetsrepresentingvariationsinbackground,scale,andclutter. the tote has one to five object instances to a low of 0.38\nwhen there are more than 26 object instances in the image. 500K+ annotations. The annotations include instance-level\nThis motivates developing algorithms that are robust against\nsegmentation masks and bounding box for two classes (ob-\nclutter and occlusion to further improve object segmentation\nject and container). Technicians with task-specific training\nperformance. generatedhigh-qualityannotationsforobjectboundariesand\nobject class which are verified by two additional quality\nassurance technicians. We divide the object segmentation dataset into three\nsubsets. The primary set, mix-object-tote, comprises 44,253\nimages and 467,225 annotations of objects in yellow and\nblue storage totes. The totes contain a heterogeneous clutter\nof objects with an average of 10.5 object segments (ranging\nfrom 1 to 50 segments) in each image. The other two sub- Fig.4.",
      "size": 982,
      "sentences": 10
    },
    {
      "id": 25,
      "content": "in yellow and\nblue storage totes. The totes contain a heterogeneous clutter\nof objects with an average of 10.5 object segments (ranging\nfrom 1 to 50 segments) in each image. The other two sub- Fig.4. Performanceonmix-object-totewithvaryingdegreeofclutter. sets, namely zoomed-out-tote-transfer-set and same-object-\nV. OBJECTIDENTIFICATION\ntransfer-set (Fig. 3(b) and (c)) enable us to understand the\nimpactofvariationindatadistribution.Thezoomed-out-tote- Objectidentification(ID)isthetaskofexactlyidentifying\ntransfer-setsubsetwith5,837imagesand43,401annotations an image segment as one of the objects within a database. captures images of containers from a different warehouse. In the robotic manipulation context, this task is applicable\nIt poses a transfer learning challenge due to significant both before and after picking the object. In the pre-pick\ndifferences in background, scale, and object distribution.",
      "size": 915,
      "sentences": 9
    },
    {
      "id": 26,
      "content": "this task is applicable\nIt poses a transfer learning challenge due to significant both before and after picking the object. In the pre-pick\ndifferences in background, scale, and object distribution. stage, identifying an object segment within the tote allows\nThe same-object-transfer-set subset contains 3,323 images accessing any stored models or attributes of the object\nand 12,664 annotations. It captures a common and visually from past experience which can be used for manipulation\nchallengingscenarioinwarehouseswheremultipleinstances planning purposes. In the post-pick stage, ID has access to\nof the same object are tightly packed in a container. the segment of the object being manipulated both within\nTo establish a performance baseline, we trained Mat- the tote as well as when it is attached to the robotic arm. terport’s implementation of Mask R-CNN [3], [33] with Accurately identifying which object is being transferred\nResNet-50 backbone [34] on the mix-object-tote dataset.",
      "size": 990,
      "sentences": 7
    },
    {
      "id": 27,
      "content": "is attached to the robotic arm. terport’s implementation of Mask R-CNN [3], [33] with Accurately identifying which object is being transferred\nResNet-50 backbone [34] on the mix-object-tote dataset. from one container to another is critical to tracking the\nDefault training schedule (for MS-COCO) and hyper- object within a warehouse, thereby maintaining a container-\nparameters were used along with a train-valid-test split of manifest. This also allows posing ID as an image retrieval\n0.7:0.15:0.15. Table I shows the results for our baseline challenge. The pick and transfer images with segments of\nexperiment. Mean average precision (mAP) for a threshold the target object (acquired using an instance segmentation\nof 0.5 (mAP ) and 0.75 (mAP ) are used to evaluate the algorithm)aretreatedasqueryimages.Thereferenceimages\n50 75\nperformance of the baseline model on test set. forallobjectsinthecontainermanifestaretreatedasgallery\nWe observe that applying model weights trained on mix- images.",
      "size": 996,
      "sentences": 8
    },
    {
      "id": 28,
      "content": "eryimages.Thereferenceimages\n50 75\nperformance of the baseline model on test set. forallobjectsinthecontainermanifestaretreatedasgallery\nWe observe that applying model weights trained on mix- images. The challenge is to compare the query images to\nobject-tote to the zoomed-out-tote-transfer-set (mAP = gallery images to find a match. 50\n0.25) and same-object-transfer-set subsets (mAP =0.11) The benchmark dataset for this task contains 235K+ la-\n50\nyields poor results. While techniques like transfer learning beledpickactivitiescorrespondingto190K+uniqueobjects. can improve performance on a new scenario when a rea- Each pick activity comprises one query image from the pick\nsonable amount of domain-specific labeled data is available, sceneanduptothreequeryimagesfromthetransferphase.A\nlabeling specifically for each variation is time-consuming, ground-truth ID annotation is automatically acquired using\n=== 페이지 5 ===\nmultiple barcode scanners that are placed in the workcell.",
      "size": 982,
      "sentences": 6
    },
    {
      "id": 29,
      "content": "phase.A\nlabeling specifically for each variation is time-consuming, ground-truth ID annotation is automatically acquired using\n=== 페이지 5 ===\nmultiple barcode scanners that are placed in the workcell. In cases where no barcode is scanned, a human operator\nmanually scans the barcode of the object after it ejects\nthe workcell. Pick activities are accompanied by a set of\nreferenceimagesforobjectsinthecontainermanifest.These\na) Packaging variation between the b) Certain object configurations in pick image (left)\nimages are captures of the object from previous operations query (left) and the gallery (right) might not be captured in gallery (middle). In-hand\nimages image (right) can be useful in such cases\nwithin the warehouse. While up to six reference images\nare sampled per object, reference images are not available\nfor some objects. Such cases are representative of scenarios\nwhen a new object is introduced into the warehouse.",
      "size": 935,
      "sentences": 6
    },
    {
      "id": 30,
      "content": "up to six reference images\nare sampled per object, reference images are not available\nfor some objects. Such cases are representative of scenarios\nwhen a new object is introduced into the warehouse. To\ntackle such cases, an ID algorithm needs to model some\nnotion of confidence and prevent false-positive prediction. A test set is sampled for evaluating baseline algorithms on c) Queried object (left) has no gallery image. d) Small variations in products can be present in the\nSimilar looking object (right) from the same container. In this case the only difference is\nthe dataset. This set contains 50,000 pick activities where at container is incorrectly selected. the coffee flavor marked in the center. least one reference image is available for the picked object. Another set (test-uncertainty) is derived from the test set by Fig.5. ChallengingcasesforObjectIdentification\nignoring reference images of the picked object for 20% of\nthe cases.",
      "size": 948,
      "sentences": 11
    },
    {
      "id": 31,
      "content": "r the picked object. Another set (test-uncertainty) is derived from the test set by Fig.5. ChallengingcasesforObjectIdentification\nignoring reference images of the picked object for 20% of\nthe cases. This set is used to evaluate the behavior of ID on\n16000 84 100\nnovel objects coming into the warehouse. 14000 82 recall@95 = 57.6% recall@95 = 30.8%\n12000 80 90\n10000 78 TABLEII 8000 76 80\nEVALUATINGTOP-KOBJECTRETRIEVAL 6000 74\n4000 72 70\n2000 70 recall@k(pre/post-pick) k=1 k=2 k=3\n0 68 60\nResNet50-RMAC[62] 71.7/72.2 81.9/82.9 87.2/88.2 >18 (16-18)(14-16)(12-14)(10-12) <10 0 10 20 30 40 50 60 70 80\nDINO-ViTS[11] 77.2/79.5 87.3/89.4 91.6/93.5\ntest-uncertainty\nResNet50-RMAC[62] 57.7/58.0 65.7/66.5 70.2/70.7\nDINO-ViTS[11] 61.9/63.6 69.9/71.6 73.3/74.8\nTable II shows results of object retrieval with baseline\nalgorithms on the two sets.",
      "size": 840,
      "sentences": 5
    },
    {
      "id": 32,
      "content": "/93.5\ntest-uncertainty\nResNet50-RMAC[62] 57.7/58.0 65.7/66.5 70.2/70.7\nDINO-ViTS[11] 61.9/63.6 69.9/71.6 73.3/74.8\nTable II shows results of object retrieval with baseline\nalgorithms on the two sets. For the first baseline, a 512d\nimage descriptor is extracted from a ResNet50 backbone\nvia aggregating features [62] pre-trained for classification\non ImageNet dataset [56]. The second baseline utilizes a\n384dfeaturevectorpre-trainedviaself-supervision[11]ona\nvision transformer. A cosine similarity is computed between\nfeature embeddings for query and gallery images to get the\nclosest match. Evaluation is performed both over pre-pick\n(pick image only) and post-pick (pick and transfer images)\nscenarios. Although transfer images are significantly differ-\nentintermsofpresentationtoreferenceimages,theyprovide\nmultiple views of the object which improves the overall\nretrievalrate.Fig.5showssomeofthechallengesassociated\nwith ID on this dataset.",
      "size": 945,
      "sentences": 6
    },
    {
      "id": 33,
      "content": "ly differ-\nentintermsofpresentationtoreferenceimages,theyprovide\nmultiple views of the object which improves the overall\nretrievalrate.Fig.5showssomeofthechallengesassociated\nwith ID on this dataset. Large variations in appearance for\nthesameobjectandthesimilaritiesbetweendifferentobjects\nmakes the dataset challenging. The challenge increases with\nthe size of container manifest as seen in Fig. 6(left). Fig. 6(right) shows the precision-recall curve obtained based on\na rank-ratio confidence metric computed as (1− c2), where\nc1\nc ,c are softmax probabilities corresponding to the first\n1 2\nand second ranked objects. The plot highlights recall rates\nat high precision values as mis-identifications can lead to\ncostly scenarios, such as an object getting lost within the\nwarehouse.",
      "size": 784,
      "sentences": 7
    },
    {
      "id": 34,
      "content": "irst\n1 2\nand second ranked objects. The plot highlights recall rates\nat high precision values as mis-identifications can lead to\ncostly scenarios, such as an object getting lost within the\nwarehouse. While methods like contrastive learning over the\ntraining set can improve the top-1 retrieval rate, achieving a\nhighrecallratewithintheprecisionconstraintswouldrequire\nmethods to perform uncertainty estimation and leverage\nadditional modalities such as text and dimensions. )%(\nnoisicerp\nrecall (%)\nsesac\n#\n)%( llacer\nrecall@99 = 7.8% recall@99 = 33.8%\n--o--recall\n--o--test --o--test-uncertainty\nContainer manifest size\nFig.6. Containermanifestsize(left)isindicativeofthenumberofimages\nthat the algorithm needs to select from. The precision-recall curve (right)\nshowstheneedforaconfidencemodeltopreventfalse-positivepredictions. VI. DEFECTDETECTION\nThe defect detection task is to identify if a robotic ma-\nnipulation activity resulted in a defect.",
      "size": 949,
      "sentences": 8
    },
    {
      "id": 35,
      "content": "urve (right)\nshowstheneedforaconfidencemodeltopreventfalse-positivepredictions. VI. DEFECTDETECTION\nThe defect detection task is to identify if a robotic ma-\nnipulation activity resulted in a defect. Two types of robot-\ninduceddefectsareincludedinthedataset:1)multi-pick,and\n2) package-defect. Multi-pick is used to describe activities\nwhere multiple objects were picked and transferred from\nthe source container to the destination container. Package-\ndefect is used to describe activities where the object pack-\naging opened and/or the object separated into multiple parts\n(deconstruction). Two subclasses, open and deconstruction,\nare defined for package-defect. Fig. 7 shows examples of\nmulti-pick and package-defect in our dataset. Multi-pick are\noften observed when there is a high degree of clutter, there\nare multiple instances of the same object, or when objects\nof significantly different sizes are placed together. Fig. 7 (a-\nc) shows package-defect on a variety of objects.",
      "size": 984,
      "sentences": 12
    },
    {
      "id": 36,
      "content": "h degree of clutter, there\nare multiple instances of the same object, or when objects\nof significantly different sizes are placed together. Fig. 7 (a-\nc) shows package-defect on a variety of objects. Defects on\ndeformable objects like plastic bags can be challenging for\nvisual detection. Our dataset comprises 19,303 images of objects from\nmultiple viewpoints (Transfer-images) and 4,070 videos of\npick-and-place activities that resulted in a defect. Videos\nare excluded from our dataset for multi-pick defect as such\ndefects are not observable along specific viewpoints. Multi-\nview Transfer-images are best suited to detect multi-pick\ndefect. On the other hand, open and deconstruction defects\ncan happen at any time during an activity.",
      "size": 739,
      "sentences": 8
    },
    {
      "id": 37,
      "content": "vable along specific viewpoints. Multi-\nview Transfer-images are best suited to detect multi-pick\ndefect. On the other hand, open and deconstruction defects\ncan happen at any time during an activity. As a result, they\narebestcapturedusingvideos.Thedatasetincludes100,000\n[표 데이터 감지됨]\n\n=== 페이지 6 ===\nTABLEIII\nBASELINEFORSINGLE-VIEWIMAGEDEFECTDETECTION\nmodel metric multi-pick package-defect combined\ncount 7,813 11,490 19,303\nResNet-50[34] recall 0.34 0.73 0.57\nfpr 0.05 0.05 0.05\nTABLEIV\nBASELINEFORVIDEODEFECTDETECTION\nmodel metric open deconstruction combined\ncount 2,951 2,165 4,070\nFig.7. Multi-viewimagesinthedefectdetectiondatasetshowing(a)–(c) MViT-B[26] recall 0.69 0.79 0.73\npackage-defectand(d)–(f)multi-pickdefectfordifferenttypesofobjects. fpr 0.23 0.03 0.13\nimagesofobjectsandvideosofactivitiesthatdonothaveany\ndefects and are defined as nominal activities. Tables III and picks are a harder to detect than package-defects. On the\nIV shows the distribution of defect types in our dataset.",
      "size": 1000,
      "sentences": 8
    },
    {
      "id": 38,
      "content": "itiesthatdonothaveany\ndefects and are defined as nominal activities. Tables III and picks are a harder to detect than package-defects. On the\nIV shows the distribution of defect types in our dataset. In other hand, results for video defect detection show that\naddition to Transfer-images, the dataset includes Pick-image open defects are harder to detect than deconstruction. There\nand Place-image that provide context for an activity. is significant scope for improvement in defect detection\nA two-step process was used to annotate data. A tech- methods to be effective in warehouses operations which\nnician operating our system labeled each activity as suc- typically require high recall (>0.95) and low fpr (<0.01). cessful/nominal (a single object transferred from the source\ncontainer to the destination container), multi-pick, open, or VII. DISCUSSIONANDFUTUREWORK\ndeconstruction defect.",
      "size": 893,
      "sentences": 9
    },
    {
      "id": 39,
      "content": "95) and low fpr (<0.01). cessful/nominal (a single object transferred from the source\ncontainer to the destination container), multi-pick, open, or VII. DISCUSSIONANDFUTUREWORK\ndeconstruction defect. Expert annotators verified the anno-\nIn this work we introduced ARMBench, a large-scale,\ntations for each activity and augmented the annotations for\nobject-centric benchmark dataset for robotic manipulation\nTransfer-images as multi-pick, or package-defect if a defect\nin warehouses. The object segmentation benchmark presents\nwas observable, and as nominal if no defect was observable\nchallenges relating to clutter, deformable and transparent\nin the image. In addition to the defect type, we also provide\npackaging as well as the problem of degrading performance\nsegmentationpolygonsfortheobjectstoenabledevelopment\nwith different backgrounds and storage configurations. The\nof models that can benefit from additional attention cues.",
      "size": 934,
      "sentences": 7
    },
    {
      "id": 40,
      "content": "lem of degrading performance\nsegmentationpolygonsfortheobjectstoenabledevelopment\nwith different backgrounds and storage configurations. The\nof models that can benefit from additional attention cues. identification benchmark presents an open set recognition\nFor video annotations, expert annotators verified the type of\nchallenge on a wide variety of objects. Additionally, images\npackage defect, i.e., open and deconstruction, observed in\nof the same object can vary significantly due to differences\nthe video. In addition to the type of defect observed in each\nin configurations and packaging variations while images of\nvideo, the index of the first frame where a defect becomes\ntwo different objects can appear similar. Missing reference\nobservable is also provided to enable development of real-\nimagesandhighprecisionrequirementmakesthebenchmark\ntime defect detection methods. well suited to evaluate uncertainty estimation algorithms.",
      "size": 940,
      "sentences": 7
    },
    {
      "id": 41,
      "content": "observable is also provided to enable development of real-\nimagesandhighprecisionrequirementmakesthebenchmark\ntime defect detection methods. well suited to evaluate uncertainty estimation algorithms. To establish a baseline for defect detection, we performed\nFinally, the defect detection benchmark presents a unique\ntwo experiments. In the first experiment, we train an image\nset of challenges such as detection of multi-pick, opening,\nclassifier with ResNet-50 [34] backbone, global average\nand deconstruction of packages. Annotations and baselines\npooling, and focal loss for predicting the type of defect\nare provided both for a single-shot as well as video-based\nobserved in the Transfer-images. In the second experiment,\ndetection of such events. we trained a multi-scale vision transformer model (MViT-B)\nOur intention is for this dataset to grow over time with a\n[26] for action classification on videos.",
      "size": 912,
      "sentences": 7
    },
    {
      "id": 42,
      "content": "d experiment,\ndetection of such events. we trained a multi-scale vision transformer model (MViT-B)\nOur intention is for this dataset to grow over time with a\n[26] for action classification on videos. Since a defect can\ngoaltoincreasethenumberofuniqueobjects,environments,\nbe introduced at any time during the manipulation process,\nand benchmark tasks. Large-scale sensor data and fine-\nwe uniformly sampled 32 frames (∼5FPS) from each video\ngrained attributes of objects will enable learning general-\nfor training. The classification head outputs a two-channel\nizable representations that could transfer to other visual\nvector that predicts binary classification on two categories:\nperception tasks. We further plan to enrich our dataset with\nopenanddeconstruction.Weusedatrain-testsplitof0.7:0.3\n3Ddataandannotations,andproposenewbenchmarktasks. formulti-pickandpackage-defects.Thenominalcategoryin\nthetrainsetwasdownsampledtomatchthesizeofthedefect\nVIII.",
      "size": 956,
      "sentences": 7
    },
    {
      "id": 43,
      "content": "ruction.Weusedatrain-testsplitof0.7:0.3\n3Ddataandannotations,andproposenewbenchmarktasks. formulti-pickandpackage-defects.Thenominalcategoryin\nthetrainsetwasdownsampledtomatchthesizeofthedefect\nVIII. ACKNOWLEDGEMENTS\ncategory to compensate for class imbalance. 10,000 samples\nfrom the nominal category were added to our test set. We would like to thank the Sparrow [1] team members\nTableIIIandIVshowperformanceofbaselinemodelsfor fordeploymentandoperationoftheroboticworkcell,Aalekh\ndefect detection on images and videos. We used recall and (Raj)RayChaudhuryandtheGo-AIteamfordataannotation\nfalse positive rate (fpr) as metrics to evaluate performance support and the Item-matrix team for curating the reference\nover defect classes. A missed defect (lower recall) is more image dataset. We would also like to thank Joey Durham,\nexpensive than classifying a nominal activity as defective Andy Marchese, Clay Flannigan, Parris Wellman, Jane Shi\n(fpr).",
      "size": 949,
      "sentences": 8
    },
    {
      "id": 44,
      "content": "lower recall) is more image dataset. We would also like to thank Joey Durham,\nexpensive than classifying a nominal activity as defective Andy Marchese, Clay Flannigan, Parris Wellman, Jane Shi\n(fpr). Results for image defect detection shows that multi- and Kapil Katyal for their valuable feedback. [표 데이터 감지됨]\n\n=== 페이지 7 ===\nREFERENCES [20] ——,“Segmentingunknown3dobjectsfromrealdepthimagesusing\nmask r-cnn trained on synthetic data,” in Proc. IEEE Int. Conf. RoboticsandAutomation(ICRA),2019. [1] “Amazon introduces sparrow—a state-of-the-art robot that handles\n[21] B. De Brabandere, D. Neven, and L. Van Gool, “Semantic instance\nmillionsofdiverseproducts,”https://tinyurl.com/2p8h4w7v. segmentation for autonomous driving,” in 2017 IEEE Conference on\n[2] “DAGM2007,”July2022,[Online;accessed12.Jul.2022].[Online]. ComputerVisionandPatternRecognitionWorkshops(CVPRW),2017,\nAvailable:https://conferences.mpi-inf.mpg.de/dagm/2007/prizes.html\npp.478–480.",
      "size": 954,
      "sentences": 11
    },
    {
      "id": 45,
      "content": "[2] “DAGM2007,”July2022,[Online;accessed12.Jul.2022].[Online]. ComputerVisionandPatternRecognitionWorkshops(CVPRW),2017,\nAvailable:https://conferences.mpi-inf.mpg.de/dagm/2007/prizes.html\npp.478–480. [3] W. Abdulla, “Mask R-CNN for object detection and instance seg-\n[22] A.Diba,M.Fayyaz,V.Sharma,M.Paluri,J.Gall,R.Stiefelhagen,and\nmentation on keras and tensorflow,” https://github.com/matterport/\nL. V. Gool, “Large scale holistic video understanding,” in European\nMaskRCNN,2017. ConferenceonComputerVision. Springer,2020,pp.593–610. [4] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, “Neural\n[23] L.Downs,A.Francis,N.Koenig,B.Kinman,R.Hickman,K.Rey-\ncodesforimageretrieval,”inEuropeanconferenceoncomputervision. mann,T.B.McHugh,andV.Vanhoucke,“Googlescannedobjects:A\nSpringer,2014,pp.584–599. high-quality dataset of 3d scanned household items,” arXiv preprint\n[5] Y. Bai, Y. Chen, W. Yu, L. Wang, and W. Zhang, “Products-\narXiv:2204.11918,2022.",
      "size": 957,
      "sentences": 8
    },
    {
      "id": 46,
      "content": "oglescannedobjects:A\nSpringer,2014,pp.584–599. high-quality dataset of 3d scanned household items,” arXiv preprint\n[5] Y. Bai, Y. Chen, W. Yu, L. Wang, and W. Zhang, “Products-\narXiv:2204.11918,2022. 10k: A large-scale product recognition dataset,” arXiv preprint\n[24] C.Eppner,S.Ho¨fer,R.Jonschkowski,R.Mart´ın-Mart´ın,A.Sieverling,\narXiv:2008.10545,2020. V.Wall,andO.Brock,“Lessonsfromtheamazonpickingchallenge:\n[6] Y.Bao,K.Song,J.Liu,Y.Wang,Y.Yan,H.Yu,andX.Li,“Triplet-\nFouraspectsofbuildingroboticsystems,”inIJCAI,2017. graph reasoning network for few-shot metal generic surface defect\n[25] M.Everingham,L.V.Gool,C.K.I.Williams,J.Winn,andA.Zisser-\nsegmentation,” IEEE Transactions on Instrumentation and Measure-\nman,“Thepascalvisualobjectclasses(voc)challenge,”International\nment,vol.70,pp.1–11,2021. Journal of Computer Vision, vol. 88, pp. 303–308, September 2009,\n[7] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, “MVTec printedversionpublicationdate:June2010.",
      "size": 976,
      "sentences": 8
    },
    {
      "id": 47,
      "content": "al\nment,vol.70,pp.1–11,2021. Journal of Computer Vision, vol. 88, pp. 303–308, September 2009,\n[7] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, “MVTec printedversionpublicationdate:June2010. AD–A comprehensive real-world dataset for unsupervised anomaly\n[26] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and\ndetection,” in 2019 IEEE/CVF Conference on Computer Vision and\nC. Feichtenhofer, “Multiscale vision transformers,” 2021 IEEE/CVF\nPatternRecognition(CVPR),2019,pp.9584–9592. InternationalConferenceonComputerVision(ICCV),pp.6804–6815,\n[8] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton, and 2021. C. Rother, “Learning 6d object pose estimation using 3d object [27] C. Fellbaum, Ed., WordNet: An Electronic Lexical Database, ser. coordinates,”inECCV,2014. Language,Speech,andCommunication. Cambridge,MA:MITPress,\n[9] B. Calli, A. Dollar, M. A. Roa, S. Srinivasa, and Y.",
      "size": 905,
      "sentences": 11
    },
    {
      "id": 48,
      "content": "ellbaum, Ed., WordNet: An Electronic Lexical Database, ser. coordinates,”inECCV,2014. Language,Speech,andCommunication. Cambridge,MA:MITPress,\n[9] B. Calli, A. Dollar, M. A. Roa, S. Srinivasa, and Y. Sun, “Guest 1998.\neditorial:Introductiontothespecialissueonbenchmarkingprotocols [28] P. Follmann, T. Bo¨ttger, P. Ha¨rtinger, R. Ko¨nig, and M. Ulrich,\nfor robotic manipulation,” IEEE Robotics and Automation Letters, “MVTec D2S: densely segmented supermarket dataset,” CoRR, vol. vol.6,no.4,pp.8678–8680,2021. abs/1804.08292, 2018. [Online]. Available: http://arxiv.org/abs/1804. [10] B. Calli, A. Singh, J. Bruce, A. Walsman, K. Konolige, S. Srini- 08292\nvasa, P. Abbeel, and A. M. Dollar, “Yale-cmu-berkeley dataset for [29] V. Gajjar, A. Gurnani, and Y. Khandhediya, “Human detection and\nroboticmanipulationresearch,”TheInternationalJournalofRobotics trackingforvideosurveillanceacognitivescienceapproach,”2017. Research,vol.36,no.3,pp.261–268,2017.",
      "size": 953,
      "sentences": 12
    },
    {
      "id": 49,
      "content": "Y. Khandhediya, “Human detection and\nroboticmanipulationresearch,”TheInternationalJournalofRobotics trackingforvideosurveillanceacognitivescienceapproach,”2017. Research,vol.36,no.3,pp.261–268,2017. [30] N. Garcia and G. Vogiatzis, “Learning non-metric visual similarity\n[11] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski, forimageretrieval,”ImageandVisionComputing,vol.82,pp.18–25,\nand A. Joulin, “Emerging properties in self-supervised vision trans- 2019.\nformers,”inProceedingsoftheInternationalConferenceonComputer [31] Y. Ge, R. Zhang, X. Wang, X. Tang, and P. Luo, “Deepfashion2: A\nVision(ICCV),2021. versatilebenchmarkfordetection,poseestimation,segmentationand\n[12] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, re-identificationofclothingimages,”inProceedingsoftheIEEE/CVF\nZ. Li, S. Savarese, M. Savva, S. Song, H. Su, et al., conference on computer vision and pattern recognition, 2019, pp. “Shapenet:Aninformation-rich3dmodelrepository,”arXivpreprint 5337–5345.",
      "size": 999,
      "sentences": 4
    },
    {
      "id": 50,
      "content": "IEEE/CVF\nZ. Li, S. Savarese, M. Savva, S. Song, H. Su, et al., conference on computer vision and pattern recognition, 2019, pp. “Shapenet:Aninformation-rich3dmodelrepository,”arXivpreprint 5337–5345. arXiv:1512.03012,2015. [32] M. Grard, L. Chen, and E. Dellandre´a, “Bicameral structuring and\n[13] L. Cheng, X. Zhou, L. Zhao, D. Li, H. Shang, Y. Zheng, P. Pan, synthetic imagery for jointly predicting instance boundaries and\nand Y. Xu, “Weakly supervised learning with side information for nearbyocclusionsfromasingleimage,”CoRR,vol.abs/1906.07480,\nnoisylabeledimages,”inEuropeanConferenceonComputerVision. 2019. [Online].Available:http://arxiv.org/abs/1906.07480\nSpringer,2020,pp.306–321. [33] K. He, G. Gkioxari, P. Dolla´r, and R. B. Girshick, “Mask\n[14] D.Colling,J.Dziedzitz,K.Furmans,P.Hopfgarten,andK.Markert, R-CNN,” CoRR, vol. abs/1703.06870, 2017. [Online]. Available:\n“Progress in autonomous picking as demonstrated by the amazon http://arxiv.org/abs/1703.06870\nroboticchallenge,”2018.",
      "size": 998,
      "sentences": 10
    },
    {
      "id": 51,
      "content": "arten,andK.Markert, R-CNN,” CoRR, vol. abs/1703.06870, 2017. [Online]. Available:\n“Progress in autonomous picking as demonstrated by the amazon http://arxiv.org/abs/1703.06870\nroboticchallenge,”2018. [34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\n[15] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, for image recognition,” CoRR, vol. abs/1512.03385, 2015. [Online]. X. Zhang, T. F. Y. Vicente, T. Dideriksen, H. Arora, et al., “Abo: Available:http://arxiv.org/abs/1512.03385\nDataset and benchmarks for real-world 3d object understanding,” in [35] T. Hodan, P. Haluza, S. Obdrza´lek, J. Matas, M. I. A. Lourakis,\nProceedings of the IEEE/CVF Conference on Computer Vision and and X. Zabulis, “T-less: An rgb-d dataset for 6d pose estimation of\nPatternRecognition,2022,pp.21126–21136. texture-less objects,” 2017 IEEE Winter Conference on Applications\n[16] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, ofComputerVision(WACV),pp.880–888,2017.",
      "size": 983,
      "sentences": 11
    },
    {
      "id": 52,
      "content": "cognition,2022,pp.21126–21136. texture-less objects,” 2017 IEEE Winter Conference on Applications\n[16] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, ofComputerVision(WACV),pp.880–888,2017. R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes [36] T.Hodan,F.Michel,E.Brachmann,W.Kehl,A.GlentBuch,D.Kraft,\ndataset for semantic urban scene understanding,” CoRR, vol. B.Drost,J.Vidal,S.Ihrke,X.Zabulis,etal.,“Bop:Benchmarkfor\nabs/1604.01685, 2016. [Online]. Available: http://arxiv.org/abs/1604. 6dobjectposeestimation,”inProceedingsoftheEuropeanconference\n01685 oncomputervision(ECCV),2018,pp.19–34. [17] N.Correll,K.E.Bekris,D.Berenson,O.Brock,A.Causo,K.Hauser, [37] H. Je´gou, M. Douze, C. Schmid, and P. Pe´rez, “Aggregating local\nK.Okada,A.Rodriguez,J.M.Romano,andP.R.Wurman,“Analysis descriptorsintoacompactimagerepresentation,”in2010IEEEcom-\nand observations from the first amazon picking challenge,” IEEE putersocietyconferenceoncomputervisionandpatternrecognition.",
      "size": 992,
      "sentences": 8
    },
    {
      "id": 53,
      "content": "urman,“Analysis descriptorsintoacompactimagerepresentation,”in2010IEEEcom-\nand observations from the first amazon picking challenge,” IEEE putersocietyconferenceoncomputervisionandpatternrecognition. TransactionsonAutomationScienceandEngineering,vol.15,no.1, IEEE,2010,pp.3304–3311. pp.172–188,2016. [38] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and\n[18] N.Correll,K.E.Bekris,D.Berenson,O.Brock,A.J.Causo,K.K. L.Fei-Fei,“Large-scalevideoclassificationwithconvolutionalneural\nHauser,K.Okada,A.Rodriguez,J.M.Romano,andP.R.Wurman, networks,”inProceedingsoftheIEEEconferenceonComputerVision\n“Analysisandobservationsfromthefirstamazonpickingchallenge,” andPatternRecognition,2014,pp.1725–1732. IEEETransactionsonAutomationScienceandEngineering,vol.15, [39] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-\npp.172–188,2018.",
      "size": 856,
      "sentences": 6
    },
    {
      "id": 54,
      "content": "challenge,” andPatternRecognition,2014,pp.1725–1732. IEEETransactionsonAutomationScienceandEngineering,vol.15, [39] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-\npp.172–188,2018. narasimhan,F.Viola,T.Green,T.Back,P.Natsev,etal.,“Thekinetics\n[19] M. Danielczuk, M. Matl, S. Gupta, A. Li, A. Lee, J. Mahler, humanactionvideodataset,”arXivpreprintarXiv:1705.06950,2017.\nand K. Goldberg, “Segmenting unknown 3d objects from real [40] F.Landi,C.G.Snoek,andR.Cucchiara,“Anomalylocalityinvideo\ndepth images using mask R-CNN trained on synthetic point surveillance,”arXivpreprintarXiv:1901.10364,2019.\nclouds,” CoRR, vol. abs/1809.05825, 2018. [Online]. Available: [41] J. Leitner, A. W. Tow, N. Su¨nderhauf, J. E. Dean, J. W. Durham,\nhttp://arxiv.org/abs/1809.05825 M. Cooper, M. Eich, C. F. Lehnert, R. Mangels, C. McCool, P. T.\n=== 페이지 8 ===\nKujala, L. Nicholson, T. T. Pham, J.",
      "size": 894,
      "sentences": 6
    },
    {
      "id": 55,
      "content": "A. W. Tow, N. Su¨nderhauf, J. E. Dean, J. W. Durham,\nhttp://arxiv.org/abs/1809.05825 M. Cooper, M. Eich, C. F. Lehnert, R. Mangels, C. McCool, P. T.\n=== 페이지 8 ===\nKujala, L. Nicholson, T. T. Pham, J. Sergeant, L. Wu, F. Zhang, [62] G. Tolias, R. Sicre, and H. Je´gou, “Particular object retrieval\nB. Upcroft, and P. Corke, “The acrv picking benchmark: A robotic with integral max-pooling of cnn activations,” arXiv preprint\nshelfpickingbenchmarktofosterreproducibleresearch,”2017IEEE arXiv:1511.05879,2015.\nInternational Conference on Robotics and Automation (ICRA), pp. [63] J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox, and\n4705–4712,2017. S. Birchfield, “Deep object pose estimation for semantic robotic\n[42] W. Li, V. Mahadevan, and N. Vasconcelos, “Anomaly detection and grasping of household objects,” CoRR, vol.",
      "size": 828,
      "sentences": 4
    },
    {
      "id": 56,
      "content": "x, and\n4705–4712,2017. S. Birchfield, “Deep object pose estimation for semantic robotic\n[42] W. Li, V. Mahadevan, and N. Vasconcelos, “Anomaly detection and grasping of household objects,” CoRR, vol. abs/1809.10790, 2018.\nlocalizationincrowdedscenes,”IEEEtransactionsonpatternanalysis [Online].Available:http://arxiv.org/abs/1809.10790\nandmachineintelligence,vol.36,no.1,pp.18–32,2013. [64] X.-S.Wei,Q.Cui,L.Yang,P.Wang,andL.Liu,“Rpc:Alarge-scale\n[43] H. Liao, T. Inomata, I. Sakuma, and T. Dohi, “3-d augmented retail product checkout dataset,” arXiv preprint arXiv:1901.07249,\nrealityfor mri-guidedsurgeryusingintegral videographyautostereo- 2019.\nscopicimageoverlay,”IEEETransactionsonBiomedicalEngineering, [65] T. Weyand, A. Araujo, B. Cao, and J. Sim, “Google landmarks\nvol.57,no.6,pp.1476–1486,2010.",
      "size": 806,
      "sentences": 4
    },
    {
      "id": 57,
      "content": "usingintegral videographyautostereo- 2019.\nscopicimageoverlay,”IEEETransactionsonBiomedicalEngineering, [65] T. Weyand, A. Araujo, B. Cao, and J. Sim, “Google landmarks\nvol.57,no.6,pp.1476–1486,2010. datasetv2-alarge-scalebenchmarkforinstance-levelrecognitionand\n[44] H. Lin, B. Li, X. Wang, Y. Shu, and S. Niu, “Automated defect retrieval,” in Proceedings of the IEEE/CVF conference on computer\ninspection of LED chip using deep convolutional neural network,” visionandpatternrecognition,2020,pp.2575–2584. Journal of Intelligent Manufacturing, vol. 30, no. 6, pp. 2525–2534, [66] Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox, “Posecnn: A\n2019. convolutionalneuralnetworkfor6dobjectposeestimationincluttered\n[45] T.-Y.Lin,M.Maire,S.J.Belongie,J.Hays,P.Perona,D.Ramanan, scenes,”ArXiv,vol.abs/1711.00199,2018. P. Dolla´r, and C. L. Zitnick, “Microsoft coco: Common objects in [67] C. Xie, Y. Xiang, A. Mousavian, and D. Fox, “Unseen object in-\ncontext,”inECCV,2014.",
      "size": 967,
      "sentences": 7
    },
    {
      "id": 58,
      "content": "nan, scenes,”ArXiv,vol.abs/1711.00199,2018. P. Dolla´r, and C. L. Zitnick, “Microsoft coco: Common objects in [67] C. Xie, Y. Xiang, A. Mousavian, and D. Fox, “Unseen object in-\ncontext,”inECCV,2014. stancesegmentationforroboticenvironments,”IEEETransactionson\n[46] W. Liu, D. L. W. Luo, and S. Gao, “Future frame prediction for Robotics,vol.37,no.5,pp.1343–1359,2021. anomaly detection – a new baseline,” in 2018 IEEE Conference on [68] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, and\nComputerVisionandPatternRecognition(CVPR),2018. J. Xiao, “Multi-view self-supervised deep learning for 6d pose esti-\n[47] Z.Liu,P.Luo,S.Qiu,X.Wang,andX.Tang,“Deepfashion:Powering mationintheamazonpickingchallenge,”in2017IEEEinternational\nrobust clothes recognition and retrieval with rich annotations,” in conference on robotics and automation (ICRA). IEEE, 2017, pp. Proceedings of the IEEE conference on computer vision and pattern 1386–1383. recognition,2016,pp.1096–1104.",
      "size": 978,
      "sentences": 8
    },
    {
      "id": 59,
      "content": "al with rich annotations,” in conference on robotics and automation (ICRA). IEEE, 2017, pp. Proceedings of the IEEE conference on computer vision and pattern 1386–1383. recognition,2016,pp.1096–1104. [69] Z.Zhang,S.Fidler,andR.Urtasun,“Instance-levelsegmentationfor\n[48] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, autonomousdrivingwithdeepdenselyconnectedmrfs,”2016. J. A. Ojea, and K. Goldberg, “Dex-net 2.0: Deep learning to\nplan robust grasps with synthetic point clouds and analytic grasp\nmetrics,” CoRR, vol. abs/1703.09312, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1703.09312\n[49] H. Oh Song, Y. Xiang, S. Jegelka, and S. Savarese, “Deep metric\nlearningvialiftedstructuredfeatureembedding,”inProceedingsofthe\nIEEE conference on computer vision and pattern recognition, 2016,\npp.4004–4012. [50] J. Peng, C. Xiao, and Y. Li, “Rp2k: A large-scale retail prod-\nuct dataset for fine-grained image classification,” arXiv preprint\narXiv:2006.12634,2020. [51] J. Philbin, O.",
      "size": 995,
      "sentences": 12
    },
    {
      "id": 60,
      "content": ", 2016,\npp.4004–4012. [50] J. Peng, C. Xiao, and Y. Li, “Rp2k: A large-scale retail prod-\nuct dataset for fine-grained image classification,” arXiv preprint\narXiv:2006.12634,2020. [51] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object\nretrieval with large vocabularies and fast spatial matching,” in 2007\nIEEEconferenceoncomputervisionandpatternrecognition. IEEE,\n2007,pp.1–8. [52] ——, “Lost in quantization: Improving particular object retrieval in\nlarge scale image databases,” in 2008 IEEE conference on computer\nvisionandpatternrecognition. IEEE,2008,pp.1–8. [53] F. Radenovic´, G. Tolias, and O. Chum, “Fine-tuning cnn image\nretrieval with no human annotation,” IEEE transactions on pattern\nanalysisandmachineintelligence,vol.41,no.7,pp.1655–1668,2018. [54] M. Ren and R. S. Zemel, “End-to-end instance segmentation and\ncountingwithrecurrentattention,”CoRR,vol.abs/1605.09410,2016.",
      "size": 903,
      "sentences": 10
    },
    {
      "id": 61,
      "content": "pattern\nanalysisandmachineintelligence,vol.41,no.7,pp.1655–1668,2018. [54] M. Ren and R. S. Zemel, “End-to-end instance segmentation and\ncountingwithrecurrentattention,”CoRR,vol.abs/1605.09410,2016. [Online].Available:http://arxiv.org/abs/1605.09410\n[55] C. Rennie, R. Shome, K. E. Bekris, and A. F. de Souza, “A dataset\nfor improved rgbd-based object detection and pose estimation for\nwarehouse pick-and-place,” IEEE Robotics and Automation Letters,\nvol.1,pp.1179–1185,2016. [56] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al., “Imagenet\nlarge scale visual recognition challenge,” International journal of\ncomputervision,vol.115,no.3,pp.211–252,2015. [57] N. O. Salscheider, “Object tracking by detection with visual and\nmotioncues,”CoRR,vol.abs/2101.07549,2021.",
      "size": 834,
      "sentences": 5
    },
    {
      "id": 62,
      "content": "ition challenge,” International journal of\ncomputervision,vol.115,no.3,pp.211–252,2015. [57] N. O. Salscheider, “Object tracking by detection with visual and\nmotioncues,”CoRR,vol.abs/2101.07549,2021. [Online].Available:\nhttps://arxiv.org/abs/2101.07549\n[58] J. Silvestre-Blanes, T. Albero-Albero, I. Miralles, R. Pe´rez-Llorens,\nandJ.Moreno,“Apublicfabricdatabasefordefectdetectionmethods\nand results,” Autex Research Journal, vol. 19, no. 4, pp. 363–374,\n2019. [Online].Available:https://doi.org/10.2478/aut-2019-0035\n[59] J. Sivic and A. Zisserman, “Video google: A text retrieval approach\ntoobjectmatchinginvideos,”innull. IEEE,2003,p.1470. [60] G. Tolias, Y. Avrithis, and H. Je´gou, “Image search with selective\nmatchkernels:aggregationacrosssingleandmultipleimages,”Inter-\nnational Journal of Computer Vision, vol. 116, no. 3, pp. 247–261,\n2016. [61] G. Tolias, T. Jenicek, and O.",
      "size": 886,
      "sentences": 13
    },
    {
      "id": 63,
      "content": "“Image search with selective\nmatchkernels:aggregationacrosssingleandmultipleimages,”Inter-\nnational Journal of Computer Vision, vol. 116, no. 3, pp. 247–261,\n2016. [61] G. Tolias, T. Jenicek, and O. Chum, “Learning and aggregating\ndeep local descriptors for instance-level recognition,” arXiv preprint\narXiv:2007.13172,2020.",
      "size": 324,
      "sentences": 6
    }
  ]
}