{
  "source": "ArXiv",
  "filename": "030_The_Multi-Agent_Pickup_and_Delivery_Problem__MAPF,.pdf",
  "total_chars": 32055,
  "total_chunks": 46,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nTHE MULTI-AGENT PICKUP AND DELIVERY PROB-\nLEM: MAPF, MARL AND ITS WAREHOUSE APPLICA-\nTIONS\nTimTsz-KitLau BiswaSengupta\nDepartmentofStatistics ZebraTechnologies\nNorthwesternUniversity EndeavourHouse,ShaftsburyAvenue,London,UK\ntimlautk@u.northwestern.edu biswa.sengupta@zebra.com\nABSTRACT\nWe study two state-of-the-art solutions to the multi-agent pickup and delivery\n(MAPD)problembasedondifferentprinciples—multi-agentpath-finding(MAPF)\nand multi-agent reinforcement learning (MARL). Specifically, a recent MAPF\nalgorithmcalledconflict-basedsearch(CBS)andacurrentMARLalgorithmcalled\nshared experience actor-critic (SEAC) are studied. While the performance of\nthesealgorithmsismeasuredusingquitedifferentmetricsintheirseparatelinesof\nwork,weaimtobenchmarkthesetwomethodscomprehensivelyinasimulated\nwarehouseautomationenvironment.",
      "size": 841,
      "sentences": 3
    },
    {
      "id": 2,
      "content": "ied. While the performance of\nthesealgorithmsismeasuredusingquitedifferentmetricsintheirseparatelinesof\nwork,weaimtobenchmarkthesetwomethodscomprehensivelyinasimulated\nwarehouseautomationenvironment. 1 INTRODUCTION\nThe multi-agent pickup and delivery (MAPD) problem in various industrial applications such as\nwarehouseautomationhaslongbeenacentralproblemtostudyinartificialintelligenceduetoits\nwidespreadreal-worldapplications. IntheMAPDproblem,variousagentsoperateinaneveryday\nenvironmentinamulti-agentsystem. Eachofthempicksupanewitemintherequestqueueand\ndelivers it to a designated delivery location. To execute these tasks, the agents need to travel in\nthe environment via their collision-free paths. To be more precise, each agent has to move from\nitscurrentlocationtothepickuplocationofarequesteditemintherequestqueueandtraveltothe\ndeliverylocationoftheitemafterthepickup. Two major approaches to tackle MAPD are multi-agent path-finding (MAPF) and multi-agent\nreinforcement learning (MARL).",
      "size": 997,
      "sentences": 8
    },
    {
      "id": 3,
      "content": "steditemintherequestqueueandtraveltothe\ndeliverylocationoftheitemafterthepickup. Two major approaches to tackle MAPD are multi-agent path-finding (MAPF) and multi-agent\nreinforcement learning (MARL). MAPF, as a more traditional solution to the MAPD problem,\ninvolvescomputingcollision-freepathsformanyagentsgiventhecurrentstatesoftheagents(e.g.,\ntheirlocations)andarepresentationoftheenvironment(e.g.,thepickupanddeliverylocationsand\nobstacles present in the environment). MAPF also finds applications in computer games, traffic\nmanagementandairportschedules. MostmethodsforsolvingtheMAPDproblemintheliterature\narebasedonMAPF,sosomehow,MAPDandMAPFareviewedasthesameproblem. Anoticeable\ndifferencebetweenthemisthatavanillaMAPFproblemassumestheplanningproblemissingle-shot,\ni.e.,theagentswillstayattheirgoallocationsoncetheyhavearrived.",
      "size": 834,
      "sentences": 6
    },
    {
      "id": 4,
      "content": ",MAPDandMAPFareviewedasthesameproblem. Anoticeable\ndifferencebetweenthemisthatavanillaMAPFproblemassumestheplanningproblemissingle-shot,\ni.e.,theagentswillstayattheirgoallocationsoncetheyhavearrived. Incontrast,anMAPDproblem\nis usually lifelong, i.e., an agent will begin delivery once after a pickup and will travel to a new\nrequesteditemlocationonceithasfinishedadelivery. ThispaperaimstotackletheMAPDproblem\nwiththemorerealisticlifelongMAPFapproaches. Due to the recent interest in MARL, the MAPD problem is also solved with MARL algorithms. However,itisgenerallyviewedasabenchmarkproblemtoshowcasetheefficiencyofnewlyproposed\nMARLalgorithmsinsteadofaspecificproblemtostudy. BasedontheformulationoftheMARL\nproblems,MARLalgorithmstackletheMAPDprobleminacompletelydifferentflavour. Theydo\nnotcomputeafullcollision-freepathbutinsteadlearnagents’policiesthatchooseactionsofthe\nagentsgiventheircurrentobservationsoftheenvironment.",
      "size": 928,
      "sentences": 8
    },
    {
      "id": 5,
      "content": "hmstackletheMAPDprobleminacompletelydifferentflavour. Theydo\nnotcomputeafullcollision-freepathbutinsteadlearnagents’policiesthatchooseactionsofthe\nagentsgiventheircurrentobservationsoftheenvironment. Theseagents’policiesarelearnedto\nmaximizetheagents’cumulative(discounted)rewards,whichareusuallyassignedwhentheagents\nhavesuccessfullypickedupordeliveredanitem. ThequalitiesoftheabovetwotypesofMAPDsolutionsareusuallyassessedusingverydifferent\nmetricsintheirlinesofwork. TheMAPF-basedsolutionsaremainlyevaluatedusingsuccessrates,\n1\n2202\nraM\n41\n]GL.sc[\n1v29070.3022:viXra\n=== 페이지 2 ===\nflowtimes(thesumofarrivaltimesofallagentsattheirgoallocations)andmakespans(themaximum\nofthearrivaltimesofallagentsattheirgoallocations). Incontrast,theMARL-basedmethodsare\nusuallyassessedwiththestandardmetricsusedinreinforcementlearning—trainingandevaluation\nreturns. Suchadiscrepancybetweentheevaluationmetricsrendersitdifficultincomparingthese\ntwotypesofsolutions.",
      "size": 950,
      "sentences": 7
    },
    {
      "id": 6,
      "content": "sare\nusuallyassessedwiththestandardmetricsusedinreinforcementlearning—trainingandevaluation\nreturns. Suchadiscrepancybetweentheevaluationmetricsrendersitdifficultincomparingthese\ntwotypesofsolutions. Giventhisdiscrepancy,inthispaper,weaimtoprovideacomprehensive\ncomparisonbetweenthesetwoseeminglydisconnectedtypesofsolutionstotheMAPDproblem. We\nparticularlycomparethelifelongversionofawell-knowncentralizedsingle-shotMAPFsolvercalled\nconflict-basedsearch(CBS)tothestate-of-the-artMARLsolvercalledsharedexperienceactor-critic\n(SEAC). 2 BACKGROUND\nInthissection,wegiveageneraloverviewoftheproblemformulationsofMAPFandMARL,which\ncan also be found from prior work (Christianos et al., 2020; 2021; Huang et al., 2021; Li et al.,\n2019a;b;2021;Liuetal.,2019). DetailsofrelatedalgorithmsaregiveninSection3. 2.1 SINGLE-SHOTMULTI-AGENTPATHFINDING\nA (single-shot) multi-agent pathfinding problem is defined by an unweighted undirected graph\nG = (V,E)andasetofnagentsA := {α ,...,α }.",
      "size": 972,
      "sentences": 7
    },
    {
      "id": 7,
      "content": "rithmsaregiveninSection3. 2.1 SINGLE-SHOTMULTI-AGENTPATHFINDING\nA (single-shot) multi-agent pathfinding problem is defined by an unweighted undirected graph\nG = (V,E)andasetofnagentsA := {α ,...,α }. Eachagentα hasastartvertexs ∈ Vand\n1 n i i\nagoalvertexg ∈V. Withtimediscretizedintotimesteps,eachagentcanonlyeithermovetoan\ni\nadjacentvertexorstayatthecurrentvertexinthegraphateachtimestep. Bothofthemoveand\nwaitactionsincuraunitcostuntiltheagenthasarrivedatitsgoalvertexandnolongermovesso\nthatthecostofeachagentisthenumberoftimestepsrequiredforitsarrivalatitsgoalvertexfrom\nitsstartvertex. Therearetwotypesofconflictsunderconsideration: (i)avertexconflict,denoted\nby(cid:104)α ,α ,v,t(cid:105),happenswhenagentsα andα areatthesamevertexv ∈Vattimestept;(ii)an\ni j i j\nedgeconflict,denotedby(cid:104)α ,α ,v ,v ,t(cid:105),occurswhenagentsα andα traversethesameedge\ni j 1 2 i j\n(v ,v )∈Einoppositedirectionsbetweentimestepstandt+1.",
      "size": 929,
      "sentences": 6
    },
    {
      "id": 8,
      "content": "vertexv ∈Vattimestept;(ii)an\ni j i j\nedgeconflict,denotedby(cid:104)α ,α ,v ,v ,t(cid:105),occurswhenagentsα andα traversethesameedge\ni j 1 2 i j\n(v ,v )∈Einoppositedirectionsbetweentimestepstandt+1. TheoverallobjectiveofMAPFis\n1 2\ntofindasetofconflict-freepathswhichmoveallagentsfromtheirstartverticestotheirgoalvertices,\nwhichareoftenreferredtoassolutions,byminimizingthesumofthecostsofalltheagents. 2.2 LIFELONGMULTI-AGENTPATHFINDING\nAs mentioned in Section 1, an MAPD problem is indeed a lifelong MAPF problem that solves\npossiblymultiplesingle-shotMAPFproblemsinaninnerloop. Thus,existingapproachesforsolving\nlifelongMAPFproblemsareusuallybasedonthoseforsolvingsingle-shotMAPFinstances,which\ncanbecategorizedintothreemaintypes. 1. AlifelongMAPFproblemisdecomposedintoasequenceofsingle-shotMAPFinstances\nwhereallagentsperformpathreplanningateverystep. 2.",
      "size": 858,
      "sentences": 7
    },
    {
      "id": 9,
      "content": "solvingsingle-shotMAPFinstances,which\ncanbecategorizedintothreemaintypes. 1. AlifelongMAPFproblemisdecomposedintoasequenceofsingle-shotMAPFinstances\nwhereallagentsperformpathreplanningateverystep. 2. AlifelongMAPFproblemisdecomposedintoasequenceofsingle-shotMAPFinstances\nwherepathreplanningisperformedonlyforagentsthathavejustpickedupordelivered\ntheiritems(Maetal.,2017b). 3. AlifelongMAPFproblemissolvedasawholeinanofflinesetting,asreductionstoother\nwell-studiedcombinatorialproblemssuchasananswersetprogrammingproblem(Nguyen\netal.,2017). SeeLietal. (2021)formoredetaileddescriptionsofthesethreetypesofsolutionstolifelongMAPF\nproblems,Felneretal.(2017);Maetal. (2017a)fordetailedsurveysonMAPF(mainlysingle-shot),\nandalsoSalzman&Stern(2020)forrecentresearchchallengesandopportunitiesinMAPFand\nMAPDproblems.",
      "size": 807,
      "sentences": 10
    },
    {
      "id": 10,
      "content": "tionstolifelongMAPF\nproblems,Felneretal.(2017);Maetal. (2017a)fordetailedsurveysonMAPF(mainlysingle-shot),\nandalsoSalzman&Stern(2020)forrecentresearchchallengesandopportunitiesinMAPFand\nMAPDproblems. 2.3 MULTI-AGENTREINFORCEMENTLEARNING\nSolutionsbasedonmulti-agentreinforcementlearning(MARL),unlikeMAPF-basedsolutions,do\nnot compute a set full collision-free paths of all the agents in the environment, but instead learn\npoliciesoftheagentswhichdecidetheactionsoftheagentsateachtimestepgiventhecurrentstates\noftheenvironment. 2\n=== 페이지 3 ===\nThis MARL problem can be formulated as a partially observable multi-agent Markov\ndecision process (a.k.a. Markov game) for n agents, which is defined by the tuple\n(N,S,{O\ni\n} i∈N,{A\ni\n} i∈N,P,{R\ni\n} i∈N), where N := {1,...,n} denotes the set of n agents, S is\nthestatespace,O:=O ×···×O isthejointobservationspace,A:=A ×···×A isthejoint\n1 n 1 n\nactionspace. Eachagenticanonlyperceivelocalobservationso ∈O whichdependonthecurrent\ni i\nstate.",
      "size": 980,
      "sentences": 6
    },
    {
      "id": 11,
      "content": "et of n agents, S is\nthestatespace,O:=O ×···×O isthejointobservationspace,A:=A ×···×A isthejoint\n1 n 1 n\nactionspace. Eachagenticanonlyperceivelocalobservationso ∈O whichdependonthecurrent\ni i\nstate. ThefunctionP: S×A→P(S),whichisknownasatransitionmodel,returnsadistribution\nonthesuccessivestategiventhecurrentstateandjointaction. Foreachagenti,therewardfunction\nR : S×A×S→Rgivesitsindividualrewardr attimestept. TheoverallMARLobjectiveisto\ni i,t\nfindanoptimaljointpoliciesoftheagents,denotedbyπ(cid:63) =(π(cid:63),...,π(cid:63)),suchthatthediscounted\n1 n\nreturnofeachagentiismaximizedwithrespecttothepoliciesofotheragents,i.e.,\n(cid:34) T (cid:35)\n(cid:88)\n(∀i∈N) π i (cid:63) ∈Arg\nπ\nm\ni\nax E πi,π\n\\\n(cid:63)\ni t=0\nγtr i,t , (1)\nwhere π := (π ,...,π ,π ,...,π ), γ ∈ (0,1] is the discount factor, and T is the total\n\\i 1 i−1 i+1 n\nnumberoftimestepsofanepisode.",
      "size": 862,
      "sentences": 5
    },
    {
      "id": 12,
      "content": "N) π i (cid:63) ∈Arg\nπ\nm\ni\nax E πi,π\n\\\n(cid:63)\ni t=0\nγtr i,t , (1)\nwhere π := (π ,...,π ,π ,...,π ), γ ∈ (0,1] is the discount factor, and T is the total\n\\i 1 i−1 i+1 n\nnumberoftimestepsofanepisode. Notethat,basedonthedifferentcontextsofMARLalgorithms,wecanaddmorestringentassump-\ntionstotheaboveformulation,e.g.,theactionspaces,theobservationspaces,ortherewardfunctions\noftheagentscanbeassumedtobeidentical(Christianosetal.,2020;Foersteretal.,2018;Rashid\netal.,2018). 3 ALGORITHMS FOR MAPF AND MARL PROBLEMS\nInthissection,wegivethedetailsofseveralpopularalgorithmsforMAPFandMARLproblems\nwhicharecomparednumericallyinSection4. 3.1 MAPFSOLVERS\n3.1.1 CONFLICT-BASEDSEARCHFORSINGLE-SHOTMAPF\nWhilethereareamultitudeofMAPFsolversdevelopedinrecentyears,conflict-basedsearch(CBS)\nanditsvariantsareamongthestrongestalgorithms.",
      "size": 819,
      "sentences": 4
    },
    {
      "id": 13,
      "content": "ction4. 3.1 MAPFSOLVERS\n3.1.1 CONFLICT-BASEDSEARCHFORSINGLE-SHOTMAPF\nWhilethereareamultitudeofMAPFsolversdevelopedinrecentyears,conflict-basedsearch(CBS)\nanditsvariantsareamongthestrongestalgorithms. Conflict-basedsearch(CBS;Sharonetal.,2015)\nisacentralizedbileveltreesearchalgorithm,whichresolvesconflictsbyaddingconstraintsatthe\nhighlevelandreplanspathsforagentsrespectingtheseconstraintsatthelowlevel. Atthehighlevel,\nCBSperformsabest-firstsearchontheconstrainttree(CT),whichisabinarysearchtree,according\ntothecostsoftheCTnodes. EachCTnodeNencompasses:\n1. asetofconstraintsN inthesearch,inwhicheachconstraintcanbeeitheravertex\nconstraints\nconstraintoranedgeconstraint(seeSection2.1fortheirdefinitions);\n2. asolutionN whichconsistsofasetofindividuallycost-minimalpathsforallagents,\nsolution\nsubjecttotheconstraintsinN ;\nconstraints\n3. acostN ofNwhichisthesumofcostsofthepathsinN ;\ncost solution\n4. asetofconflictsN betweenanytwopathsinN .",
      "size": 940,
      "sentences": 5
    },
    {
      "id": 14,
      "content": "individuallycost-minimalpathsforallagents,\nsolution\nsubjecttotheconstraintsinN ;\nconstraints\n3. acostN ofNwhichisthesumofcostsofthepathsinN ;\ncost solution\n4. asetofconflictsN betweenanytwopathsinN . conflicts solution\nAtthehighlevel,CBSbeginswithonlyonenodewithanemptyconstraintsetandexpandstheCT\nbyexpandingaCTnodewiththelowestcostN . AfterchoosingsuchaCTnodetoexpand,CBS\ncost\nfindsthesetofconflictsN inN . Iftherearenone,CBSterminatesandreturnsN . conflicts solution solution\nOtherwise,CBSrandomlychoosesoneoftheconflictstoresolvebysplittingNintotwochildCT\nnodes. Ineachofthesetwochildren,weaddCTnodes,anadditionalvertexoredgeconstrainton\noneofthetwoconflictingagentstoN ofthecorrespondingchildnode,dependingonthetype\nconstraints\noftheconflict. Thisisdonesimilarlyfortheotherconflictingagentanditscorrespondingchildnode.",
      "size": 823,
      "sentences": 7
    },
    {
      "id": 15,
      "content": "redgeconstrainton\noneofthetwoconflictingagentstoN ofthecorrespondingchildnode,dependingonthetype\nconstraints\noftheconflict. Thisisdonesimilarlyfortheotherconflictingagentanditscorrespondingchildnode. Atthelowlevel,aftertheadditionoftheconstraints,pathreplanningisperformedinN whenever\nsolution\nnecessaryviaalow-levelsearchsuchascooperativeA*search(Silver,2005),whilekeepingother\npathsunchanged. AchildCTnodewillbeprunedifthislow-levelsearchcannotfindanypaththat\nsatisfiestheconstraints. ImprovedversionsofCBShavealsobeendevelopedtoimproveitsefficiency. ImprovedCBS(ICBS;\nBoyarskietal.,2015)prioritizestheconflictstosplitonateachCTnodeN,whereasCBSH(Felner\n3\n=== 페이지 4 ===\netal.,2018)acceleratesthehigh-levelsearchthroughanadditionaladmissibleheuristicinsteadof\nalwayschoosingtoexpandtheCTnodewiththelowestcostN . Seealsoe.g.,Bareretal. (2014);\ncost\nHuangetal.(2021);Lietal.(2019a;b;2021);Maetal. (2019a)formoredetailsandrecentadvances\nofCBSanditsvariantsforMAPF.",
      "size": 961,
      "sentences": 9
    },
    {
      "id": 16,
      "content": "wayschoosingtoexpandtheCTnodewiththelowestcostN . Seealsoe.g.,Bareretal. (2014);\ncost\nHuangetal.(2021);Lietal.(2019a;b;2021);Maetal. (2019a)formoredetailsandrecentadvances\nofCBSanditsvariantsforMAPF. WhiletheaboveCBSalgorithmsaremainlyforsingle-shotMAPFproblems,acommonapproach\nto solve lifelong MAPF is to stitch a sequence of single-shot MAPF instances together by using\naMAPFsolvertoreplanwheneveratleastoneagentisassignedtoanewtargetlocationateach\ntimestep,seee.g.,Liuetal.(2019). Sincereplanningtimegrowsexponentiallywiththenumber\nofagents,reducingreplanningfrequenciessuchasplanningpathswithinafinitewindow(Lietal.,\n2021)improvesthescalabilityoflifelongMAPF. 3.2 MARLALGORITHMS\n3.2.1 POLICYGRADIENTANDACTOR-CRITICALGORITHMS\nThepolicygradientalgorithmsuchasREINFORCE(Williams,1992)isamodel-freereinforcement\nlearningalgorithmwhichlearnsanoptimalpolicyπ ,usuallyparameterizedbyθ.",
      "size": 883,
      "sentences": 7
    },
    {
      "id": 17,
      "content": "3.2.1 POLICYGRADIENTANDACTOR-CRITICALGORITHMS\nThepolicygradientalgorithmsuchasREINFORCE(Williams,1992)isamodel-freereinforcement\nlearningalgorithmwhichlearnsanoptimalpolicyπ ,usuallyparameterizedbyθ. Theexpected\nθ\nreturnisdefinedthroughJ(θ):=E\ns∼Dπ,a∼πθ(·|s)\n[Qπθ(s,a)],with\n(cid:34) (cid:88) T (cid:12) (cid:12) (cid:35)\nQπθ(s,a):=E\na∼πθ(·|s)\nγkr\nt+k+1\n(cid:12)\n(cid:12)\ns\nt\n=s,a\nt\n=a ,\n(cid:12)\nk=0\nwhereDπ istheon-policystatedistributionunderπ. TomaximizetheexpectedreturnJ,wecompute\nthegradientoftheobjectiveviathepolicygradienttheorem(Suttonetal.,2000),whichgives\n∇ θ J(θ)=E s∼Dπ,a∼πθ(·|s) [Qπθ(s,a)∇ θ logπ θ (s,a)]. WhiletheMarkovpropertyisnotusedincomputingpolicygradients,sothattheycanbeusedin\npartiallyobservablesettings,theestimationofpolicygradientsoftensufferfromhighvariance. To\nachievevariancereduction,actor-criticalgorithmsestimateMonteCarloreturnswithavaluefunction\nVπ(s)parameterizedbyφ.",
      "size": 906,
      "sentences": 5
    },
    {
      "id": 18,
      "content": "iallyobservablesettings,theestimationofpolicygradientsoftensufferfromhighvariance. To\nachievevariancereduction,actor-criticalgorithmsestimateMonteCarloreturnswithavaluefunction\nVπ(s)parameterizedbyφ. Hence,anactor-criticalgorithmunderamulti-agentpartiallyobservable\nφ\nsettingmakesuseofthefollowingpolicylossforagenti\nL(θ ):=−logπ (ai|oi)· (cid:2) ri+γV (oi )−V (oi) (cid:3) ,\ni θi t t t φ i t+1 φ i t\nwherethevaluefunctionV minimizes\nφ\ni\n(cid:13) (cid:13)2\nL(φ ):=(cid:13)V (oi)−yφ i(cid:13) with yφ i :=ri+γV (oi ). i (cid:13) φ i t i (cid:13) 2 i t φ i t+1\nInanimplementation,boththepoliciesandvaluefunctionsareparameterizedbyneuralnetworks. A2C (Mnih et al., 2016) is used with n-step rewards, parallel trajectory sampling and entropy\nregularization. 3.2.2 SHAREDEXPERIENCEACTOR-CRITIC\nBasedontheactor-criticalgorithmsdescribedabove,sharedexperienceactor-critic(SEAC;Chris-\ntianosetal.,2020)isproposedforefficientlearningusingsharedexperienceamongagents.",
      "size": 957,
      "sentences": 6
    },
    {
      "id": 19,
      "content": "2 SHAREDEXPERIENCEACTOR-CRITIC\nBasedontheactor-criticalgorithmsdescribedabove,sharedexperienceactor-critic(SEAC;Chris-\ntianosetal.,2020)isproposedforefficientlearningusingsharedexperienceamongagents. Themain\nmeritofsharingexperienceamongagentsisthatagentscanlearnfromtheexperiencesofotheragents\nwithouthavingthesamerewards. InSEAC,thetrajectoriesofotheragentsareusedasoff-policy\ndata,andimportancesamplingwithabehaviouralpolicyρisusedtocorrectfortheoff-policydata. DetailedalgorithmicdescriptionsofSEACcanbefoundinChristianosetal.(2020). 3.2.3 OTHERMARLALGORITHMS\nAstheMAPDprobleminvolvescooperationamongagents,apopularparadigminsuchacooperative\nMARLproblemistheCentralizedTrainingwithDecentralizedExecution. Allagentscanaccessdata\nfromallotheragentsduringtrainingbutnotatexecutiontime. InadditiontoSEACimplementedin\nthispaper,MARLalgorithmsofthistypealsoincludeMADDPG(Loweetal.,2017),Q-MIX(Rashid\netal.,2018)andCOMA(Foersteretal.,2018).",
      "size": 937,
      "sentences": 7
    },
    {
      "id": 20,
      "content": "lotheragentsduringtrainingbutnotatexecutiontime. InadditiontoSEACimplementedin\nthispaper,MARLalgorithmsofthistypealsoincludeMADDPG(Loweetal.,2017),Q-MIX(Rashid\netal.,2018)andCOMA(Foersteretal.,2018). AddedtotheabovegeneralMARLalgorithms,two\nrecentworksbySartorettietal.(2019)andDamanietal. (2021)developspecificMARLalgorithms\ntothelifelongMAPFproblem. 4\n=== 페이지 5 ===\n4 NUMERICAL EXPERIMENTS\nWeevaluatetheMAPF-basedandMARL-basedmethodsonasimulatedroboticwarehouseenvi-\nronment,whichismodifiedfromtheMulti-RobotWarehouseEnvironment(RWARE;Papoudakis\netal.,2021). ThemodificationsaremainlyformoreefficienttrainingofSEACandtheapplicability\nofthelifelongversionofCBS.First,agentsneedtopickupanddeliverrequestedshelves(items)\ntothedeliverylocationsbutdonotneedtoreturntheshelvestoemptyshelvesbeforetravellingto\nnewpickuplocations. Furthermore,thenumberofdeliverylocationsisincreasedsothatagentscan\ndeliveritemsinanylocationofthebottomrow.",
      "size": 932,
      "sentences": 7
    },
    {
      "id": 21,
      "content": "locationsbutdonotneedtoreturntheshelvestoemptyshelvesbeforetravellingto\nnewpickuplocations. Furthermore,thenumberofdeliverylocationsisincreasedsothatagentscan\ndeliveritemsinanylocationofthebottomrow. Thismodificationisdonebecauseeachagentshould\nhaveitsdistinctgoallocationinCBS.Finally,toaddresstheissueofsparserewardswhentraining\nMARLalgorithms,eachagentwillbeassigneda+1rewardwhenpickinguparequesteditemanda\n+2rewardwhendeliveringitemstoadeliverylocationsuccessfully. Otherspecificationssuchasthe\nobservations,actionsanddynamicsremainthesameasinRWARE. ThemodifiedroboticwarehouseenvironmentisvisualizedinFigure1,inthreedifferentsizes(small,\nmediumandlarge). Eachagentishexagonal,withablacklineindicatingitsfacingdirection. When\nanagentisnotcarryinganyiteminyellow,itstargetistomovetoarequesteditemthatisgreen\nthenpickitup. Anagentcarryinganordereditemisindicatedbyred,anditstargetistodeliverthe\nitemtoanylocationofthegreybottomrow.",
      "size": 933,
      "sentences": 8
    },
    {
      "id": 22,
      "content": "entisnotcarryinganyiteminyellow,itstargetistomovetoarequesteditemthatisgreen\nthenpickitup. Anagentcarryinganordereditemisindicatedbyred,anditstargetistodeliverthe\nitemtoanylocationofthegreybottomrow. Anewitemwillberandomlygeneratedintherequest\nqueuerightafterthedeliveryofanobjectsothatthenumberofitemsintherequestqueueremains\nunchanged. Eachagentrepeatsthispickupanddeliverycycleuntilanepisodeofafixednumberof\ntimestepsends. Whenanagentisnotcarryinganyitem,itcanmovethroughanycoordinatesinthe\nenvironment,includingundertheunrequestedpurpleshelves. Itcanonlymoveacrossthecorridors\nandthedeliveryrowifitcarriesanitem. Wealsoassumedanequalnumberofagentsandrequested\nitemsintheenvironmenttosimplifythetasks. (a)small;5agents (b)medium;5agents (c)large;5agents\nFigure1: ThemodifiedRWAREenvironmentofdifferentsizes.",
      "size": 810,
      "sentences": 8
    },
    {
      "id": 23,
      "content": "item. Wealsoassumedanequalnumberofagentsandrequested\nitemsintheenvironmenttosimplifythetasks. (a)small;5agents (b)medium;5agents (c)large;5agents\nFigure1: ThemodifiedRWAREenvironmentofdifferentsizes. RegardingtheMAPF-basedmethod,wehaveimplementedacentralizedsingle-shotMAPFalgorithm,\nthevanillasingle-shotCBSalgorithm,andsolvedthelifelongMAPFproblembyreplanningwhenever\nanyagenthasanewgoallocation. Thissolutionwouldimplythatreplanningbecomesmorefrequent\nwheneverthenumberofagentsincreasesortheenvironmentbecomesdenser. Ontheotherhand,asa\nMARL-basedmethodforcomparisonpurposes,wetrainaSEACalgorithmbasedonthismodified\nenvironmentwithalesssparserewarddesign. Empiricallyweobservethatthismodificationleadsto\nlowersamplecomplexitythantheoriginalone. Alltheexperimentsarerunonamachinewithan\nIntelXeonE5-2699v4CPU,asingleTeslaV100GPU(onlyusedinSEAC),and540GBRAM.",
      "size": 857,
      "sentences": 8
    },
    {
      "id": 24,
      "content": "Empiricallyweobservethatthismodificationleadsto\nlowersamplecomplexitythantheoriginalone. Alltheexperimentsarerunonamachinewithan\nIntelXeonE5-2699v4CPU,asingleTeslaV100GPU(onlyusedinSEAC),and540GBRAM. OneremarkabledifferencebetweentheimplementationsoflifelongCBSandSEACisthatthevanilla\nCBSishomogeneousamongallagentssothatallitemsexcepttherequestedonesareviewedas\nobstaclesintheenvironment. Thisimpliesallagents,regardlessofthestatusofcarryingitemsornot,\ncannotmoveunderanyunrequestedshelves. Thissignificantlyreducesthenumberoffeasiblepaths\n5\n=== 페이지 6 ===\ninthelifelongCBSsolver. Therefore,itisexpectedthatthelifelongCBSsolverwillfailwhenever\ntheenvironmentistoodense,i.e.,thenumberofagentsrelativetothesizeofthedomainistoolarge. WecomparelifelongCBSandSEACattesttime,consideringallthreesizesofthewarehouse,with\n2,5and8agentsinlifelongCBSand5,10and15agentsinSEAC.WeobservethatlifelongCBS\ndoesnotscaleto10ormoreagentsinanyofthesizesoftheenvironmentduetotime-consuming\nreplanningschemes.",
      "size": 986,
      "sentences": 7
    },
    {
      "id": 25,
      "content": "esizesofthewarehouse,with\n2,5and8agentsinlifelongCBSand5,10and15agentsinSEAC.WeobservethatlifelongCBS\ndoesnotscaleto10ormoreagentsinanyofthesizesoftheenvironmentduetotime-consuming\nreplanningschemes. Weevaluatetheirperformancewithfiverandomseeds,eachwithfourepisodes\nof500-timesteps. Thesetwoalgorithmsarecomparedusingthefollowingfivemetrics(averaged\noverallepisodesandrandomseeds),whicharecommonlyusedineitherMAPForMARLbutnot\nboth: (i)meanflowtimeforthefirstdelivery(thesumofarrivaltimesofallagentsattheirdelivery\nlocations,intermsofthenumberoftimestepsrequired);(ii)meanmakespanforthefirstdelivery\n(themaximumofthearrivaltimesofallagentsattheirdeliverylocations,intermsofthenumber\noftimestepsrequired);(iii)meanepisodiccumulativerewardperagentinoneepisode;(iv)mean\nnumberofsuccessfullydelivereditemsofeachagentinoneepisode; (v)meanepisodictime(in\nseconds). ThesemetricsforlifelongCBSandSEACaregiveninTables1and2respectively. Table1: Metrics(s.e.) forlifelongCBS.",
      "size": 964,
      "sentences": 6
    },
    {
      "id": 26,
      "content": "n\nnumberofsuccessfullydelivereditemsofeachagentinoneepisode; (v)meanepisodictime(in\nseconds). ThesemetricsforlifelongCBSandSEACaregiveninTables1and2respectively. Table1: Metrics(s.e.) forlifelongCBS. LifelongCBS\nMetric n small medium large\n2 66.60(7.58) 75.20(17.68) 76.00(17.13)\nMean\n5 178.60(16.40) 196.20(29.10) 209.80(11.38)\nflowtime\n8 307.00(53.43) 345.60(37.71) 339.80(22.38)\n2 41.40(5.30) 44.60(10.29) 44.80(11.08)\nMean\n5 51.60(5.01) 55.40(9.30) 60.40(7.33)\nmakespan\n8 61.60(8.50) 71.80(9.89) 66.20(4.07)\nMean 2 36.02(2.98) 35.30(2.09) 27.95(1.89)\ncumulative 5 31.19(1.24) 30.63(4.59) 26.09(1.36)\nreward 8 27.19(1.39) 28.02(1.19) 23.40(1.16)\nMean 2 30.30(14.22) 30.20(13.83) 23.90(11.01)\n#delivered 5 27.07(12.63) 27.87(15.10) 22.48(10.43)\nitems 8 24.50(11.61) 24.46(11.29) 20.38(9.44)\nMean 2 508.40(0.97) 510.57(0.78) 513.53(1.26)\nepisodic 5 567.34(79.62) 637.12(340.26) 565.88(29.60)\ntime 8 2618.11(4866.68) 1391.95(1372.68) 2371.99(2409.23)\nFromTable1,weobservethat,ingeneral,boththemeanflowtimeandmakespanincreasewiththe\nnumberofagentsandthesizeoftheenvironment.",
      "size": 1073,
      "sentences": 5
    },
    {
      "id": 27,
      "content": ".26) 565.88(29.60)\ntime 8 2618.11(4866.68) 1391.95(1372.68) 2371.99(2409.23)\nFromTable1,weobservethat,ingeneral,boththemeanflowtimeandmakespanincreasewiththe\nnumberofagentsandthesizeoftheenvironment. Theincreasedenvironmentsizeleadstoaless\ndenseenvironment,somoretimestepsarerequiredforcompletingdeliveriesduetolongertravel\ndistances. Ontheotherhand,theincreaseinthenumberofagentsgivesrisetoadenserenvironment\nsothattherearefewerpossiblecollision-freepaths. Agentsneedtotravellongdistancestoavoid\ncollisionswhicharemorelikelytooccurinadenserenvironment. Inaddition,themeancumulative\nrewardandthemeannumberofdelivereditemsofeachagentperepisodedecreasewiththenumber\nofagentsandthesizeoftheenvironment. Withinafixedtimeof500ineveryepisode,eachagent\ncanonlydeliverfeweritemsinanenvironmentwithmoreagentsoroflargersize,thusreceivinga\nsmallerreward.",
      "size": 843,
      "sentences": 6
    },
    {
      "id": 28,
      "content": "creasewiththenumber\nofagentsandthesizeoftheenvironment. Withinafixedtimeof500ineveryepisode,eachagent\ncanonlydeliverfeweritemsinanenvironmentwithmoreagentsoroflargersize,thusreceivinga\nsmallerreward. More notably, while the mean episodic times of lifelong CBS are close in different sizes of the\nenvironment,themeanepisodictimegrowssignificantlywiththenumberofagents,alsosuffering\nfromhighvariance. SinceourimplementationoflifelongCBSisonline,episodictimesincludethe\ntimerequiredforbothreplanningandagents’movementintheenvironment. Attributedtoadenser\nenvironment,replanningisperformedathigherfrequencieswithmoreagents. Eachreplanningis\nalsomoretime-consumingasmoreconflictshavetoberesolved. Thehighvarianceofthemean\nepisodictimeswithlargenumbersofagentsalsoindicatesthatlifelongCBS’sperformanceisnot\nrobustenoughindifferentinstancesoftheenvironment. ForSEAC,weobservefromTable2similarvariationsofthemeanflowtimeandthemeanmakespan.",
      "size": 931,
      "sentences": 8
    },
    {
      "id": 29,
      "content": "thlargenumbersofagentsalsoindicatesthatlifelongCBS’sperformanceisnot\nrobustenoughindifferentinstancesoftheenvironment. ForSEAC,weobservefromTable2similarvariationsofthemeanflowtimeandthemeanmakespan. However,withmoreagentsintheenvironment,agentstrainedusingSEACcandelivermoreitems\n6\n=== 페이지 7 ===\nTable2: Metrics(s.e.) forSEAC.",
      "size": 327,
      "sentences": 4
    },
    {
      "id": 30,
      "content": "eobservefromTable2similarvariationsofthemeanflowtimeandthemeanmakespan. However,withmoreagentsintheenvironment,agentstrainedusingSEACcandelivermoreitems\n6\n=== 페이지 7 ===\nTable2: Metrics(s.e.) forSEAC. SEAC\nMetric n small medium large\n5 274.15(53.04) 367.00(137.32) 555.90(182.28)\nMean\n10 482.55(88.30) 763.90(223.34) 805.45(146.45)\nflowtime\n15 882.15(201.03) 846.20(127.07) 1030.80(268.63)\n5 88.50(27.37) 142.50(65.43) 221.70(93.63)\nMean\n10 94.80(33.62) 203.65(101.51) 177.85(58.99)\nmakespan\n15 185.75(90.75) 146.45(44.56) 305.05(110.42)\nMean 5 37.01(2.53) 25.98(5.04) 12.26(3.06)\ncumulative 10 39.60(2.01) 21.37(4.03) 19.42(4.47)\nreward 15 31.02(3.93) 23.29(4.08) 8.18(1.17)\nMean 5 10.00(9.69) 14.20(33.69) 5.71(14.21)\n#delivered 10 8.41(5.72) 5.12(7.84) 8.69(22.86)\nitems 15 12.90(19.53) 16.77(34.98) 30.77(59.61)\nMean 5 509.75(0.23) 511.34(0.39) 513.50(0.40)\nepisodic 10 512.77(0.36) 514.43(0.42) 516.66(0.45)\ntime 15 515.67(0.50) 517.59(0.43) 519.48(0.46)\nwithmoreagentsonaverageinanepisode.",
      "size": 994,
      "sentences": 4
    },
    {
      "id": 31,
      "content": "34.98) 30.77(59.61)\nMean 5 509.75(0.23) 511.34(0.39) 513.50(0.40)\nepisodic 10 512.77(0.36) 514.43(0.42) 516.66(0.45)\ntime 15 515.67(0.50) 517.59(0.43) 519.48(0.46)\nwithmoreagentsonaverageinanepisode. ThisisaremarkabledifferencefromlifelongCBS,which\nhassignificantlyfewerpossiblepathsinthewarehousethanSEAC. Furthermore,sincetheagents’policiesinSEACarelearnedduringtrainingandthemeanepisodic\ntimesarecomputedattesttime,onlyagents’environmentalmovementsaccountfortheepisodictime. Therefore,themeanepisodictimesareverycloseregardlessofthesizeoftheenvironmentorthe\nnumberofagents. ThismakesSEACamoreviableandefficientsolutioninpracticeasrobotshaveto\nstayidleduringreplanninginlifelongCBS. ComparinglifelongCBSandSEACfromTables1and2for5agents,spendingsimilartimeineach\nepisode,lifelongCBShassignificantlysmallermeanflowtimeandmeanmakespan,whilehaving\nsignificantlymoreitemsdeliveredinanepisodebyeachagentonaverage.",
      "size": 909,
      "sentences": 6
    },
    {
      "id": 32,
      "content": "ACfromTables1and2for5agents,spendingsimilartimeineach\nepisode,lifelongCBShassignificantlysmallermeanflowtimeandmeanmakespan,whilehaving\nsignificantlymoreitemsdeliveredinanepisodebyeachagentonaverage. ThisisbecauseCBS-based\nmethodsplantheshortestcollision-freepathforeveryindividualagentfromitsstartlocationtoits\ntargetlocation,whichshouldbemoreefficientthanSEACwhichonlygivesthemostprobableaction\nforeachagentbasedonthecurrentobservationoftheenvironmentateverytimestep(i.e.,noplanning\nahead). However,dependingonthedensityoftheenvironment,MAPF-basedmethodssufferfrom\ntheissueofscalability. ReplanninginlifelongCBSmighttakeaverylongtimeinpracticeoreven\nfailinvariousinstancesinarelativelydenseenvironment. ThissuggeststhatlifelongCBSisamore\nefficientsolverfortheMAPDproblemwithfeweragentsandinlessdenseenvironments,whereas\nSEACshouldbeusedwhenmanyagentsareused. However, weshouldnotethatthetrainingof\nSEACagentsalsocostsasignificantamountoftime,whichmainlyriseswiththenumberofagents.",
      "size": 982,
      "sentences": 6
    },
    {
      "id": 33,
      "content": "entsandinlessdenseenvironments,whereas\nSEACshouldbeusedwhenmanyagentsareused. However, weshouldnotethatthetrainingof\nSEACagentsalsocostsasignificantamountoftime,whichmainlyriseswiththenumberofagents. 5 DISCUSSION AND FUTURE WORK\nWhileweimplementedthelifelongCBSwiththeprinciplethatreplanningisperformedwhenever\nanyoneoftheagentschangesitstargetlocations,thiscurrentimplementationisfarfromefficient. In\nadditiontotheimprovedvariantsofCBSmentionedinSection3,variousrecentworksinMAPF\naddressdifferentperspectivesandimprovetheefficienciesofMAPFsolvers,seee.g.,Greshleretal. (2021);Ho¨nigetal.(2019);Huangetal.(2021);Maetal.(2019b);Shaharetal.(2021);Wuetal. (2021). Furthermore,acrucialandvaluableresearchdirectiontopursueistouselifelongMAPFmethodsto\nimprovethesampleefficiencyofMARLalgorithmsforsolvingtheMAPDproblem.",
      "size": 813,
      "sentences": 7
    },
    {
      "id": 34,
      "content": "etal.(2019b);Shaharetal.(2021);Wuetal. (2021). Furthermore,acrucialandvaluableresearchdirectiontopursueistouselifelongMAPFmethodsto\nimprovethesampleefficiencyofMARLalgorithmsforsolvingtheMAPDproblem. Forinstance,\nthesolutionsbasedonlifelongMAPFsolverscanbeusedasexpertdemonstrationdatatoderivea\npolicy,i.e.,(multi-agent)imitationlearning(Ho&Ermon,2016;Linetal.,2021;Songetal.,2018;\nWangetal.,2021). Furthermore,whentherewardfunctionsarehardtodesignoreachagenthas\n7\n=== 페이지 8 ===\nnoaccesstotherewardsorgoalsofotheragents,wecanuseexpertdemonstrationdatatolearnthe\nrewards,i.e.,(multi-agent)inversereinforcementlearning(Filosetal.,2021;Yuetal.,2019).",
      "size": 647,
      "sentences": 5
    },
    {
      "id": 35,
      "content": "oreachagenthas\n7\n=== 페이지 8 ===\nnoaccesstotherewardsorgoalsofotheragents,wecanuseexpertdemonstrationdatatolearnthe\nrewards,i.e.,(multi-agent)inversereinforcementlearning(Filosetal.,2021;Yuetal.,2019). The MAPDproblem considered inthis paper isa relativelysimple instance whereall agents are\nassumedtobehomogeneous: allagentscanpickupanyrequesteditemanddeliveritemsatany\ndeliverylocation.Insteadoftheimplicitassumptionofallagentsbeinghomogeneous,amorerealistic\nyetcomplicatedscenarioistoallowagentstohavedifferentabilitiesandgoals: eachagentcanonly\npickupadesignatedtypeofitem. ArecentMARLalgorithmbasedonselectiveparametersharing\n(SePS;Christianosetal.,2021)isdesignedtohandlesuchsettings,inwhichparametersharingis\nperformedamongindividualgroupsofhomogeneousagentswithgroupingperformedautomatically\nusinganunsupervisedclusteringalgorithmbasedontheabilitiesandgoalsoftheagents. Itremains\ntoseehowSePScomparestolifelongMAPF-basedsolutionsinmorecomplicatedMAPDproblems.",
      "size": 965,
      "sentences": 4
    },
    {
      "id": 36,
      "content": "hgroupingperformedautomatically\nusinganunsupervisedclusteringalgorithmbasedontheabilitiesandgoalsoftheagents. Itremains\ntoseehowSePScomparestolifelongMAPF-basedsolutionsinmorecomplicatedMAPDproblems. 8\n=== 페이지 9 ===\nREFERENCES\nMaxBarer,GuniSharon,RoniStern,andArielFelner. Suboptimalvariantsoftheconflict-based\nsearchalgorithmforthemulti-agentpathfindingproblem. InAnnualSymposiumonCombinatorial\nSearch(SoCS),2014. EliBoyarski,ArielFelner,RoniStern,GuniSharon,DavidTolpin,OdedBetzalel,andEyalShimony. ICBS:Improvedconflict-basedsearchalgorithmformulti-agentpathfinding. InProceedingsof\ntheInternationalJointConferenceonArtificialIntelligence(IJCAI),2015. FilipposChristianos,LukasScha¨fer,andStefanoV.Albrecht.Sharedexperienceactor-criticformulti-\nagentreinforcementlearning. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),\n2020. FilipposChristianos,GeorgiosPapoudakis,ArrasyRahman,andStefanoV.Albrecht. Scalingmulti-\nagentreinforcementlearningwithselectiveparametersharing.",
      "size": 983,
      "sentences": 12
    },
    {
      "id": 37,
      "content": "esinNeuralInformationProcessingSystems(NeurIPS),\n2020. FilipposChristianos,GeorgiosPapoudakis,ArrasyRahman,andStefanoV.Albrecht. Scalingmulti-\nagentreinforcementlearningwithselectiveparametersharing. InProceedingsoftheInternational\nConferenceonMachineLearning(ICML),2021. MehulDamani,ZhiyaoLuo,EmersonWenzel,andGuillaumeSartoretti. PRIMAL : Pathfinding\n2\nviareinforcementandimitationmulti-agentlearning-Lifelong. IEEERoboticsandAutomation\nLetters,6(2):2666–2673,2021. Ariel Felner, Roni Stern, Solomon Eyal Shimony, Eli Boyarski, Meir Goldenberg, Guni Sharon,\nNathanSturtevant,GlennWagner,andPavelSurynek. Search-basedoptimalsolversforthemulti-\nagentpathfindingproblem: Summaryandchallenges. InAnnualSymposiumonCombinatorial\nSearch(SoCS),2017. ArielFelner,JiaoyangLi,EliBoyarski,HangMa,LironCohen,T.K.SatishKumar,andSvenKoenig. Adding heuristics to conflict-based search for multi-agent path finding. In Proceedings of the\nInternationalConferenceonAutomatedPlanningandScheduling,2018.",
      "size": 984,
      "sentences": 13
    },
    {
      "id": 38,
      "content": ",LironCohen,T.K.SatishKumar,andSvenKoenig. Adding heuristics to conflict-based search for multi-agent path finding. In Proceedings of the\nInternationalConferenceonAutomatedPlanningandScheduling,2018. AngelosFilos,ClareLyle,YarinGal,SergeyLevine,NatashaJaques,andGregoryFarquhar. PsiPhi-\nlearning: Reinforcementlearningwithdemonstrationsusingsuccessorfeaturesandinversetem-\nporaldifferencelearning. InProceedingsoftheInternationalConferenceonMachineLearning\n(ICML),2021. JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson. Counterfactualmulti-agentpolicygradients. InProceedingsoftheAAAIConferenceonArtificial\nIntelligence,2018. NirGreshler,OfirGordon,OrenSalzman,andNahumShimkin.Cooperativemulti-agentpathfinding:\nBeyondpathplanningandcollisionavoidance. arXivpreprintarXiv:2105.10993,2021. JonathanHoandStefanoErmon. Generativeadversarialimitationlearning. InAdvancesinNeural\nInformationProcessingSystems(NeurIPS),2016.",
      "size": 953,
      "sentences": 14
    },
    {
      "id": 39,
      "content": "athplanningandcollisionavoidance. arXivpreprintarXiv:2105.10993,2021. JonathanHoandStefanoErmon. Generativeadversarialimitationlearning. InAdvancesinNeural\nInformationProcessingSystems(NeurIPS),2016. WolfgangHo¨nig,ScottKiesel,AndrewTinka,JosephW.Durham,andNoraAyanian. Persistentand\nrobustexecutionofMAPFschedulesinwarehouses. IEEERoboticsandAutomationLetters,4(2):\n1125–1131,2019. TaoanHuang,BistraDilkina,andSvenKoenig. Learningtoresolveconflictsformulti-agentpath\nfindingwithconflict-basedsearch.InProceedingsoftheAAAIConferenceonArtificialIntelligence,\n2021. JiaoyangLi,ArielFelner,EliBoyarski,HangMa,andSvenKoenig. Improvedheuristicsformulti-\nagentpathfindingwithconflict-basedsearch. InProceedingsoftheInternationalJointConference\nonArtificialIntelligence(IJCAI),2019a. JiaoyangLi,DanielHarabor,PeterJ.Stuckey,ArielFelner,HangMa,andSvenKoenig.",
      "size": 850,
      "sentences": 14
    },
    {
      "id": 40,
      "content": "entpathfindingwithconflict-basedsearch. InProceedingsoftheInternationalJointConference\nonArtificialIntelligence(IJCAI),2019a. JiaoyangLi,DanielHarabor,PeterJ.Stuckey,ArielFelner,HangMa,andSvenKoenig. Disjoint\nsplittingformulti-agentpathfindingwithconflict-basedsearch.InProceedingsoftheInternational\nConferenceonAutomatedPlanningandScheduling(ICAPS),2019b. JiaoyangLi,AndrewTinka,ScottKiesel,JosephW.Durham,T.K.SatishKumar,andSvenKoenig. Lifelongmulti-agentpathfindinginlarge-scalewarehouses.InProceedingsoftheAAAIConference\nonArtificialIntelligence,2021. 9\n=== 페이지 10 ===\nAlexTongLin,MarkJ.Debord,KatiaEstabridis,GaryHewer,GuidoMontufar,andStanleyOsher. Decentralizedmulti-agentsbyimitationofacentralizedcontroller. InProceedingsoftheAnnual\nConferenceonMathematicalandScientificMachineLearning,2021. MinghuaLiu, HangMa, JiaoyangLi, andSven Koenig. Taskand pathplanningformulti-agent\npickupanddelivery.",
      "size": 902,
      "sentences": 11
    },
    {
      "id": 41,
      "content": "edcontroller. InProceedingsoftheAnnual\nConferenceonMathematicalandScientificMachineLearning,2021. MinghuaLiu, HangMa, JiaoyangLi, andSven Koenig. Taskand pathplanningformulti-agent\npickupanddelivery. InProceedingsoftheInternationalJointConferenceonAutonomousAgents\nandMultiagentSystems(AAMAS),2019. RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch. Multi-agentactor-\ncriticformixedcooperative-competitiveenvironments.AdvancesinNeuralInformationProcessing\nSystems(NeurIPS),2017. HangMa, SvenKoenig, NoraAyanian, LironCohen, WolfgangHo¨nig, TKKumar, TanselUras,\nHongXu,CraigTovey,andGuniSharon. Overview: Generalizationsofmulti-agentpathfinding\ntoreal-worldscenarios. arXivpreprintarXiv:1702.05515,2017a. Hang Ma, Jiaoyang Li, T.K. Satish Kumar, and Sven Koenig. Lifelong multi-agent path finding\nforonlinepickupanddeliverytasks. In ProceedingsoftheInternationalJointConference on\nAutonomousAgentsandMultiagentSystems(AAMAS),2017b.",
      "size": 942,
      "sentences": 14
    },
    {
      "id": 42,
      "content": "Satish Kumar, and Sven Koenig. Lifelong multi-agent path finding\nforonlinepickupanddeliverytasks. In ProceedingsoftheInternationalJointConference on\nAutonomousAgentsandMultiagentSystems(AAMAS),2017b. HangMa,DanielHarabor,PeterJ.Stuckey,JiaoyangLi,andSvenKoenig. Searchingwithconsistent\nprioritizationformulti-agentpathfinding. InProceedingsoftheAAAIConferenceonArtificial\nIntelligence,2019a. Hang Ma, Wolfgang Ho¨nig, TK Satish Kumar, Nora Ayanian, and Sven Koenig. Lifelong path\nplanningwithkinematicconstraintsformulti-agentpickupanddelivery. InProceedingsofthe\nAAAIConferenceonArtificialIntelligence,2019b. VolodymyrMnih,AdriaPuigdomenechBadia,MehdiMirza,AlexGraves,TimothyLillicrap,Tim\nHarley,DavidSilver,andKorayKavukcuoglu. Asynchronousmethodsfordeepreinforcement\nlearning. InProceedingsoftheInternationalConferenceonMachineLearning(ICML),2016. VanNguyen,PhilippObermeier,TranCaoSon,TorstenSchaub,andWilliamYeoh.Generalizedtarget\nassignmentandpathfindingusinganswersetprogramming.",
      "size": 986,
      "sentences": 13
    },
    {
      "id": 43,
      "content": "edingsoftheInternationalConferenceonMachineLearning(ICML),2016. VanNguyen,PhilippObermeier,TranCaoSon,TorstenSchaub,andWilliamYeoh.Generalizedtarget\nassignmentandpathfindingusinganswersetprogramming. InProceedingsoftheInternational\nJointConferenceonArtificialIntelligence(IJCAI),2017. GeorgiosPapoudakis,FilipposChristianos,LukasScha¨fer,andStefanoV.Albrecht. Benchmark-\ning multi-agent deep reinforcement learning algorithms in cooperative tasks. arXiv preprint\narXiv:2006.07869,2021. TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,JakobFoerster,andShi-\nmonWhiteson. QMIX:Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcement\nlearning. InProceedingsoftheInternationalConferenceonMachineLearning(ICML),2018. OrenSalzmanandRoniStern. Researchchallengesandopportunitiesinmulti-agentpathfindingand\nmulti-agentpickupanddeliveryproblems. InProceedingsoftheInternationalJointConference\nonAutonomousAgentsandMultiagentSystems(AAMAS),2020.",
      "size": 962,
      "sentences": 12
    },
    {
      "id": 44,
      "content": "Researchchallengesandopportunitiesinmulti-agentpathfindingand\nmulti-agentpickupanddeliveryproblems. InProceedingsoftheInternationalJointConference\nonAutonomousAgentsandMultiagentSystems(AAMAS),2020. GuillaumeSartoretti,JustinKerr,YunfeiShi,GlennWagner,T.K.SatishKumar,SvenKoenig,and\nHowieChoset. Primal: Pathfindingviareinforcementandimitationmulti-agentlearning. IEEE\nRoboticsandAutomationLetters,4(3):2378–2385,2019. TomerShahar,ShashankShekhar,DorAtzmon,AbdallahSaffidine,BrendanJuba,andRoniStern. Safemulti-agentpathfindingwithtimeuncertainty. JournalofArtificialIntelligenceResearch,70:\n923–954,2021. GuniSharon,RoniStern,ArielFelner,andNathanR.Sturtevant. Conflict-basedsearchforoptimal\nmulti-agentpathfinding. ArtificialIntelligence,219:40–66,2015. DavidSilver. Cooperativepathfinding. AIIDE,1:117–122,2005. JiamingSong,HongyuRen,DorsaSadigh,andStefanoErmon. Multi-agentgenerativeadversarial\nimitationlearning. arXivpreprintarXiv:1807.09936,2018.",
      "size": 953,
      "sentences": 17
    },
    {
      "id": 45,
      "content": ". DavidSilver. Cooperativepathfinding. AIIDE,1:117–122,2005. JiamingSong,HongyuRen,DorsaSadigh,andStefanoErmon. Multi-agentgenerativeadversarial\nimitationlearning. arXivpreprintarXiv:1807.09936,2018. 10\n=== 페이지 11 ===\nRichard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradi-\nent methods for reinforcement learning with function approximation. In Advances in Neural\nInformationProcessingSystems(NeurIPS),2000. Hongwei Wang, Lantao Yu, Zhangjie Cao, and Stefano Ermon. Multi-agent imitation learning\nwithcopulas. InJointEuropeanConferenceonMachineLearningandKnowledgeDiscoveryin\nDatabases(ECML-KDD),2021. RonaldJ.Williams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcement\nlearning. MachineLearning,8(3):229–256,1992. XiaohuWu,YihaoLiu,XueyanTang,WentongCai,FuningBai,GilbertKhonstantine,andGuopeng\nZhao. Multi-agentpickupanddeliverywithtaskdeadlines. InProceedingsoftheInternational\nSymposiumonCombinatorialSearch(SoCS),2021.",
      "size": 985,
      "sentences": 19
    },
    {
      "id": 46,
      "content": "YihaoLiu,XueyanTang,WentongCai,FuningBai,GilbertKhonstantine,andGuopeng\nZhao. Multi-agentpickupanddeliverywithtaskdeadlines. InProceedingsoftheInternational\nSymposiumonCombinatorialSearch(SoCS),2021. LantaoYu,JiamingSong,andStefanoErmon. Multi-agentadversarialinversereinforcementlearning. InProceedingsoftheInternationalConferenceonMachineLearning(ICML),2019. 11",
      "size": 363,
      "sentences": 7
    }
  ]
}