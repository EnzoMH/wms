{
  "source": "ArXiv",
  "filename": "048_PRIMAL__Pathfinding_via_Reinforcement_and_Imitatio.pdf",
  "total_chars": 45323,
  "total_chunks": 67,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nPRIMAL: Pathfinding via Reinforcement and Imitation\nMulti-Agent Learning\nGuillaume Sartoretti1, Justin Kerr1, Yunfei Shi1, Glenn Wagner2,\nT. K. Satish Kumar3, Sven Koenig3, and Howie Choset1\nAbstract—Multi-agent path finding (MAPF) is an essential\ncomponent of many large-scale, real-world robot deployments,\nfrom aerial swarms to warehouse automation. However, de-\nspite the community’s continued efforts, most state-of-the-art\nMAPF planners still rely on centralized planning and scale\npoorly past a few hundred agents. Such planning approaches\nare maladapted to real-world deployments, where noise and\nuncertainty often require paths be recomputed online, which\nis impossible when planning times are in seconds to min-\nutes. We present PRIMAL, a novel framework for MAPF\nthat combines reinforcement and imitation learning to teach\nfully-decentralized policies, where agents reactively plan paths\nonline in a partially-observable world while exhibiting implicit\ncoordination.",
      "size": 991,
      "sentences": 4
    },
    {
      "id": 2,
      "content": "combines reinforcement and imitation learning to teach\nfully-decentralized policies, where agents reactively plan paths\nonline in a partially-observable world while exhibiting implicit\ncoordination. This framework extends our previous work on Fig. 1. Example problem where 100 simulated robots (white dots) must\ndistributed learning of collaborative policies by introducing computeindividualcollision-freepathsinalarge,factory-likeenvironment. demonstrationsofanexpertMAPFplannerduringtraining,as\nwellascarefulrewardshapingandenvironmentsampling.Once\nenvironments(suchasFig.1),wheretheycanresultindead-\nlearned,theresultingpolicycanbecopiedontoanynumberof\nagents and naturally scales to different team sizes and world and livelocks [5]. dimensions. We present results on randomized worlds with up Extending our previous work on distributed reinforce-\nto 1024 agents and compare success rates against state-of-the- ment learning (RL) for multiple agents in shared environ-\nart MAPF planners.",
      "size": 990,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "th up Extending our previous work on distributed reinforce-\nto 1024 agents and compare success rates against state-of-the- ment learning (RL) for multiple agents in shared environ-\nart MAPF planners. Finally, we experimentally validate the\nments [11], [12], the main contribution of this paper intro-\nlearned policies in a hybrid simulation of a factory mockup,\nduces PRIMAL, a novel hybrid framework for decentralized\ninvolving both real-world and simulated robots. MAPF that combines RL [13] and imitation learning (IL)\nI. INTRODUCTION fromanexpertcentralizedMAPFplanner.Inthisframework,\nagents learn to take into account the consequences of their\nGiventherapiddevelopmentofaffordablerobotswithem-\nposition on other agents, in order to favor movements that\nbedded sensing and computation capabilities, manufacturing\nwill benefit the whole team and not only themselves.",
      "size": 870,
      "sentences": 4
    },
    {
      "id": 4,
      "content": "entofaffordablerobotswithem-\nposition on other agents, in order to favor movements that\nbedded sensing and computation capabilities, manufacturing\nwill benefit the whole team and not only themselves. That\napplications will soon regularly involve the deployment of\nis, by simultaneously learning to plan efficient single-agent\nthousands of robots [1], [2]. To support these applications,\npaths (mostly via RL), and to imitate a centralized expert\nsignificant research effort has been devoted to multi-agent\n(IL), agents ultimately learn a decentralized policy where\npath finding (MAPF) [3], [4], [5], [6] for deployment in\nthey still exhibit implicit coordination during online path\ndistributioncentersandpotentialuseforairplanetaxiing[7],\nplanningwithouttheneedforexplicitcommunicationamong\n[8]. However, as the number of agents in the system grows,\nagents. Since multiple agents learn a common, single-agent\nso does the complexity of coordinating them.",
      "size": 953,
      "sentences": 5
    },
    {
      "id": 5,
      "content": "theneedforexplicitcommunicationamong\n[8]. However, as the number of agents in the system grows,\nagents. Since multiple agents learn a common, single-agent\nso does the complexity of coordinating them. Current state-\npolicy, the final learned policy can be copied onto any\nof-the-art optimal planners can plan for several hundreds\nnumber of agents. Additionally, we consider the case where\nof agents, and the community is now settling for bounded\nagents evolve in a partially-observable world, where they\nsuboptimal planners as a potential solution for even larger\ncan only observe the world in a limited field of view (FOV)\nmulti-agent systems [3], [9]. Another common approach\naround themselves. We present the results of an extensive\nis to rely on reactive planners, which do not plan joint\nsetofsimulationexperimentsandshowthatthefinal,trained\npaths for all agents before execution, but rather correct\npolicies naturally scale to various team and world sizes.",
      "size": 961,
      "sentences": 7
    },
    {
      "id": 6,
      "content": "ners, which do not plan joint\nsetofsimulationexperimentsandshowthatthefinal,trained\npaths for all agents before execution, but rather correct\npolicies naturally scale to various team and world sizes. We\nindividualpathsonlinetoavoidcollisions[5],[10].However,\nfurther highlight cases where PRIMAL outperforms other\nsuch planners often prove inefficient in cluttered factory\nstate-of-the-artMAPFplannersandcaseswhereitstruggles. G. Sartoretti, H. Choset, J. Kerr, Y. Shi are with the Robotics We also present experimental results of the trained policy in\nInstitute at Carnegie Mellon University, Pittsburgh, PA 15213, USA. a hybrid simulation of a factory mockup. {gsartore,jgkerr,yunfeischoset}@andrew.cmu.edu. The paper is structured as follows: In Section II, we sum-\nG. Wagner is with the Commonwealth Scientific and Industrial\nResearch Organisation (CSIRO), Pullenvale QLD 4069, Australia, marizethestate-of-the-artinMAPFandmulti-agentRL.We\nglenn.s.wagner@gmail.com.",
      "size": 969,
      "sentences": 6
    },
    {
      "id": 7,
      "content": ". Wagner is with the Commonwealth Scientific and Industrial\nResearch Organisation (CSIRO), Pullenvale QLD 4069, Australia, marizethestate-of-the-artinMAPFandmulti-agentRL.We\nglenn.s.wagner@gmail.com. detailhowMAPFiscastintheRLframeworkinSectionIII,\nT.K.S.KumarandS.KoenigarewiththeComputerScienceDepartment\nand how learning is carried out in Section IV. Section V\nat the University of Southern California, Los Angeles, CA 90089, USA. tkskwork@gmail.com, skoenig@usc.edu. presents our results, and Section VI concluding remarks. 9102\nbeF\n02\n]OR.sc[\n3v13530.9081:viXra\n=== 페이지 2 ===\nII. PRIORWORK of dimensionality: most joint approaches fail as the state-\naction spaces explode combinatorially, requiring impractical\nA. Multi-Agent Path Finding (MAPF)\namounts of training data to converge [24]. In this context,\nMAPF is an NP-hard problem even when approximat- many recent work have focused on decentralized policy\ning optimal solutions [14], [15].",
      "size": 947,
      "sentences": 9
    },
    {
      "id": 8,
      "content": "F)\namounts of training data to converge [24]. In this context,\nMAPF is an NP-hard problem even when approximat- many recent work have focused on decentralized policy\ning optimal solutions [14], [15]. MAPF planners can be learning [25], [26], [27], [28], [29], where agents each learn\nbroadly classified into three categories: coupled, decoupled, theirownpolicy,whichshouldencompassameasureofagent\nand dynamically-coupled approaches. Coupled approaches cooperation,atleastduringtraining.Onesuchapproachisto\n(e.g., standard A∗), which treat the multi-agent system as trainagentstopredictotheragents’actions[26],[27],which\na single, very high dimensional agent, greatly suffer from generally scales poorly as the team size increases. In most\nan exponential growth in planning complexity.",
      "size": 784,
      "sentences": 5
    },
    {
      "id": 9,
      "content": "otheragents’actions[26],[27],which\na single, very high dimensional agent, greatly suffer from generally scales poorly as the team size increases. In most\nan exponential growth in planning complexity. Hence we cases, some form of centralized learning is involved, where\nfocus on decoupled and dynamically-coupled, state-of-the- the sum of experience of all agents can be used towards\nart planners for large MAPF problems. training a common aspect of the problem (e.g., network\nDecoupled approaches compute individual paths for each outputorvalue/advantagecalculation)[25],[27],[28].When\nagent, and then adjust these paths to avoid collisions. Since centrally learning a network output, parameter sharing has\nindividual paths can be planned, as well as adjusted for beenusedtoenablefasterandmorestabletrainingbysharing\ncollisions, in low-dimensional search spaces, decoupled ap- the weights of some of the layers of the neural net [25].",
      "size": 934,
      "sentences": 5
    },
    {
      "id": 10,
      "content": "ned, as well as adjusted for beenusedtoenablefasterandmorestabletrainingbysharing\ncollisions, in low-dimensional search spaces, decoupled ap- the weights of some of the layers of the neural net [25]. In\nproaches can rapidly find paths for large multi-agent sys- actor-critic approaches, for example, the critic output of the\ntems [5], [16]. Velocity planners fix the individual path that network is often trained centrally with parameter sharing,\nwill be followed by each agent, then find a velocity profile sinceitappliestoallagentsinthesystem,andhasbeenused\nalong those paths that avoids collisions [6], [10]. In partic- to train cooperation between agents [25], [27].",
      "size": 670,
      "sentences": 4
    },
    {
      "id": 11,
      "content": "agent, then find a velocity profile sinceitappliestoallagentsinthesystem,andhasbeenused\nalong those paths that avoids collisions [6], [10]. In partic- to train cooperation between agents [25], [27]. Centralized\nular, ORCA [5] adapts the agents’ velocity magnitudes and learningcanalsohelpwhendealingwithpartially-observable\ndirections online to avoid collisions, on top of individually- systems, by aggregating all the agents’ observations into a\nplanned single-agent paths, and recent work has focused on single learning process [25], [27], [28]. such an obstacle avoidance approach using reinforcement\nSecond, many existing approaches rely on explicit com-\nlearning (RL) [10]. Priority planners assign a priority to\nmunication among agents, to share observations or selected\neach agent, and plan individual paths in decreasing order of\nactions during training and sometimes also during policy\npriority, each time treating higher priority agents as moving\nexecution [26], [27], [28].",
      "size": 984,
      "sentences": 5
    },
    {
      "id": 12,
      "content": "nt, and plan individual paths in decreasing order of\nactions during training and sometimes also during policy\npriority, each time treating higher priority agents as moving\nexecution [26], [27], [28]. In our previous work [11], [12],\nobstacles [17], [18], [19]. The main drawback of decoupled\nwe focused on extending the state-of-the-art asynchronous\napproaches is that the low-dimensional search spaces used\nadvantage actor-critic (A3C) algorithm to enable multiple\nonly represent a small portion of the joint configuration\nagents to learn a common, homogeneous policy in shared\nspace, meaning that these approaches cannot be complete\nenvironments without the need for any explicit agent com-\n(i.e., find paths for all solvable problems) [20]. munication.Thatis,theagentshadaccesstothefullstateof\nSeveralrecentapproachesliebetweencoupledanddecou- the system (fully-observable world), and treated each other\npled approaches: they allow for richer agent-agent behaviors as moving obstacles.",
      "size": 988,
      "sentences": 4
    },
    {
      "id": 13,
      "content": "stateof\nSeveralrecentapproachesliebetweencoupledanddecou- the system (fully-observable world), and treated each other\npled approaches: they allow for richer agent-agent behaviors as moving obstacles. There, stabilizing learning is key: the\nthan can be achieved with decoupled planners, while avoid- learning gradients obtained by agents experiencing the same\ning planning in the joint configuration space. A common episode in the same environment are often very correlated\napproach followed by dynamically coupled approaches is and destabilized the learning process. To prevent this, we\nto grow the search space as necessary during planning [3], relied on experience replay [30] and carefully randomized\n[21]. Conflict-Based Search (CBS) and its variants [4], [21] episode initialization. However, we did not train agents to\nplansforindividualagentsandconstructsasetofconstraints exhibit any form of coordination.",
      "size": 913,
      "sentences": 6
    },
    {
      "id": 14,
      "content": "lict-Based Search (CBS) and its variants [4], [21] episode initialization. However, we did not train agents to\nplansforindividualagentsandconstructsasetofconstraints exhibit any form of coordination. That is, in our previous\nto find optimal or near-optimal solutions without exploring extension of A3C, agents collaborate (i.e., work towards\nhigher-dimensionalspaces.ExtendingstandardA∗ toMAPF, a common goal) but do not explicitly cooperate (i.e., take\nM∗ and its variants [3] first plan paths for individual agents actions to benefit the whole group and not only themselves). and then project these individual plans forward through\nIn our work, we propose to rely on imitation learning\ntime searching for collisions.",
      "size": 718,
      "sentences": 4
    },
    {
      "id": 15,
      "content": "tions to benefit the whole group and not only themselves). and then project these individual plans forward through\nIn our work, we propose to rely on imitation learning\ntime searching for collisions. The configuration space is\n(IL)ofanexpertcentralizedplanner(ODrM*)totrainagents\nonly locally expanded around any collision between single-\nto exhibit coordination, without the need for explicit com-\nagent plans, where joint planning is performed through\nmunication,inapartially-observableworld.Wealsopropose\n(usually limited) backtracking to solve the collision and\na carefully crafted reward structure and a way to sample the\nresume single-agent plans. In particular, OD-recursive-M∗\nchallenges used to train the agents.",
      "size": 721,
      "sentences": 4
    },
    {
      "id": 16,
      "content": "ed) backtracking to solve the collision and\na carefully crafted reward structure and a way to sample the\nresume single-agent plans. In particular, OD-recursive-M∗\nchallenges used to train the agents. The resulting, trained\n(ODrM*)[22]canfurtherreducethesetofagentsforwhich\npolicy is executed by each agent based on locally gathered\njointplanningisnecessary,bybreakingitdownintoindepen-\ninformation but still allows agents to exhibit cooperative be-\ndent collision sets, combined with Operator Decomposition\nhavior, and is also robust against agent failures or additions. (OD) [23] to keep the branching factor small during search. III.",
      "size": 635,
      "sentences": 5
    },
    {
      "id": 17,
      "content": "perative be-\ndent collision sets, combined with Operator Decomposition\nhavior, and is also robust against agent failures or additions. (OD) [23] to keep the branching factor small during search. III. POLICYREPRESENTATION\nB. Multi-Agent Reinforcement Learning (MARL)\nThe first and most important problem encountered when Inthissection,wepresenthowtheMAPFproblemiscast\ntransitioningfromsingle-tomulti-agentlearningisthecurse intotheRLframework.Wedetailtheobservationandaction\n=== 페이지 3 ===\nspaces of each agent, the reward structure and the neural\nnetwork that represents the policy to be learned. World state\nA. Observation Space\nWe consider a partially-observable discrete gridworld,\nwhere agents can only observe the state of the world in a |v|\nlimited FOV centered around themselves (10×10 FOV in\npractice). We believe that considering a partially-observable Magnitude\nworld is an important step towards real-world robot deploy-\nment.",
      "size": 936,
      "sentences": 7
    },
    {
      "id": 18,
      "content": "in a |v|\nlimited FOV centered around themselves (10×10 FOV in\npractice). We believe that considering a partially-observable Magnitude\nworld is an important step towards real-world robot deploy-\nment. In scenarios where the full map of the environment is\nv^\navailable (e.g., automated warehouses), it is always possible\nto train agents with full observability of the system by v\nusingasufficientlylargeFOV.Additionally,assumingafixed\nFOV can allow the policy to generalize to arbitrary world\nsizes and also helps to reduce the input dimension to the\nUnit vector\nneural network. However, an agent needs to have access to\ninformationaboutitsgoal,whichisoftenoutsideofitsFOV. To this end, it has access to both a unit vector pointing\ntowards its goal and Euclidean distance to its goal at all\ntimes (see Figure 2). In the limited FOV, we separate the available information\ninto different channels to simplify the agents’ learning task.",
      "size": 931,
      "sentences": 6
    },
    {
      "id": 19,
      "content": "wards its goal and Euclidean distance to its goal at all\ntimes (see Figure 2). In the limited FOV, we separate the available information\ninto different channels to simplify the agents’ learning task. Specifically, each observation consists of binary matrices\nrepresenting the obstacles, the positions of other agents, Obstacles Agents' Neighbors' Agent's\nthe agent’s own goal location (if within the FOV), and the positions goals goal\nFig. 2. Observation space of each agent (here, for the light blue agent). position of other observable agents’ goals. When agents are\nAgents are displayed as colored squares, their goals as similarly-colored\nclose to the edges of the world, obstacles are added at all stars,andobstaclesasgreysquares.Eachagentonlyhasaccesstoalimited\npositions outside the world’s boundaries.",
      "size": 809,
      "sentences": 7
    },
    {
      "id": 20,
      "content": "their goals as similarly-colored\nclose to the edges of the world, obstacles are added at all stars,andobstaclesasgreysquares.Eachagentonlyhasaccesstoalimited\npositions outside the world’s boundaries. field of view (FOV) centered around its position, in which information is\nbrokendownintochannels:positionsofobstacles,positionofnearbyagents,\ngoal positions of these nearby agents (projected onto the boundary of the\nB. Action Space\nFOVifoutsideoftheFOV),andpositionofitsgoalifwithintheFOV.Note\nAgents take discrete actions in the gridworld: moving howthebottomrowoftheobstaclechannelhasbeenfilledwithobstacles,\nsincethesepositionsareoutsideoftheworld’sboundaries.Eachagentalso\none cell in one of the four cardinal directions or staying\nhasaccesstoanormalizedvectorpointingtoitsgoal(oftenoutsideofits\nstill. At each timestep, certain actions may be invalid, such FOV) and its magnitude (distance to goal), as a natural way to let agents\nas moving into a wall or another agent.",
      "size": 975,
      "sentences": 4
    },
    {
      "id": 21,
      "content": "sgoal(oftenoutsideofits\nstill. At each timestep, certain actions may be invalid, such FOV) and its magnitude (distance to goal), as a natural way to let agents\nas moving into a wall or another agent. During training, learntoselecttheirgeneraldirectionoftravel. actionsaresampledonlyfromvalidactionsandanadditional\nfoundthatremovingthisaspectoftherewardfunctionledto\nloss function aids in learning this information. We exper-\npoor convergence, which might be the case due to conflicts\nimentally observed that this approach enables more stable\nbetween the RL and IL gradients.",
      "size": 574,
      "sentences": 5
    },
    {
      "id": 22,
      "content": "tion aids in learning this information. We exper-\npoor convergence, which might be the case due to conflicts\nimentally observed that this approach enables more stable\nbetween the RL and IL gradients. Though invalid moves\ntraining, compared to giving negative rewards to agents for\n(moving back to the previous cell, or into an obstacle) are\nselectinginvalidmoves.Additionally,tocombatconvergence\nfiltered out of the action space during training as described\nto oscillating policies, agents are prevented during training\nin Section III-B because agents act sequentially in a random\nfrom returning to the location they occupied at the last\norder, it is still possible for them to collide, e.g., when\ntimestep(agentscanstillstaystillduringmultiplesuccessive\nmultiple agents choose to move to the same location at\ntimesteps). This is necessary to encourage exploration and\nthe same timestep. Agent collisions result in a −2 reward. learn effective policies (even when also using IL).",
      "size": 979,
      "sentences": 6
    },
    {
      "id": 23,
      "content": "move to the same location at\ntimesteps). This is necessary to encourage exploration and\nthe same timestep. Agent collisions result in a −2 reward. learn effective policies (even when also using IL). Agents receive a +20 reward for finishing an episode, i.e.,\nIfanagentselectsaninvalidmoveduringtesting,itinstead\nwhen all agents are on their goals simultaneously. stays still for that timestep. In practice, agents very rarely\nselect invalid moves once fully trained, showing that they TABLEI\neffectively learn the set of valid actions in each state. SIMPLEREWARDSTRUCTURE. Action Reward\nC. Reward Structure\nMove[N/E/S/W] -0.3\nOur reward function (Table I) follows the same intuition AgentCollision -2.0\nthat most reward functions for gridworlds use, where agents\nNoMovement(on/offgoal) 0.0/-0.5\nare punished for each timestep they are not resting on goal,\nFinishEpisode +20.0\nleading to the strategy of reaching their goals as quickly\nD. Actor-Critic Network\nas possible.",
      "size": 971,
      "sentences": 9
    },
    {
      "id": 24,
      "content": "t(on/offgoal) 0.0/-0.5\nare punished for each timestep they are not resting on goal,\nFinishEpisode +20.0\nleading to the strategy of reaching their goals as quickly\nD. Actor-Critic Network\nas possible. We penalize agents slightly more for staying\nstill than for moving, which is necessary to encourage Our work relies on the asynchronous advantage actor-\nexploration.Eventhoughimitationassistsinexploration,we critic (A3C) algorithm [31] and extends our previous work\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\n10x10x4 2x1 12x1 Fully connected\nInput Tensors Goal Position Fully connected ReLU ReLU 512x1\n512x1 LSTM 1x1 Sigmoid\n10x10x128 5x5x128 2x2x256 Fully connected\nblocking\nConv2D Conv2D MaxPool\n512x1 ReLU 512x1 512x1\n10x10x128 5x5x256\nFully connected Fully connected Fully connected\nConv2D Conv2D\n10x10x128 5x5x128 5x5x256 500x1 512x1 Softmax 5x1 1x1\nConv2D MaxPool Conv2D Conv2D Concatenate Policy Value\nFig.3. Theneuralnetworkconsistsof7convolutionallayersinterleavedwithmaxpoolinglayers,followedbyanLSTM.",
      "size": 995,
      "sentences": 3
    },
    {
      "id": 25,
      "content": "x5x128 5x5x256 500x1 512x1 Softmax 5x1 1x1\nConv2D MaxPool Conv2D Conv2D Concatenate Policy Value\nFig.3. Theneuralnetworkconsistsof7convolutionallayersinterleavedwithmaxpoolinglayers,followedbyanLSTM. on distributed learning for multiple agents in shared en- and stabilize training. First, the blocking prediction output\nvironments [11], [12]. We use a deep neural network to is updated by minimizing L , the log likelihood of\nblocking\napproximate the agent’s policy, which maps the current predicting incorrectly. Second, we define the loss function\nobservationofitssurroundingstothenextactiontotake.This L to minimize the log likelihood of selecting an invalid\nvalid\nnetwork has multiple outputs, one of them being the actual move [11], as mentioned in Section III-B. policy and the others only being used toward training it. IV. LEARNING\nWe use the 6-layer convolutional network pictured in Fig.",
      "size": 897,
      "sentences": 9
    },
    {
      "id": 26,
      "content": "one of them being the actual move [11], as mentioned in Section III-B. policy and the others only being used toward training it. IV. LEARNING\nWe use the 6-layer convolutional network pictured in Fig. 3,\ntaking inspiration from VGGnet [32], using several small In this section, we detail our distributed framework for\n3×3 kernels between each max-pooling layer. learning MAPF with implicit agent coordination. The RL\nportion of our framework builds upon our previous work\nSpecifically, the two inputs to the neural network – the\non distributed RL for multiple agents in shared environ-\nlocal observation and the goal direction/distance – are pre-\nments [11], [12]. In our work, we introduce an IL module\nprocessedindependently,beforebeingconcatenatedhalf-way\nthroughtheneuralnetwork.Thefour-channelmatrices(10× that allows agents to learn from expert demonstrations. 10×4 tensor) representing the local observation are passed A.",
      "size": 927,
      "sentences": 9
    },
    {
      "id": 27,
      "content": "forebeingconcatenatedhalf-way\nthroughtheneuralnetwork.Thefour-channelmatrices(10× that allows agents to learn from expert demonstrations. 10×4 tensor) representing the local observation are passed A. Coordination Learning\nthrough two stages of three convolutions and maxpooling,\nOne of the key challenges in training a decentralized\nfollowed by a last convolutional layer. In parallel, the goal\npolicy is to encourage agents to act selflessly, even though\nunit vector and magnitude are passed through one fully-\nit may be detrimental to their immediate maximization of\nconnected (fc) layer. The concatenation of both of these\nreward. In particular, agents typically display undesirable\npre-processed inputs is then passed through two fc layers,\nselfish behavior when stopped on their goals while blocking\nwhich is finally fed into a long-short-term memory (LSTM)\nother agents’ access to their own goals. A naive implemen-\ncell with output size 512.",
      "size": 948,
      "sentences": 7
    },
    {
      "id": 28,
      "content": "ish behavior when stopped on their goals while blocking\nwhich is finally fed into a long-short-term memory (LSTM)\nother agents’ access to their own goals. A naive implemen-\ncell with output size 512. A residual shortcut [33] connects\ntation of our previous work [11], where agents distributedly\nthe output of the concatenation layer to the input layer of\nlearn a fully selfish policy, fails in dense environments with\nthe LSTM. The output layers consist of the policy neurons\nmany narrow environmental features where the probability\nwithsoftmaxactivation,thevalueoutput,andafeaturelayer\nofblockingotheragentsishigh.Thatis,agentssimplylearn\nusedtotraineachagenttoknowwhetheritisblockingother\nto move as fast as possible to their goals, and then to never\nagentsfromreachingtheirgoals(detailedinSectionIV-A.1).",
      "size": 807,
      "sentences": 4
    },
    {
      "id": 29,
      "content": "ishigh.Thatis,agentssimplylearn\nusedtotraineachagenttoknowwhetheritisblockingother\nto move as fast as possible to their goals, and then to never\nagentsfromreachingtheirgoals(detailedinSectionIV-A.1). move away from it, not even to let other agents access their\nDuring training, the policy, value, and “blocking” outputs\nown goals (despite the fact that this would end the episode\nareupdatedinbatcheveryn=256stepsorwhenanepisode\nearlier, which would result in higher rewards for all agents). finishes. As is common, the value is updated to match the\ntotal discounted return (R = (cid:80)k γir ) by minimizing: Many of the current multi-agent training techniques ad-\nt i=0 t+i dressing this selfishness problem are invalidated by the\nT\n(cid:88) size of the environments and the limited FOV of agents. L = (V(o ;θ)−R )2.",
      "size": 817,
      "sentences": 5
    },
    {
      "id": 30,
      "content": "he current multi-agent training techniques ad-\nt i=0 t+i dressing this selfishness problem are invalidated by the\nT\n(cid:88) size of the environments and the limited FOV of agents. L = (V(o ;θ)−R )2. (1)\nV t t\nSharedcritics[27]haveproveneffectiveatmulti-agentcredit\nt=0\nTo update the policy, we use an approximation of the assignment.However,thesemethodsaretypicallyusedwhen\nadvantage function by bootstrapping using the value func- agentshavealmostfullinformationabouttheirenvironment. tion: A(o ,a ;θ)= (cid:80)k−1γir +γkV(o ;θ)−V(o ;θ) In our highly decentralized scenario, assigning credit to\nt t i=0 t+i k+t t\n(where k is bounded by the batch size T).",
      "size": 656,
      "sentences": 4
    },
    {
      "id": 31,
      "content": "rmationabouttheirenvironment. tion: A(o ,a ;θ)= (cid:80)k−1γir +γkV(o ;θ)−V(o ;θ) In our highly decentralized scenario, assigning credit to\nt t i=0 t+i k+t t\n(where k is bounded by the batch size T). We also add an agents may be confusing when they cannot observe the\nentropy term H(π(o)) to the policy loss, which has been source of the penalty, for example, when an agent cannot\nshown to encourage exploration and discourage premature observe that a long hallway is a dead-end, yet the universal\nconvergence[34]bypenalizingapolicythatalwayschooses critic sharply decreases the value function. Another popular\nthe same actions. The policy loss reads multi-agent training technique is to apply joint rewards to\nagentsinanattempttohelpthemrealizethebenefitoftaking\nT\n(cid:88)\nL =σ ·H(π(o))− log(P(a |π,o;θ)A(o ,a ;θ)) (2) personal sacrifices to benefit the team [35], [12]. We briefly\nπ H t t t\ntried to assign joint rewards to agents within the same FOV.",
      "size": 954,
      "sentences": 6
    },
    {
      "id": 32,
      "content": "oftaking\nT\n(cid:88)\nL =σ ·H(π(o))− log(P(a |π,o;θ)A(o ,a ;θ)) (2) personal sacrifices to benefit the team [35], [12]. We briefly\nπ H t t t\ntried to assign joint rewards to agents within the same FOV. t=0\nwithasmallentropyweightσ (σ =0.01inpractice).We However,thisproducednonoticeabledifferenceinbehavior,\nH H\nrely on two additional loss functions which help to guide soweabandoneditinfavorofthemethodsdescribedbelow. [표 데이터 감지됨]\n\n=== 페이지 5 ===\nTo successfully teach agents collaborative behavior, we\nReinforcement Learning\nrely on three methods: applying a penalty for encouraging\nother agents’ movement (called the “blocking penalty”), World\nusing expert demonstrations during training, and tailoring\no r o r o r\nthe random environments during training to expose agents 1 1 ... ... n n\nto more difficult cluttered scenarios.",
      "size": 826,
      "sentences": 4
    },
    {
      "id": 33,
      "content": "penalty”), World\nusing expert demonstrations during training, and tailoring\no r o r o r\nthe random environments during training to expose agents 1 1 ... ... n n\nto more difficult cluttered scenarios. We emphasize that, Agent 1 Agent Agent n\n...\nwithout all three methods, the learning process is either\nunstable (no learning) or converges to a worse policy than\nAC Net 1 AC Net AC Net n\n...\nwith all three, as is apparent in Fig. 5. 1) Blocking Penalty: First, we augment the reward func-\ntion shown in Table I with a sharp penalty (−2 in practice) a a a\n1 ... n\nif an agent decides to stay on goal while preventing another\nagentfromreachingitsgoal.Theintuitionbehindthisreward RL/IL\nis to provide an incentive for agents to leave their goals, Switch\noffsetting the (selfish) local maximum agents experience\na a a\nwhile resting on goal.",
      "size": 836,
      "sentences": 4
    },
    {
      "id": 34,
      "content": "hingitsgoal.Theintuitionbehindthisreward RL/IL\nis to provide an incentive for agents to leave their goals, Switch\noffsetting the (selfish) local maximum agents experience\na a a\nwhile resting on goal. Our definition of blocking includes 1 ... n\ncases where an agent is not just preventing another agent\no\nfrom reaching its goal, but also cases where an agent delays 1...n\nExpert (M*)\nanother agent significantly (in practice, by 10 or more steps\nto match the size of the agents’ FOV). This looser definition Imitation Learning\nof blocking is necessary because of the agents’ small FOV. Fig.4.",
      "size": 591,
      "sentences": 4
    },
    {
      "id": 35,
      "content": "gent significantly (in practice, by 10 or more steps\nto match the size of the agents’ FOV). This looser definition Imitation Learning\nof blocking is necessary because of the agents’ small FOV. Fig.4. StructureofourhybridRL/ILapproach.Inthebeginningofeach\nAlthough an alternate route might exists around the agent in episode,arandomdrawdetermineswhethertheepisodewillbeRL-orIL-\nlarger worlds, it is illogical to move around the agent when based,andthe“switch”(inthemiddle)issetaccordingly.FortheRL-based\ncoordination could lead to shorter a path, especially if the\nlearning,ateachtimestep,eachagent(1,..,n)drawsitsobservationoiand\nreward ri for its previous action from the world (learning environment)\nalternate route lies outside the agent’s FOV (and therefore is and uses the observation to select an action ai via its own copy of the\nuncertain).",
      "size": 848,
      "sentences": 4
    },
    {
      "id": 36,
      "content": "revious action from the world (learning environment)\nalternate route lies outside the agent’s FOV (and therefore is and uses the observation to select an action ai via its own copy of the\nuncertain). neuralnetwork.Theactionsofdifferentagentsareexecutedsequentiallyin\narandomorder.Sinceagentsoftenpushandpullweightsfromacommon,\nWe use standard A∗ to determine the length of an agent’s\nshared neural network, they ultimately share the same weights in their\npath from its current position to its goal and then that of individual nets. For the IL-based learning, an expert centralized planner\nits path when each one of the other agents is removed from coordinatesallagentsduringtheepisode,whosebehaviortheagentslearn\ntoimitate,allowingthemtolearncoordinatedbehaviors. the world. If the second path is shorter than the first one by\nmore than 10 steps, that other agent is considered blocking.",
      "size": 887,
      "sentences": 5
    },
    {
      "id": 37,
      "content": "sebehaviortheagentslearn\ntoimitate,allowingthemtolearncoordinatedbehaviors. the world. If the second path is shorter than the first one by\nmore than 10 steps, that other agent is considered blocking. The “blocking” output of the network is trained to predict wherelearningagentsonlyhaveaccesstoafinitesetofpre-\nwhen an agent is blocking others, to implicitly provide the recorded expert trajectories. The heuristic used in ODrM*\nagentwithan“explanation”oftheextrapenaltyitwillincur inherently helps generate high-quality paths with respect\nin this case. to our reward structure (Table I), where agents move to\n2) Combining RL and IL: Second, combining RL and IL their goals as quickly as possible (while avoiding collisions)\nhasbeenshowntoleadtofaster,morestabletrainingaswell and rest on it. Therefore, the RL/IL gradients are naturally\nas higher-quality solutions in robot manipulation [36], [37], coherent, thus avoiding oscillations in the learning process. [38].",
      "size": 967,
      "sentences": 8
    },
    {
      "id": 38,
      "content": "ngaswell and rest on it. Therefore, the RL/IL gradients are naturally\nas higher-quality solutions in robot manipulation [36], [37], coherent, thus avoiding oscillations in the learning process. [38]. These advantages are likely due to the fact that IL can Leveraging demonstrations is a necessary component of\nhelp to quickly identify high-quality regions of the agents our system: without it, learning progresses far slower and\nstate-action space, while RL can further improve the policy converges to a significantly worse solution. However, we\nby freely exploring these regions. In our work, we randomly experimented with various IL proportions (10-50% by in-\nselectinthebeginningofeachepisodewhetheritwillinvolve crements of 10%) and observed that the RL/IL ratio does\nRL or IL (thus setting the central switch in the middle of not seem to affect the performance of the trained policy by\nFig. 4).",
      "size": 899,
      "sentences": 7
    },
    {
      "id": 39,
      "content": "eritwillinvolve crements of 10%) and observed that the RL/IL ratio does\nRL or IL (thus setting the central switch in the middle of not seem to affect the performance of the trained policy by\nFig. 4). Such demonstrations are generated dynamically by much.Finally,althoughwecouldusedynamicmethodssuch\nrelying on the centralized planner ODrM* [3] (with (cid:15)=2). as DA GGER [40] or confident inference [41] because of the\nA trajectory of observations and actions T ∈ (O × A)n availability of a real-time planner, we chose to use behavior\nis obtained for each agent, and we minimize the behavior cloningbecauseofitssimplicityandeaseofimplementation. cloning loss: It is unclear whether using such methods would lead to a\nT\n1 (cid:88) performanceincrease,andwillbethesubjectoffutureworks. L =− log(P(a |π,o ;θ)).",
      "size": 810,
      "sentences": 6
    },
    {
      "id": 40,
      "content": "itssimplicityandeaseofimplementation. cloning loss: It is unclear whether using such methods would lead to a\nT\n1 (cid:88) performanceincrease,andwillbethesubjectoffutureworks. L =− log(P(a |π,o ;θ)). (3)\nbc T t t 3) Environment Sampling: Finally, during training, we\nt=0\nOur implementation deviates from [36], [39] in that we randomize both the sizes and obstacle densities of worlds\ncombine off-policy behavior cloning with on-policy actor- in the beginning of each episode. We found that uniformly\ncriticlearning,ratherthanwithoff-policydeepdeterministic sampling the size and densities of worlds did not expose\npolicy gradient. We explored this approach since we can the agents to enough situations in which coordination is\ncheaply generate expert demonstrations online in the begin- necessary because of the relative sparsity of agent-agent\nning of a new training episode, as opposed to other work interactions.",
      "size": 915,
      "sentences": 6
    },
    {
      "id": 41,
      "content": "ination is\ncheaply generate expert demonstrations online in the begin- necessary because of the relative sparsity of agent-agent\nning of a new training episode, as opposed to other work interactions. We therefore sample both the size and the\n[표 데이터 감지됨]\n\n=== 페이지 6 ===\nobstacle density from a distribution that favors smaller and\n256\ndenserenvironments,forcingtheagentstolearncoordination\nsince they experience agent-agent interactions more often. 200\n150\nB. Training Details\n100\n1) Environment: The size of the square environment is\nrandomly selected in the beginning of each episode to be 50\neither 10, 40, or 70, with a probability distribution that 0\n0 1 2 3.14\nmakes 10-sized worlds twice as likely. The obstacle density Episode .",
      "size": 735,
      "sentences": 5
    },
    {
      "id": 42,
      "content": "ndomly selected in the beginning of each episode to be 50\neither 10, 40, or 70, with a probability distribution that 0\n0 1 2 3.14\nmakes 10-sized worlds twice as likely. The obstacle density Episode . 105\nis randomly selected from a triangular distribution between\n0and50%,withthepeakcenteredat33%.Theplacementof\nobstacles,agents,andgoalsisuniformlyatrandomacrossthe\nenvironment, with the caveat that each agent had to be able\ntoreachitsgoal.Thatis,eachagentisinitiallyplacedinthe\nsame connected region as its goal. It is possible that agents\ntrain in impossible environments (e.g., two agents might be\nspawned in the same narrow connected region, each on the\nother’s goal), although highly unlikely. The actions of the\nagents are executed sequentially in a random order at each\ntimestep to ensure that they have equal priority (i.e., race\nconditions are resolved randomly).",
      "size": 873,
      "sentences": 5
    },
    {
      "id": 43,
      "content": "although highly unlikely. The actions of the\nagents are executed sequentially in a random order at each\ntimestep to ensure that they have equal priority (i.e., race\nconditions are resolved randomly). 2) Parameters: We use a discount factor (γ) of 0.95, an\nepisode length of 256, and a batch size of 128 so that up to\ntwo gradient updates are performed each episode per agent. The probability of observing a demonstration is 50% per\nepisode. We use the Nadam optimizer [42] with a learning\nrate beginning at 2 · 10−5 and decaying proportionally to\nthe inverse square root of episode count. We train in 3\nindependentenvironmentswith8agentseach,synchronizing\nagentsinthesameenvironmentinthebeginningofeachstep\nandallowingthemtoactinparallel.Trainingwasperformed\nat the Pittsburgh Supercomputing Center (PSC) [43] on 7\ncores of a Intel Xeon E5-2695 and one NVIDIA K80 GPU,\nandlastedaround20days.Thefullcodeusedtotrainagents\nis available at https://goo.gl/T627XD.",
      "size": 958,
      "sentences": 6
    },
    {
      "id": 44,
      "content": "at the Pittsburgh Supercomputing Center (PSC) [43] on 7\ncores of a Intel Xeon E5-2695 and one NVIDIA K80 GPU,\nandlastedaround20days.Thefullcodeusedtotrainagents\nis available at https://goo.gl/T627XD. V. RESULTS\nIn this section, we present the results of an extensive set\nof simulations comparing PRIMAL against state-of-the-art\nMAPF planners in gridworlds. These tests are performed\nin environments with varying obstacle densities, grid sizes,\nand team sizes. Finally, we present experimental results\nfor a scenario featuring both physical and simulated robots\nplanning paths online in an indoor factory mockup. A. Comparison with Other MAPF Planners\nForourexperiments,weselectedCBS[21]asouroptimal,\ncentralized planner, ODrM* [3] as suboptimal, centralized\noption (with inflation factors (cid:15) = 1.5 and (cid:15) = 10), and\nORCA [5] as fully-decoupled velocity planner.",
      "size": 873,
      "sentences": 6
    },
    {
      "id": 45,
      "content": "ectedCBS[21]asouroptimal,\ncentralized planner, ODrM* [3] as suboptimal, centralized\noption (with inflation factors (cid:15) = 1.5 and (cid:15) = 10), and\nORCA [5] as fully-decoupled velocity planner. Note that\nall other planners have access to the whole state of the\nsystem, whereas PRIMAL assumes that each agent only\nhas partial observability of the system. World sizes are\n{10,20,40,80,160}, densities {0,0.1,0.2,0.3}, and team\nsizes {4,8,...,1024}. We placed no more than 32 agents in\n10-sizedworlds,nomorethan128agentsin20-sizedworlds,\nand no more than 1024 agents in 40-sized worlds. htgnel\nedosipE\nMean episode length during training\nFull PRIMAL\nNo Environment Sampling\nNo Blocking Penalty\nNo Imitation Learning\nExpert (ODrM*)\nFig. 5. Mean episode length during training, lower is better. The dotted\nline shows the baseline, obtained from the expert ODrM* planner.",
      "size": 871,
      "sentences": 8
    },
    {
      "id": 46,
      "content": "ling\nNo Blocking Penalty\nNo Imitation Learning\nExpert (ODrM*)\nFig. 5. Mean episode length during training, lower is better. The dotted\nline shows the baseline, obtained from the expert ODrM* planner. When\nweremoveeitherenvironmentsampling,theblockingpenalties,orimitation\nlearningfromourapproach,thepolicyconvergestoaworsesolution. In our experiments, we compared the success rates of\nthe different planners, that is whether they complete a\ngiven problem within a given amount of wall clock time\nor timesteps. For CBS and ODrM*, we used a timeout of\n300sand60s,respectively,tomatchpreviousresults[3].We\ndivided the timeout by 5 for ODrM* because we used a\nC++ implementation which was experimentally measured\nto be about 5 times faster than the previously used Python\nimplementation. For ORCA, we use a timeout of 60s but\nterminate early when all agents are in a deadlock (defined\nas all agents being stuck for more than 120s simulation\ntime, which corresponds to 10s physical time).",
      "size": 983,
      "sentences": 8
    },
    {
      "id": 47,
      "content": "For ORCA, we use a timeout of 60s but\nterminate early when all agents are in a deadlock (defined\nas all agents being stuck for more than 120s simulation\ntime, which corresponds to 10s physical time). Finally, for\nPRIMAL, we let the agents plan individual paths for up to\n256 timesteps for 10- to 40-sized worlds, 384 timesteps for\n80-sized worlds, and 512 timesteps for 160-sized worlds. Experiments for the conventional planners were carried out\non a single desktop computer, equipped with an AMD\nThreadripper2990WXwith64logicalcoresclockedat4Ghz\nand64GbofRAM.ExperimentsforPRIMALwerepartially\nrun on the same computer, which is also equipped with 3\nGPUs (NVIDIA Titan V, GTX 1080Ti and 1070Ti), as well\nas on a simple desktop with an Intel i7-7700K, 16Gb RAM\nand an NVIDIA GTX 1070.",
      "size": 784,
      "sentences": 3
    },
    {
      "id": 48,
      "content": "partially\nrun on the same computer, which is also equipped with 3\nGPUs (NVIDIA Titan V, GTX 1080Ti and 1070Ti), as well\nas on a simple desktop with an Intel i7-7700K, 16Gb RAM\nand an NVIDIA GTX 1070. Basedonourresults,wefirstnoticethatourapproachper-\nformsextremelywellinlowobstacledensities,whereagents\ncan easily go around each other, but is easily outperformed\nin dense environments, where joint actions seem necessary\nfor agents to reach their goals (which sometimes requires\ndrastic path changes). Similarly,but with significantly worse\nperformance, ORCA cannot protect against deadlocks and\nperforms very poorly in most scenarios involving more than\n16 agents and any obstacles, due to its fully-decoupled,\nreactive nature. Second, we notice that, since our training\ninvolves worlds of varying sizes but a constant team size,\nagents are inherently exposed to a small variability in agent\ndensity within their FOV.",
      "size": 919,
      "sentences": 4
    },
    {
      "id": 49,
      "content": "e nature. Second, we notice that, since our training\ninvolves worlds of varying sizes but a constant team size,\nagents are inherently exposed to a small variability in agent\ndensity within their FOV. In our results, we observed that\nagents perform more poorly as the number of nearby agents\nincreases in their FOV (small worlds, large teams), an\neffect we believe could be corrected by varying the team\nsizes during training. This will be investigated in future\nworks. However, we expect traditional planners to generally\noutperformourapproachinsmall(10-20-sized)worlds,even\nwith larger teams. Third, we notice that the paths generated\nby PRIMAL are sometimes more than twice as long as\n[표 데이터 감지됨]\n\n=== 페이지 7 ===\nthe paths of the other planners’. However, other planners\n100\nallow moves that the agents cannot take in our definition of\nthe MAPF problem: agents can follow each other with no\nempty space between them, can swap around (similar to a\n50\nrunabout),etc.",
      "size": 965,
      "sentences": 7
    },
    {
      "id": 50,
      "content": "nners\n100\nallow moves that the agents cannot take in our definition of\nthe MAPF problem: agents can follow each other with no\nempty space between them, can swap around (similar to a\n50\nrunabout),etc. [3],whichleadstoshorterpaths.Additionally,\nvisual inspection of the cases where PRIMAL generates\nlonger paths shows that most agents move to their goals 0\n4 8 16 32 64 128 256 512 1024\neffectively, except for a few laggards. Finally, since agents\nare never exposed to worlds larger than 70 × 70 during\ntraining, they seem to perform extremely poorly in larger\nworlds during testing (≥ 80-sized). However, by capping\nthegoaldistanceintheagents’state,PRIMAL’ssuccessrate\nin larger worlds can be drastically improved. In the results\npresented here for 80- and 160-sized worlds, the distance to\ngoal is capped at 75 (empirically set) in the agents’ state. Example videos of near-optimal and severely sub-optimal\nplans for PRIMAL in various environments are available\nat https://goo.gl/T627XD.",
      "size": 988,
      "sentences": 6
    },
    {
      "id": 51,
      "content": "o\ngoal is capped at 75 (empirically set) in the agents’ state. Example videos of near-optimal and severely sub-optimal\nplans for PRIMAL in various environments are available\nat https://goo.gl/T627XD. Due to space constraints, we choose to discuss the three\nmain scenarios shown in Fig. 6: a case where PRIMAL\nstronglyoutperformsallotherplanners,onewherePRIMAL\nslightly outperforms them, and one where PRIMAL strug-\ngles.Thecompletesetofresults(forallteamsizes,obstacles\ndensities,andworldsizes)canbefoundathttps://goo. gl/APktNk and contains the path lengths generated by the\ndifferent planners as well as the planning times. First, in a\nlarge world with no obstacles (160×160), centralized plan-\nners especially struggle since the joint configuration space\nquickly grows to encompass all agents, making planning for\nmorethan100agentsverytime-consuming.PRIMAL,onthe\nother hand, can easily deal with teams up to 1024 agents,\nwith a near-perfect success rate.",
      "size": 957,
      "sentences": 6
    },
    {
      "id": 52,
      "content": "quickly grows to encompass all agents, making planning for\nmorethan100agentsverytime-consuming.PRIMAL,onthe\nother hand, can easily deal with teams up to 1024 agents,\nwith a near-perfect success rate. Second, in a medium-sized\nworldwithlowobstacledensity,thecentralizedplannerscan\neasilyplanforafewhundredagents.PRIMAL’ssuccessrate\nstarts decreasing earlier than that of the other planners, but\nremains above 60% for cases with 512 agents, whereas all\notherplannersperformpoorly.Third,inasmallerworldthat\nis very densely populated with obstacles, all planners can\nonly handle up to 64 agents, but PRIMAL starts to struggle\npast 8 agents, whereas ODrM* can handle up to 64 agents. However,evenwhenPRIMALcannotfinishafullproblem,it\nusuallymanagestobringmanyagentstotheirgoalsquickly,\nwith only a few failing to reach their goals.",
      "size": 826,
      "sentences": 3
    },
    {
      "id": 53,
      "content": "8 agents, whereas ODrM* can handle up to 64 agents. However,evenwhenPRIMALcannotfinishafullproblem,it\nusuallymanagestobringmanyagentstotheirgoalsquickly,\nwith only a few failing to reach their goals. At this point, a\nconventionalplannercouldbeusedtocompletetheproblem,\nwhich has become simple for a graph-based solver since\nmost agents should remain motionless at their goals. Future\nwork will investigate the combination of PRIMAL with a\ncompleteplannertoleveragethefast,decentralizedplanning\nof PRIMAL while guaranteeing completeness. B. Experimental Validation\nWe also implemented PRIMAL on a small fleet of au-\ntonomous ground vehicles (AGVs) evolving in a factory\nmockup. In this hybrid system, two physical robots evolve\nalongsidetwo(then,half-waythroughtheexperiment,three)\nsimulated ones.",
      "size": 796,
      "sentences": 7
    },
    {
      "id": 54,
      "content": "mall fleet of au-\ntonomous ground vehicles (AGVs) evolving in a factory\nmockup. In this hybrid system, two physical robots evolve\nalongsidetwo(then,half-waythroughtheexperiment,three)\nsimulated ones. The physical robots have access to the\nposition of simulated robots, and vice-versa, as they all plan\n]%[\netaR\nsseccuS\nSucces rates - obstacle density 0.0, world size 160\n100\n50\n0\n4 8 16 32 64 128 256 512 1024\n]%[\netaR\nsseccuS\nSucces rates - obstacle density 0.1, world size 80\nFig. 6. Success rates of the different planners in our three scenarios. PRIMAL outperforms all planners in the top obstacle-free world, slightly\noutperforms the others in low-obstacle-density worlds, and is strongly\noutperformedinthehigh-obstacle-densityworld. Fig.7.",
      "size": 745,
      "sentences": 7
    },
    {
      "id": 55,
      "content": "os. PRIMAL outperforms all planners in the top obstacle-free world, slightly\noutperforms the others in low-obstacle-density worlds, and is strongly\noutperformedinthehigh-obstacle-densityworld. Fig.7. Snapshotofthephysicalandsimulatedrobotsevolvinginthefactory\nmockup.Left:overhead(top)andside(bottom)viewsofthemockupand\nrobots.Right:visualizationshowingtheobstacles(blacksolids),therobots\n(bluecircles),theirgoals(bluesquares),andcurrentmoves(greensquares). their next actions online using our decentralized approach. PRIMAL shows clear online capabilities, as the planning\ntime per step and per agent is well below 0.1s on a standard\nGPU (and well below 0.2s on a CPU). Fig. 7 shows our\nfactory mockup and simulation environment. The full video\nis available at https://goo.gl/T627XD. VI. CONCLUSION\nIn this paper, we presented PRIMAL, a new approach\nto multi-agent path finding, which relies on combining dis-\ntributed reinforcement learning and imitation learning from\na centralized expert planner.",
      "size": 1000,
      "sentences": 11
    },
    {
      "id": 56,
      "content": "n this paper, we presented PRIMAL, a new approach\nto multi-agent path finding, which relies on combining dis-\ntributed reinforcement learning and imitation learning from\na centralized expert planner. Through an extensive set of\nexperiments, we showed how PRIMAL scales to various\nteam sizes, world sizes and obstacle densities, despite only\n[표 데이터 감지됨]\n\n=== 페이지 8 ===\ngivingagentsaccesstolocalinformationabouttheworld.In [12] G. Sartoretti, Y. Shi, W. Paivine, M. Travers, and H. Choset, “Dis-\nlow obstacle-density environments, we further showed how tributed learning for the decentralized control of articulated mobile\nrobots,”inICRA,2018,pp.3789–3794. PRIMALexhibitson-parperformance,andevenoutperforms\n[13] R.S.SuttonandA.G.Barto,“Reinforcementlearning:Anintroduc-\nstate-of-the-art MAPF planners in some cases, even though tion,”ABradfordBook,1998. these have access to the whole state of the system.",
      "size": 904,
      "sentences": 4
    },
    {
      "id": 57,
      "content": "[13] R.S.SuttonandA.G.Barto,“Reinforcementlearning:Anintroduc-\nstate-of-the-art MAPF planners in some cases, even though tion,”ABradfordBook,1998. these have access to the whole state of the system. Finally, [14] H.Ma,D.Harabor,J.Li,andS.Koenig,“SearchingwithConsistent\nwe presented an example where we deployed PRIMAL on PrioritizationforMulti-AgentPathFinding,”inAAAI,2019. [15] S.LaValle,PlanningAlgorithms. CambridgeUniversityPress,2006. physical and simulated robots in a factory mockup, showing\n[16] S.Leroy,J.-P.Laumond,andT.Sime´on,“MultiplePathCoordination\nhow robots can benefit from our online, local-information- for Mobile Robots: A Geometric Algorithm,” in IJCAI, 1999, pp. based, decentralized MAPF approach. 1118–1123.",
      "size": 734,
      "sentences": 8
    },
    {
      "id": 58,
      "content": "e´on,“MultiplePathCoordination\nhow robots can benefit from our online, local-information- for Mobile Robots: A Geometric Algorithm,” in IJCAI, 1999, pp. based, decentralized MAPF approach. 1118–1123. [17] H.Ma,C.A.Tovey,G.Sharon,T.S.Kumar,andS.Koenig,“Multi-\nFutureworkwillfocusonadaptingourtrainingprocedure\nAgentPathFindingwithPayloadTransfersandthePackage-Exchange\nto factory-like environments, with low to medium obsta- Robot-RoutingProblem,”inAAAI,2016,pp.3166–3173. cle density but where parts of the environment are very [18] M. Ca´p, P. Nova´k, M. Selecky´, J. Faigl, and J. Vok´ınek, “Asyn-\nsparse and other parts highly-structured (such as corridors, chronousdecentralizedprioritizedplanningforcoordinationinmulti-\nrobotsystem,”inProceedingsofIROS,2013,pp.3822–3829. aisles, etc.).",
      "size": 791,
      "sentences": 7
    },
    {
      "id": 59,
      "content": "“Asyn-\nsparse and other parts highly-structured (such as corridors, chronousdecentralizedprioritizedplanningforcoordinationinmulti-\nrobotsystem,”inProceedingsofIROS,2013,pp.3822–3829. aisles, etc.). We also believe that extending our approach\n[19] M. Erdmann and T. Lozano-Perez, “On multiple moving objects,”\nto receding-horizon planning, where agents plan ahead for Algorithmica,vol.2,no.1,pp.477–521,1987. severalactions,mayhelptoimprovetheperformanceofPRI- [20] G.SanchezandJ.Latombe,“UsingaPRMplannertocomparecentral-\nizedanddecoupledplanningformulti-robotsystems,”inProceedings\nMAL by teaching agents to explicitly coordinate their paths. ofICRA,vol.2,2002,pp.2112–2119. [21] G. Sharon, R. Stern, A. Felner, and N. Sturtevant, “Conflict-based\nsearchforoptimalmulti-agentpathfinding,”inProc.ofAAAI,2012. ACKNOWLEDGMENTS\n[22] C. Ferner, G. Wagner, and H. Choset, “ODrM*: optimal multirobot\npathplanninginlowdimensionalsearchspaces,”inICRA,2013,pp.",
      "size": 951,
      "sentences": 7
    },
    {
      "id": 60,
      "content": "rchforoptimalmulti-agentpathfinding,”inProc.ofAAAI,2012. ACKNOWLEDGMENTS\n[22] C. Ferner, G. Wagner, and H. Choset, “ODrM*: optimal multirobot\npathplanninginlowdimensionalsearchspaces,”inICRA,2013,pp. Detailed comments from anonymous referees contributed to\n3854–3859. the presentation and quality of this paper. This research was [23] T. Standley, “Finding Optimal Solutions to Cooperative Pathfinding\nsupported by the CMU Manufacturing Futures Initiative, Problems,”inProceedingsofAAAI,2010,pp.173–178. made possible by the Richard King Mellon Foundation. [24] L.Busoniu,R.Babusˇka,andB.DeSchutter,“Multi-agentreinforce-\nmentlearning:Anoverview,”InnovationsinMulti-AgentSystemsand\nJustin Kerr was a CMU SURF student, funded by the Na- Applications-1,vol.310,pp.183–221,2010. tional Science Foundation (NSF).",
      "size": 808,
      "sentences": 8
    },
    {
      "id": 61,
      "content": "ntreinforce-\nmentlearning:Anoverview,”InnovationsinMulti-AgentSystemsand\nJustin Kerr was a CMU SURF student, funded by the Na- Applications-1,vol.310,pp.183–221,2010. tional Science Foundation (NSF). The research at USC was [25] J. K. Gupta, M. Egorov, and M. Kochenderfer, “Cooperative multi-\nsupported by NSF grants 1409987, 1724392, 1817189, and agent control using deep reinforcement learning,” in AAMAS, 2017,\npp.66–83. 1837779. This work used the Bridges system, supported by\n[26] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch,\nNSF grant ACI-1445606 at the Pittsburgh Supercomputing “Multi-agent actor-critic for mixed cooperative-competitive environ-\nCenter [43]. ments,”inNIPS,2017,pp.6382–6393. [27] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. White-\nson, “Counterfactual multi-agent policy gradients,” arXiv preprint\nREFERENCES 1705.08926,2017.",
      "size": 887,
      "sentences": 7
    },
    {
      "id": 62,
      "content": ". ments,”inNIPS,2017,pp.6382–6393. [27] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. White-\nson, “Counterfactual multi-agent policy gradients,” arXiv preprint\nREFERENCES 1705.08926,2017. [28] J.Foerster,I.A.Assael,N.deFreitas,andS.Whiteson,“Learningto\n[1] M. Rubenstein, A. Cornejo, and R. Nagpal, “Programmable self- communicatewithdeepmulti-agentreinforcementlearning,”inNIPS,\nassembly in a thousand-robot swarm,” Science, vol. 345, no. 6198, 2016,pp.2137–2145. pp.795–799,2014. [29] F.S.MeloandM.Veloso,“HeuristicplanningfordecentralizedMDPs\n[2] A. Howard, L. E. Parker, and G. S. Sukhatme, “Experiments with withsparseinteractions,”inDARS,2013,pp.329–343. a Large Heterogeneous Mobile Robot Team: Exploration, Mapping,\n[30] T.Schaul,J.Quan,I.Antonoglou,andD.Silver,“Prioritizedexperi-\nDeployment and Detection,” The International Journal of Robotics\nencereplay,”arXivpreprint1511.05952,2015. Research,vol.25,no.5-6,pp.431–447,2006.",
      "size": 948,
      "sentences": 10
    },
    {
      "id": 63,
      "content": ",J.Quan,I.Antonoglou,andD.Silver,“Prioritizedexperi-\nDeployment and Detection,” The International Journal of Robotics\nencereplay,”arXivpreprint1511.05952,2015. Research,vol.25,no.5-6,pp.431–447,2006. [31] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\n[3] G.WagnerandH.Choset,“Subdimensionalexpansionformultirobot\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\npathplanning,”ArtificialIntelligence,vol.219,pp.1–24,2015. reinforcementlearning,”inICML,2016,pp.1928–1937. [4] M. Barer, G. Sharon, R. Stern, and A. Felner, “Suboptimal variants\n[32] K. Simonyan and A. Zisserman, “Very deep convolutional networks\noftheconflict-basedsearchalgorithmforthemulti-agentpathfinding\nforlarge-scaleimagerecognition,”arXivpreprint1409.1556,2014. problem,”inProceedingsofSoCS,2014. [33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\n[5] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha, “Reciprocal\nimagerecognition,”inCVPR,2016,pp.770–778.",
      "size": 983,
      "sentences": 10
    },
    {
      "id": 64,
      "content": "eedingsofSoCS,2014. [33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\n[5] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha, “Reciprocal\nimagerecognition,”inCVPR,2016,pp.770–778. n-bodycollisionavoidance,”inRoboticsResearch,2011,pp.3–19. [6] R.Cui,B.Gao,andJ.Guo,“Pareto-optimalcoordinationofmultiple [34] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz,\nrobotswithsafetyguarantees,”AutonomousRobots,vol.32,no.3,pp. “Reinforcementlearningthroughasynchronousadvantageactor-critic\n189–205,2011. onaGPU,”2016. [7] J.L.Baxter,E.Burke,J.M.Garibaldi,andM.Norman,“Multi-robot [35] P.YingandL.Dehua,“Improvementwithjointrewardsonmulti-agent\nsearchandrescue:Apotentialfieldbasedapproach,”inAutonomous cooperativereinforcementlearning,”inCASCON,2008,pp.536–539. RobotsandAgents,2007,pp.9–16.",
      "size": 816,
      "sentences": 11
    },
    {
      "id": 65,
      "content": "dL.Dehua,“Improvementwithjointrewardsonmulti-agent\nsearchandrescue:Apotentialfieldbasedapproach,”inAutonomous cooperativereinforcementlearning,”inCASCON,2008,pp.536–539. RobotsandAgents,2007,pp.9–16. [36] A.Nair,B.McGrew,M.Andrychowicz,W.Zaremba,andP.Abbeel,\n[8] H. Balakrishnan and Y. Jung, “A framework for coordinated surface “Overcoming exploration in reinforcement learning with demonstra-\noperations planning at Dallas-Fort Worth International Airport,” in tions,”arXivpreprint1709.10089,2017. AIAAGNC,vol.3,2007,pp.2382–2400. [37] A. Rajeswaran, V. Kumar, A. Gupta, J. Schulman, E. Todorov, and\n[9] K.-H. C. Wang and A. Botea, “MAPP: a scalable multi-agent path S.Levine,“Learningcomplexdexterousmanipulationwithdeeprein-\nplanning algorithm with tractability and completeness guarantees,” forcement learning and demonstrations,” arXiv preprint 1709.10087,\nJournalofArtificialIntelligenceResearch,vol.42,pp.55–90,2011. 2017.",
      "size": 930,
      "sentences": 6
    },
    {
      "id": 66,
      "content": "anning algorithm with tractability and completeness guarantees,” forcement learning and demonstrations,” arXiv preprint 1709.10087,\nJournalofArtificialIntelligenceResearch,vol.42,pp.55–90,2011. 2017. [10] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decentralized non- [38] Y.Gao,H.Xu,J.Lin,F.Yu,S.Levine,andT.Darrell,“Reinforcement\ncommunicating multiagent collision avoidance with deep reinforce- learningfromimperfectdemonstrations,”arXiv:1802.05313,2018.\nmentlearning,”inICRA,2017,pp.285–292. [39] M. Vecer´ık, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot,\n[11] G. Sartoretti, Y. Wu, W. Paivine, T. K. S. Kumar, S. Koenig, and N. Heess, T. Rotho¨rl, T. Lampe, and M. A. Riedmiller, “Leveraging\nH.Choset,“Distributedreinforcementlearningformulti-robotdecen- demonstrationsfordeepreinforcementlearningonroboticsproblems\ntralizedcollectiveconstruction,”inDARS,2018,pp.35–49. withsparserewards,”arXivpreprint1707.08817,2017.",
      "size": 934,
      "sentences": 5
    },
    {
      "id": 67,
      "content": "rcementlearningformulti-robotdecen- demonstrationsfordeepreinforcementlearningonroboticsproblems\ntralizedcollectiveconstruction,”inDARS,2018,pp.35–49. withsparserewards,”arXivpreprint1707.08817,2017. === 페이지 9 ===\n[40] S.Ross,G.Gordon,andD.Bagnell,“Areductionofimitationlearning\nandstructuredpredictiontono-regretonlinelearning,”inICAIS,2011,\npp.627–635. [41] S.ChernovaandM.Veloso,“Confidence-basedpolicylearningfrom\ndemonstrationusingGaussianmixturemodels,”inAAMAS. [42] T.Dozat,“IncorporatingNesterovmomentumintoAdam,”2016. [43] N.A.Nystrom,M.J.Levine,R.Z.Roskies,andJ.R.Scott,“Bridges:\na uniquely flexible HPC resource for new communities and data\nanalytics,”inProceedingsoftheXSEDEConf.,2015,pp.30:1–30:8.",
      "size": 710,
      "sentences": 6
    }
  ]
}