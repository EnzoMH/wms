{
  "source": "ArXiv",
  "filename": "020_RDE__A_Hybrid_Policy_Framework_for_Multi-Agent_Pat.pdf",
  "total_chars": 27913,
  "total_chunks": 39,
  "chunks": [
    {
      "id": 1,
      "content": "=== 페이지 1 ===\nRDE: A Hybrid Policy Framework for Multi-Agent Path Finding Problem\nJianqi Gao ID , Yanjie Li ID , Member, IEEE, Xiaoqing Yang ID , and Mingshan Tan ID\nAbstract—Multi-agent path finding (MAPF) is an abstract\nmodel for the navigation of multiple robots in warehouse\nautomation, where multiple robots plan collision-free paths\nfrom the start to goal positions. Reinforcement learning (RL)\nhas been employed to develop partially observable distributed\nMAPF policies that can be scaled to any number of agents. However, RL-based MAPF policies often get agents stuck in\ndeadlock due to warehouse automation’s dense and structured\nobstacles. This paper proposes a novel hybrid MAPF policy,\nRDE, based on switching among the RL-based MAPF policy,\nthe Distance heat map (DHM)-based policy and the Escape\npolicy. The RL-based policy is used for coordination among\nFig.1.",
      "size": 875,
      "sentences": 5
    },
    {
      "id": 2,
      "content": "l hybrid MAPF policy,\nRDE, based on switching among the RL-based MAPF policy,\nthe Distance heat map (DHM)-based policy and the Escape\npolicy. The RL-based policy is used for coordination among\nFig.1. Thetwo-dimensionalschematicofthewarehouseautomation.The\nagents.Incontrast,whennootheragentsareintheagent’sfield\norangecirclesrepresenttherobots.Thegreysquaresrepresenttheinventory\nof view, it can get the next action by querying the DHM. The\npods(obstacles),andtheonesmarkedwithlightredstarsaretobemoved. escapepolicythatrandomlyselectsvalidactionscanhelpagents\nTheinventorystationswithmanipulatorsareshownontheleftside.The\nescape the deadlock. We conduct simulations on warehouse-like robot enters the bottom of the inventory pod, lifts it, and carries it to its\nstructured grid maps using state-of-the-art RL-based MAPF designatedlocation. policies(DHCandDCC),whichshowthatRDEcansignificantly\nimprove their performance.",
      "size": 920,
      "sentences": 7
    },
    {
      "id": 3,
      "content": "tory pod, lifts it, and carries it to its\nstructured grid maps using state-of-the-art RL-based MAPF designatedlocation. policies(DHCandDCC),whichshowthatRDEcansignificantly\nimprove their performance. Index Terms—Path Planning for Multiple Mobile Robots\nofview(FOV).Conversely,RL-basedMAPFpolicymaylead\nor Agents, Planning, Scheduling and Coordination, Collision\nto agent detours. Avoidance. This paper introduces a hybrid MAPF policy called RDE\nto address the difficulties outlined earlier. RDE combines the\nI. INTRODUCTION\nRL-based policy with the DHM-based policy and the escape\nThe widespread adoption of e-commerce and logistics has\npolicy.TheRL-basedpolicymainlymanagescooperationand\nledtotheincreaseduseofwarehouseautomation.Asdepicted\navoids agent conflicts, while the DHM-based policy handles\nin Fig. 1, a flexible multi-robot system [1], [2] is employed\nsimple scenarios, where no other agents are within an agent’s\nfor this purpose. By providing start and goal positions for a FOV.",
      "size": 991,
      "sentences": 9
    },
    {
      "id": 4,
      "content": "handles\nin Fig. 1, a flexible multi-robot system [1], [2] is employed\nsimple scenarios, where no other agents are within an agent’s\nfor this purpose. By providing start and goal positions for a FOV. We can directly query the DHM1 to determine the next\ngroupofwarehouserobots,MAPFcanplanmultiplecollision-\naction where the agent is closest to the goal position and\nfree paths [3]. Traditional MAPF policies [4]–[9] have been\ntrying to stay straight. Additionally, if an agent is trapped in\nextensively researched and developed, but their real-time\na deadlock state, the escape policy that relies on randomly\nperformance deteriorates as the number of agents increases,\nselecting actions is activated to help resolve the issue. To\nparticularly in large-scale warehouse automation.",
      "size": 777,
      "sentences": 7
    },
    {
      "id": 5,
      "content": "licy that relies on randomly\nperformance deteriorates as the number of agents increases,\nselecting actions is activated to help resolve the issue. To\nparticularly in large-scale warehouse automation. verifytheeffectivenessofRDE,weutilizestate-of-the-artRL-\nResearchers have recently used RL to acquire distributed\nbased MAPF policies (DHC [14] and DCC [15]) to conduct\nMAPF policies, where each agent’s action is based on its\nsimulation experiments on warehouse-like structured grid\npartial observation space [10]–[15]. The RL-based MAPF\nmaps,andtheresultsshowthatRDEcansignificantlyimprove\npolicy can be applied to any number of agents or map sizes\ntheir performance. without requiring retraining. When agents conflict, the RL-\nTheremainderofthispaperisstructuredasfollows.Section\nbased MAPF policy will punish them with negative rewards\nII introduces the research work related to MAPF and lifelong\nand make them stay at their current positions. The agent\nMAPF.",
      "size": 962,
      "sentences": 7
    },
    {
      "id": 6,
      "content": "dasfollows.Section\nbased MAPF policy will punish them with negative rewards\nII introduces the research work related to MAPF and lifelong\nand make them stay at their current positions. The agent\nMAPF. Section III presents the problem formulation. Section\nretakes new actions based on the observations in the next\nIVdescribesthehybridMAPFframework.SectionVpresents\nstep.However,iftheagent’sobservationsdonotchangeinthe\nthe simulation experiment results. Section VI provides the\nnext step, the RL-based policy will make the agent take the\nconclusions and future work. same action as before, leading to a path conflict again. When\nthis process continues, the agent will get stuck in a deadlock, II. RELATEDWORK\nwhichcanleadtothefailureofMAPF.Moreover,thebenefits\nA. Traditional MAPF\nof cooperative coordination using RL-based policy becomes\nFinding an optimal MAPF solution has been proven to be\ninsignificant if the agent has no other agents within its field\nNP-hard [17].",
      "size": 969,
      "sentences": 9
    },
    {
      "id": 7,
      "content": "nal MAPF\nof cooperative coordination using RL-based policy becomes\nFinding an optimal MAPF solution has been proven to be\ninsignificant if the agent has no other agents within its field\nNP-hard [17]. According to whether the optimal solution can\nAllauthorsarewiththeDepartmentofControlScienceandEngineering,\nHarbinInstituteofTechnology(Shenzhen),Shenzhen518055,China(e-mail: 1To obtain a DHM with the goal position as the source, we employ\ngaojianqi205a@stu.hit.edu.cn; autolyj@hit.edu.cn; yxqsheep@gmail.com; the Dijkstra [16] algorithm to compute the minimum distance to all grid\ntanmingshan033@gmail.com. positions.DetailscanbeseeninsectionIV-C.\n3202\nvoN\n3\n]OR.sc[\n1v82710.1132:viXra\n[표 데이터 감지됨]\n\n=== 페이지 2 ===\nbeobtained,wecandividethetraditionalMAPFpoliciesinto uses different MAPF policies for them. Some studies use\noptimalandsub-optimal.TheoptimalMAPFpolicies[4],[5], hierarchical policies to solve MAPF problems. First, the map\n[18], [19] also satisfies completeness.",
      "size": 976,
      "sentences": 5
    },
    {
      "id": 8,
      "content": "rent MAPF policies for them. Some studies use\noptimalandsub-optimal.TheoptimalMAPFpolicies[4],[5], hierarchical policies to solve MAPF problems. First, the map\n[18], [19] also satisfies completeness. CBS [5] is the most is spatially divided into small areas. The high-level performs\ncommonly used optimal MAPF policy, and researchers have global planning for each agent, and the low-level solves the\nmade many improvements to CBS. However, as the map size MAPF of each small area. HMAPP [36] generates the high-\nor agent’s number grows, finding optimal solutions becomes level plan for each agent from a randomly picked shortest\nchallenging. A number of sub-optimal policies have been path from its start position to its goal position and then uses\nproposed,whichcanbedividedintoboundedandunbounded. ECBS [8] to find conflict-free subpaths in every region. The cost of the bounded sub-optimal policy’s solution is no\nmore than (1+ϵ)×c , where c is the optimal cost and III.",
      "size": 973,
      "sentences": 10
    },
    {
      "id": 9,
      "content": "dintoboundedandunbounded. ECBS [8] to find conflict-free subpaths in every region. The cost of the bounded sub-optimal policy’s solution is no\nmore than (1+ϵ)×c , where c is the optimal cost and III. PROBLEMFORMULATION\nopt opt\nϵ > 0 is the sub-optimality factor [8]. The bounded sub-\nA. MAPF\noptimal policies are generally derived from optimal MAPF\npolicies [20]–[22]. Bounded sub-optimal policies can, under TheinputofMAPFincludesanundirectedconnectedgraph\ncertain conditions, improve the speed of solving and provide G(V,E) and an agent set N ={1,...,i,...,m}. Every agent\nsome guarantee of the quality of the solution. Unbounded ihasastartvertex,v i s ∈V andauniquegoalvertex,v i g ∈V. sub-optimal MAPF policies [7], [23]–[26] can get solutions At each discrete time step t = 0,...,∞, agent i is located\nfaster, but the quality of the solution is not guaranteed. in vertex v and takes action at i .",
      "size": 901,
      "sentences": 10
    },
    {
      "id": 10,
      "content": "APF policies [7], [23]–[26] can get solutions At each discrete time step t = 0,...,∞, agent i is located\nfaster, but the quality of the solution is not guaranteed. in vertex v and takes action at i . Then agent i moves to an\nadjacent vertex v′ that meets (v,v′) ∈ E or stays in its\nB. Learning-based MAPF current vertex v. The action can be represented as a:v →v′\nLearning-based MAPF policies are distributed methods, or a(v) = v′. The path of agent i can be represented by a\nwhereeveryagenttakesactionbasedonitspartialobservation sequence of actions l i = (a1 i ,a2 i ,...,at i ). A MAPF solution\nspace. PRIMAL [10] combines RL with ODrM∗ [4]-based can be represented as L = (l 1 ,l 2 ,...,l m ). We assume all\nimitation learning (IL) to get a distributed MAPF policy. agents simultaneously move one grid or stand still at each\nBased on the framework of PRIMAL, many learning-based time step. In this paper, the world is a two-dimensional\nMAPF policies are proposed.",
      "size": 967,
      "sentences": 9
    },
    {
      "id": 11,
      "content": ". agents simultaneously move one grid or stand still at each\nBased on the framework of PRIMAL, many learning-based time step. In this paper, the world is a two-dimensional\nMAPF policies are proposed. MAPPER [11] and G2RL four-connected unit grid map where every vertex has four\n[12] use A∗-based shortest path to guide the policy learning. adjacentvertices.Thepathcostofagentiisthediscrete-time\nAgentswhodeviatefromthispathwillbepenalized.However, tg when the agent i reaches the goal vertex vg. Conflict is\ni i\nA∗-based shortest paths are not unique and not optimal usually used in MAPF to represent the path plan collision\nglobally, destabilizing the learning process and the multi- of different agents. The main conflicts in MAPF are edge\nagentt implicit coordination. Instead of using a CNN [27] as conflict ⟨i,j,v,v′,t⟩ and vertex conflict ⟨i,j,v,t⟩. theencoderoftheobservedstate,[28]proposesatransformer-\nB. Learning-based MAPF\nbased [29] policy network for feature extraction.",
      "size": 983,
      "sentences": 9
    },
    {
      "id": 12,
      "content": "sing a CNN [27] as conflict ⟨i,j,v,v′,t⟩ and vertex conflict ⟨i,j,v,t⟩. theencoderoftheobservedstate,[28]proposesatransformer-\nB. Learning-based MAPF\nbased [29] policy network for feature extraction. In addition,\nin the above policies each agent regards other agents as AsshowninFig.2,learning-basedMAPFcanbetreatedas\ndynamic obstacles, leading to the environment’s instability a partially observable Markov decision process (POMDPs)2\nand making the training process challenging to converge. [37] solved with RL. At each timestep t, the ith agent has\nResearchers have recently enabled one agent to communicate an observation ot, which only provides partial information\ni\nwith other agents. In [13], a graph neural network is used to of G(V,E), and then we independently compute an action\nhelp agents communicate with each other.",
      "size": 828,
      "sentences": 6
    },
    {
      "id": 13,
      "content": "which only provides partial information\ni\nwith other agents. In [13], a graph neural network is used to of G(V,E), and then we independently compute an action\nhelp agents communicate with each other. Based on [13] at by the policy π shared by all agents:\nθ\nand MAPPER [11], MAGAT [30] and AB-Mapper [31]\nmake use of the attention mechanism to assess the relative at i ∼π θ (cid:0) ot i (cid:1) , (1)\nimportance of agent. DHC [14] uses graph convolution\nwhere θ denotes the policy parameters. To find the policy\nmechanism to communicate between agents. Instead of\nπ , we minimize the expectation of the total path cost of all\nbroadcastcommunicationinFOV,DCC[15]firstevaluatesthe θ\nagents, which is defined as:\nimportance of information and then selects some information\nto aggregate. (cid:20) (cid:21)\nargminE max |l ||π , (2)\ni θ\nC. Hybrid MAPF Solver πθ 1≤i≤m\nOne policies cannot effectively address every variant of where |l | is path cost of agent i.\ni\nthe classical MAPF problem.",
      "size": 983,
      "sentences": 7
    },
    {
      "id": 14,
      "content": "cid:20) (cid:21)\nargminE max |l ||π , (2)\ni θ\nC. Hybrid MAPF Solver πθ 1≤i≤m\nOne policies cannot effectively address every variant of where |l | is path cost of agent i.\ni\nthe classical MAPF problem. Some researchers attempt to\ncombine multiple MAPF policies to solve MAPF problems. 2Formally, a POMDP can be represented as\n(cid:68) (cid:69)\nIn [32], the software is proposed to plan paths for thousands N,S,{Ai}N\ni=1\n,{Ri}N\ni=1\n,{Oi}N\ni=1\n,P,Z,γ . N is the number\nof trains within a few minutes, which incorporates many of agents. S is the state space containing information about agents\nstate-of-the-art MAPF policies. SWARM-MAPF [33] uses a A nd = e A nv 1 ir × on . m .. e × nts A . n A i i s t r h e e pr s e p s a e c n e ts of th j e oin a t c a ti c o t n ion s s p . a R ce i : of S a × ge A nt → i, R an i d s\nswarm-based policy [34] in open areas and CBS [5] in therewardfunctionforagenti.{Oi}istheobservationspaceofagenti.",
      "size": 934,
      "sentences": 10
    },
    {
      "id": 15,
      "content": "h j e oin a t c a ti c o t n ion s s p . a R ce i : of S a × ge A nt → i, R an i d s\nswarm-based policy [34] in open areas and CBS [5] in therewardfunctionforagenti.{Oi}istheobservationspaceofagenti. areas with dense obstacles, which improves the efficiency P(s′|s,a):S×A×S→[0,1]isthestatetransitionfunction,whichis\nusedtodescribetheprobabilitythatagentinstatestakesactionaandthen\nof solving problems. The spatially distributed multi-agent\ntransitstostates′.Z:S×A→Oi istheobservationfunction.γ∈[0,1]\nplanner [35] identifies high and low-contention areas and isthediscountfactor. === 페이지 3 ===\nEnvironment 3) Deadlock: In warehouse automation, encountering\ndeadlock states T like stagnation and oscillation is possible\nd\ndue to the obstacles’ high density and structured features\n[38]. These deadlock states can ultimately fail MAPF. Stagnation.Iftheagentstaysatanon-goalpositionforover\nn steps, we consider the agent in a state of stagnation.",
      "size": 942,
      "sentences": 7
    },
    {
      "id": 16,
      "content": "igh density and structured features\n[38]. These deadlock states can ultimately fail MAPF. Stagnation.Iftheagentstaysatanon-goalpositionforover\nn steps, we consider the agent in a state of stagnation. When\nother agents block the warehouse aisle, stagnation happens\nas the agent cannot find a path. We test different values for\nPolicy Network\nn and find that n=4 can improve performance. Oscillation. When a collision occurs between two agents,\nthey may take the same action to avoid it, causing a collision\nagain in the next step. If this process continues, neither agent\ncan reach the goal position. We call this situation oscillation. As illustrated in Fig. 4, the green agent moves to the right\ngrid first when time t = 1, followed by the orange agent. Policy Optimization When time t=2 arrives, both agents move to the left grid\nbased on their partial observations. In the next time step\nt = 3, both agents return to the right grid. The single-step\nFig.2.",
      "size": 958,
      "sentences": 14
    },
    {
      "id": 17,
      "content": "cy Optimization When time t=2 arrives, both agents move to the left grid\nbased on their partial observations. In the next time step\nt = 3, both agents return to the right grid. The single-step\nFig.2. AnoverviewofRL-basedMAPFpolicy.Ateachtimestept,agent\nireceivesitspartialobservationspaceot fromtheenvironment,whichis oscillationshowninFig.4isthemostcommonintheaisleof\ni\ndenotedbyot\ni\n∈RWFOV×HFOV,whereWFOV andHFOV represent the warehouse and has the most significant impact on MAPF. the width and height of the FOV. Then an action at is generated by the\ni Other larger oscillations occur rarely, so we do not consider\nsharedpolicyπ θ.Whentheagentreachesthenewpositioninthenexttime\nstept+1,itwillreceivearewardrt anditsnewobservationspaceot+1. them in this paper. i i\nWerepeattheaboveprocessuntiltheagentreachesthegoalposition. IV. HYBRIDMAPFPOLICY\nThis section presents the hybrid MAPF policy, RDE,\n(a) t=1 (b) t=2 (c) t=3\ndepicted in Fig. 3.",
      "size": 943,
      "sentences": 11
    },
    {
      "id": 18,
      "content": "them in this paper. i i\nWerepeattheaboveprocessuntiltheagentreachesthegoalposition. IV. HYBRIDMAPFPOLICY\nThis section presents the hybrid MAPF policy, RDE,\n(a) t=1 (b) t=2 (c) t=3\ndepicted in Fig. 3. RDE flawlessly combines the RL-based\npolicy π with the DHM-based policy π and the escape Fig.4. Anexampleoftheagentinastateofoscillation. θ H\npolicy π . s\nB. RL-based Policy\nRDE can be adapted to any RL-based MAPF policy. The\nComplex Simple Deadlock\nstate-of-the-art RL-based MAPF policies used in this paper\nare shown in Table I. DHC [14] and DCC [15] perform\nexcellently than other MAPF policies and use DHM-based\nheuristic observation channels. We can directly leverage the\nalreadyestablishedDHMtobuildaDHM-basedpolicy,which\nWith other Without other Deadlock state\nagents in FOV agents in FOV occurs makes DHC and DCC ideal for combination with RDE. TABLEI\nHybrid MAPF\nPolicy COMPARISONOFRDEANDBASELINES. RL-based Policy DHM-based Policy Escape Policy\nSolver Learning Comm.",
      "size": 976,
      "sentences": 13
    },
    {
      "id": 19,
      "content": "FOV agents in FOV occurs makes DHC and DCC ideal for combination with RDE. TABLEI\nHybrid MAPF\nPolicy COMPARISONOFRDEANDBASELINES. RL-based Policy DHM-based Policy Escape Policy\nSolver Learning Comm. Guidance Encoder TrainEnv. DHC[14] DuelingDQN[39] ✓ Heuristic CNN Random\nFig. 3. Overview of RDE. The RL-based policy is utilized in complex DCC[15] DuelingDQN[39] ✓ Heuristic CNN Random\nscenariosTc,whiletheDHM-basedpolicyisusedforsimplescenariosTs. TheescapepolicyisemployedindeadlockstatescenariosT d.\nC. DHM-Based Policy\nIf no other agents exist in an agent’s FOV, RL-based\nA. Classification of Sceniarios\nMAPF policy π loses its inherent collaborative advantage. θ\n1) Complex: When there are other agents in the field of Additionally, the dense and structured obstacles in warehouse\nview of the agent, the agent not only pays attention to the automation often cause agents to get stuck in deadlock using\ncollision with obstacles, but also considers the cooperation RL-based MAPF policies.",
      "size": 991,
      "sentences": 11
    },
    {
      "id": 20,
      "content": "f the agent, the agent not only pays attention to the automation often cause agents to get stuck in deadlock using\ncollision with obstacles, but also considers the cooperation RL-based MAPF policies. with other agents. We call this scenario as complex T c . The DHM-based policy π H is designed in response to the\n2) Simple: When there are no other agents in the FOV of above problems. The policy uses the Dijkstra [16] algorithm\nthe agent, the agent only pays attention to the collision with to calculate the shortest distance from the goal position to\nobstacles. We call this scenario simple T . all grid positions, resulting in a DHM with the goal position\ns\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\nAlgorithm 1: The escape policy π for MAPF.",
      "size": 731,
      "sentences": 7
    },
    {
      "id": 21,
      "content": "the goal position to\nobstacles. We call this scenario simple T . all grid positions, resulting in a DHM with the goal position\ns\n[표 데이터 감지됨]\n\n=== 페이지 4 ===\nAlgorithm 1: The escape policy π for MAPF. e\nInput: Agents set N ={1,...,i,...,m}, goals set\nVg ={v ,...,v ,...,v }, the position set of each\n1 i m\nagent for the past five time steps\nVp = (cid:8) vt−4,vt−3,vt−2,vt−1,vt(cid:9) ,∀i∈N, Action\ni i i i i i\nset A′ ={up,down,left,right}\nOutput: Actions list: A= (cid:8) at,...,at,...,at (cid:9)\n1 i m\n1 for i←1 to m do\n2 if v i t ̸=v i g then\nFig.5. The10×10DHMwithastarrepresentingthegoalpositionand 3 if v i t−1 =v i t−3 & v i t−2 =v i t−4 then\nthedarkbluesquareindicatingtheobstacles.OntheDHM,theheatvalue 4 at i ←random(A′);\noftheobstaclepositionisinfinite.Ifthegoalpositionisthesame,wecan 5 end\ncalltheexistingDHMdirectly. 6 end\n7 end\nas the source. As shown in Fig.",
      "size": 871,
      "sentences": 7
    },
    {
      "id": 22,
      "content": "ngtheobstacles.OntheDHM,theheatvalue 4 at i ←random(A′);\noftheobstaclepositionisinfinite.Ifthegoalpositionisthesame,wecan 5 end\ncalltheexistingDHMdirectly. 6 end\n7 end\nas the source. As shown in Fig. 5, by querying the DHM,\nwe can determine the shortest distances between the four\ngrids adjacent to the agent’s current and goal positions. The Simulationiscarriedoutonasingledesktopcomputerwith\nposition closest to the goal position is then selected as the 32 GB memory, Intel® CoreTM i7-9700 CPU @ 3.00GHz\nagent’s action at . × 8 processes and GeForce RTX 2060 SUPER/PCIe/SSE2\ni,s\nMoreover, when the number of at ∈ At is more than graphics. The single desktop computer has a Ubuntu 16.04\ni,s i,s\none, we prioritize the action that allows the agent to go LTS system. straight, which satisfies:\nTABLEII\n−−−−→ −−−−→\nvt−1vt =vtvt+1, (3) MAPSETS.",
      "size": 841,
      "sentences": 8
    },
    {
      "id": 23,
      "content": "he single desktop computer has a Ubuntu 16.04\ni,s i,s\none, we prioritize the action that allows the agent to go LTS system. straight, which satisfies:\nTABLEII\n−−−−→ −−−−→\nvt−1vt =vtvt+1, (3) MAPSETS. i i i i\nwhere\n−\nv\n−\nt−\n−−\n1\n→\nvt represents the current move direction of agent\nMap AgentDensityρa(10−2)∗\ni, and\n−\nv\n−i\ntv\n−\nt\n−\n+\n→\n1\ni\nrepresents the future move direction of agent\nSize Type ρo † 10 30 50 70\ni i Sparse 0.419 1.49 4.47 7.44 10.42\ni. If none of the actions at ∈At can satisfy Eq. (3), the 34×34\ni,s i,s Dense 0.476 1.65 4.95 8.25 11.56\nagent randomly selects an action from At . As shown in Fig. 6, We try to go straight to reduce robot w\ni,\ne\ns\nar and actual time\n†\n∗\nThedensityofobstaclesρointhemapreferstothepercentageofstaicobstacles. Theagentdensityρaistheproportionofallagentstothevacantpositionson\ncosts. themap. A.",
      "size": 839,
      "sentences": 10
    },
    {
      "id": 24,
      "content": "reduce robot w\ni,\ne\ns\nar and actual time\n†\n∗\nThedensityofobstaclesρointhemapreferstothepercentageofstaicobstacles. Theagentdensityρaistheproportionofallagentstothevacantpositionson\ncosts. themap. A. Setup for Testing\nAsshowninFig.7,wechoosetwokindsofwarehouse-like\nstructuredmapsfortesting:SparseandDense.TableIIshows\nthe obstacle density of two kinds of map are both larger than\n0.4, much higher than that of other studies [10]–[14] (up\nto 0.3). There are more path conflicts among agents. In the\nsmall-scale scenarios, the map size is 34×34, and number\nFig.6. Twopathswiththesamecost.Eventhoughthedashedandsolid of agents is 10, 30, 50, and 70, respectively. We randomly\npathshavethesamecost,robotsthatfollowthedashedpathswilltakelonger\ngenerate 1000 test instances with the same map size and the\nandcausemoredamagetotheirhardwareinreal-worldsituations.Therefore,\nreducingthenumberofturnsisessentialtooptimizeitsperformanceand number of agents for the small-scale scenarios.",
      "size": 976,
      "sentences": 9
    },
    {
      "id": 25,
      "content": "same map size and the\nandcausemoredamagetotheirhardwareinreal-worldsituations.Therefore,\nreducingthenumberofturnsisessentialtooptimizeitsperformanceand number of agents for the small-scale scenarios. We set the\nreducewearandtear. maximum time step size for the small-scale scenario to 150. As mentioned in Section IV-B, we use state-of-the-art\nD. Escape Policy RL-based policies, DHC [14] and DCC [15], to verify\nthe effectiveness of RDE. We directly use the policy\nWhen implementing the RL-based policy in warehouse\nnetwork models of DHC and DCC trained on random\nautomation,thereisahigherprobabilityforagenttoencounter\nmaps without retraining on warehouse-like structured maps. adeadlockstate.Weintroduceanescapepolicyπ toenhance\ne During the test, we combined DHC, DCC with the DHM-\nthe agent’s capability of escaping such a state.",
      "size": 834,
      "sentences": 6
    },
    {
      "id": 26,
      "content": "etraining on warehouse-like structured maps. adeadlockstate.Weintroduceanescapepolicyπ toenhance\ne During the test, we combined DHC, DCC with the DHM-\nthe agent’s capability of escaping such a state. As shown in\nbased policy (DHC+DHM, DCC+DHM) and then com-\nAlgorithm 1, if the agent is stuck in a deadlock state while\nbined DHC, DCC with the DHM-based and escape policies\nplanning, we compel it to randomly choose an action from\n(DHC+DHM+Escape, DCC+DHM+Escape). the available action space, thereby assisting it in breaking\nfree from the deadlock states. B. Metrics\nV. SIMULATIONEXPERIMENT Success rate (SSR). One MAPF instance is deemed\nIn this section, we show the experiment results of RDE. unsuccessful when the time step surpasses the maximum\nWe implemented the experiments with python. value and not all agents reach their goals. We usually use\n[표 데이터 감지됨]\n\n=== 페이지 5 ===\n(a) DHC,Sparse (b) DHC,Dense (c) DCC,Sparse (d) DCC,Dense\nFig.7.",
      "size": 943,
      "sentences": 9
    },
    {
      "id": 27,
      "content": "imum\nWe implemented the experiments with python. value and not all agents reach their goals. We usually use\n[표 데이터 감지됨]\n\n=== 페이지 5 ===\n(a) DHC,Sparse (b) DHC,Dense (c) DCC,Sparse (d) DCC,Dense\nFig.7. ThecomparisonofSSRforallpoliciesinthesmall-scalescenarios. the SSR of solving multiple MAPF instances to evaluate the policies, DHC and DCC. Simulation results show that RDE\nperformance of the MAPF policy. SSR can be defined as: can further improve the success rate of DHC and DCC. In\nn RDE, we use an event-based heuristic method for policy\nSSR= s, (4)\nn switching.However,itmaybechallengingtocopewithmore\nt\ncomplexscenarios.Inthefuture,wewillseekmoreintelligent\nwhere n is the number of successfully solved MAPF\ns\nways to switch policies. instances I and n is the total number of instances I . s t t\nREFERENCES\nC. Results\n[1] P.R.Wurman,R.D’Andrea,andM.Mountz,“Coordinatinghundreds\nAs shown in Fig.",
      "size": 900,
      "sentences": 10
    },
    {
      "id": 28,
      "content": "solved MAPF\ns\nways to switch policies. instances I and n is the total number of instances I . s t t\nREFERENCES\nC. Results\n[1] P.R.Wurman,R.D’Andrea,andM.Mountz,“Coordinatinghundreds\nAs shown in Fig. 7, RDE can effectively improve the\nofcooperative,autonomousvehiclesinwarehouses,”AIMag.,vol.29,\nsuccess rate of DHC and DCC on two different warehouse- no.1,pp.9–9,2008. like structured grid maps. As the number of agents increases, [2] J. Berger and N. Lo, “An innovative multi-agent search-and-rescue\npathplanningapproach,”Computers&OperationsResearch,vol.53,\nthe success rate of all policies decreases. In particular, when\npp.24–31,2015. the number of agents exceeds 50, the success rates of DHC [3] H.Ma,“Graph-basedmulti-robotpathfindingandplanning,”Current\nandDCCdropsharply.BycombiningDHC,DCCwithDHM Robot.Reports,vol.3,pp.77–84,2022.",
      "size": 839,
      "sentences": 8
    },
    {
      "id": 29,
      "content": "he number of agents exceeds 50, the success rates of DHC [3] H.Ma,“Graph-basedmulti-robotpathfindingandplanning,”Current\nandDCCdropsharply.BycombiningDHC,DCCwithDHM Robot.Reports,vol.3,pp.77–84,2022. [4] C. Ferner, G. Wagner, and H. Choset, “ODrM∗ optimal multirobot\nandescapepolicies,thesharpdeclineinsuccessratehasbeen\npathplanninginlowdimensionalsearchspaces,”inProc.IEEEInt. effectively alleviated. As shown in Fig. 7(a) and 7(b), when Conf.Robot.Autom.,2013,pp.3854–3859. the number of agents is less than 30, the DHM-based policy [5] G.Sharon,R.Stern,A.Felner,andN.R.Sturtevant,“Conflict-based\nsearchforoptimalmulti-agentpathfinding,”Artif.Intell.,vol.219,pp. can further improve the success rate of DHC. Similarly, by\n40–66,2015. comparingFig.7(a)with7(b),andFig.7(c)with7(d),wecan [6] L.Cohen,M.Greco,H.Ma,C.Hernandez,A.Felner,T.S.Kumar,\nalso find that the DHM-based policy improves the success andS.Koenig,“Anytimefocalsearchwithapplications,”inProc.Int.",
      "size": 963,
      "sentences": 9
    },
    {
      "id": 30,
      "content": "ndFig.7(c)with7(d),wecan [6] L.Cohen,M.Greco,H.Ma,C.Hernandez,A.Felner,T.S.Kumar,\nalso find that the DHM-based policy improves the success andS.Koenig,“Anytimefocalsearchwithapplications,”inProc.Int. JointConf.Artif.Intell.,2018,pp.1434–1441. rateoftheRL-basedpolicymoresignificantlyonsparsemaps\n[7] D.Silver,“Cooperativepathfinding,”inProc.AAAIConf.Artif.Intell. than on dense maps. It is because when the agent density or Interact.Digit.Entertain.,2005,pp.117–122. obstacledensityislow,thepossibilityoftherebeingnoagent [8] M.Barer,G.Sharon,R.Stern,andA.Felner,“Suboptimalvariants\noftheconflict-basedsearchalgorithmforthemulti-agentpathfinding\nin the agent’s FOV becomes higher. The DHM-based policy\nproblem,”inProc.Int.Symp.Comb.Search,2014,pp.19–27. can help the agent take optimal actions to approach the target [9] P.Surynek,“Towardsoptimalcooperativepathplanninginhardsetups\nposition. From Fig.",
      "size": 901,
      "sentences": 9
    },
    {
      "id": 31,
      "content": "problem,”inProc.Int.Symp.Comb.Search,2014,pp.19–27. can help the agent take optimal actions to approach the target [9] P.Surynek,“Towardsoptimalcooperativepathplanninginhardsetups\nposition. From Fig. 7, we can see that the addition of the throughsatisfiabilitysolving,”inPacificRimInt.Conf.Artif.Intell.,\n2012,pp.564–576. escape policy has significantly improved the success rates of\n[10] G.Sartoretti,J.Kerr,Y.Shi,G.Wagner,T.S.Kumar,S.Koenig,and\nDHC and DCC on two different maps. Especially on dense H. Choset, “PRIMAL: Pathfinding via reinforcement and imitation\nmaps, when the number of agents reaches 70, the success multi-agent learning,” IEEE Robot. Autom. Lett., vol. 4, no. 3, pp. 2378–2385,2019. rates of both DHC and DCC increase by 15%.",
      "size": 748,
      "sentences": 12
    },
    {
      "id": 32,
      "content": "nt and imitation\nmaps, when the number of agents reaches 70, the success multi-agent learning,” IEEE Robot. Autom. Lett., vol. 4, no. 3, pp. 2378–2385,2019. rates of both DHC and DCC increase by 15%. Deadlock is\n[11] Z. Liu, B. Chen, H. Zhou, G. Koushik, M. Hebert, and D. Zhao,\nthe key reason for the failure of MAPF solution, and escape “MAPPER:Multi-agentpathplanningwithevolutionaryreinforcement\npoliciesbasedonrandomactionselectioncaneffectivelyhelp learninginmixeddynamicenvironments,”inProc.IEEE/RSJInt.Conf. Intell.RobotsSyst.,2020,pp.11748–11754. agents escape from deadlock. [12] B.Wang,Z.Liu,Q.Li,andA.Prorok,“Mobilerobotpathplanningin\ndynamicenvironmentsthroughgloballyguidedreinforcementlearning,”\nVI. CONCLUSIONSANDFUTUREWORK\nIEEERobot.Autom.Lett.,vol.5,no.4,pp.6932–6939,2020. This paper introduces a hybrid MAPF policy framework [13] Q.Li,F.Gama,A.Ribeiro,andA.Prorok,“Graphneuralnetworks\nfordecentralizedmulti-robotpathplanning,”inProc.IEEE/RSJInt.",
      "size": 965,
      "sentences": 13
    },
    {
      "id": 33,
      "content": "5,no.4,pp.6932–6939,2020. This paper introduces a hybrid MAPF policy framework [13] Q.Li,F.Gama,A.Ribeiro,andA.Prorok,“Graphneuralnetworks\nfordecentralizedmulti-robotpathplanning,”inProc.IEEE/RSJInt. called RDE, which combines RL-based policy with DHM-\nConf.Intell.RobotsSyst.,2020,pp.11785–11792. based and escape policy for warehouse automation. The RL- [14] Z. Ma, Y. Luo, and H. Ma, “Distributed heuristic multi-agent path\nbased policy can handle cooperative coordination between findingwithcommunication,”inProc.IEEEInt.Conf.Robot.Autom.,\n2021,pp.8699–8705. agents, while the DHM-based policy is suitable for situations\n[15] Z. Ma, Y. Luo, and J. Pan, “Learning selective communication for\nwhere no other agents are within the FOV. In a deadlock, multi-agentpathfinding,”IEEERobot.Autom.Lett.,vol.7,no.2,pp. agents can use escape policy to escape from the deadlock. 1455–1462,2021. [16] H.Wang,Y.Yu,andQ.Yuan,“Applicationofdijkstraalgorithmin\nRDE can be adapted to any RL-based MAPF policy.",
      "size": 995,
      "sentences": 10
    },
    {
      "id": 34,
      "content": ".,vol.7,no.2,pp. agents can use escape policy to escape from the deadlock. 1455–1462,2021. [16] H.Wang,Y.Yu,andQ.Yuan,“Applicationofdijkstraalgorithmin\nRDE can be adapted to any RL-based MAPF policy. This\nrobotpath-planning,”inProc.IEEEInt.Conf.Mech.Automat.Control\npaper combines RDE with state-of-the-art RL-based MAPF Eng.,2011,pp.1067–1069. === 페이지 6 ===\n[17] J.YuandS.LaValle,“Structureandintractabilityofoptimalmulti- planning,”IEEETrans.Industr.Inform.,vol.19,no.10,pp.10233–\nrobotpathplanningongraphs,”inProc.AAAIConf.Artif.Intell.,2013, 10243,2023.\npp.1443–1449. [29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\n[18] G.Sharon,R.Stern,M.Goldenberg,andA.Felner,“Theincreasing Gomez,Ł.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”2017. costtreesearchforoptimalmulti-agentpathfinding,”Artif.Intell.,vol. [30] Q.Li,W.Lin,Z.Liu,andA.Prorok,“Message-awaregraphattention\n195,pp.470–495,2013. networksforlarge-scalemulti-robotpathplanning,”IEEERobot.Autom.",
      "size": 979,
      "sentences": 10
    },
    {
      "id": 35,
      "content": "oroptimalmulti-agentpathfinding,”Artif.Intell.,vol. [30] Q.Li,W.Lin,Z.Liu,andA.Prorok,“Message-awaregraphattention\n195,pp.470–495,2013. networksforlarge-scalemulti-robotpathplanning,”IEEERobot.Autom. [19] J.YuandS.M.LaValle,“Optimalmultirobotpathplanningongraphs: Lett.,vol.6,no.3,pp.5533–5540,2021. Complete algorithms and effective heuristics,” IEEE Trans. Robot., [31] H.Guan,Y.Gao,M.Zhao,Y.Yang,F.Deng,andT.L.Lam,“Ab-\nvol.32,no.5,pp.1163–1177,2016. mapper: Attention and bicnet based multi-agent path planning for\n[20] G.WagnerandH.Choset,“Subdimensionalexpansionformultirobot dynamic environment,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots\npathplanning,”Artif.Intell.,vol.219,pp.1–24,2015. Syst.,2022,pp.13799–13806. [21] F. Aljalaud and N. Sturtevant, “Finding bounded suboptimal multi-\n[32] J.Li,Z.Chen,Y.Zheng,S.-H.Chan,D.Harabor,P.J.Stuckey,H.Ma,\nagentpathplanningsolutionsusingincreasingcosttreesearch,”inInt.",
      "size": 921,
      "sentences": 13
    },
    {
      "id": 36,
      "content": ". [21] F. Aljalaud and N. Sturtevant, “Finding bounded suboptimal multi-\n[32] J.Li,Z.Chen,Y.Zheng,S.-H.Chan,D.Harabor,P.J.Stuckey,H.Ma,\nagentpathplanningsolutionsusingincreasingcosttreesearch,”inInt. andS.Koenig,“Scalablerailplanningandreplanning:Winningthe\nSymp.Comb.Search,SoCS,2013,pp.203–204. 2020flatlandchallenge,”inProc.Int.Conf.AutomatedPlan.Sched.,\n[22] M.Rahman,M.A.Alam,M.M.Islam,I.Rahman,M.M.Khan,and\n2021,pp.477–485. T.Iqbal,“Anadaptiveagent-specificsub-optimalboundingapproach\n[33] J.Li,K.Sun,H.Ma,A.Felner,T.S.Kumar,andS.Koenig,“Moving\nformulti-agentpathfinding,”IEEEAccess,vol.10,pp.22226–22237,\nagents in formation in congested environments,” in Proc. Int. Joint\n2022. Conf.Auton.AgentsMultiagentSyst.,2020,pp.726–734.",
      "size": 735,
      "sentences": 8
    },
    {
      "id": 37,
      "content": "Koenig,“Moving\nformulti-agentpathfinding,”IEEEAccess,vol.10,pp.22226–22237,\nagents in formation in congested environments,” in Proc. Int. Joint\n2022. Conf.Auton.AgentsMultiagentSyst.,2020,pp.726–734. [23] H. Ma, C. Tovey, G. Sharon, T. S. Kumar, and S. Koenig, “Multi-\n[34] A.Jain,D.Ghose,andP.P.Menon,“Achievingadesiredcollective\nagentpathfindingwithpayloadtransfersandthepackage-exchange\ncentroidbyaformationofagentsmovinginacontrollableforcefield,”\nrobot-routingproblem,”inProc.AAAIConf.Artif.Intell.,2016,pp. inIndianControlConference,2016,pp.182–187. 3166–3173. [35] C.WiltandA.Botea,“Spatiallydistributedmultiagentpathplanning,”\n[24] K. Okumura, M. Machida, X. Défago, and Y. Tamura, “Priority\ninProc.Int.Conf.AutomatedPlan.Sched.,ICAPS,2014,pp.332–340. inheritancewithbacktrackingforiterativemulti-agentpathfinding,”in\nProc.Int.JointConf.Artif.Intell.,2019,pp.535–542.",
      "size": 875,
      "sentences": 9
    },
    {
      "id": 38,
      "content": "nd Y. Tamura, “Priority\ninProc.Int.Conf.AutomatedPlan.Sched.,ICAPS,2014,pp.332–340. inheritancewithbacktrackingforiterativemulti-agentpathfinding,”in\nProc.Int.JointConf.Artif.Intell.,2019,pp.535–542. [36] H. Zhang, M. Yao, Z. Liu, J. Li, L. Terr, S.-H. Chan, T. S. Kumar,\n[25] J.Li,Z.Chen,D.Harabor,P.J.Stuckey,andS.Koenig,“Anytime andS.Koenig,“Ahierarchicalapproachtomulti-agentpathfinding,”\nmulti-agentpathfindingvialargeneighborhoodsearch,”inProc.Int. inInt.Symp.Comb.Search,SoCS,2021,pp.209–211. JointConf.Auton.AgentsMultiagentSyst.,2021,pp.1581–1583. [37] M. L. Littman, “Markov games as a framework for multi-agent\n[26] ——,“Mapf-lns2:Fastrepairingformulti-agentpathfindingvialarge reinforcementlearning,”inMachinelearningproceedings,1994,pp. neighborhood search,” in Proc. AAAI Conf. Artif. Intell., 2022, pp. 157–163. 10256–10265.",
      "size": 838,
      "sentences": 12
    },
    {
      "id": 39,
      "content": "2:Fastrepairingformulti-agentpathfindingvialarge reinforcementlearning,”inMachinelearningproceedings,1994,pp. neighborhood search,” in Proc. AAAI Conf. Artif. Intell., 2022, pp. 157–163. 10256–10265. [38] Y. Xu, Y. Li, Q. Liu, J. Gao, Y. Liu, and M. Chen, “Multi-agent\n[27] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of pathfindingwithlocalandglobalguidance,”in2021IEEEInternational\nconvolutionalneuralnetworks:Analysis,applications,andprospects,” ConferenceonNetworking,SensingandControl(ICNSC),vol.1,2021,\nIEEETransactionsonNeuralNetworksandLearningSystems,vol.33, pp.1–7. no.12,pp.6999–7019,2022. [39] Z.Wang,T.Schaul,M.Hessel,H.Hasselt,M.Lanctot,andN.Freitas,\n[28] L.Chen,Y.Wang,Z.Miao,Y.Mo,M.Feng,Z.Zhou,andH.Wang, “Dueling network architectures for deep reinforcement learning,” in\n“Transformer-basedimitativereinforcementlearningformultirobotpath Internationalconferenceonmachinelearning,2016,pp.1995–2003.",
      "size": 926,
      "sentences": 10
    }
  ]
}