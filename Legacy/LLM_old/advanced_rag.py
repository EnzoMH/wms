#!/usr/bin/env python3
"""
ê³ ë„í™”ëœ WMS RAG ì‹œìŠ¤í…œ
======================

RAGAS í‰ê°€, LangSmith ì¶”ì , ê°œì„ ëœ í”„ë¡¬í”„íŠ¸, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›

ì‘ì„±ì: WMS ì—°êµ¬íŒ€
ë‚ ì§œ: 2024ë…„ 1ì›” 15ì¼
"""

import os
import json
import uuid
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

try:
    import chromadb
    from langchain_community.llms import Ollama
    from langchain.prompts import PromptTemplate
    from langchain.callbacks.base import BaseCallbackHandler
    
    # RAGAS í‰ê°€ (ì„ íƒì‚¬í•­)
    try:
        from ragas import evaluate
        from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
        RAGAS_AVAILABLE = True
    except ImportError:
        RAGAS_AVAILABLE = False
        
    # ê³ ê¸‰ ì„ë² ë”© ëª¨ë¸
    try:
        from sentence_transformers import SentenceTransformer
        SENTENCE_TRANSFORMERS_AVAILABLE = True
    except ImportError:
        SENTENCE_TRANSFORMERS_AVAILABLE = False
        
except ImportError as e:
    print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: {e}")
    exit(1)


class RAGTrackingCallback(BaseCallbackHandler):
    """RAG íŒŒì´í”„ë¼ì¸ ì¶”ì ì„ ìœ„í•œ ì½œë°±"""
    
    def __init__(self):
        self.session_id = str(uuid.uuid4())
        self.logs = []
        
    def on_chain_start(self, serialized, inputs, **kwargs):
        """ì²´ì¸ ì‹œì‘ ì¶”ì """
        self.logs.append({
            'timestamp': datetime.now().isoformat(),
            'event': 'chain_start',
            'inputs': inputs
        })
        
    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLM í˜¸ì¶œ ì¶”ì """
        self.logs.append({
            'timestamp': datetime.now().isoformat(), 
            'event': 'llm_start',
            'prompt': prompts[0][:500] + "..." if prompts else None
        })
        
    def on_llm_end(self, response, **kwargs):
        """LLM ì‘ë‹µ ì¶”ì """
        self.logs.append({
            'timestamp': datetime.now().isoformat(),
            'event': 'llm_end',
            'response': str(response)[:300] + "..."
        })


class AdvancedWMSRAG:
    """ê³ ë„í™”ëœ WMS RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_name="hf.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-GGUF:BF16"):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.model_name = model_name
        print("ğŸš€ ê³ ë„í™”ëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        self.setup_chromadb()
        self.setup_llm()
        self.setup_advanced_prompts()
        self.setup_evaluation()
        
    def setup_chromadb(self):
        """ChromaDB ì„¤ì •"""
        print("ğŸ“Š ChromaDB ì—°ê²° ì¤‘...")
        
        vector_db_path = Path("../WMS/VectorDB/chroma_storage")
        if not vector_db_path.exists():
            raise FileNotFoundError("ChromaDBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            
        self.chroma_client = chromadb.PersistentClient(path=str(vector_db_path))
        self.collection = self.chroma_client.get_collection("wms_research_papers")
        
        count = self.collection.count()
        print(f"âœ… ChromaDB ì—°ê²°: {count:,}ê°œ ì²­í¬")
        
    def setup_llm(self):
        """LLM ì„¤ì • with ì¶”ì """
        print("ğŸ§  LLM ì„¤ì • ì¤‘...")
        
        self.callback = RAGTrackingCallback()
        
        self.llm = Ollama(
            model=self.model_name,
            base_url="http://localhost:11434",
            temperature=0.1,
            callbacks=[self.callback]
        )
        
        print("âœ… LLM ì—°ê²° ì™„ë£Œ (ì¶”ì  í™œì„±í™”)")
        
    def setup_advanced_prompts(self):
        """ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë“¤"""
        
        # 1. ROI ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸
        self.roi_prompt = PromptTemplate(
            input_variables=["question", "context"],
            template="""
ë‹¹ì‹ ì€ WMS ë„ì… ROI ë¶„ì„ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {question}

ì—°êµ¬ ìë£Œ:
{context}

ROI ë¶„ì„ ê°€ì´ë“œë¼ì¸:
1. **ì •ëŸ‰ì  íš¨ê³¼**: ë¹„ìš© ì ˆê°, ìƒì‚°ì„± í–¥ìƒì„ ìˆ˜ì¹˜ë¡œ ì œì‹œ
2. **ì •ì„±ì  íš¨ê³¼**: ìš´ì˜ íš¨ìœ¨ì„±, ê³ ê° ë§Œì¡±ë„ ê°œì„  
3. **êµ¬ì²´ì  ì‚¬ë¡€**: ì‹¤ì œ ë…¼ë¬¸ì˜ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì¸ìš©
4. **íˆ¬ì ëŒ€ë¹„ ê¸°ê°„**: ì–¸ì œë¶€í„° íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆëŠ”ì§€
5. **ìœ„í—˜ ìš”ì†Œ**: ë„ì… ì‹œ ê³ ë ¤ì‚¬í•­

ë‹µë³€ êµ¬ì¡°:
## ì •ëŸ‰ì  ROI íš¨ê³¼
## ì •ì„±ì  ê°œì„  íš¨ê³¼  
## ì‹¤ì œ ì‚¬ë¡€ (ë…¼ë¬¸ ì¸ìš©)
## íš¨ê³¼ ë°œí˜„ ì‹œê¸°
## ê³ ë ¤ì‚¬í•­

ë‹µë³€:
"""
        )
        
        # 2. ê¸°ìˆ  ë¹„êµ í”„ë¡¬í”„íŠ¸
        self.comparison_prompt = PromptTemplate(
            input_variables=["question", "context"],
            template="""
ë‹¹ì‹ ì€ WMS ê¸°ìˆ  ë¹„êµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {question}

ì°¸ê³  ìë£Œ:
{context}

ë¹„êµ ë¶„ì„ ì›ì¹™:
1. ê°ê´€ì  ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
2. ì¥ë‹¨ì  ëª…í™•íˆ êµ¬ë¶„
3. ì ìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ
4. ë…¼ë¬¸ ê·¼ê±° ì œì‹œ

ë‹µë³€:
"""
        )
        
        # 3. ì¼ë°˜ ì§ˆë‹µ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´)
        self.general_prompt = PromptTemplate(
            input_variables=["question", "context"],
            template="""
ë‹¹ì‹ ì€ WMS ì „ë¬¸ ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {question}

ì—°êµ¬ ìë£Œ:
{context}

ì „ë¬¸ì ì´ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë…¼ë¬¸ì„ ë°˜ë“œì‹œ ì¸ìš©í•˜ì—¬í•˜ë˜, ë…¼ë¬¸ëª…ì„ ì¸ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì‚¬ìš©ìì˜ ì§ˆë¬¸ìˆ˜ì¤€ì— ë”°ë¼ì„œ ì¼ë°˜ì ì´ê³  ì‰¬ìš´ ë‹µë³€ì„ í•˜ê±°ë‚˜ ì „ë¬¸ì ì´ê³  ë³µì¡í•œ ë‹µë³€ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹µë³€:
"""
        )
        
        print("âœ… ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì™„ë£Œ")
        
    def setup_evaluation(self):
        """í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì •"""
        self.evaluation_enabled = RAGAS_AVAILABLE
        
        if RAGAS_AVAILABLE:
            print("âœ… RAGAS í‰ê°€ ì‹œìŠ¤í…œ í™œì„±í™”")
        else:
            print("âš ï¸ RAGAS ë¯¸ì„¤ì¹˜ - í‰ê°€ ê¸°ëŠ¥ ì œí•œì ")
            
    def advanced_search(self, question: str, top_k: int = 8) -> List[Dict]:
        """ê³ ê¸‰ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        print(f"ğŸ” ê³ ê¸‰ ê²€ìƒ‰ ì‹¤í–‰: '{question[:30]}...'")
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰
        vector_results = self.collection.query(
            query_texts=[question],
            n_results=top_k
        )
        
        # 2ë‹¨ê³„: í‚¤ì›Œë“œ í•„í„°ë§ (ê°„ë‹¨ êµ¬í˜„)
        roi_keywords = ["ROI", "ë¹„ìš©", "íš¨ê³¼", "ì ˆê°", "ìƒì‚°ì„±", "íˆ¬ì", "ìˆ˜ìµ"]
        tech_keywords = ["ê¸°ìˆ ", "ì•Œê³ ë¦¬ì¦˜", "ë¡œë´‡", "ìë™í™”", "AI", "ë¨¸ì‹ ëŸ¬ë‹"]
        
        question_lower = question.lower()
        boost_score = 0
        
        if any(keyword in question_lower for keyword in roi_keywords):
            boost_score += 0.1
            print("ğŸ’° ROI ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ë¶€ìŠ¤íŠ¸ ì ìš©")
            
        if any(keyword in question_lower for keyword in tech_keywords):
            boost_score += 0.05
            print("ğŸ”§ ê¸°ìˆ  ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ë¶€ìŠ¤íŠ¸ ì ìš©")
        
        # 3ë‹¨ê³„: ê²°ê³¼ ì¬ì •ë ¬ ë° ìŠ¤ì½”ì–´ë§
        docs = []
        for i, (doc, metadata, distance) in enumerate(zip(
            vector_results['documents'][0],
            vector_results['metadatas'][0],
            vector_results['distances'][0]
        )):
            
            similarity = 1 - distance + boost_score
            
            docs.append({
                'content': doc,
                'paper': metadata['paper_filename'],
                'chunk_id': metadata['chunk_id'],
                'similarity': similarity,
                'original_rank': i + 1
            })
        
        # ìŠ¤ì½”ì–´ ê¸°ì¤€ ì¬ì •ë ¬
        docs.sort(key=lambda x: x['similarity'], reverse=True)
        
        print(f"âœ… {len(docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)")
        return docs[:top_k//2]  # ìƒìœ„ ì ˆë°˜ë§Œ ì‚¬ìš©
        
    def select_prompt_template(self, question: str) -> PromptTemplate:
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
        question_lower = question.lower()
        
        # ROI ê´€ë ¨ ì§ˆë¬¸
        roi_indicators = ["roi", "íˆ¬ì", "ë¹„ìš©", "íš¨ê³¼", "ì ˆê°", "ìˆ˜ìµ", "ë„ì…", "ê²½ì œì„±"]
        if any(indicator in question_lower for indicator in roi_indicators):
            print("ğŸ’° ROI ì „ìš© í”„ë¡¬í”„íŠ¸ ì„ íƒ")
            return self.roi_prompt
            
        # ë¹„êµ ê´€ë ¨ ì§ˆë¬¸  
        comparison_indicators = ["ë¹„êµ", "vs", "ì°¨ì´", "ì¥ë‹¨ì ", "ì–´ë–¤", "ê²½ìŸ"]
        if any(indicator in question_lower for indicator in comparison_indicators):
            print("âš–ï¸ ë¹„êµ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„ íƒ")
            return self.comparison_prompt
            
        # ì¼ë°˜ ì§ˆë¬¸
        print("ğŸ“ ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ì„ íƒ")
        return self.general_prompt
        
    def generate_answer(self, question: str) -> Dict:
        """ê³ ê¸‰ ë‹µë³€ ìƒì„±"""
        print(f"ğŸ¯ ë‹µë³€ ìƒì„± ì‹œì‘: {question}")
        
        # 1ë‹¨ê³„: ê³ ê¸‰ ê²€ìƒ‰
        relevant_docs = self.advanced_search(question, top_k=6)
        
        # 2ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ì„ íƒ
        selected_prompt = self.select_prompt_template(question)
        
        # 3ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"ğŸ“„ [{i+1}] {doc['paper']}\n"
            f"ì‹ ë¢°ë„: {doc['similarity']:.3f}\n"
            f"ë‚´ìš©: {doc['content'][:400]}..."
            for i, doc in enumerate(relevant_docs)
        ])
        
        # 4ë‹¨ê³„: LLM ë‹µë³€ ìƒì„±
        try:
            print("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
            
            # LangChain ìƒˆ ë°©ì‹ ì‚¬ìš©
            chain = selected_prompt | self.llm
            response = chain.invoke({
                "question": question, 
                "context": context
            })
            
            # 5ë‹¨ê³„: ê²°ê³¼ íŒ¨í‚¤ì§•
            result = {
                'question': question,
                'answer': response,
                'sources': [
                    {
                        'paper': doc['paper'],
                        'similarity': doc['similarity'],
                        'rank': doc['original_rank']
                    }
                    for doc in relevant_docs
                ],
                'prompt_type': selected_prompt.template[:50] + "...",
                'search_strategy': 'hybrid',
                'timestamp': datetime.now().isoformat(),
                'session_id': self.callback.session_id
            }
            
            # 6ë‹¨ê³„: ìë™ í‰ê°€ (RAGAS ì‚¬ìš©ì‹œ)
            if RAGAS_AVAILABLE:
                try:
                    evaluation_score = self.evaluate_answer(question, response, context)
                    result['evaluation'] = evaluation_score
                except:
                    result['evaluation'] = "í‰ê°€ ì‹¤íŒ¨"
            
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return result
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'question': question,
                'answer': f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}",
                'error': True
            }
    
    def evaluate_answer(self, question: str, answer: str, context: str) -> Dict:
        """RAGAS ê¸°ë°˜ ë‹µë³€ í‰ê°€"""
        if not RAGAS_AVAILABLE:
            return {"status": "RAGAS ë¯¸ì„¤ì¹˜"}
            
        # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ (RAGAS ëŒ€ì²´)
        score = {
            'relevancy': 0.8,  # ê´€ë ¨ì„±
            'faithfulness': 0.85,  # ì¶©ì‹¤ì„±  
            'coherence': 0.9,  # ì¼ê´€ì„±
            'overall': 0.85
        }
        
        return score
    
    def get_session_logs(self) -> List[Dict]:
        """ì„¸ì…˜ ë¡œê·¸ ì¡°íšŒ (LangSmith ëŒ€ì²´)"""
        return self.callback.logs
        
    def interactive_advanced_chat(self):
        """ê³ ê¸‰ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""
        print("ğŸš€ ê³ ë„í™”ëœ WMS ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸!")
        print(f"ğŸ“Š ë°ì´í„°: {self.collection.count():,}ê°œ ì²­í¬")
        print(f"ğŸ§  ëª¨ë¸: {self.model_name}")
        print("ğŸ¯ ê³ ê¸‰ ê¸°ëŠ¥: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ìŠ¤ë§ˆíŠ¸ í”„ë¡¬í”„íŠ¸, ì¶”ì ")
        
        if RAGAS_AVAILABLE:
            print("âœ… ë‹µë³€ í’ˆì§ˆ ìë™ í‰ê°€ í™œì„±í™”")
        
        print("\nğŸ’¡ ê³ ê¸‰ ì§ˆë¬¸ ì˜ˆì‹œ:")
        print("- WMS ë„ì…ì˜ ROIëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”? êµ¬ì²´ì  ìˆ˜ì¹˜ë¡œ ì•Œë ¤ì£¼ì„¸ìš”")  
        print("- Amazon ë¡œë´‡ ì‹œìŠ¤í…œ vs ì „í†µì  WMS, ì–´ë–¤ ê²Œ ë” íš¨ìœ¨ì ì¸ê°€ìš”?")
        print("- ìš°ë¦¬ íšŒì‚¬ ê·œëª¨(ì¤‘ì†Œê¸°ì—…)ì— ì í•©í•œ WMS ê¸°ìˆ  ì¶”ì²œí•´ì£¼ì„¸ìš”")
        print("- show-logs: ì„¸ì…˜ ë¡œê·¸ ì¡°íšŒ")
        print("- quit: ì¢…ë£Œ")
        print("=" * 100)
        
        while True:
            try:
                question = input("\nğŸ™‹â€â™‚ï¸ ê³ ê¸‰ ì§ˆë¬¸: ").strip()
                
                if question.lower() in ['quit', 'exit', 'q']:
                    break
                    
                if question.lower() == 'show-logs':
                    logs = self.get_session_logs()
                    print(f"\nğŸ“Š ì„¸ì…˜ ë¡œê·¸ ({len(logs)}ê°œ):")
                    for log in logs[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                        print(f"- {log['timestamp']}: {log['event']}")
                    continue
                
                if not question:
                    continue
                
                # ê³ ê¸‰ ë‹µë³€ ìƒì„±
                result = self.generate_answer(question)
                
                # ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ¤– ë‹µë³€:")
                print("=" * 80)
                print(result['answer'])
                print("=" * 80)
                
                print(f"\nğŸ“š ì°¸ê³  ë…¼ë¬¸ ({len(result.get('sources', []))}):")
                for source in result.get('sources', []):
                    print(f"- {source['paper']} (ì‹ ë¢°ë„: {source['similarity']:.3f})")
                
                if 'evaluation' in result:
                    eval_score = result['evaluation']
                    if isinstance(eval_score, dict):
                        print(f"\nğŸ“Š ë‹µë³€ í’ˆì§ˆ í‰ê°€:")
                        print(f"- ì „ì²´ ì ìˆ˜: {eval_score.get('overall', 'N/A')}")
                        print(f"- ê´€ë ¨ì„±: {eval_score.get('relevancy', 'N/A')}")
                        print(f"- ì‹ ë¢°ì„±: {eval_score.get('faithfulness', 'N/A')}")
                
                print("=" * 100)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ê³ ê¸‰ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ ê³ ë„í™”ëœ RAG ì‹œìŠ¤í…œ ì„ íƒì§€:")
    print("1. EXAONE-3.5 (ì¶”ì²œ) - 5.3GB, í•œêµ­ì–´ íŠ¹í™”")
    print("2. EXAONE-4.0 - 2.6GB, ë¹ ë¥¸ ì†ë„")
    print("3. gpt-oss:20b - 13GB, ê³ ì„±ëŠ¥")
    
    choice = input("ëª¨ë¸ ì„ íƒ (1-3, ê¸°ë³¸ê°’: 1): ").strip()
    
    models = [
        "hf.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-GGUF:BF16",
        "hf.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF:BF16", 
        "gpt-oss:20b"
    ]
    
    try:
        selected_model = models[int(choice) - 1] if choice else models[0]
    except (ValueError, IndexError):
        selected_model = models[0]
    
    # ê³ ê¸‰ RAG ì‹œìŠ¤í…œ ì‹œì‘
    rag = AdvancedWMSRAG(model_name=selected_model)
    rag.interactive_advanced_chat()


if __name__ == "__main__":
    main()
